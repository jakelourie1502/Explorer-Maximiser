append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  6.053050607442856
printing an ep nov before normalisation:  4.90636597761854
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.014]
 [0.013]] [[4.739]
 [4.739]
 [4.739]
 [6.37 ]
 [4.739]] [[0.528]
 [0.528]
 [0.528]
 [1.329]
 [0.528]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.986005902290344
printing an ep nov before normalisation:  7.697233557701111
siam score:  -0.005312045125968077
printing an ep nov before normalisation:  6.290841192800016
printing an ep nov before normalisation:  7.568838596343994
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
deleting a thread, now have 2 threads
Frames:  1220 train batches done:  29 episodes:  95
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.24522564
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  1220 train batches done:  75 episodes:  95
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.01321152278356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.05852980536008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.  0.  0.2 0.6]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.60646987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Sims:  6 1 epoch:  2808 pick best:  False frame count:  2808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.63884604
siam score:  -0.6429135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
siam score:  -0.6568956
siam score:  -0.6538524
siam score:  -0.65328264
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6305536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5455, 0.0320, 0.0006, 0.1947, 0.2272], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0675, 0.8124, 0.0384, 0.0369, 0.0449], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0030,     0.9178,     0.0559,     0.0226],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2001, 0.0032, 0.0246, 0.5820, 0.1901], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3870, 0.0170, 0.0286, 0.2562, 0.3113], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.  0.6 0.  0.4 0. ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.59490496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6172003
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.75564765930176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.272755773800334
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.6749409
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.67278326
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6744415
siam score:  -0.6743388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.87065184052881
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.67941546
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5837, 0.0029, 0.0169, 0.1558, 0.2408], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0428, 0.8071, 0.0162, 0.0311, 0.1028], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9797,     0.0159,     0.0042],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2451, 0.0030, 0.0832, 0.4646, 0.2041], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4425, 0.0025, 0.0017, 0.2337, 0.3197], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.5463, 0.0022, 0.0076, 0.2291, 0.2147], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0251,     0.8894,     0.0007,     0.0045,     0.0803],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0002,     0.9437,     0.0465,     0.0093],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1493, 0.0028, 0.0697, 0.5849, 0.1933], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3986, 0.0032, 0.0617, 0.2419, 0.2945], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6779265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.114991664886475
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.950818170470754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.78483251826749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  19.57194064846383
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.704943756606806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7040811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.44264841079712
printing an ep nov before normalisation:  58.811193368382405
siam score:  -0.7065031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.194]
 [19.706]
 [24.194]
 [24.194]
 [24.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.012]
 [34.012]
 [34.012]
 [34.722]
 [34.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  65.63938173233227
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.735]
 [63.15 ]
 [43.634]
 [32.258]
 [63.15 ]] [[0.815]
 [2.469]
 [1.333]
 [0.671]
 [2.469]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[90.064]
 [90.064]
 [90.064]
 [90.064]
 [92.573]] [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.84 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.20804595947266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.47435860846039
printing an ep nov before normalisation:  51.14028698593518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.4246062811432125
printing an ep nov before normalisation:  70.85802759874751
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.684]
 [41.633]
 [49.194]
 [47.916]
 [54.811]] [[0.775]
 [0.907]
 [1.073]
 [1.045]
 [1.197]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.14678955078125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.573084354400635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.36065864562988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.11762416235137
printing an ep nov before normalisation:  45.59780074689854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6824971
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.673574
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.114]
 [52.529]
 [60.203]
 [49.671]
 [52.055]] [[0.172]
 [0.202]
 [0.269]
 [0.176]
 [0.197]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.225494413729706
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.2218665601211
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.32228323138278
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.299427976687184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72428316
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7296543
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.45961653317684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.39469162623088
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73647016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.762713941902376
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.634]
 [61.634]
 [73.593]
 [61.634]
 [61.634]] [[0.733]
 [0.733]
 [0.93 ]
 [0.733]
 [0.733]]
actions average: 
K:  3  action  0 :  tensor([    0.6833,     0.0067,     0.0005,     0.1056,     0.2039],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0329,     0.9353,     0.0007,     0.0072,     0.0240],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9969,     0.0010,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2524, 0.0006, 0.0022, 0.5261, 0.2187], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2164, 0.0008, 0.0807, 0.3414, 0.3608], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.906128406524658
printing an ep nov before normalisation:  45.727426641420685
printing an ep nov before normalisation:  0.3538836268251089
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
deleting a thread, now have 1 threads
Frames:  9250 train batches done:  1082 episodes:  792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.503]
 [46.45 ]
 [39.968]
 [34.503]
 [35.144]] [[0.856]
 [1.421]
 [1.115]
 [0.856]
 [0.887]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.70855767
siam score:  -0.70451075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.76743399006684
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.11412355519269113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72071767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.497291124467694
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.7142247
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71005416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.43629896226402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [72.836]
 [ 0.   ]] [[-0.69 ]
 [-0.69 ]
 [-0.69 ]
 [ 1.118]
 [-0.69 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  3.2577090476172543
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.273]
 [43.273]
 [43.273]
 [43.273]
 [40.011]] [[1.626]
 [1.626]
 [1.626]
 [1.626]
 [1.396]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([0.7198, 0.0076, 0.0019, 0.0910, 0.1797], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0077, 0.9641, 0.0041, 0.0010, 0.0231], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0007,     0.9639,     0.0209,     0.0145],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1178, 0.0016, 0.0085, 0.7380, 0.1341], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.4003, 0.0112, 0.0009, 0.2538, 0.3338], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73506254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.168]
 [46.136]
 [51.599]
 [53.699]
 [47.831]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.846]
 [48.338]
 [37.265]
 [26.556]
 [48.338]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.482263444188696
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.6078, 0.0019, 0.0018, 0.1682, 0.2203], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0079,     0.9794,     0.0001,     0.0004,     0.0122],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0005,     0.9751,     0.0161,     0.0084],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.2493,     0.0003,     0.0010,     0.5115,     0.2380],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4687, 0.0007, 0.0053, 0.2289, 0.2964], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.273824250949476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  89.6100959324209
printing an ep nov before normalisation:  31.789824027752054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.686793328004597
main train batch thing paused
add a thread
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Adding thread: now have 2 threads
printing an ep nov before normalisation:  14.00902181151742
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.145]
 [95.737]
 [90.141]
 [91.767]
 [90.873]] [[1.103]
 [1.619]
 [1.501]
 [1.535]
 [1.517]]
siam score:  -0.7338265
printing an ep nov before normalisation:  19.188415307775916
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.465886191315136
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.04 ]
 [50.885]
 [69.518]
 [61.617]
 [51.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.74976205283114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.73140246
actions average: 
K:  1  action  0 :  tensor([    0.6254,     0.0039,     0.0003,     0.1542,     0.2162],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0208,     0.9649,     0.0003,     0.0000,     0.0140],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0018, 0.0206, 0.7939, 0.0914, 0.0923], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1793, 0.0511, 0.0358, 0.4785, 0.2554], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3019, 0.0127, 0.0347, 0.2217, 0.4289], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.323]
 [46.027]
 [49.926]
 [49.203]
 [45.402]] [[0.578]
 [0.778]
 [0.914]
 [0.889]
 [0.756]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.75942039489746
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.531]
 [11.709]
 [14.538]
 [26.515]
 [14.51 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.358555937568575
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 30.095102649629876
printing an ep nov before normalisation:  52.283330463599086
siam score:  -0.73171806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 44.01375619789283
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.529256188375705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.5525,     0.0740,     0.0004,     0.1712,     0.2019],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0256, 0.8937, 0.0359, 0.0034, 0.0414], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9846,     0.0090,     0.0062],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2923, 0.0046, 0.0189, 0.4609, 0.2233], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2471, 0.0248, 0.0347, 0.2902, 0.4033], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.743443133901195
deleting a thread, now have 1 threads
Frames:  12588 train batches done:  1473 episodes:  1049
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.30580575953996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73920816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.6269,     0.0006,     0.0013,     0.1629,     0.2083],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0354,     0.9220,     0.0002,     0.0002,     0.0421],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0009,     0.9822,     0.0107,     0.0061],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1154, 0.0013, 0.0162, 0.6871, 0.1800], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2879, 0.0205, 0.0455, 0.2740, 0.3721], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.76421070098877
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.73405516
printing an ep nov before normalisation:  0.08093944788129193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.04532564769231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.01819782211463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.31055742134764
actions average: 
K:  2  action  0 :  tensor([0.6404, 0.0038, 0.0008, 0.1642, 0.1908], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0768, 0.8582, 0.0027, 0.0015, 0.0608], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0034,     0.0001,     0.9044,     0.0576,     0.0345],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1410, 0.0013, 0.0013, 0.6335, 0.2229], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3046, 0.0369, 0.0094, 0.2700, 0.3791], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6682, 0.0016, 0.0174, 0.1468, 0.1660], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0103,     0.9792,     0.0015,     0.0003,     0.0086],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9650,     0.0155,     0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2310, 0.0087, 0.0016, 0.4848, 0.2739], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3353, 0.0013, 0.0029, 0.2747, 0.3857], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.5968,     0.0014,     0.0002,     0.1740,     0.2276],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0362,     0.9246,     0.0034,     0.0000,     0.0358],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0077,     0.9827,     0.0041,     0.0054],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2041, 0.0006, 0.0196, 0.4869, 0.2886], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2476, 0.0192, 0.0427, 0.2964, 0.3941], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.93072773080015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.346]
 [25.346]
 [35.704]
 [25.346]
 [25.346]] [[0.875]
 [0.875]
 [1.537]
 [0.875]
 [0.875]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.969149057735308
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.73985004425049
actions average: 
K:  4  action  0 :  tensor([    0.5919,     0.0887,     0.0005,     0.1230,     0.1958],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0023, 0.9476, 0.0022, 0.0326, 0.0152], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0007,     0.9339,     0.0467,     0.0182],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1588,     0.0003,     0.0156,     0.6480,     0.1774],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2286, 0.1437, 0.0662, 0.2927, 0.2687], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.871]
 [24.924]
 [13.193]
 [21.697]
 [23.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.792 0.042 0.042]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.117]
 [45.117]
 [45.117]
 [63.063]
 [45.117]] [[0.61 ]
 [0.61 ]
 [0.61 ]
 [1.333]
 [0.61 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.7477142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.253788390903146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.189028633965385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.26087296298574
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  90.05809964542273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.605]
 [21.605]
 [21.605]
 [25.011]
 [21.605]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.016]
 [34.016]
 [41.509]
 [34.016]
 [34.016]] [[1.097]
 [1.097]
 [1.595]
 [1.097]
 [1.097]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.50544118022001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.485655784606934
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.99282455444336
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.55075399519451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.709]
 [61.985]
 [54.14 ]
 [56.545]
 [54.14 ]] [[1.436]
 [1.6  ]
 [1.396]
 [1.458]
 [1.396]]
siam score:  -0.73618585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74028254
printing an ep nov before normalisation:  44.83028236597412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6005, 0.0068, 0.0008, 0.1229, 0.2689], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0127,     0.9730,     0.0003,     0.0000,     0.0140],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0015,     0.9620,     0.0157,     0.0207],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1961, 0.0014, 0.0051, 0.5224, 0.2750], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2796, 0.0015, 0.0007, 0.2787, 0.4395], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  44.67195243840857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7418283
printing an ep nov before normalisation:  2.1190967371751412
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.02488559183338
UNIT TEST: sample policy line 217 mcts : [0.083 0.25  0.208 0.375 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7368898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.99272092021329
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.615568164072471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.32991629548411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([0.7110, 0.0431, 0.0011, 0.1009, 0.1439], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9986,     0.0001,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9492,     0.0339,     0.0169],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0804, 0.0029, 0.0035, 0.7617, 0.1515], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2844, 0.0173, 0.0025, 0.2991, 0.3967], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74718755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.289]
 [26.516]
 [27.655]
 [22.749]
 [21.825]] [[0.57 ]
 [0.923]
 [1.   ]
 [0.668]
 [0.606]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.657810555927114
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5092, 0.0488, 0.0012, 0.1590, 0.2817], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9966,     0.0000,     0.0001,     0.0029],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0111, 0.0420, 0.9003, 0.0166, 0.0299], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1212,     0.0003,     0.0220,     0.5987,     0.2578],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1941, 0.1323, 0.0230, 0.2144, 0.4362], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  8.434697680154452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.47204650414089
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.529393693439246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  27.7626167732353
printing an ep nov before normalisation:  25.576934814453125
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7456584
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.10780906677246
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74538636
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.074]
 [40.707]
 [30.586]
 [29.32 ]
 [35.83 ]] [[1.3  ]
 [1.195]
 [0.743]
 [0.687]
 [0.977]]
printing an ep nov before normalisation:  28.67139847891366
printing an ep nov before normalisation:  20.147506155875732
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.518]
 [35.518]
 [35.518]
 [35.518]
 [35.518]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.09743174627263
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5243, 0.0085, 0.0007, 0.1642, 0.3023], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0582, 0.8763, 0.0057, 0.0014, 0.0584], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9939,     0.0028,     0.0033],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0396, 0.0169, 0.1001, 0.6208, 0.2226], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3037, 0.0946, 0.0661, 0.1951, 0.3406], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.313]
 [14.763]
 [25.987]
 [29.016]
 [22.963]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.26993147532146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.89480861604778
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.921008746367264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.764506676874703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7421707
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.97576352215759
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.2574291229248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6272, 0.0181, 0.0007, 0.1275, 0.2265], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9933,     0.0003,     0.0002,     0.0055],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0052,     0.9025,     0.0018,     0.0904],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1477, 0.0006, 0.0601, 0.5800, 0.2116], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2125, 0.0046, 0.0798, 0.2812, 0.4219], grad_fn=<DivBackward0>)
siam score:  -0.7404932
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7088,     0.0330,     0.0001,     0.0602,     0.1979],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9990,     0.0003,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0011,     0.9953,     0.0005,     0.0031],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0944, 0.0006, 0.0423, 0.5568, 0.3059], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1899, 0.0407, 0.0562, 0.2074, 0.5057], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.7948055267334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 73.4958034240429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.80446977223896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.64353755760904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.357518672943115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.988774935404464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.69772259250743
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.7010,     0.0046,     0.0002,     0.1385,     0.1557],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0345,     0.9009,     0.0018,     0.0001,     0.0626],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9954,     0.0025,     0.0017],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1592, 0.0015, 0.0052, 0.5736, 0.2605], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2359, 0.0033, 0.0050, 0.3033, 0.4525], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7423136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  0  action  0 :  tensor([    0.7291,     0.0006,     0.0003,     0.0937,     0.1763],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9889,     0.0094,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9972,     0.0015,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1763, 0.0009, 0.0205, 0.5835, 0.2188], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3808, 0.0010, 0.0053, 0.2157, 0.3972], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.39942781019572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7384649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.49677085876465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.017665919991195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.2636651931315
siam score:  -0.74297774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.298521995544434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.6]
 [36.6]
 [36.6]
 [36.6]
 [36.6]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
printing an ep nov before normalisation:  54.37452230880237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.381265111408311
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.16629496362213
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  59.45392493874667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.49959752860021
siam score:  -0.74367744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.56505584716797
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.824619134682735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.93391219019145
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  73.89174584993269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.027983576868943302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.61163411046995
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6505,     0.0018,     0.0002,     0.0964,     0.2512],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9993,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9873,     0.0075,     0.0050],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1193,     0.0008,     0.0004,     0.6169,     0.2625],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3262, 0.0071, 0.0014, 0.2473, 0.4180], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.18760061264038
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.74368066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.74068075
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  33.375205734155834
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.053]
 [49.053]
 [49.053]
 [49.053]
 [49.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7001,     0.0108,     0.0003,     0.1163,     0.1726],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0166,     0.9083,     0.0006,     0.0005,     0.0739],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9708,     0.0124,     0.0168],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1267, 0.0007, 0.0039, 0.5727, 0.2961], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3824, 0.0282, 0.0010, 0.2605, 0.3279], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  99.63482635301729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.509]
 [47.432]
 [54.918]
 [62.983]
 [64.73 ]] [[1.758]
 [1.186]
 [1.49 ]
 [1.818]
 [1.889]]
printing an ep nov before normalisation:  15.784525300215932
Starting evaluation
STARTED EXPV TRAINING ON FRAME NO.  20054
printing an ep nov before normalisation:  58.32197083571625
printing an ep nov before normalisation:  52.63750052234779
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6461,     0.0148,     0.0004,     0.1082,     0.2305],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0015,     0.9934,     0.0010,     0.0003,     0.0038],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9895,     0.0053,     0.0051],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1006, 0.0020, 0.0367, 0.5325, 0.3281], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3250, 0.0159, 0.0030, 0.2036, 0.4525], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.84568259504687
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.65203404176436
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72999346
printing an ep nov before normalisation:  24.561495780944824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.42549291949899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.792221546173096
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.91185154610668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.68877410888672
printing an ep nov before normalisation:  27.758998377756992
printing an ep nov before normalisation:  16.548764625726875
printing an ep nov before normalisation:  34.32734795447607
printing an ep nov before normalisation:  37.74630749567172
siam score:  -0.7499174
printing an ep nov before normalisation:  52.00502395629883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.51318672967872
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20054
actions average: 
K:  0  action  0 :  tensor([    0.5628,     0.0003,     0.0005,     0.1481,     0.2884],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0044,     0.9804,     0.0001,     0.0000,     0.0152],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9998,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0825,     0.0002,     0.0027,     0.7579,     0.1567],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.3659,     0.0004,     0.0011,     0.2017,     0.4308],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.93639873982732
printing an ep nov before normalisation:  29.662389755249023
siam score:  -0.74100465
printing an ep nov before normalisation:  38.667451848148026
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74903923
printing an ep nov before normalisation:  53.52043922670611
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.945479181159378
printing an ep nov before normalisation:  45.618861186221764
using explorer policy with actor:  1
printing an ep nov before normalisation:  83.4445257829885
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.78217363357544
actions average: 
K:  1  action  0 :  tensor([    0.7473,     0.0004,     0.0002,     0.0803,     0.1718],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0034,     0.9672,     0.0008,     0.0002,     0.0284],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9991,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0800,     0.0002,     0.0249,     0.7381,     0.1568],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3420, 0.0035, 0.0039, 0.1739, 0.4768], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.477]
 [47.477]
 [47.477]
 [47.477]
 [47.477]] [[1.628]
 [1.628]
 [1.628]
 [1.628]
 [1.628]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.15694913965175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7532354
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.128]
 [43.532]
 [38.415]
 [36.238]
 [35.887]] [[1.36 ]
 [1.644]
 [1.375]
 [1.26 ]
 [1.242]]
siam score:  -0.7546846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.23684298349146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.375789803586635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.78211259841919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7853,     0.0017,     0.0002,     0.0628,     0.1501],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0046,     0.9900,     0.0001,     0.0000,     0.0054],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0093,     0.9667,     0.0037,     0.0199],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1064, 0.0012, 0.0058, 0.6104, 0.2762], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2917, 0.0278, 0.0067, 0.3249, 0.3490], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.25516499130097
printing an ep nov before normalisation:  75.14020177649726
printing an ep nov before normalisation:  59.28577409895723
printing an ep nov before normalisation:  49.69202013887863
printing an ep nov before normalisation:  9.514415860176086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.67154049792658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.10815355482376
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.251933097839355
printing an ep nov before normalisation:  49.97477408682557
printing an ep nov before normalisation:  70.81781181369496
line 256 mcts: sample exp_bonus 75.27715713310904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.08611679077148
printing an ep nov before normalisation:  73.58462987549241
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7396321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.087222251354795
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.09717662841243
siam score:  -0.73986053
printing an ep nov before normalisation:  55.19933852960125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73817396
using explorer policy with actor:  1
deleting a thread, now have 1 threads
Frames:  23215 train batches done:  2721 episodes:  1849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.48655640253753
printing an ep nov before normalisation:  43.49536753645662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.139384226216514
actions average: 
K:  1  action  0 :  tensor([0.6797, 0.0009, 0.0032, 0.1161, 0.2001], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9895,     0.0000,     0.0000,     0.0067],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9975,     0.0009,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1666, 0.0006, 0.0016, 0.5593, 0.2718], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3423, 0.0010, 0.0121, 0.2834, 0.3612], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7078,     0.0174,     0.0003,     0.1026,     0.1720],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9841,     0.0054,     0.0001,     0.0087],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.9911,     0.0048,     0.0038],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1291, 0.0011, 0.0324, 0.5836, 0.2539], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1962, 0.0597, 0.0169, 0.3197, 0.4074], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.89217573816772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.20963159394214
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.599]
 [23.308]
 [22.782]
 [38.498]
 [18.788]] [[0.336]
 [0.328]
 [0.316]
 [0.703]
 [0.217]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.74946212768555
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.51973819732666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.66829517834422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.925550394930035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.333 0.042 0.25  0.208 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.989033913074728
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6839,     0.0003,     0.0004,     0.1119,     0.2035],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0062,     0.9921,     0.0000,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9962,     0.0027,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1751,     0.0004,     0.0004,     0.6354,     0.1887],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3114, 0.0007, 0.0004, 0.3441, 0.3434], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.71805513723711
printing an ep nov before normalisation:  29.401573374048716
printing an ep nov before normalisation:  47.622054933138706
printing an ep nov before normalisation:  32.410961128404914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.854747173852502
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.932563932074572
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  42.316403556947016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7581387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.42078699384417
printing an ep nov before normalisation:  65.50406180848128
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.842]
 [79.415]
 [75.842]
 [75.842]
 [75.842]] [[1.558]
 [1.667]
 [1.558]
 [1.558]
 [1.558]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.077632146883468
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7612171
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.575]
 [32.575]
 [43.153]
 [32.575]
 [32.575]] [[0.707]
 [0.707]
 [1.27 ]
 [0.707]
 [0.707]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.415123270440645
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.955732113802156
printing an ep nov before normalisation:  26.690711975097656
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.581]
 [53.499]
 [53.499]
 [53.499]
 [53.499]] [[1.667]
 [1.618]
 [1.618]
 [1.618]
 [1.618]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.599038124084473
actions average: 
K:  4  action  0 :  tensor([0.6105, 0.0058, 0.0006, 0.1398, 0.2432], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9717,     0.0010,     0.0000,     0.0268],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9787,     0.0099,     0.0113],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2078, 0.0015, 0.0024, 0.5634, 0.2248], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.4210, 0.0059, 0.0320, 0.1575, 0.3836], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.352413177490234
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73287886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  16.278841678720315
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.576]
 [59.32 ]
 [62.727]
 [63.812]
 [62.541]] [[1.09 ]
 [1.009]
 [1.131]
 [1.17 ]
 [1.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75065917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.99714138909231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7139,     0.0021,     0.0002,     0.1144,     0.1694],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9965,     0.0014,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1231,     0.0005,     0.0008,     0.6473,     0.2284],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1937, 0.0009, 0.0007, 0.3830, 0.4218], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6222, 0.0033, 0.0022, 0.0908, 0.2814], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0301,     0.9397,     0.0008,     0.0003,     0.0291],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9993,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1745, 0.0006, 0.0179, 0.5237, 0.2833], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2922, 0.0646, 0.0074, 0.2371, 0.3987], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.996802825919964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.44768808768616
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.69794738292694
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.98948851625046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.802]
 [54.048]
 [59.629]
 [55.325]
 [52.012]] [[0.551]
 [0.556]
 [0.667]
 [0.581]
 [0.515]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.859]
 [15.168]
 [10.183]
 [11.859]
 [12.33 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.089]
 [25.735]
 [37.288]
 [36.586]
 [36.089]] [[1.627]
 [1.16 ]
 [1.681]
 [1.649]
 [1.627]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.597]
 [28.652]
 [34.557]
 [35.191]
 [32.36 ]] [[0.965]
 [0.773]
 [1.158]
 [1.2  ]
 [1.015]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76118684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.77077017909816
siam score:  -0.76234454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.246979360031894
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  31.91572666168213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.6997263192451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.5755,     0.0005,     0.0003,     0.1247,     0.2990],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0013,     0.9981,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0104,     0.9105,     0.0606,     0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0916, 0.0012, 0.0023, 0.6456, 0.2593], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1210, 0.0132, 0.0038, 0.4275, 0.4345], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.31404658196292
printing an ep nov before normalisation:  48.34668148381565
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6542, 0.0131, 0.0014, 0.1347, 0.1967], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0026,     0.9954,     0.0000,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.9697,     0.0114,     0.0185],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1413,     0.0003,     0.0056,     0.4876,     0.3652],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3280, 0.0086, 0.0055, 0.2902, 0.3678], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.158]
 [36.96 ]
 [40.792]
 [36.96 ]
 [38.269]] [[1.278]
 [1.335]
 [1.612]
 [1.335]
 [1.43 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7403629
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.63989440824148
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.53694825239472
siam score:  -0.74961334
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.604]
 [24.604]
 [24.604]
 [24.604]
 [24.604]] [[32.798]
 [32.798]
 [32.798]
 [32.798]
 [32.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5901, 0.0253, 0.0009, 0.1407, 0.2431], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     1.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0029,     0.9945,     0.0018,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0947,     0.0005,     0.0004,     0.7721,     0.1323],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2689, 0.0240, 0.0047, 0.3451, 0.3573], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 49.089395674058544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.69278812408447
printing an ep nov before normalisation:  41.27925773959164
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.372]
 [27.525]
 [28.388]
 [27.525]
 [27.525]] [[0.547]
 [0.373]
 [0.384]
 [0.373]
 [0.373]]
siam score:  -0.7531368
printing an ep nov before normalisation:  37.93054252874751
printing an ep nov before normalisation:  62.45536720524313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.33270072937012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7037697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7020176
siam score:  -0.69758767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6954415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.966]
 [28.131]
 [30.068]
 [29.771]
 [29.062]] [[0.711]
 [0.465]
 [0.526]
 [0.517]
 [0.495]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.59595497731899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.77139606432703
printing an ep nov before normalisation:  39.41868829749309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.6938645
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.242282390594482
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.801]
 [17.491]
 [36.649]
 [23.326]
 [17.746]] [[0.08 ]
 [0.088]
 [0.289]
 [0.149]
 [0.09 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.759133
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.11490697291893
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.997]
 [33.997]
 [33.997]
 [40.253]
 [33.997]] [[1.184]
 [1.184]
 [1.184]
 [1.599]
 [1.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.837502344359255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.81321430206299
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.86 ]
 [33.86 ]
 [45.765]
 [33.86 ]
 [33.86 ]] [[1.006]
 [1.006]
 [1.645]
 [1.006]
 [1.006]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.201]
 [ 0.   ]
 [28.352]
 [ 0.   ]
 [ 0.   ]] [[ 0.722]
 [-0.   ]
 [ 0.509]
 [-0.   ]
 [-0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.80089071926762
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.357]
 [61.357]
 [61.357]
 [61.357]
 [61.357]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
printing an ep nov before normalisation:  67.38666973386489
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.582]
 [59.582]
 [59.582]
 [59.582]
 [59.582]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76128244
printing an ep nov before normalisation:  35.57042121887207
printing an ep nov before normalisation:  40.552387666305435
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76218444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.004548296367801186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7560582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75502515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.53267961533951
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.951775639017534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.301714212502844
printing an ep nov before normalisation:  55.45740564123052
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.641]
 [55.641]
 [55.641]
 [55.641]
 [55.641]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
printing an ep nov before normalisation:  37.232190251922404
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  26.56914393107096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7566019
actions average: 
K:  4  action  0 :  tensor([0.7048, 0.0066, 0.0008, 0.0912, 0.1966], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0020,     0.9884,     0.0000,     0.0001,     0.0095],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9227,     0.0510,     0.0262],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0791, 0.0199, 0.0258, 0.6293, 0.2459], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2137, 0.0357, 0.0205, 0.3656, 0.3644], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7164,     0.0007,     0.0002,     0.1073,     0.1754],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1110, 0.0007, 0.0182, 0.6501, 0.2200], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2547, 0.0213, 0.0055, 0.2961, 0.4224], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.28229993490076
printing an ep nov before normalisation:  36.87915325164795
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.334]
 [36.334]
 [36.334]
 [36.334]
 [36.334]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.59 ]
 [57.59 ]
 [59.589]
 [57.59 ]
 [57.59 ]] [[1.416]
 [1.416]
 [1.48 ]
 [1.416]
 [1.416]]
printing an ep nov before normalisation:  58.58660697937012
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.63094737375774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6449,     0.0005,     0.0003,     0.1526,     0.2017],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9796,     0.0023,     0.0004,     0.0174],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9902,     0.0055,     0.0041],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0518, 0.0009, 0.0665, 0.7313, 0.1495], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2469, 0.0009, 0.0019, 0.3428, 0.4075], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75789595
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.432]
 [73.432]
 [73.432]
 [73.432]
 [73.432]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.519]
 [29.203]
 [29.223]
 [28.329]
 [26.01 ]] [[0.704]
 [0.783]
 [0.784]
 [0.742]
 [0.634]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.29709657031112
printing an ep nov before normalisation:  30.514291354215217
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.07871554487265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.437]
 [39.623]
 [39.623]
 [65.248]
 [39.623]] [[1.484]
 [0.318]
 [0.318]
 [1.475]
 [0.318]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75503904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.13 ]
 [49.775]
 [66.994]
 [49.775]
 [49.775]] [[1.369]
 [1.032]
 [1.651]
 [1.032]
 [1.032]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.284]
 [41.536]
 [42.386]
 [42.367]
 [41.536]] [[1.932]
 [1.898]
 [1.936]
 [1.936]
 [1.898]]
siam score:  -0.7606515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.39669722924031
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.802]
 [46.085]
 [48.611]
 [43.25 ]
 [39.322]] [[0.189]
 [0.238]
 [0.261]
 [0.212]
 [0.175]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77514505
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.33482128295461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.102]
 [38.102]
 [63.722]
 [38.102]
 [38.102]] [[0.288]
 [0.288]
 [0.569]
 [0.288]
 [0.288]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.93727808213664
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.8799,     0.0001,     0.0016,     0.0484,     0.0699],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0015,     0.9957,     0.0000,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0006,     0.9846,     0.0094,     0.0054],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1607, 0.0008, 0.0028, 0.5296, 0.3061], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3140, 0.0013, 0.0037, 0.2206, 0.4604], grad_fn=<DivBackward0>)
siam score:  -0.75092405
printing an ep nov before normalisation:  32.1571466148915
siam score:  -0.7491006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[9.617]
 [9.617]
 [9.617]
 [9.617]
 [9.617]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 1.3190036283520124e-11
0.0 1.3077596631288099e-11
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.4980421540681493e-11
0.0 0.0
0.0 9.842794285842918e-12
printing an ep nov before normalisation:  47.34397530555725
siam score:  -0.7590451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.368]
 [21.453]
 [26.16 ]
 [19.165]
 [19.195]] [[0.778]
 [0.784]
 [1.087]
 [0.636]
 [0.638]]
printing an ep nov before normalisation:  11.88039338981838
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.477]
 [28.756]
 [29.438]
 [28.531]
 [32.296]] [[0.922]
 [0.938]
 [0.979]
 [0.925]
 [1.148]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  21.730425357818604
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76102614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.23 ]
 [55.118]
 [55.118]
 [55.118]
 [55.118]] [[1.369]
 [1.63 ]
 [1.63 ]
 [1.63 ]
 [1.63 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.01871327135619
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77615553
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.34221935272217
siam score:  -0.76610166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.607]
 [41.688]
 [41.688]
 [41.688]
 [41.688]] [[0.604]
 [0.591]
 [0.591]
 [0.591]
 [0.591]]
printing an ep nov before normalisation:  35.20332988740258
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.514027256309014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.612]
 [28.764]
 [45.243]
 [44.726]
 [43.348]] [[1.788]
 [1.179]
 [1.855]
 [1.834]
 [1.777]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.451]
 [35.667]
 [47.443]
 [48.928]
 [47.304]] [[1.801]
 [1.142]
 [1.749]
 [1.825]
 [1.742]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.446679957031375
siam score:  -0.7688623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.41405387749298
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 32.052281588751036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.47747693842801
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.746]
 [35.784]
 [32.573]
 [30.081]
 [36.291]] [[0.703]
 [1.   ]
 [0.842]
 [0.719]
 [1.025]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.36940672881234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.54658222198486
printing an ep nov before normalisation:  28.95992340737378
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.750530004501343
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.79898325602213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.653775190865467
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.17478740402208
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.026]
 [17.494]
 [26.737]
 [21.071]
 [15.8  ]] [[0.263]
 [0.274]
 [0.48 ]
 [0.353]
 [0.236]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.06177247654157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.26450639291651
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.6761,     0.0013,     0.0004,     0.1294,     0.1927],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0110,     0.9825,     0.0000,     0.0000,     0.0065],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9886,     0.0046,     0.0068],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1129, 0.0007, 0.0016, 0.5833, 0.3014], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.3094,     0.0032,     0.0004,     0.2564,     0.4307],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.59616100017306
siam score:  -0.76504743
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.127118885123753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.832397783050574
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.73518887678347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.243614548074866
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.244650978257965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.203331612685076
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.083 0.75  0.042]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  42.6451765803101
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.89662265777588
printing an ep nov before normalisation:  27.9808804450256
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.40083138756892
printing an ep nov before normalisation:  14.214759567419861
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7427668
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.0299243482982
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.317]
 [46.607]
 [42.607]
 [48.112]
 [44.748]] [[1.22 ]
 [0.905]
 [0.761]
 [0.96 ]
 [0.838]]
printing an ep nov before normalisation:  49.82667033241553
printing an ep nov before normalisation:  49.69112462261854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.184174821707586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.303248495931157
printing an ep nov before normalisation:  24.666462917852613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.113]
 [49.826]
 [63.167]
 [59.306]
 [54.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.97357940429861
printing an ep nov before normalisation:  30.970496102492096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.373410947644956
printing an ep nov before normalisation:  40.07421086730153
printing an ep nov before normalisation:  20.84916114807129
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  46.53307713722643
printing an ep nov before normalisation:  40.997159481048584
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.4881534576416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.99]
 [46.99]
 [46.99]
 [46.99]
 [46.99]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.78484344624715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  20.634751319885254
printing an ep nov before normalisation:  10.538797378540039
printing an ep nov before normalisation:  7.052081394724965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.13298033600706
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.1885051197107
printing an ep nov before normalisation:  25.557756924710027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.45889496803284
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.665879249572754
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.93138260084615
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.678902338693675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.768303078199644
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  22.048189029226606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.27978860377803
printing an ep nov before normalisation:  27.419832530479475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.25009495023157
line 256 mcts: sample exp_bonus 24.7505018805409
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.996750847687355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.098985007566853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7716765
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.396771639387733
actions average: 
K:  1  action  0 :  tensor([    0.6563,     0.0003,     0.0004,     0.1083,     0.2347],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0101,     0.9683,     0.0001,     0.0004,     0.0212],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9843,     0.0079,     0.0073],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0947,     0.0004,     0.0231,     0.6395,     0.2423],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2595, 0.0008, 0.0086, 0.2999, 0.4312], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.405144605845226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.2923165838071
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.6327, 0.0012, 0.0007, 0.1140, 0.2513], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9967,     0.0000,     0.0000,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9369,     0.0358,     0.0272],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1718, 0.0007, 0.0008, 0.6275, 0.1993], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3681, 0.0028, 0.0009, 0.2512, 0.3770], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.333]
 [32.333]
 [52.18 ]
 [32.333]
 [48.784]] [[0.155]
 [0.155]
 [0.722]
 [0.155]
 [0.625]]
line 256 mcts: sample exp_bonus 42.0022406578064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.78685333568413
printing an ep nov before normalisation:  34.035083898157566
printing an ep nov before normalisation:  39.86311977021568
printing an ep nov before normalisation:  30.199387005965654
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.42163997226291
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.36827949946013
printing an ep nov before normalisation:  23.161251752261016
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.672023636954172
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.285]
 [16.285]
 [31.708]
 [16.285]
 [16.285]] [[0.357]
 [0.357]
 [1.   ]
 [0.357]
 [0.357]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.68 ]
 [58.061]
 [64.674]
 [63.651]
 [62.707]] [[1.384]
 [1.246]
 [1.444]
 [1.413]
 [1.385]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.1165847559589
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [0. ]
 [1.5]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.5]
 [1.5]
 [1.5]
 [0. ]
 [1.5]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 1 threads
Frames:  39858 train batches done:  4667 episodes:  2918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.773895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7253,     0.0004,     0.0004,     0.1015,     0.1724],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9986,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9980,     0.0009,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1577,     0.0005,     0.0003,     0.5758,     0.2657],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2464, 0.0012, 0.0007, 0.3079, 0.4437], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.381694793701172
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.548622126613608
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.465267023028744
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.54330617251661
printing an ep nov before normalisation:  46.437392779145846
printing an ep nov before normalisation:  42.829992247746844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.665536859287077
siam score:  -0.7655463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.764401870878935
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.61687176441845
siam score:  -0.76546186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7654042
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6431,     0.0003,     0.0004,     0.1604,     0.1958],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9992,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0012,     0.9948,     0.0020,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1374,     0.0003,     0.0003,     0.6726,     0.1893],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3011, 0.0020, 0.0005, 0.3100, 0.3864], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.827617168426514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.68537278493926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.3326510265108
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.90472552947159
siam score:  -0.7541318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.936214406982362
siam score:  -0.7601288
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.279]
 [37.649]
 [12.638]
 [37.649]
 [37.649]] [[0.056]
 [0.333]
 [0.037]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76945895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.931540834785196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 0.0
0.0 6.919363292971541e-13
0.0 -8.303235953176889e-13
0.0 5.708474763623056e-13
0.0 3.1396610968339834e-12
0.0 -1.1416949386280128e-12
0.0 4.497586137612182e-13
0.0 0.0
0.0 0.0
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.66694151413284
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.244835875771305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.80850889810825
siam score:  -0.7546579
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.60009332709315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.113]
 [23.113]
 [23.113]
 [23.113]
 [23.113]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.167 0.5   0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7636801
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76797533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.6948,     0.0015,     0.0004,     0.1315,     0.1718],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9984,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0267,     0.9718,     0.0005,     0.0011],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0814,     0.0007,     0.0380,     0.7145,     0.1653],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3087, 0.0194, 0.0022, 0.2791, 0.3906], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.35848369415237
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.697710487435046
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.96454931726192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.71888189526154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7079,     0.0004,     0.0003,     0.1112,     0.1803],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0059,     0.9771,     0.0000,     0.0021,     0.0150],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9875,     0.0086,     0.0039],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.2064,     0.0005,     0.0002,     0.5376,     0.2552],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.3470,     0.0008,     0.0002,     0.3071,     0.3449],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.92860527109514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.592852608952713
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.771]
 [40.771]
 [40.771]
 [40.771]
 [40.771]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.811106167725825
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.506086973313913
printing an ep nov before normalisation:  61.49330995680759
using explorer policy with actor:  1
siam score:  -0.7769666
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.08043956756592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7780729
printing an ep nov before normalisation:  28.674153322486656
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7859932
line 256 mcts: sample exp_bonus 24.332303189380973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.626]
 [50.9  ]
 [45.764]
 [39.626]
 [39.626]] [[0.409]
 [0.678]
 [0.556]
 [0.409]
 [0.409]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7756786
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.247]
 [24.288]
 [23.386]
 [30.247]
 [30.247]] [[1.956]
 [1.333]
 [1.239]
 [1.956]
 [1.956]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77534103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.446832315707425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7822264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  16.355637717031613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  30.15425443649292
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.202]
 [42.812]
 [45.997]
 [51.119]
 [44.202]] [[0.472]
 [0.443]
 [0.509]
 [0.615]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.27542190696065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.147]
 [59.608]
 [72.559]
 [68.682]
 [61.41 ]] [[0.707]
 [0.633]
 [0.903]
 [0.822]
 [0.671]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77585524
printing an ep nov before normalisation:  41.05579376220703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7724507
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.766582
printing an ep nov before normalisation:  45.843906487989116
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.426]
 [31.895]
 [37.987]
 [43.533]
 [38.335]] [[0.779]
 [0.35 ]
 [0.519]
 [0.672]
 [0.528]]
siam score:  -0.7654591
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.81588665421805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.08288847700962
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.7034,     0.0016,     0.0002,     0.1390,     0.1558],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0173, 0.9448, 0.0062, 0.0035, 0.0283], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0169,     0.9760,     0.0041,     0.0030],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1096,     0.0005,     0.1698,     0.5175,     0.2027],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1797, 0.0028, 0.1635, 0.2516, 0.4024], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7606705
printing an ep nov before normalisation:  0.028466795316717253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.31306156251351
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.513]
 [65.389]
 [64.559]
 [64.811]
 [65.389]] [[0.887]
 [1.   ]
 [0.976]
 [0.983]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.86296152447846
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.149]
 [30.149]
 [30.149]
 [30.149]
 [30.149]] [[0.619]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.279215683833215
printing an ep nov before normalisation:  38.71576988818978
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.73499066988132
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.272]
 [35.272]
 [35.272]
 [35.272]
 [35.272]] [[1.05]
 [1.05]
 [1.05]
 [1.05]
 [1.05]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77042085
printing an ep nov before normalisation:  71.3883647146247
actions average: 
K:  1  action  0 :  tensor([    0.7799,     0.0002,     0.0004,     0.1269,     0.0927],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0021,     0.9958,     0.0000,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9988,     0.0006,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0722,     0.0002,     0.0044,     0.6970,     0.2263],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3307, 0.0405, 0.0022, 0.2706, 0.3561], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.4868745803833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7614676
actions average: 
K:  3  action  0 :  tensor([    0.7486,     0.0085,     0.0004,     0.0716,     0.1710],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0021, 0.9601, 0.0011, 0.0047, 0.0320], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9689,     0.0238,     0.0073],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0978, 0.0016, 0.0018, 0.6679, 0.2309], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1484, 0.0199, 0.0829, 0.4003, 0.3485], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.39103651046753
printing an ep nov before normalisation:  41.581140324845386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7883,     0.0092,     0.0002,     0.0711,     0.1313],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9960,     0.0000,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0184,     0.9461,     0.0168,     0.0186],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1715,     0.0001,     0.0003,     0.7347,     0.0935],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1998, 0.0104, 0.0024, 0.2698, 0.5176], grad_fn=<DivBackward0>)
siam score:  -0.7665355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.52169990539551
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.48]
 [33.48]
 [26.42]
 [33.48]
 [33.48]] [[1.006]
 [1.006]
 [0.667]
 [1.006]
 [1.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.513349585030134
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76871973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.403]
 [43.403]
 [58.495]
 [43.403]
 [43.403]] [[0.777]
 [0.777]
 [1.291]
 [0.777]
 [0.777]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.364]
 [30.781]
 [29.059]
 [29.69 ]
 [28.865]] [[0.893]
 [1.053]
 [0.939]
 [0.981]
 [0.926]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.2813256985731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.6527,     0.0002,     0.0004,     0.1526,     0.1941],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0059, 0.9784, 0.0012, 0.0011, 0.0134], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9973,     0.0017,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0810,     0.0002,     0.0021,     0.6983,     0.2184],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.2438,     0.0004,     0.0081,     0.3133,     0.4344],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.58514528293661
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.3351518133422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.9991103710131
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  14.668133726117889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76507384
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.899775909023166
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6155, 0.0014, 0.0007, 0.1727, 0.2097], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9969,     0.0001,     0.0003,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9939,     0.0030,     0.0030],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0476,     0.0001,     0.0028,     0.8702,     0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2915, 0.0013, 0.0020, 0.3370, 0.3683], grad_fn=<DivBackward0>)
siam score:  -0.76508087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7110,     0.0004,     0.0002,     0.0607,     0.2278],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9968,     0.0000,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9814,     0.0120,     0.0064],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1157, 0.0077, 0.0451, 0.5440, 0.2876], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3844, 0.0006, 0.0382, 0.2066, 0.3703], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7644623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  53.14207810697478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.861]
 [41.532]
 [46.097]
 [44.398]
 [42.932]] [[0.25 ]
 [0.272]
 [0.331]
 [0.309]
 [0.29 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.612]
 [26.823]
 [45.226]
 [29.503]
 [26.823]] [[0.398]
 [0.436]
 [1.006]
 [0.519]
 [0.436]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.361056862598154
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [55.778]
 [ 0.   ]] [[-0.605]
 [-0.605]
 [-0.605]
 [ 1.105]
 [-0.605]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.76411104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76384336
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.95405218353027
actions average: 
K:  0  action  0 :  tensor([    0.7145,     0.0077,     0.0003,     0.0717,     0.2057],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9991,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0010,     0.9949,     0.0021,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0680,     0.0003,     0.0001,     0.8018,     0.1298],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2869, 0.0025, 0.0005, 0.3106, 0.3996], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.80558483113724
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7678694
printing an ep nov before normalisation:  66.66560140453883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.63428765908577
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7586,     0.0073,     0.0002,     0.1074,     0.1264],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0023,     0.9918,     0.0000,     0.0000,     0.0058],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9943,     0.0026,     0.0031],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0776,     0.0003,     0.0004,     0.7508,     0.1708],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2907, 0.0177, 0.0026, 0.3246, 0.3643], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.74735368775517
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.90575679377206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7679232
printing an ep nov before normalisation:  34.55114126205444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.82317999921916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.584298390929526
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.57687789518698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.87000131607056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.77512264251709
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76853544
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.643960088461586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  12.09657029219378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  19.92108253272221
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.57084083557129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.52484941482544
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8383,     0.0006,     0.0002,     0.0629,     0.0979],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9869,     0.0061,     0.0006,     0.0060],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1195, 0.0013, 0.0063, 0.6688, 0.2040], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2247, 0.0091, 0.0269, 0.3879, 0.3514], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.29632041254452
siam score:  -0.7755471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.95952388027095
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actions average: 
K:  3  action  0 :  tensor([    0.7148,     0.0003,     0.0007,     0.0987,     0.1855],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0028,     0.9948,     0.0000,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0031,     0.9349,     0.0320,     0.0301],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1331,     0.0002,     0.0004,     0.7592,     0.1071],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.2719,     0.0004,     0.0014,     0.4199,     0.3064],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.85104126290517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.30952188600496
siam score:  -0.784344
printing an ep nov before normalisation:  28.404102325439453
printing an ep nov before normalisation:  53.4572522086558
printing an ep nov before normalisation:  37.86522940808217
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  22.46894928500815
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7765947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.336126145359604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.992990493774414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  2  action  0 :  tensor([    0.7971,     0.0026,     0.0003,     0.0616,     0.1384],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9942,     0.0003,     0.0000,     0.0053],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9967,     0.0010,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0529,     0.0002,     0.0134,     0.7792,     0.1542],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2497, 0.0457, 0.0277, 0.2670, 0.4100], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7874713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.62496811884944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.15525806857756
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7278,     0.0005,     0.0015,     0.0849,     0.1852],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0040,     0.9785,     0.0082,     0.0001,     0.0092],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9996,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0805,     0.0003,     0.0046,     0.6718,     0.2428],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2866, 0.0016, 0.0026, 0.2638, 0.4455], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.4667630795805
printing an ep nov before normalisation:  65.96945843381823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.808]
 [25.662]
 [30.555]
 [26.214]
 [24.399]] [[0.911]
 [0.985]
 [1.408]
 [1.033]
 [0.875]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.34539269324616
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.401]
 [25.084]
 [21.447]
 [17.464]
 [22.046]] [[1.16 ]
 [1.609]
 [1.26 ]
 [0.879]
 [1.318]]
siam score:  -0.77120835
siam score:  -0.7705406
printing an ep nov before normalisation:  24.502835954938618
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.99 ]
 [55.99 ]
 [58.809]
 [55.99 ]
 [55.99 ]] [[1.118]
 [1.118]
 [1.202]
 [1.118]
 [1.118]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.51199571610334
siam score:  -0.77466464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7752051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.11929382622505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.251]
 [30.251]
 [30.251]
 [30.251]
 [30.251]] [[0.81]
 [0.81]
 [0.81]
 [0.81]
 [0.81]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.735]
 [34.882]
 [34.882]
 [34.882]
 [34.882]] [[0.535]
 [0.309]
 [0.309]
 [0.309]
 [0.309]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  17.591020931326092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.778434
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7808527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6327,     0.0010,     0.0006,     0.1627,     0.2030],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0088,     0.9831,     0.0000,     0.0002,     0.0078],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9958,     0.0024,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0739,     0.0004,     0.0010,     0.6989,     0.2257],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1461, 0.0014, 0.0008, 0.4358, 0.4161], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  37.55195140838623
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.39296360350948
siam score:  -0.7635195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.96547365223517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.63750243136738
printing an ep nov before normalisation:  26.128182411193848
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6705,     0.0114,     0.0003,     0.1230,     0.1948],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9917,     0.0000,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0017,     0.9976,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0805, 0.0016, 0.0045, 0.6181, 0.2953], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2469, 0.0010, 0.0042, 0.3378, 0.4101], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.24570658106964
siam score:  -0.7743985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.4164995799617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7787449
printing an ep nov before normalisation:  34.58209723205129
siam score:  -0.77858025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  39.80800955550833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 35.308370172443674
printing an ep nov before normalisation:  20.111911848592822
printing an ep nov before normalisation:  28.420743942260742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7847142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.963]
 [14.755]
 [19.075]
 [10.423]
 [18.814]] [[1.81 ]
 [1.226]
 [1.711]
 [0.741]
 [1.681]]
actions average: 
K:  0  action  0 :  tensor([    0.8325,     0.0004,     0.0002,     0.0558,     0.1112],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9975,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0005,     0.9924,     0.0023,     0.0047],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0413, 0.0063, 0.0122, 0.6417, 0.2985], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2852, 0.0037, 0.0023, 0.3424, 0.3664], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7456, 0.0069, 0.0032, 0.0952, 0.1490], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9897,     0.0003,     0.0001,     0.0092],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0009,     0.0274,     0.9547,     0.0037,     0.0133],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1584, 0.0009, 0.0020, 0.5092, 0.3296], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3300, 0.0136, 0.0020, 0.2358, 0.4186], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.037105940709225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77204674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.995464902806383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8278,     0.0107,     0.0002,     0.0582,     0.1032],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9993,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9960,     0.0026,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0469,     0.0003,     0.0005,     0.7414,     0.2110],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2420, 0.0142, 0.0009, 0.3119, 0.4310], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.38653978023112
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.05 ]
 [38.05 ]
 [42.242]
 [38.05 ]
 [38.05 ]] [[1.325]
 [1.325]
 [1.617]
 [1.325]
 [1.325]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.444009109920337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.74314912725338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8194,     0.0002,     0.0002,     0.0853,     0.0948],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0306,     0.9432,     0.0001,     0.0000,     0.0262],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0016,     0.9697,     0.0045,     0.0239],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0840, 0.0020, 0.0007, 0.6446, 0.2687], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.2513,     0.0005,     0.0004,     0.3238,     0.4240],
       grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  36.96820020675659
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.728596448898315
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77543455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.2579437996197953
line 256 mcts: sample exp_bonus 55.700213739083296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.215]
 [41.77 ]
 [41.77 ]
 [41.77 ]
 [41.77 ]] [[1.163]
 [1.009]
 [1.009]
 [1.009]
 [1.009]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.813]
 [45.813]
 [48.13 ]
 [47.377]
 [45.813]] [[1.478]
 [1.478]
 [1.582]
 [1.548]
 [1.478]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.155]
 [30.104]
 [29.676]
 [31.798]
 [25.871]] [[1.058]
 [0.996]
 [0.971]
 [1.096]
 [0.749]]
printing an ep nov before normalisation:  46.892460928030026
printing an ep nov before normalisation:  34.54841079972549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.667 0.125 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.230494540135275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7790593
printing an ep nov before normalisation:  18.01434756121978
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.562664750812516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7906, 0.0062, 0.0017, 0.0597, 0.1418], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0045,     0.9912,     0.0003,     0.0001,     0.0039],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9992,     0.0005,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0930,     0.0002,     0.0011,     0.7127,     0.1930],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2417, 0.0008, 0.0027, 0.3738, 0.3809], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.41935443878174
printing an ep nov before normalisation:  32.715598026393685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7793335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.3244941238338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.91113877315047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.41137669645969
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7787889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77856857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.321790878848994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.338]
 [40.199]
 [44.02 ]
 [42.597]
 [38.796]] [[1.105]
 [1.485]
 [1.784]
 [1.673]
 [1.375]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7887838
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7793873
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.465403238781214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.869850400031217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7801829
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.03781390691996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.50688798227742
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7793871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.243972884284126
printing an ep nov before normalisation:  27.436375344945777
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.64108482684031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.68090615427206
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.7714,     0.0010,     0.0006,     0.0975,     0.1296],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0070, 0.9397, 0.0093, 0.0031, 0.0409], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9959,     0.0024,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0818,     0.0005,     0.0055,     0.7383,     0.1739],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2342, 0.0010, 0.0180, 0.4294, 0.3174], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7833649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78520775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.941]
 [31.941]
 [59.696]
 [31.941]
 [31.941]] [[0.554]
 [0.554]
 [1.04 ]
 [0.554]
 [0.554]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.771]
 [41.771]
 [58.42 ]
 [41.771]
 [41.771]] [[0.952]
 [0.952]
 [1.333]
 [0.952]
 [0.952]]
printing an ep nov before normalisation:  55.8278242746989
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.60913719623504
printing an ep nov before normalisation:  71.41214400742108
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.631402254104614
printing an ep nov before normalisation:  70.88901558754664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.285987660116454
printing an ep nov before normalisation:  37.564130534002906
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.842637394917347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.995]
 [46.761]
 [43.296]
 [45.771]
 [43.78 ]] [[1.371]
 [1.359]
 [1.174]
 [1.306]
 [1.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.617527803057875
actions average: 
K:  3  action  0 :  tensor([    0.8243,     0.0005,     0.0004,     0.0616,     0.1132],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0055,     0.9879,     0.0000,     0.0000,     0.0066],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0244,     0.9680,     0.0011,     0.0065],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0689, 0.0009, 0.0290, 0.6685, 0.2326], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.2280,     0.0006,     0.0004,     0.3211,     0.4499],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7810057
printing an ep nov before normalisation:  34.1840065585874
printing an ep nov before normalisation:  40.70358484890889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.152490670196336
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.29903397177929
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.1578631401062
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7645,     0.0007,     0.0001,     0.0701,     0.1645],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9988,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9780,     0.0030,     0.0186],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0968,     0.0006,     0.0059,     0.6315,     0.2653],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2392, 0.0046, 0.0051, 0.3734, 0.3777], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.40603188032963
printing an ep nov before normalisation:  30.74833518216381
actions average: 
K:  3  action  0 :  tensor([0.7720, 0.0141, 0.0031, 0.0878, 0.1230], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0025,     0.9967,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9871,     0.0043,     0.0085],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0565,     0.0003,     0.0011,     0.6465,     0.2956],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2308, 0.0047, 0.0005, 0.2979, 0.4660], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7671518
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76534075
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.59924752934037
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.99922466278076
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.34206247329712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.965254249866575
actions average: 
K:  4  action  0 :  tensor([    0.7401,     0.0103,     0.0003,     0.0944,     0.1548],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0071, 0.9350, 0.0240, 0.0026, 0.0312], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0006,     0.9592,     0.0236,     0.0164],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0486,     0.0004,     0.0015,     0.7789,     0.1707],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2233, 0.0095, 0.0437, 0.3679, 0.3556], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.039684860372343
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.64273590441869
UNIT TEST: sample policy line 217 mcts : [0.208 0.125 0.125 0.167 0.375]
printing an ep nov before normalisation:  45.59696253255076
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.001813881728815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78527606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.767992973327637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.8516,     0.0008,     0.0007,     0.0318,     0.1152],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0078,     0.9802,     0.0003,     0.0001,     0.0115],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9477,     0.0280,     0.0241],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0375,     0.0002,     0.0011,     0.8201,     0.1410],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1978, 0.0005, 0.0020, 0.4235, 0.3761], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.499601138406454
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.718]
 [47.972]
 [55.278]
 [48.585]
 [43.723]] [[0.412]
 [0.452]
 [0.58 ]
 [0.462]
 [0.377]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.4913130541467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.31864295053897
printing an ep nov before normalisation:  63.816802576943886
printing an ep nov before normalisation:  61.46030492572899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.026439739340727
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.471121585353
Starting evaluation
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.036]
 [45.019]
 [44.916]
 [44.853]
 [37.476]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  43.48857842035859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.165678378751835
printing an ep nov before normalisation:  33.62850697772541
printing an ep nov before normalisation:  63.18970141685549
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.053]
 [23.933]
 [31.396]
 [28.967]
 [23.997]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.329]
 [37.329]
 [37.329]
 [37.329]
 [37.329]] [[37.329]
 [37.329]
 [37.329]
 [37.329]
 [37.329]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.07140369076346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.84742259592622
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7713902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.224]
 [59.836]
 [59.836]
 [59.836]
 [59.836]] [[1.924]
 [1.737]
 [1.737]
 [1.737]
 [1.737]]
printing an ep nov before normalisation:  27.71775680124449
printing an ep nov before normalisation:  51.8690720456225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.91563811620145
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 8.04721951023338e-11
0.0 2.2792382697149475e-10
0.0 2.3387447939748943e-11
0.0 0.0
0.0 4.73284449468021e-11
0.0 2.577462827135349e-11
0.0 2.577462827135349e-11
0.0 0.0
0.0 7.459073632062667e-11
0.0 1.1188610451154975e-10
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.22777950292097
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.097]
 [79.343]
 [79.673]
 [83.284]
 [82.413]] [[1.671]
 [1.705]
 [1.714]
 [1.814]
 [1.79 ]]
siam score:  -0.7796261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.56363809108734
printing an ep nov before normalisation:  34.16481908048439
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.39554711463169
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.882]
 [58.82 ]
 [45.882]
 [45.882]
 [45.882]] [[1.355]
 [1.737]
 [1.355]
 [1.355]
 [1.355]]
printing an ep nov before normalisation:  31.031925678253174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.779291702115046
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.623399981057084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.74223735428256
printing an ep nov before normalisation:  23.415949401089765
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.197175866634502
printing an ep nov before normalisation:  17.598111924383076
printing an ep nov before normalisation:  47.67137820905777
printing an ep nov before normalisation:  45.18968143274129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7736669
printing an ep nov before normalisation:  50.769422839784674
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[13.962]
 [13.962]
 [13.962]
 [13.962]
 [13.962]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.780798
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.201]
 [32.961]
 [44.264]
 [40.084]
 [38.082]] [[1.314]
 [0.884]
 [1.556]
 [1.307]
 [1.188]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.416744232177734
printing an ep nov before normalisation:  43.64383601108141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.70470525319673
siam score:  -0.7848114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.578699747288717
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.289048671722412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.720613834704245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.42977178724292
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.04124505882051
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.02585990323924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.69383384663614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.745]
 [35.768]
 [35.768]
 [35.768]
 [35.768]] [[1.763]
 [1.166]
 [1.166]
 [1.166]
 [1.166]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.72129383283452
siam score:  -0.7760054
printing an ep nov before normalisation:  71.90293428443303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 57.65833016611416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  45.32556194554132
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.714]
 [34.639]
 [34.639]
 [34.639]
 [34.639]] [[1.372]
 [1.156]
 [1.156]
 [1.156]
 [1.156]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.08196694510324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.92502144299147
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.65531698452272
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.97252416610718
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77269334
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.080649104889574
printing an ep nov before normalisation:  28.928585052490234
printing an ep nov before normalisation:  64.3381864033504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.667 0.25  0.042 0.042 0.   ]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.7441,     0.0004,     0.0004,     0.1124,     0.1426],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0215,     0.9769,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.9341,     0.0350,     0.0306],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1337,     0.0025,     0.0006,     0.7173,     0.1459],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2055, 0.0014, 0.0017, 0.3915, 0.3999], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78078085
printing an ep nov before normalisation:  30.225428932840682
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  4  action  0 :  tensor([    0.8010,     0.0035,     0.0006,     0.0608,     0.1341],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9971,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9885,     0.0046,     0.0065],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0718,     0.0004,     0.0089,     0.7592,     0.1598],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2166, 0.0058, 0.0007, 0.4488, 0.3283], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.167931529208573
siam score:  -0.7765896
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.778963
printing an ep nov before normalisation:  54.2506217956543
actions average: 
K:  0  action  0 :  tensor([    0.7316,     0.0004,     0.0007,     0.0801,     0.1872],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9911,     0.0020,     0.0000,     0.0064],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0033, 0.0025, 0.9465, 0.0169, 0.0308], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0570,     0.0003,     0.0005,     0.7346,     0.2075],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2693, 0.0125, 0.0010, 0.3303, 0.3869], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.2877595650616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.537635070268564
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.96 ]
 [25.888]
 [32.518]
 [32.719]
 [23.668]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.419]
 [51.419]
 [49.59 ]
 [51.419]
 [51.419]] [[1.   ]
 [1.   ]
 [0.932]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.65978468167592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.3680936241982
printing an ep nov before normalisation:  21.530814397206832
printing an ep nov before normalisation:  39.02282522184049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.694941135800775
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.40708991526851
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.522]
 [34.627]
 [36.482]
 [35.356]
 [35.606]] [[0.921]
 [0.986]
 [1.095]
 [1.029]
 [1.043]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.645177312407036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.69857573078295
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.477321545842905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.89548016472566
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.68115520477295
printing an ep nov before normalisation:  37.218336264292404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77708906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.23282365741797
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  39.228904247283936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.955623149871826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.56154392557665
printing an ep nov before normalisation:  69.31942194773677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.183934688568115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.125339139351205
printing an ep nov before normalisation:  54.39551000103116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7873645
printing an ep nov before normalisation:  24.216227057754352
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.46779744030658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7872597
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7867995
printing an ep nov before normalisation:  47.9196275983538
printing an ep nov before normalisation:  42.034084057117816
printing an ep nov before normalisation:  75.92535216148029
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.09759788178155
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.41017818450928
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.26308087958579
printing an ep nov before normalisation:  62.80625560568119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.545599726495766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.145]
 [29.96 ]
 [24.025]
 [21.977]
 [39.19 ]] [[1.   ]
 [0.719]
 [0.397]
 [0.286]
 [1.219]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.751841456067318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.870209411605515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.56622308815922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.17332553546996
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.09731960296631
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.72054594433177
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.782]
 [72.005]
 [60.832]
 [52.517]
 [51.459]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  41.3023688264493
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78037375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.496917114864505
printing an ep nov before normalisation:  23.814811675112853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7895446
printing an ep nov before normalisation:  53.74029989618842
siam score:  -0.79157656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.163]
 [56.163]
 [56.163]
 [56.163]
 [56.163]] [[0.563]
 [0.563]
 [0.563]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  56.56998171098782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.512]
 [33.224]
 [31.26 ]
 [35.889]
 [31.26 ]] [[0.582]
 [0.487]
 [0.43 ]
 [0.564]
 [0.43 ]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  70.99235100497906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  59.534925718117734
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  52.305120218012426
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.74753344
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.515]
 [30.422]
 [45.523]
 [39.096]
 [28.016]] [[0.295]
 [0.26 ]
 [0.508]
 [0.403]
 [0.221]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.076050285019285
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  39.50473574750272
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.69908347122218
deleting a thread, now have 1 threads
Frames:  69914 train batches done:  8190 episodes:  4990
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.7583412
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.484032783607176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.7568744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.7571077
printing an ep nov before normalisation:  71.82206011409178
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  63.79318579395252
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.7810,     0.0036,     0.0004,     0.0827,     0.1323],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0214, 0.8922, 0.0207, 0.0145, 0.0512], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9990,     0.0004,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0468,     0.0003,     0.0081,     0.7605,     0.1843],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1387, 0.0072, 0.0217, 0.3915, 0.4408], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.08691071393878
siam score:  -0.8003993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.44674015045166
printing an ep nov before normalisation:  52.75842600790995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  30.730351747314867
printing an ep nov before normalisation:  65.86597810845002
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
actions average: 
K:  1  action  0 :  tensor([    0.7511,     0.0002,     0.0003,     0.0901,     0.1583],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9992,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9949,     0.0026,     0.0023],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0987,     0.0003,     0.0005,     0.6835,     0.2171],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1650, 0.0563, 0.0059, 0.3417, 0.4311], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.083 0.208 0.5  ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8182375
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[49.227]
 [37.617]
 [37.617]
 [37.617]
 [37.617]] [[1.368]
 [0.862]
 [0.862]
 [0.862]
 [0.862]]
printing an ep nov before normalisation:  50.163876784207766
siam score:  -0.8232997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.8226631
printing an ep nov before normalisation:  33.38357448577881
printing an ep nov before normalisation:  39.365058863400094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.82379526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
siam score:  -0.8345805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  33.93784874265185
printing an ep nov before normalisation:  31.296668862342703
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.785619942936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
printing an ep nov before normalisation:  41.97403025151265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8494344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([    0.7411,     0.0106,     0.0005,     0.0992,     0.1486],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9989,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9171,     0.0440,     0.0389],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0472,     0.0007,     0.0009,     0.7720,     0.1792],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2309, 0.0014, 0.0041, 0.4328, 0.3309], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.6893, 0.0209, 0.0012, 0.1002, 0.1884], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9859,     0.0045,     0.0096],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0562, 0.0043, 0.0016, 0.7310, 0.2069], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2397, 0.0142, 0.0041, 0.3160, 0.4260], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 48.94010274073544
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
using explorer policy with actor:  1
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.7793,     0.0008,     0.0008,     0.1052,     0.1139],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0130,     0.9827,     0.0000,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0004,     0.9861,     0.0091,     0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0945, 0.0025, 0.0019, 0.7297, 0.1715], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2614, 0.0042, 0.0027, 0.3542, 0.3775], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  26.70735268325276
printing an ep nov before normalisation:  42.30290002858475
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.001]] [[25.548]
 [28.512]
 [25.548]
 [30.67 ]
 [25.548]] [[0.902]
 [1.116]
 [0.902]
 [1.273]
 [0.902]]
printing an ep nov before normalisation:  6.78408415310822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.84756607
printing an ep nov before normalisation:  22.270466315844438
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.202]
 [41.202]
 [41.202]
 [41.202]
 [41.202]] [[1.534]
 [1.534]
 [1.534]
 [1.534]
 [1.534]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.60463352219531
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[27.352]
 [25.182]
 [25.791]
 [24.028]
 [27.653]] [[0.972]
 [0.815]
 [0.859]
 [0.732]
 [0.994]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  22.814219424812702
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  16.579755197245273
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.724106788635254
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  36.73495510943919
siam score:  -0.85681397
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.003]
 [0.004]] [[43.3  ]
 [43.3  ]
 [43.3  ]
 [61.535]
 [43.3  ]] [[0.656]
 [0.656]
 [0.656]
 [1.178]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.08861315050931
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  66.86120925284857
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8911
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[41.14]
 [41.14]
 [41.14]
 [41.14]
 [41.14]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
siam score:  -0.8782576
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  35.55283546447754
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.67408466339111
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8636744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.86043286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  26.88760257153365
actions average: 
K:  3  action  0 :  tensor([    0.7299,     0.0222,     0.0007,     0.0600,     0.1873],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0027,     0.9866,     0.0001,     0.0001,     0.0104],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0037,     0.9896,     0.0016,     0.0052],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1462,     0.0003,     0.0261,     0.5769,     0.2505],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0734, 0.0391, 0.0049, 0.3844, 0.4983], grad_fn=<DivBackward0>)
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  38.920820709190664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  31.697278022766113
printing an ep nov before normalisation:  38.564223796425814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  2  action  0 :  tensor([    0.8047,     0.0001,     0.0001,     0.1045,     0.0906],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9978,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9983,     0.0004,     0.0013],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1462,     0.0004,     0.0017,     0.5591,     0.2926],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1899, 0.0379, 0.0032, 0.2851, 0.4839], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  3.278889419443658
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  42.76140313405579
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.01735966301095
printing an ep nov before normalisation:  36.616844306529885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.001]
 [0.009]] [[18.161]
 [18.161]
 [18.161]
 [15.781]
 [18.161]] [[1.457]
 [1.457]
 [1.457]
 [1.258]
 [1.457]]
actions average: 
K:  1  action  0 :  tensor([    0.8029,     0.0153,     0.0001,     0.0809,     0.1009],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0021,     0.9693,     0.0025,     0.0008,     0.0255],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0022,     0.9876,     0.0036,     0.0061],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0954,     0.0006,     0.0293,     0.6779,     0.1967],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1864, 0.0036, 0.1056, 0.2962, 0.4082], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  22.374236583709717
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.435]
 [50.395]
 [45.934]
 [30.887]
 [30.435]] [[0.095]
 [0.194]
 [0.172]
 [0.097]
 [0.095]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([    0.7868,     0.0006,     0.0003,     0.1214,     0.0910],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0164,     0.9695,     0.0000,     0.0018,     0.0122],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9661,     0.0037,     0.0300],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0992, 0.0010, 0.0028, 0.6154, 0.2816], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1869, 0.0361, 0.0212, 0.4154, 0.3404], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[40.314]
 [40.314]
 [40.314]
 [40.314]
 [40.314]] [[1.29]
 [1.29]
 [1.29]
 [1.29]
 [1.29]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.29630923271179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 61.49699794368346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8096011
siam score:  -0.8052314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  58.63011906827443
actions average: 
K:  1  action  0 :  tensor([    0.8052,     0.0006,     0.0003,     0.0627,     0.1311],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9989,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9983,     0.0011,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0626,     0.0002,     0.0006,     0.7716,     0.1650],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2447, 0.0024, 0.0101, 0.2722, 0.4705], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.82046616
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8327612
printing an ep nov before normalisation:  31.63238738561596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  37.88712024688721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[55.507]
 [38.517]
 [29.639]
 [31.219]
 [32.856]] [[0.714]
 [0.495]
 [0.381]
 [0.401]
 [0.422]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.8274,     0.0003,     0.0002,     0.0552,     0.1169],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9228,     0.0005,     0.0009,     0.0745],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0146,     0.9372,     0.0340,     0.0142],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0731, 0.0024, 0.0018, 0.7157, 0.2070], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1682, 0.0017, 0.0013, 0.3376, 0.4912], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  63.39481942105863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.404403316706784
printing an ep nov before normalisation:  29.35965435927379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 76.7495003653255
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[52.486]
 [47.16 ]
 [47.16 ]
 [47.16 ]
 [47.16 ]] [[1.34 ]
 [1.135]
 [1.135]
 [1.135]
 [1.135]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.004]
 [0.007]
 [0.007]] [[72.33 ]
 [68.385]
 [68.398]
 [71.258]
 [72.895]] [[1.271]
 [1.19 ]
 [1.186]
 [1.248]
 [1.281]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.89662194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.006]
 [0.008]
 [0.008]] [[40.024]
 [40.024]
 [58.033]
 [40.024]
 [40.024]] [[1.207]
 [1.207]
 [2.006]
 [1.207]
 [1.207]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.57112879496819
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.8057,     0.0003,     0.0003,     0.0884,     0.1054],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0010, 0.8889, 0.0019, 0.0323, 0.0759], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0006,     0.9964,     0.0005,     0.0025],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0296,     0.0005,     0.0263,     0.7586,     0.1850],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2322, 0.0024, 0.0082, 0.3294, 0.4277], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([0.7266, 0.0013, 0.0009, 0.1108, 0.1603], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0019,     0.9919,     0.0000,     0.0000,     0.0061],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0010,     0.0002,     0.9741,     0.0069,     0.0178],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0931, 0.0007, 0.0009, 0.6058, 0.2995], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.1512,     0.0007,     0.0001,     0.4279,     0.4201],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  67.44224795862402
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.7022,     0.0020,     0.0006,     0.1336,     0.1615],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9980,     0.0002,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9987,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0524,     0.0005,     0.0026,     0.7666,     0.1778],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2343, 0.0027, 0.0906, 0.2942, 0.3782], grad_fn=<DivBackward0>)
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  60.62314510345459
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  28.83563024795526
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([    0.9184,     0.0128,     0.0000,     0.0141,     0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0128,     0.9808,     0.0002,     0.0001,     0.0061],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9981,     0.0009,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0791,     0.0005,     0.0665,     0.6384,     0.2155],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1503, 0.0056, 0.0074, 0.3027, 0.5340], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[48.109]
 [61.665]
 [78.298]
 [57.758]
 [55.802]] [[0.431]
 [0.71 ]
 [1.054]
 [0.63 ]
 [0.59 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.8900212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.66922034729324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  28.55929732322693
siam score:  -0.8795518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  61.83364055407742
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.42521905899048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.00211032231649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  48.16629347028477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.59999465942383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.8574,     0.0011,     0.0001,     0.0483,     0.0932],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0024,     0.9940,     0.0002,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0006,     0.9970,     0.0014,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0636,     0.0001,     0.0250,     0.7527,     0.1585],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1254, 0.0006, 0.0092, 0.4439, 0.4210], grad_fn=<DivBackward0>)
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  47.89380752541001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[61.319]
 [61.319]
 [71.22 ]
 [61.319]
 [61.319]] [[1.065]
 [1.065]
 [1.334]
 [1.065]
 [1.065]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.851841
siam score:  -0.8505954
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  20.351314544677734
actions average: 
K:  4  action  0 :  tensor([0.7910, 0.0012, 0.0009, 0.0713, 0.1357], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9848,     0.0000,     0.0000,     0.0135],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0016,     0.9940,     0.0004,     0.0040],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0565, 0.0017, 0.0111, 0.7577, 0.1729], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1439, 0.0336, 0.0075, 0.3597, 0.4553], grad_fn=<DivBackward0>)
siam score:  -0.8511319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[28.03 ]
 [28.03 ]
 [47.906]
 [28.03 ]
 [28.03 ]] [[0.39 ]
 [0.39 ]
 [0.914]
 [0.39 ]
 [0.39 ]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.85610336
printing an ep nov before normalisation:  28.45002045295777
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.002]
 [0.003]] [[62.767]
 [64.893]
 [64.893]
 [56.884]
 [64.893]] [[1.49 ]
 [1.571]
 [1.571]
 [1.261]
 [1.571]]
printing an ep nov before normalisation:  63.17647873194011
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.14561605453491
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  53.76415982740853
siam score:  -0.8619118
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.001]
 [0.002]
 [0.003]] [[51.799]
 [32.548]
 [47.662]
 [37.496]
 [32.441]] [[0.896]
 [0.393]
 [0.786]
 [0.522]
 [0.39 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[25.322]
 [ 0.   ]
 [30.534]
 [ 0.   ]
 [ 0.   ]] [[ 0.393]
 [-0.562]
 [ 0.589]
 [-0.562]
 [-0.562]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.001]
 [0.003]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.84968567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[52.587]
 [36.022]
 [36.726]
 [42.603]
 [48.218]] [[0.898]
 [0.42 ]
 [0.441]
 [0.61 ]
 [0.772]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8456401
siam score:  -0.84553754
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[26.056]
 [33.265]
 [43.039]
 [33.444]
 [30.323]] [[0.346]
 [0.559]
 [0.848]
 [0.564]
 [0.472]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.8247,     0.0007,     0.0007,     0.0525,     0.1213],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0126,     0.9500,     0.0315,     0.0001,     0.0059],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0022,     0.9924,     0.0023,     0.0032],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0652, 0.0010, 0.0396, 0.6765, 0.2178], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1310, 0.0009, 0.0178, 0.4809, 0.3694], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
Starting evaluation
using explorer policy with actor:  0
printing an ep nov before normalisation:  56.72354654290652
printing an ep nov before normalisation:  46.42477429172616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
printing an ep nov before normalisation:  33.95181960939627
printing an ep nov before normalisation:  22.563458637441943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8512991
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  67.0087603938259
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84532183
printing an ep nov before normalisation:  23.898114537163202
printing an ep nov before normalisation:  64.84089475403775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
siam score:  -0.84884614
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.84767866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8494167
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  33.809852600097656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.84643984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.501556668962756
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.848727
printing an ep nov before normalisation:  84.7118176467527
printing an ep nov before normalisation:  53.4032531679006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8480687
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.693188419862665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([    0.8244,     0.0002,     0.0002,     0.0885,     0.0866],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0022,     0.9882,     0.0043,     0.0001,     0.0051],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9734,     0.0160,     0.0106],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1338,     0.0002,     0.0006,     0.6594,     0.2060],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1999, 0.0029, 0.0145, 0.3209, 0.4617], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  26.188775841462014
siam score:  -0.8338388
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.8161,     0.0598,     0.0002,     0.0487,     0.0752],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9063,     0.0888,     0.0004,     0.0043],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9984,     0.0005,     0.0011],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1004, 0.0015, 0.0128, 0.6498, 0.2355], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0724, 0.0575, 0.0967, 0.4285, 0.3450], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  62.05662359820705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.02831189882082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  40.450289990420885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.8177727
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[50.605]
 [66.403]
 [85.308]
 [66.413]
 [75.185]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.81978494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  61.26919985151576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([    0.7751,     0.0192,     0.0002,     0.0993,     0.1062],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9826,     0.0019,     0.0006,     0.0145],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0277,     0.9461,     0.0152,     0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0635,     0.0004,     0.0018,     0.6966,     0.2376],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3591, 0.0038, 0.0016, 0.2933, 0.3421], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  31.013511315550595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.728163637614585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
printing an ep nov before normalisation:  31.92361553211799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  57.331038637508186
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  10.979815648716999
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  70.18412820708868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
actions average: 
K:  2  action  0 :  tensor([    0.8832,     0.0002,     0.0001,     0.0400,     0.0765],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0230, 0.9226, 0.0068, 0.0010, 0.0466], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0070,     0.9726,     0.0151,     0.0053],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0930,     0.0004,     0.0192,     0.7002,     0.1873],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1682, 0.0031, 0.0027, 0.3746, 0.4515], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.28486430644989
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.002]
 [0.002]] [[51.233]
 [58.629]
 [51.233]
 [50.923]
 [49.74 ]] [[1.107]
 [1.41 ]
 [1.107]
 [1.095]
 [1.046]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.79189867
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([0.7717, 0.0150, 0.0036, 0.0909, 0.1188], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9968,     0.0001,     0.0001,     0.0029],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0052,     0.9715,     0.0088,     0.0145],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0222, 0.0007, 0.0744, 0.6796, 0.2231], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0471, 0.0120, 0.0065, 0.3701, 0.5644], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[31.695]
 [36.367]
 [31.695]
 [31.695]
 [31.695]] [[1.235]
 [1.638]
 [1.235]
 [1.235]
 [1.235]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[33.393]
 [35.052]
 [26.116]
 [31.695]
 [30.746]] [[1.375]
 [1.518]
 [0.75 ]
 [1.23 ]
 [1.148]]
printing an ep nov before normalisation:  21.195369311624187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  91.916633712656
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  21.18562936782837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  41.630111552432815
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
UNIT TEST: sample policy line 217 mcts : [0.083 0.042 0.792 0.042 0.042]
printing an ep nov before normalisation:  45.383375768331405
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  30.88896455174603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  31.661446639966996
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.9192,     0.0001,     0.0001,     0.0267,     0.0539],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0066,     0.9502,     0.0002,     0.0000,     0.0430],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0047,     0.9355,     0.0312,     0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1002, 0.0023, 0.0008, 0.7086, 0.1881], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1850, 0.0058, 0.0799, 0.3573, 0.3720], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.83667883568022
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[41.935]
 [44.346]
 [36.727]
 [44.346]
 [44.346]] [[1.538]
 [1.668]
 [1.258]
 [1.668]
 [1.668]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.8046, 0.0022, 0.0268, 0.0958, 0.0707], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9984,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9996,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1138,     0.0002,     0.0148,     0.7302,     0.1411],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2475, 0.0323, 0.0092, 0.3421, 0.3689], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
siam score:  -0.8028592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
line 256 mcts: sample exp_bonus 0.003342807780086332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.82164873820602
printing an ep nov before normalisation:  20.320643071811304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  48.569051462606936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  35.90888747588582
actions average: 
K:  4  action  0 :  tensor([    0.8062,     0.0015,     0.0004,     0.0738,     0.1181],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9998,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0361,     0.9550,     0.0002,     0.0086],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0171,     0.0003,     0.0097,     0.7786,     0.1944],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2313, 0.0047, 0.0029, 0.3498, 0.4112], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  64.71908155674734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.80557084
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([    0.7375,     0.0009,     0.0002,     0.0683,     0.1931],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0028,     0.9926,     0.0000,     0.0001,     0.0045],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9919,     0.0065,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0284,     0.0001,     0.0097,     0.8145,     0.1472],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1952, 0.0081, 0.0161, 0.2585, 0.5220], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.10322778683148
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.74870723468723
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.847421077724366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7980292
actions average: 
K:  4  action  0 :  tensor([    0.9059,     0.0153,     0.0001,     0.0380,     0.0407],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9872,     0.0003,     0.0001,     0.0120],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0058,     0.9765,     0.0048,     0.0129],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0692, 0.0009, 0.0047, 0.6233, 0.3018], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1655, 0.0089, 0.0152, 0.2203, 0.5902], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7982263
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.91792083656028
printing an ep nov before normalisation:  52.50605334712926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7948404
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[58.65 ]
 [56.257]
 [56.257]
 [56.257]
 [56.257]] [[1.453]
 [1.365]
 [1.365]
 [1.365]
 [1.365]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.32268306251431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.525190675644751
printing an ep nov before normalisation:  31.17461228863494
actions average: 
K:  0  action  0 :  tensor([    0.6709,     0.0016,     0.0002,     0.1351,     0.1922],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0012, 0.9709, 0.0026, 0.0167, 0.0087], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9875,     0.0075,     0.0048],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1720, 0.0007, 0.0221, 0.5703, 0.2349], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2246, 0.0044, 0.0577, 0.2549, 0.4584], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8040092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[38.571]
 [40.943]
 [46.958]
 [46.705]
 [44.419]] [[1.229]
 [1.304]
 [1.495]
 [1.488]
 [1.415]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.00642609315492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.29774939263158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.0674577169033
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78356355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.652]
 [32.751]
 [30.684]
 [28.068]
 [29.79 ]] [[1.063]
 [1.072]
 [0.89 ]
 [0.66 ]
 [0.812]]
printing an ep nov before normalisation:  53.13695506090577
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.68766887569299
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.457]
 [36.457]
 [34.69 ]
 [36.774]
 [36.457]] [[1.882]
 [1.882]
 [1.79 ]
 [1.898]
 [1.882]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.297410570702795
siam score:  -0.78222525
siam score:  -0.78113157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.969173431396484
printing an ep nov before normalisation:  56.78257972851203
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.35391022924386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.509697408895416
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.33433495077174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.356]
 [59.356]
 [68.666]
 [59.356]
 [59.356]] [[1.201]
 [1.201]
 [1.48 ]
 [1.201]
 [1.201]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  84.27741969718078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7808802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.30269145965576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.8291,     0.0001,     0.0001,     0.0925,     0.0783],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0044,     0.9933,     0.0000,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1225, 0.0007, 0.0036, 0.6158, 0.2574], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0925, 0.0131, 0.0340, 0.1512, 0.7092], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.570472517174686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.3510252434007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.5576564590191
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.892]
 [55.42 ]
 [66.892]
 [41.442]
 [31.313]] [[0.036]
 [0.537]
 [0.732]
 [0.3  ]
 [0.128]]
printing an ep nov before normalisation:  60.95834748553784
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.739]
 [26.471]
 [24.292]
 [13.225]
 [12.982]] [[0.031]
 [0.139]
 [0.123]
 [0.042]
 [0.04 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  77.82182364716373
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.172]
 [32.388]
 [35.295]
 [35.173]
 [32.268]] [[0.225]
 [0.289]
 [0.333]
 [0.331]
 [0.287]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.14325414052782
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.78031814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.83949525015695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.056]
 [21.452]
 [31.221]
 [19.046]
 [15.305]] [[0.197]
 [0.404]
 [1.   ]
 [0.258]
 [0.029]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.963494300842285
printing an ep nov before normalisation:  25.220643692891542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.14631040382446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.47280996263774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.7959,     0.0005,     0.0004,     0.0832,     0.1201],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0013,     0.9936,     0.0000,     0.0000,     0.0050],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0021,     0.9931,     0.0026,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0676, 0.0007, 0.0013, 0.6797, 0.2507], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0901, 0.0531, 0.0072, 0.3073, 0.5423], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.008]
 [46.008]
 [46.008]
 [46.331]
 [42.861]] [[1.16 ]
 [1.16 ]
 [1.16 ]
 [1.176]
 [0.998]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8534,     0.0035,     0.0001,     0.0287,     0.1143],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0048, 0.8642, 0.0016, 0.0346, 0.0948], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9959,     0.0021,     0.0020],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0720, 0.0010, 0.0221, 0.6786, 0.2263], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1153, 0.0036, 0.0018, 0.3259, 0.5534], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  94.92936020229824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.62592538263038
actions average: 
K:  3  action  0 :  tensor([    0.7817,     0.0010,     0.0005,     0.0873,     0.1296],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9933,     0.0009,     0.0003,     0.0054],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0006,     0.9462,     0.0143,     0.0389],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0629,     0.0004,     0.0008,     0.7455,     0.1904],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2838, 0.0008, 0.0006, 0.3331, 0.3816], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.570428415449676
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[34.087]
 [34.087]
 [90.267]
 [34.087]
 [34.087]] [[0.273]
 [0.273]
 [1.129]
 [0.273]
 [0.273]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.425180148050146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.00521183013916
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.984935343265533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7846295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77890104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.88501585290243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.905]
 [66.905]
 [73.029]
 [66.905]
 [66.905]] [[1.451]
 [1.451]
 [1.667]
 [1.451]
 [1.451]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.10762882232666
actions average: 
K:  4  action  0 :  tensor([    0.8707,     0.0087,     0.0001,     0.0337,     0.0867],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9450,     0.0515,     0.0001,     0.0033],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0091, 0.0316, 0.9292, 0.0193, 0.0108], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0755,     0.0005,     0.0012,     0.7338,     0.1891],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1091, 0.0397, 0.0033, 0.2442, 0.6036], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7849199
printing an ep nov before normalisation:  36.8566308364103
siam score:  -0.78403527
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.7813549
siam score:  -0.78182906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.665]
 [37.665]
 [46.905]
 [37.665]
 [37.665]] [[0.975]
 [0.975]
 [1.377]
 [0.975]
 [0.975]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.90805625915527
line 256 mcts: sample exp_bonus 30.77617303339171
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  87.05958141863098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.358]
 [34.358]
 [34.358]
 [34.358]
 [34.358]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  24.81238603591919
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.951]
 [32.951]
 [32.951]
 [32.951]
 [32.951]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.036]
 [22.396]
 [32.535]
 [15.315]
 [21.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  45.295446642765675
printing an ep nov before normalisation:  40.67510833216572
printing an ep nov before normalisation:  25.890536174672594
printing an ep nov before normalisation:  56.241984526074724
printing an ep nov before normalisation:  26.199593544006348
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.40530590171308
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7689307
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77131456
siam score:  -0.7733204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7695047
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.624441909514378
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76762205
siam score:  -0.7664024
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.36997259737686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.76853925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.21448337921944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8361,     0.0010,     0.0002,     0.0460,     0.1168],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0014, 0.9811, 0.0023, 0.0017, 0.0135], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9990,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0593,     0.0003,     0.0030,     0.6703,     0.2671],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2251, 0.0070, 0.0011, 0.3307, 0.4360], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76153624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.636]
 [26.636]
 [28.14 ]
 [26.636]
 [26.636]] [[1.109]
 [1.109]
 [1.172]
 [1.109]
 [1.109]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.71191905742521
printing an ep nov before normalisation:  37.98624515533447
printing an ep nov before normalisation:  21.579970200294202
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.316263539932994
printing an ep nov before normalisation:  58.53392482898592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.834]
 [38.834]
 [54.25 ]
 [38.834]
 [38.834]] [[0.553]
 [0.553]
 [0.9  ]
 [0.553]
 [0.553]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.5233232776279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8763,     0.0029,     0.0004,     0.0306,     0.0897],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9980,     0.0000,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9991,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0767,     0.0006,     0.0020,     0.7279,     0.1927],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2982, 0.0026, 0.0032, 0.2471, 0.4489], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.3415007724704
printing an ep nov before normalisation:  37.7071036431064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76791024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7600455
actions average: 
K:  1  action  0 :  tensor([0.6285, 0.0011, 0.0006, 0.0943, 0.2754], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9970,     0.0001,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0013,     0.9914,     0.0042,     0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0579, 0.0007, 0.0008, 0.6367, 0.3039], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1939, 0.0020, 0.0012, 0.2694, 0.5335], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.57613314864733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 83.15297099504863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.072]
 [79.81 ]
 [80.645]
 [80.464]
 [81.181]] [[0.926]
 [1.099]
 [1.117]
 [1.113]
 [1.129]]
printing an ep nov before normalisation:  75.36240578680837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78292197
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.3157527220400311
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7688626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.185]
 [48.252]
 [48.252]
 [48.252]
 [48.252]] [[1.522]
 [1.325]
 [1.325]
 [1.325]
 [1.325]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.062]
 [39.869]
 [33.062]
 [33.062]
 [33.062]] [[1.336]
 [1.758]
 [1.336]
 [1.336]
 [1.336]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.861]
 [42.016]
 [38.688]
 [38.37 ]
 [38.688]] [[1.728]
 [1.681]
 [1.498]
 [1.48 ]
 [1.498]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.71296057773321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.932]
 [34.63 ]
 [31.627]
 [29.339]
 [33.932]] [[1.286]
 [1.333]
 [1.129]
 [0.973]
 [1.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.7265682220459
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8139,     0.0009,     0.0001,     0.0361,     0.1489],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0029,     0.9909,     0.0000,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9282,     0.0417,     0.0301],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0818,     0.0004,     0.0134,     0.6796,     0.2247],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1992, 0.0688, 0.0030, 0.2801, 0.4489], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.512939689842696
printing an ep nov before normalisation:  70.44771468699136
siam score:  -0.7736156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7761353
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.54484796818069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.28942396770249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.604251742362976
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.413137223470983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.522919460566044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7707808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.37727044499633
printing an ep nov before normalisation:  39.291720390319824
actions average: 
K:  4  action  0 :  tensor([    0.8141,     0.0019,     0.0001,     0.0383,     0.1456],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0014,     0.9875,     0.0000,     0.0000,     0.0110],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9976,     0.0011,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0660,     0.0004,     0.0053,     0.6700,     0.2584],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1548, 0.0098, 0.0013, 0.4247, 0.4094], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.73864185111992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.12590597798193
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  7.7727168305930014
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.16 ]
 [65.16 ]
 [71.421]
 [65.16 ]
 [68.491]] [[1.638]
 [1.638]
 [1.801]
 [1.638]
 [1.725]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.084500248554725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9341,     0.0015,     0.0001,     0.0250,     0.0394],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0013,     0.9935,     0.0022,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0083,     0.0003,     0.9308,     0.0176,     0.0430],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1224,     0.0005,     0.0006,     0.6839,     0.1925],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2595, 0.0077, 0.0006, 0.3445, 0.3877], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.76964773400513
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  63.566071116063355
printing an ep nov before normalisation:  41.631191596544994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.08985759980253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.973974412864564
actions average: 
K:  0  action  0 :  tensor([    0.7841,     0.0006,     0.0004,     0.0582,     0.1567],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9989,     0.0003,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0043,     0.9939,     0.0011,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1102, 0.0014, 0.0211, 0.6860, 0.1812], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1728, 0.0013, 0.0007, 0.5139, 0.3113], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.96656513214111
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75885713
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7608431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7604462
siam score:  -0.7601097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.14799118041992
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78212893
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7820206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.648312091827393
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[37.914]
 [35.626]
 [55.275]
 [41.978]
 [36.423]] [[0.638]
 [0.566]
 [1.191]
 [0.768]
 [0.591]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0007],
        [0.0010],
        [0.0006],
        [0.0000],
        [0.0156],
        [0.0000],
        [0.0039],
        [0.0000],
        [0.0007],
        [0.0004]], dtype=torch.float64)
0.0 0.000655564515393532
0.0 0.0010095766352516422
0.0 0.0005578753469445095
0.0 0.0
0.0 0.015610219204349195
0.0 0.0
0.0 0.003913051445590306
0.0 0.0
0.0 0.0007187772296768523
0.0 0.0003781346343050583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  1.2272664794380719
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[29.679]
 [29.679]
 [33.214]
 [29.679]
 [29.679]] [[1.67 ]
 [1.67 ]
 [2.002]
 [1.67 ]
 [1.67 ]]
siam score:  -0.77889144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.781899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.439083576202393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.658972084022074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.349562803360996
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.003]
 [0.002]
 [0.001]] [[16.591]
 [15.333]
 [42.251]
 [29.714]
 [17.283]] [[0.112]
 [0.064]
 [1.109]
 [0.621]
 [0.139]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[77.312]
 [69.252]
 [94.057]
 [93.107]
 [88.925]] [[1.322]
 [1.154]
 [1.674]
 [1.654]
 [1.567]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[39.134]
 [53.871]
 [36.724]
 [52.963]
 [46.369]] [[0.357]
 [0.492]
 [0.335]
 [0.484]
 [0.423]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[20.104]
 [29.305]
 [26.902]
 [23.293]
 [17.484]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
actions average: 
K:  3  action  0 :  tensor([    0.9036,     0.0013,     0.0001,     0.0207,     0.0742],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0027,     0.9800,     0.0006,     0.0005,     0.0162],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9842,     0.0081,     0.0076],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0393,     0.0002,     0.0015,     0.7420,     0.2171],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2251, 0.0014, 0.0034, 0.2937, 0.4763], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.27324690302783
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.548]
 [28.436]
 [47.072]
 [34.801]
 [30.643]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 54.670267596695496
actions average: 
K:  3  action  0 :  tensor([    0.8502,     0.0128,     0.0003,     0.0562,     0.0806],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9698,     0.0062,     0.0019,     0.0212],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0028,     0.9591,     0.0173,     0.0208],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0745, 0.0240, 0.0183, 0.7589, 0.1242], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0991, 0.0680, 0.0328, 0.2854, 0.5146], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.651966075001546
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.733336296221204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7699487
siam score:  -0.7681366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7791,     0.0003,     0.0003,     0.0778,     0.1425],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9987,     0.0001,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0006,     0.9813,     0.0094,     0.0086],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0409,     0.0005,     0.0002,     0.7258,     0.2325],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0115, 0.0053, 0.0032, 0.1499, 0.8301], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76388776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.14372195802473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.271714576529845
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.99489381463298
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.495718175538855
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[55.896]
 [55.896]
 [55.896]
 [62.363]
 [55.896]] [[1.295]
 [1.295]
 [1.295]
 [1.55 ]
 [1.295]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.50617785846248
siam score:  -0.77283144
printing an ep nov before normalisation:  50.98472732299711
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.001]] [[43.05 ]
 [68.012]
 [84.284]
 [81.679]
 [60.   ]] [[0.549]
 [1.007]
 [1.305]
 [1.257]
 [0.86 ]]
printing an ep nov before normalisation:  88.06385594002346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7801153
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.86320406471503
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[52.574]
 [52.148]
 [52.148]
 [52.148]
 [52.148]] [[1.334]
 [1.319]
 [1.319]
 [1.319]
 [1.319]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.539]
 [39.539]
 [39.539]
 [39.539]
 [39.539]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  27.660675048828125
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.761555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.71795585768585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.171207427978516
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7656605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76226544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.986416218070252
printing an ep nov before normalisation:  55.28880023649159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.407307698719926
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.347]
 [28.347]
 [28.347]
 [28.347]
 [28.347]] [[1.66]
 [1.66]
 [1.66]
 [1.66]
 [1.66]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8558,     0.0005,     0.0001,     0.0679,     0.0756],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9987,     0.0005,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9953,     0.0045,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1013,     0.0003,     0.0017,     0.7047,     0.1920],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2624, 0.0110, 0.0019, 0.2337, 0.4910], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.65089931902752
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.352]
 [65.352]
 [77.337]
 [65.352]
 [65.352]] [[0.254]
 [0.254]
 [0.331]
 [0.254]
 [0.254]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.524308148263124
printing an ep nov before normalisation:  34.37727966939742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0005],
        [0.0004],
        [0.0005],
        [0.0000],
        [0.0005],
        [0.0000],
        [0.0000],
        [0.0007],
        [0.0000],
        [0.0003]], dtype=torch.float64)
0.0 0.00045908494565314277
0.0 0.0004466476190409667
0.0 0.00045908494565314277
0.0 0.0
0.0 0.00048354248709080367
0.0 0.0
0.0 0.0
0.0 0.0007151507934499707
0.0 0.0
0.0 0.0003198094106328685
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.981]
 [28.925]
 [35.428]
 [35.481]
 [34.048]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.73989715668352
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8295,     0.0003,     0.0001,     0.0613,     0.1088],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9732,     0.0159,     0.0005,     0.0093],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9820,     0.0130,     0.0049],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1219,     0.0003,     0.0006,     0.7100,     0.1672],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1334, 0.0007, 0.0593, 0.2878, 0.5188], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.636401721197835
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8141,     0.0002,     0.0001,     0.0678,     0.1178],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9937,     0.0012,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9975,     0.0013,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0901,     0.0004,     0.0014,     0.6894,     0.2188],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2301, 0.0006, 0.0441, 0.2513, 0.4738], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.477478680875365
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  39.75834369659424
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[39.607]
 [38.803]
 [45.739]
 [39.585]
 [35.957]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76439047
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.72575032242864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.516895464291583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.15671666463216
printing an ep nov before normalisation:  45.67565227020609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.22963047027588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7906, 0.0211, 0.0015, 0.0723, 0.1145], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9914,     0.0036,     0.0001,     0.0047],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0096,     0.9660,     0.0112,     0.0131],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0307, 0.0029, 0.0261, 0.6796, 0.2607], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1496, 0.0118, 0.0068, 0.2923, 0.5396], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 26.304575079775347
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]] [[25.006]
 [28.397]
 [33.56 ]
 [35.357]
 [26.133]] [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.946537307055287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.150787459978666
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.100043608556554
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[28.241]
 [27.928]
 [46.77 ]
 [36.747]
 [35.336]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
siam score:  -0.76559263
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7746321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.2646451661162
siam score:  -0.77604795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7775,     0.0006,     0.0002,     0.0369,     0.1848],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0102,     0.9766,     0.0005,     0.0002,     0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0021,     0.9960,     0.0006,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0886,     0.0001,     0.0002,     0.7771,     0.1341],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1665, 0.0018, 0.0347, 0.2143, 0.5827], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.24363127072776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.91682985093858
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.7803,     0.0004,     0.0004,     0.1068,     0.1121],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0221,     0.9548,     0.0001,     0.0002,     0.0228],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0180,     0.9619,     0.0068,     0.0133],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0997,     0.0002,     0.0006,     0.7280,     0.1714],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2541, 0.0151, 0.0297, 0.2002, 0.5008], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7836606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.39336782037233
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7708986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.35933349178073
printing an ep nov before normalisation:  77.56772377562616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.60120312597447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7663221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7849,     0.0012,     0.0002,     0.0955,     0.1182],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9978,     0.0002,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0036,     0.9939,     0.0002,     0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1327,     0.0003,     0.0255,     0.6380,     0.2034],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1051, 0.0466, 0.0075, 0.2988, 0.5420], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7921,     0.0011,     0.0002,     0.0599,     0.1467],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0067,     0.9617,     0.0003,     0.0276,     0.0037],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9901,     0.0092,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1142,     0.0003,     0.0006,     0.7318,     0.1530],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2416, 0.0206, 0.0626, 0.1768, 0.4985], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  96.02824867237403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8233,     0.0014,     0.0002,     0.0570,     0.1180],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0010, 0.0053, 0.9195, 0.0022, 0.0721], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0514,     0.0003,     0.0017,     0.7126,     0.2340],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1465, 0.0023, 0.0052, 0.3753, 0.4707], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 63.13131620333225
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.458 0.208 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7743509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.23095854227458
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.903]
 [59.903]
 [59.903]
 [59.903]
 [59.903]] [[79.851]
 [79.851]
 [79.851]
 [79.851]
 [79.851]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.583 0.208 0.083 0.042]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7620983
siam score:  -0.7613305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7961,     0.0018,     0.0004,     0.0742,     0.1276],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0033,     0.9830,     0.0019,     0.0002,     0.0116],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0004,     0.9931,     0.0015,     0.0050],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0207,     0.0006,     0.0583,     0.7802,     0.1401],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2179, 0.0532, 0.0203, 0.3772, 0.3313], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  21.87982888624444
printing an ep nov before normalisation:  54.74570261226694
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.25  0.083 0.5  ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77102447
siam score:  -0.7718222
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7748996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.375 0.208 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.26377573041735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.9446681467414
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.01533278895267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
line 256 mcts: sample exp_bonus 47.39386930565786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.47345397789526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8736,     0.0039,     0.0006,     0.0640,     0.0579],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9805,     0.0001,     0.0001,     0.0189],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0010,     0.9872,     0.0004,     0.0114],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0550, 0.0015, 0.0117, 0.8106, 0.1213], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0918, 0.0709, 0.0165, 0.3163, 0.5045], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.07644626072475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00047203056567468593
printing an ep nov before normalisation:  48.338299035636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.2231725299788
printing an ep nov before normalisation:  34.3724872353184
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.431]
 [52.431]
 [52.431]
 [52.431]
 [52.431]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[57.943]
 [62.433]
 [62.548]
 [57.943]
 [48.863]] [[0.991]
 [1.153]
 [1.157]
 [0.991]
 [0.663]]
printing an ep nov before normalisation:  56.06660653194453
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.988850919424188
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  18.70410442352295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
siam score:  -0.74835837
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.8357,     0.0099,     0.0003,     0.0463,     0.1078],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9990,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0025, 0.0132, 0.9695, 0.0020, 0.0129], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0435, 0.0018, 0.1064, 0.6920, 0.1562], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1197, 0.0177, 0.0200, 0.3071, 0.5354], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  39.28126500859202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[34.448]
 [34.448]
 [40.121]
 [34.448]
 [34.448]] [[1.2  ]
 [1.2  ]
 [1.612]
 [1.2  ]
 [1.2  ]]
siam score:  -0.74056226
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  64.21900345103096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.002]]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  2  action  0 :  tensor([    0.8823,     0.0006,     0.0003,     0.0015,     0.1153],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0032,     0.9865,     0.0003,     0.0002,     0.0098],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0004,     0.9977,     0.0009,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0393,     0.0005,     0.0034,     0.7719,     0.1849],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1440, 0.0066, 0.0891, 0.3151, 0.4451], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.38168597633049
siam score:  -0.75377953
printing an ep nov before normalisation:  5.992375300851194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
line 256 mcts: sample exp_bonus 45.952546161385975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  85.66332583457972
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
siam score:  -0.75754255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  25.770074129104614
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[75.152]
 [75.152]
 [70.361]
 [74.752]
 [75.152]] [[1.841]
 [1.841]
 [1.619]
 [1.822]
 [1.841]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  14.415855565745971
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  36.56340952504563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7702417
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
siam score:  -0.768936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  4  action  0 :  tensor([    0.9194,     0.0041,     0.0002,     0.0055,     0.0708],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9929,     0.0045,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0093,     0.9599,     0.0007,     0.0302],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0542, 0.0048, 0.1609, 0.6219, 0.1582], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1639, 0.0209, 0.0721, 0.1693, 0.5739], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  40.790813406381034
printing an ep nov before normalisation:  43.50861564061705
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.007]
 [0.005]
 [0.005]] [[39.003]
 [39.003]
 [46.472]
 [39.003]
 [39.003]] [[1.243]
 [1.243]
 [1.674]
 [1.243]
 [1.243]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  50.28877430723988
printing an ep nov before normalisation:  3.4851552671523223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  25.980538805869458
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[36.821]
 [36.821]
 [36.821]
 [36.821]
 [36.821]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
siam score:  -0.7691484
printing an ep nov before normalisation:  26.763147777981228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  3  action  0 :  tensor([    0.8717,     0.0007,     0.0004,     0.0713,     0.0559],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0015,     0.9535,     0.0429,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0016,     0.9633,     0.0040,     0.0311],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0577,     0.0004,     0.0017,     0.8393,     0.1009],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0919, 0.0714, 0.0073, 0.3490, 0.4805], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[27.024]
 [33.809]
 [33.809]
 [33.809]
 [33.809]] [[2.002]
 [2.958]
 [2.958]
 [2.958]
 [2.958]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  1
siam score:  -0.7731254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.8248,     0.0003,     0.0001,     0.0495,     0.1254],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9976,     0.0003,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0002,     0.9695,     0.0016,     0.0282],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1672,     0.0003,     0.0005,     0.6147,     0.2174],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0944, 0.0009, 0.0333, 0.2835, 0.5879], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.7936,     0.0029,     0.0001,     0.0557,     0.1477],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9924,     0.0005,     0.0000,     0.0069],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0010,     0.9920,     0.0058,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0468,     0.0004,     0.0230,     0.7582,     0.1716],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0030, 0.0097, 0.0226, 0.0880, 0.8768], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  63.72078682230216
siam score:  -0.770835
siam score:  -0.7681288
printing an ep nov before normalisation:  65.96139018550463
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.002]
 [0.003]] [[52.988]
 [52.988]
 [63.232]
 [58.963]
 [52.988]] [[1.285]
 [1.285]
 [1.532]
 [1.428]
 [1.285]]
printing an ep nov before normalisation:  33.65166294992327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  33.72483302388325
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[52.607]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.185]
 [-0.789]
 [-0.789]
 [-0.789]
 [-0.789]]
printing an ep nov before normalisation:  21.729820115225657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.007]
 [0.009]
 [0.015]
 [0.007]] [[28.323]
 [32.433]
 [39.109]
 [35.467]
 [34.526]] [[0.856]
 [1.117]
 [1.556]
 [1.324]
 [1.254]]
siam score:  -0.76236343
printing an ep nov before normalisation:  83.4224194378672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  46.94010762313382
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]]
using another actor
printing an ep nov before normalisation:  85.80324097141968
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  33.994145321781296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.031]
 [0.007]
 [0.007]] [[60.42 ]
 [60.42 ]
 [77.761]
 [60.42 ]
 [60.42 ]] [[1.185]
 [1.185]
 [1.652]
 [1.185]
 [1.185]]
printing an ep nov before normalisation:  84.11198314020197
UNIT TEST: sample policy line 217 mcts : [0.292 0.583 0.042 0.042 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  73.94205349018942
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[44.9]
 [ 0. ]
 [ 0. ]
 [ 0. ]
 [ 0. ]] [[ 0.707]
 [-0.219]
 [-0.219]
 [-0.219]
 [-0.219]]
printing an ep nov before normalisation:  25.92200040817261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  3  action  0 :  tensor([    0.9281,     0.0098,     0.0002,     0.0008,     0.0611],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9990,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0054,     0.9902,     0.0013,     0.0032],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0550,     0.0006,     0.0083,     0.7371,     0.1991],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1821, 0.0416, 0.0020, 0.2000, 0.5743], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 48.62317860772387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  73.40576077054124
printing an ep nov before normalisation:  42.51233819891795
printing an ep nov before normalisation:  43.27059981949194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  49.83683149295678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  92.42746001130926
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.016]
 [0.011]
 [0.001]
 [0.006]] [[42.688]
 [39.045]
 [62.48 ]
 [55.166]
 [53.593]] [[0.959]
 [0.887]
 [1.405]
 [1.232]
 [1.202]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  74.91795615927484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  2  action  0 :  tensor([    0.8255,     0.0010,     0.0006,     0.0648,     0.1081],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9930,     0.0003,     0.0000,     0.0059],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0031,     0.9895,     0.0047,     0.0027],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0379,     0.0003,     0.0039,     0.7293,     0.2287],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1842, 0.0029, 0.0866, 0.2910, 0.4354], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  49.767865881812206
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.60511527877969
printing an ep nov before normalisation:  28.07337522506714
printing an ep nov before normalisation:  23.50494704472724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  73.86965081033918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.005]
 [0.005]
 [0.005]] [[32.102]
 [51.312]
 [32.102]
 [32.102]
 [32.102]] [[0.656]
 [1.342]
 [0.656]
 [0.656]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using another actor
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  25.81333415290353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  49.516510260217814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
siam score:  -0.78622246
printing an ep nov before normalisation:  18.688437295073633
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  68.6494425077891
printing an ep nov before normalisation:  77.67392176414047
using another actor
printing an ep nov before normalisation:  67.02791194505846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78513473
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[53.13]
 [53.13]
 [55.74]
 [53.13]
 [53.13]] [[1.466]
 [1.466]
 [1.603]
 [1.466]
 [1.466]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  38.6179872718285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  73.38180508265258
printing an ep nov before normalisation:  43.913764531801064
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[74.778]
 [74.778]
 [74.778]
 [74.778]
 [74.778]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  44.30183637737951
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 68.91677262775463
line 256 mcts: sample exp_bonus 21.411606550216675
actions average: 
K:  1  action  0 :  tensor([    0.8295,     0.0004,     0.0002,     0.0634,     0.1065],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9941,     0.0002,     0.0000,     0.0041],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9688,     0.0079,     0.0229],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1425, 0.0010, 0.0460, 0.6396, 0.1709], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1705, 0.0010, 0.0586, 0.2998, 0.4701], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  27.42388929639544
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[20.277]
 [19.147]
 [18.697]
 [19.147]
 [19.147]] [[1.167]
 [1.049]
 [1.004]
 [1.049]
 [1.049]]
printing an ep nov before normalisation:  31.469125259945255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[60.028]
 [60.028]
 [60.028]
 [60.028]
 [60.028]] [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
actions average: 
K:  0  action  0 :  tensor([0.8083, 0.0135, 0.0012, 0.1194, 0.0576], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9940,     0.0005,     0.0000,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9975,     0.0004,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0780, 0.0012, 0.0046, 0.6808, 0.2354], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1787, 0.0014, 0.0262, 0.2671, 0.5266], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.95439043807898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  38.199786984770014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.35900578139528
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.808657
printing an ep nov before normalisation:  38.702079314502924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
printing an ep nov before normalisation:  40.433440481172916
actions average: 
K:  0  action  0 :  tensor([    0.7995,     0.0003,     0.0001,     0.0875,     0.1125],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9993,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0036,     0.9639,     0.0063,     0.0261],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0219,     0.0003,     0.0016,     0.7305,     0.2457],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1064, 0.0182, 0.0622, 0.1029, 0.7103], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
from probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
printing an ep nov before normalisation:  52.85965521508588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
siam score:  -0.8126516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
printing an ep nov before normalisation:  46.44449348775307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[42.759]
 [42.759]
 [42.759]
 [42.759]
 [42.759]] [[1.018]
 [1.018]
 [1.018]
 [1.018]
 [1.018]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
from probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.06671943976210959, 0.2666138935712237, 0.466508347380338]
actions average: 
K:  1  action  0 :  tensor([    0.7876,     0.0038,     0.0001,     0.0424,     0.1660],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9975,     0.0001,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0007,     0.9974,     0.0011,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0366,     0.0001,     0.0006,     0.8071,     0.1555],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0484, 0.0448, 0.1050, 0.1560, 0.6458], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8276448
printing an ep nov before normalisation:  60.9573480304062
from probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
using another actor
printing an ep nov before normalisation:  39.2953824564088
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
line 256 mcts: sample exp_bonus 59.6177206849494
printing an ep nov before normalisation:  44.221521747829286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
printing an ep nov before normalisation:  26.563573672637055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.0714753518132806, 0.21426232409335966, 0.4998362686535179]
printing an ep nov before normalisation:  31.486434936523438
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  44.779679356272425
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  49.61520409098479
printing an ep nov before normalisation:  26.49670320510859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
using another actor
from probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[59.449]
 [59.449]
 [59.449]
 [59.449]
 [59.449]] [[1.688]
 [1.688]
 [1.688]
 [1.688]
 [1.688]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  55.98170103588192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  42.99683147969184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  101.63515128925766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
printing an ep nov before normalisation:  43.89983867180395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.006]
 [0.006]]
printing an ep nov before normalisation:  60.498361587524414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.07411767542583843, 0.1851764649148323, 0.518352833381814]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  14.063217639923096
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.0 0.0161
printing an ep nov before normalisation:  22.566952512092154
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
printing an ep nov before normalisation:  40.28506949157031
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.99360275268555
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.028]
 [0.023]
 [0.026]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.028]
 [0.023]
 [0.026]
 [0.026]]
printing an ep nov before normalisation:  31.292270802982284
printing an ep nov before normalisation:  20.222413539886475
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.04 ]
 [0.012]
 [0.021]
 [0.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.023]
 [0.04 ]
 [0.012]
 [0.021]
 [0.092]]
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
using another actor
printing an ep nov before normalisation:  84.3213091199388
printing an ep nov before normalisation:  19.059328251774588
siam score:  -0.8576868
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
printing an ep nov before normalisation:  37.36815929412842
actions average: 
K:  2  action  0 :  tensor([    0.7811,     0.0007,     0.0002,     0.0704,     0.1476],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0068,     0.9907,     0.0003,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9837,     0.0081,     0.0079],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0207,     0.0001,     0.0019,     0.8872,     0.0901],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1075, 0.0362, 0.0015, 0.2708, 0.5840], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 50.11738307982165
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.0741220601663335, 0.1851755879667333, 0.5183361713679328]
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  31.208593898971728
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.311]
 [0.195]
 [0.195]] [[40.196]
 [40.196]
 [45.005]
 [40.196]
 [40.196]] [[1.306]
 [1.306]
 [1.667]
 [1.306]
 [1.306]]
line 256 mcts: sample exp_bonus 58.72246693316593
maxi score, test score, baseline:  0.0161 0.4 0.4
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.059]
 [0.   ]
 [0.029]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.029]
 [0.059]
 [0.   ]
 [0.029]
 [0.031]]
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[62.378]
 [62.378]
 [62.378]
 [62.378]
 [62.378]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [0.676]]
printing an ep nov before normalisation:  43.7723663716981
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.042]
 [0.042]
 [0.042]
 [0.042]] [[44.994]
 [26.722]
 [26.722]
 [26.722]
 [26.722]] [[1.428]
 [0.624]
 [0.624]
 [0.624]
 [0.624]]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
printing an ep nov before normalisation:  41.556543774074974
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[25.429]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.383]
 [-0.835]
 [-0.835]
 [-0.835]
 [-0.835]]
printing an ep nov before normalisation:  25.78038454055786
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.4 0.4
siam score:  -0.899408
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.10823009870184
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  49.31439170784577
printing an ep nov before normalisation:  49.703388576454316
printing an ep nov before normalisation:  56.8061463039867
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
printing an ep nov before normalisation:  27.676898691502608
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.07580273237946394, 0.16666666666666669, 0.5301224038154776]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06415828660659674, 0.06415828660659674, 0.06415828660659674, 0.06415828660659674, 0.29480214174175406, 0.44856471183185886]
printing an ep nov before normalisation:  24.700548453227853
siam score:  -0.8953435
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06415828660659674, 0.06415828660659674, 0.06415828660659674, 0.06415828660659674, 0.29480214174175406, 0.44856471183185886]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.9148,     0.0233,     0.0001,     0.0001,     0.0617],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0011, 0.0265, 0.9567, 0.0011, 0.0146], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0448, 0.0026, 0.0412, 0.8068, 0.1046], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1469, 0.0099, 0.0171, 0.2268, 0.5994], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.24505807496831677, 0.48023229987326727]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.46 ]
 [0.212]
 [0.417]] [[26.795]
 [26.795]
 [25.669]
 [32.121]
 [26.795]] [[1.935]
 [1.935]
 [1.894]
 [2.131]
 [1.935]]
using another actor
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.24505807496831677, 0.48023229987326727]
printing an ep nov before normalisation:  50.886499056384544
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.11227864723672
printing an ep nov before normalisation:  27.478215579995368
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.24505807496831677, 0.48023229987326727]
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.8378,     0.0024,     0.0001,     0.0797,     0.0800],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9909,     0.0022,     0.0002,     0.0049],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0028,     0.9648,     0.0001,     0.0322],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0209,     0.0006,     0.0485,     0.7452,     0.1849],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2011, 0.0161, 0.0090, 0.2059, 0.5678], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.24505807496831677, 0.48023229987326727]
printing an ep nov before normalisation:  11.65157918722457
using explorer policy with actor:  1
siam score:  -0.8928459
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.06867740628960399, 0.24505807496831677, 0.48023229987326727]
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [1.214]
 [0.945]
 [0.945]] [[19.563]
 [19.563]
 [19.409]
 [19.563]
 [19.563]] [[2.164]
 [2.164]
 [2.417]
 [2.164]
 [2.164]]
printing an ep nov before normalisation:  16.769941483425473
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.2280395966454141, 0.4910664394114745]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.2280395966454141, 0.4910664394114745]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.2280395966454141, 0.4910664394114745]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.07022349098577783, 0.2280395966454141, 0.4910664394114745]
printing an ep nov before normalisation:  50.98684804388535
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.565275056021555
maxi score, test score, baseline:  0.0161 0.4 0.4
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.029]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[34.061]
 [45.755]
 [34.061]
 [34.061]
 [34.061]] [[0.03 ]
 [0.029]
 [0.03 ]
 [0.03 ]
 [0.03 ]]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0714751095231749, 0.0714751095231749, 0.0714751095231749, 0.0714751095231749, 0.21426244523841256, 0.49983711666888775]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  25.71313648088271
using another actor
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.20288112579647669, 0.5070825824868808]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.20288112579647669, 0.5070825824868808]
printing an ep nov before normalisation:  29.010009765625
using explorer policy with actor:  1
from probs:  [0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.20288112579647669, 0.5070825824868808]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.20288112579647669, 0.5070825824868808]
printing an ep nov before normalisation:  49.35301264090391
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.07250907292916065, 0.20288112579647669, 0.5070825824868808]
siam score:  -0.9051146
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.079]
 [0.032]
 [0.032]
 [0.032]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.032]
 [0.079]
 [0.032]
 [0.032]
 [0.032]]
printing an ep nov before normalisation:  42.229720556043105
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06671730767522234, 0.10669705127180007, 0.10669705127180007, 0.06671730767522234, 0.18665653846495553, 0.46651474364099965]
printing an ep nov before normalisation:  15.987724136318057
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06415565954727721, 0.10259728721704826, 0.1410389148868193, 0.06415565954727721, 0.17948054255659035, 0.44857193624498765]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06415565954727721, 0.10259728721704826, 0.1410389148868193, 0.06415565954727721, 0.17948054255659035, 0.44857193624498765]
printing an ep nov before normalisation:  70.03327173784908
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  0.7633427855295594
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0617838146427978, 0.09880129182769269, 0.1358187690125876, 0.0617838146427978, 0.20985372338237737, 0.4319585864917468]
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  84.61267151789522
printing an ep nov before normalisation:  86.1332567795236
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0617838146427978, 0.09880129182769269, 0.1358187690125876, 0.0617838146427978, 0.20985372338237737, 0.4319585864917468]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0617838146427978, 0.09880129182769269, 0.1358187690125876, 0.0617838146427978, 0.20985372338237737, 0.4319585864917468]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.017]
 [0.013]
 [0.017]
 [0.017]] [[32.986]
 [31.934]
 [37.985]
 [31.934]
 [31.934]] [[0.43 ]
 [0.386]
 [0.526]
 [0.386]
 [0.386]]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0617838146427978, 0.09880129182769269, 0.1358187690125876, 0.0617838146427978, 0.20985372338237737, 0.4319585864917468]
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.048]
 [0.396]
 [0.048]
 [0.048]] [[32.461]
 [32.461]
 [53.659]
 [32.461]
 [32.461]] [[0.325]
 [0.325]
 [0.936]
 [0.325]
 [0.325]]
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  28.648645537240167
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.0617838146427978, 0.09880129182769269, 0.1358187690125876, 0.0617838146427978, 0.20985372338237737, 0.4319585864917468]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.214070694222297
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]] [[29.016]
 [29.016]
 [29.016]
 [29.016]
 [29.016]] [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.024]]
printing an ep nov before normalisation:  14.347629912979487
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0161 0.4 0.4
actions average: 
K:  1  action  0 :  tensor([    0.8593,     0.0168,     0.0003,     0.0475,     0.0761],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0238,     0.9438,     0.0136,     0.0001,     0.0187],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0004,     0.9679,     0.0128,     0.0189],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0114,     0.0002,     0.0067,     0.7891,     0.1927],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2075, 0.0179, 0.0012, 0.1689, 0.6044], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.624]
 [1.053]
 [0.22 ]
 [0.164]
 [0.624]] [[33.848]
 [34.776]
 [34.522]
 [35.54 ]
 [33.848]] [[2.066]
 [2.564]
 [1.712]
 [1.732]
 [2.066]]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06116690876084795, 0.1277983348066282, 0.1277983348066282, 0.06116690876084795, 0.1944297608524084, 0.42763975201263915]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06116690876084795, 0.1277983348066282, 0.1277983348066282, 0.06116690876084795, 0.1944297608524084, 0.42763975201263915]
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06116690876084795, 0.1277983348066282, 0.1277983348066282, 0.06116690876084795, 0.1944297608524084, 0.42763975201263915]
from probs:  [0.06116690876084795, 0.1277983348066282, 0.1277983348066282, 0.06116690876084795, 0.1944297608524084, 0.42763975201263915]
printing an ep nov before normalisation:  44.72480773925781
actions average: 
K:  1  action  0 :  tensor([    0.9491,     0.0035,     0.0000,     0.0030,     0.0443],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0034,     0.9950,     0.0007,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0056,     0.9534,     0.0027,     0.0382],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0319,     0.0001,     0.0037,     0.8631,     0.1012],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1946, 0.0245, 0.0205, 0.2030, 0.5574], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.01832560029344
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06116690876084795, 0.1277983348066282, 0.1277983348066282, 0.06116690876084795, 0.1944297608524084, 0.42763975201263915]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  23.613516566092503
using another actor
siam score:  -0.9166916
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]]
printing an ep nov before normalisation:  34.04815494649442
maxi score, test score, baseline:  0.0161 0.4 0.4
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
from probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  30.24976081158799
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  18.51870062959071
printing an ep nov before normalisation:  51.67810120386787
using another actor
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.0161 0.4 0.4
printing an ep nov before normalisation:  37.69713318997191
printing an ep nov before normalisation:  19.685737714190754
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[49.829]
 [49.829]
 [49.829]
 [49.829]
 [49.829]] [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]]
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.078]
 [0.048]
 [0.078]
 [0.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.078]
 [0.078]
 [0.048]
 [0.078]
 [0.078]]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.0161 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  35.011231899261475
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
Printing some Q and Qe and total Qs values:  [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]] [[50.332]
 [50.332]
 [50.332]
 [50.332]
 [50.332]] [[2.713]
 [2.713]
 [2.713]
 [2.713]
 [2.713]]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
from probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  29.954957962036133
using explorer policy with actor:  1
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.067]
 [0.067]
 [0.067]
 [0.067]] [[32.323]
 [32.323]
 [32.323]
 [32.323]
 [32.323]] [[0.738]
 [0.738]
 [0.738]
 [0.738]
 [0.738]]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
using another actor
from probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.068]
 [0.056]
 [0.068]
 [0.068]] [[23.547]
 [23.547]
 [41.881]
 [23.547]
 [23.547]] [[0.427]
 [0.427]
 [0.947]
 [0.427]
 [0.427]]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  50.67732977934208
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  35.262125916153984
printing an ep nov before normalisation:  42.67645837723571
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
printing an ep nov before normalisation:  44.171109370400124
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06255422522991497, 0.12502169009196598, 0.12502169009196598, 0.06255422522991497, 0.187489154954017, 0.437359014402221]
printing an ep nov before normalisation:  78.96599736400111
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.06377834654876645, 0.12257167233042372, 0.12257167233042372, 0.06377834654876645, 0.18136499811208096, 0.44593496412953876]
printing an ep nov before normalisation:  47.67256500975802
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06377834654876645, 0.12257167233042372, 0.12257167233042372, 0.06377834654876645, 0.18136499811208096, 0.44593496412953876]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
using explorer policy with actor:  1
printing an ep nov before normalisation:  11.999143691809172
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06377834654876645, 0.12257167233042372, 0.12257167233042372, 0.06377834654876645, 0.18136499811208096, 0.44593496412953876]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0648664689891919, 0.12039384954054179, 0.12039384954054179, 0.0648664689891919, 0.17592123009189162, 0.4535581328486411]
printing an ep nov before normalisation:  63.32307243843017
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.0648664689891919, 0.12039384954054179, 0.12039384954054179, 0.0648664689891919, 0.17592123009189162, 0.4535581328486411]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.0648664689891919, 0.12039384954054179, 0.12039384954054179, 0.0648664689891919, 0.17592123009189162, 0.4535581328486411]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 30.604328513145447
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.0658400638743835, 0.11844524793992255, 0.11844524793992255, 0.0658400638743835, 0.17105043200546155, 0.46037894436592636]
printing an ep nov before normalisation:  31.055467178524943
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.0658400638743835, 0.11844524793992255, 0.11844524793992255, 0.0658400638743835, 0.17105043200546155, 0.46037894436592636]
using another actor
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.0658400638743835, 0.11844524793992255, 0.11844524793992255, 0.0658400638743835, 0.17105043200546155, 0.46037894436592636]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.517]
 [0.667]
 [0.517]
 [0.357]] [[60.929]
 [60.929]
 [52.643]
 [60.929]
 [64.259]] [[0.517]
 [0.517]
 [0.667]
 [0.517]
 [0.357]]
printing an ep nov before normalisation:  34.46722200428015
printing an ep nov before normalisation:  22.916083383764438
siam score:  -0.9271802
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06415478383091464, 0.11541072524879063, 0.11541072524879063, 0.06415478383091464, 0.19229463737560468, 0.44857434446498484]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([    0.8478,     0.0171,     0.0000,     0.0297,     0.1054],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9886,     0.0029,     0.0002,     0.0066],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0001,     0.9407,     0.0360,     0.0227],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0031,     0.0001,     0.0019,     0.8747,     0.1202],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0588,     0.0004,     0.0027,     0.0786,     0.8595],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.23008388943142
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
printing an ep nov before normalisation:  37.347989082336426
printing an ep nov before normalisation:  39.487209667424594
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.138]
 [0.009]
 [0.109]
 [0.111]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.101]
 [0.138]
 [0.009]
 [0.109]
 [0.111]]
printing an ep nov before normalisation:  55.025673688976475
printing an ep nov before normalisation:  33.629817962646484
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
Printing some Q and Qe and total Qs values:  [[1.199]
 [1.311]
 [1.193]
 [1.199]
 [1.199]] [[20.77 ]
 [17.123]
 [21.219]
 [20.77 ]
 [20.77 ]] [[2.632]
 [2.324]
 [2.678]
 [2.632]
 [2.632]]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
maxi score, test score, baseline:  0.018099999999999998 0.4 0.4
using another actor
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
maxi score, test score, baseline:  0.0201 0.4 0.4
probs:  [0.06509185072023316, 0.11384776237452125, 0.11384776237452125, 0.06509185072023316, 0.18698162985595335, 0.4551391439545379]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[58.616]
 [58.616]
 [58.616]
 [58.616]
 [58.616]] [[1.832]
 [1.832]
 [1.832]
 [1.832]
 [1.832]]
printing an ep nov before normalisation:  71.84799508995933
printing an ep nov before normalisation:  77.2426775096414
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06354477023912647, 0.1349368523812697, 0.11113949166722194, 0.06354477023912647, 0.18253157380936516, 0.4443025416638904]
siam score:  -0.9294298
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.062069667283401554, 0.13180100020557828, 0.13180100020557828, 0.062069667283401554, 0.17828855548702943, 0.4339701095350108]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.062069667283401554, 0.13180100020557828, 0.13180100020557828, 0.062069667283401554, 0.17828855548702943, 0.4339701095350108]
printing an ep nov before normalisation:  47.645941092393095
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.712]
 [0.388]
 [0.388]] [[69.353]
 [69.353]
 [83.215]
 [69.353]
 [69.353]] [[1.67 ]
 [1.67 ]
 [2.332]
 [1.67 ]
 [1.67 ]]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
printing an ep nov before normalisation:  28.793654234756396
printing an ep nov before normalisation:  12.897351080680146
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
from probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
from probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
printing an ep nov before normalisation:  35.440806384846155
using explorer policy with actor:  0
printing an ep nov before normalisation:  84.74911663880724
printing an ep nov before normalisation:  56.58859957177334
Printing some Q and Qe and total Qs values:  [[0.669]
 [0.669]
 [0.504]
 [0.669]
 [0.669]] [[75.634]
 [75.634]
 [79.411]
 [75.634]
 [75.634]] [[0.669]
 [0.669]
 [0.504]
 [0.669]
 [0.669]]
printing an ep nov before normalisation:  65.22573058406238
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
printing an ep nov before normalisation:  28.793593720021388
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.06066163294192236, 0.15152309042027462, 0.12880772605068655, 0.06066163294192236, 0.17423845478986266, 0.42410746285533135]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.144]
 [0.17 ]
 [0.113]
 [0.109]] [[26.728]
 [26.456]
 [22.95 ]
 [34.776]
 [23.999]] [[0.295]
 [0.42 ]
 [0.375]
 [0.555]
 [0.336]]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.052]
 [0.052]
 [0.053]
 [0.052]] [[41.065]
 [30.497]
 [26.706]
 [30.368]
 [30.497]] [[0.055]
 [0.052]
 [0.052]
 [0.053]
 [0.052]]
siam score:  -0.92377067
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
printing an ep nov before normalisation:  31.28727817206307
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
printing an ep nov before normalisation:  27.706539740941132
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
printing an ep nov before normalisation:  0.20610263940312734
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.432]
 [0.898]
 [0.432]
 [0.432]] [[68.445]
 [68.445]
 [62.314]
 [68.445]
 [68.445]] [[2.267]
 [2.267]
 [2.495]
 [2.267]
 [2.267]]
printing an ep nov before normalisation:  64.94397101264211
printing an ep nov before normalisation:  45.585312843322754
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
using another actor
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
printing an ep nov before normalisation:  22.355241775512695
maxi score, test score, baseline:  0.022099999999999998 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
printing an ep nov before normalisation:  41.62098256415499
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.95725158017726
printing an ep nov before normalisation:  62.00918187391816
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.059316194768799166, 0.14815796461531017, 0.14815796461531017, 0.059316194768799166, 0.17036840707693793, 0.41468327415484335]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.060339490858225715, 0.14540123150497847, 0.14540123150497847, 0.060339490858225715, 0.16666666666666666, 0.4218518886069251]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.061279262569204386, 0.14286951090272357, 0.14286951090272357, 0.061279262569204386, 0.16326707298610332, 0.4284353800700407]
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.909]
 [0.909]
 [0.909]
 [0.909]] [[71.504]
 [71.504]
 [71.504]
 [71.504]
 [71.504]] [[2.702]
 [2.702]
 [2.702]
 [2.702]
 [2.702]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.061279262569204386, 0.14286951090272357, 0.14286951090272357, 0.061279262569204386, 0.16326707298610332, 0.4284353800700407]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
Printing some Q and Qe and total Qs values:  [[0.497]
 [1.004]
 [0.809]
 [0.292]
 [0.298]] [[53.605]
 [37.474]
 [49.434]
 [54.198]
 [52.47 ]] [[1.844]
 [1.718]
 [1.992]
 [1.661]
 [1.6  ]]
using explorer policy with actor:  1
from probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
printing an ep nov before normalisation:  18.457101744710112
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
printing an ep nov before normalisation:  16.419052422835552
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
actions average: 
K:  1  action  0 :  tensor([    0.9403,     0.0039,     0.0002,     0.0152,     0.0404],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9916,     0.0019,     0.0038,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0010,     0.9664,     0.0066,     0.0256],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0321,     0.0003,     0.0022,     0.8499,     0.1155],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1846, 0.0357, 0.0118, 0.2110, 0.5569], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
Printing some Q and Qe and total Qs values:  [[0.167]
 [0.105]
 [0.11 ]
 [0.111]
 [0.111]] [[35.124]
 [38.838]
 [39.709]
 [39.393]
 [38.447]] [[1.125]
 [1.278]
 [1.334]
 [1.316]
 [1.262]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06214533516942472, 0.14053633379235617, 0.14053633379235617, 0.06214533516942472, 0.16013408344808902, 0.4345025786283492]
printing an ep nov before normalisation:  33.58298423644624
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06095246149653727, 0.13783551980208592, 0.13783551980208592, 0.06095246149653727, 0.1762770489548602, 0.42614698844789345]
from probs:  [0.06095246149653727, 0.13783551980208592, 0.13783551980208592, 0.06095246149653727, 0.1762770489548602, 0.42614698844789345]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06095246149653727, 0.13783551980208592, 0.13783551980208592, 0.06095246149653727, 0.1762770489548602, 0.42614698844789345]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.06095246149653727, 0.13783551980208592, 0.13783551980208592, 0.06095246149653727, 0.1762770489548602, 0.42614698844789345]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.90307724
printing an ep nov before normalisation:  23.658940832190563
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0241 0.4 0.4
printing an ep nov before normalisation:  71.451813351037
printing an ep nov before normalisation:  62.70684527935755
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.058699291322713454, 0.1327340629871385, 0.1327340629871385, 0.058699291322713454, 0.20676883465156357, 0.4103644567287325]
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.205]
 [0.405]
 [0.246]
 [0.246]] [[42.436]
 [43.838]
 [38.592]
 [43.188]
 [43.188]] [[1.496]
 [1.631]
 [1.553]
 [1.637]
 [1.637]]
siam score:  -0.9081242
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.05869929132271345, 0.1327340629871385, 0.1327340629871385, 0.05869929132271345, 0.20676883465156357, 0.41036445672873245]
using another actor
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.058699291322713454, 0.13273406298713852, 0.13273406298713852, 0.058699291322713454, 0.20676883465156357, 0.4103644567287325]
printing an ep nov before normalisation:  59.71824280700268
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.058699291322713454, 0.13273406298713852, 0.13273406298713852, 0.058699291322713454, 0.20676883465156357, 0.4103644567287325]
printing an ep nov before normalisation:  48.98493451154683
siam score:  -0.9148328
printing an ep nov before normalisation:  35.207118961442035
Printing some Q and Qe and total Qs values:  [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.122]
 [0.122]
 [0.122]
 [0.122]
 [0.122]]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.05869929132271345, 0.1327340629871385, 0.1327340629871385, 0.05869929132271345, 0.20676883465156357, 0.41036445672873245]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 41.76176396295384
maxi score, test score, baseline:  0.0241 0.4 0.4
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.60320048750229
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
Printing some Q and Qe and total Qs values:  [[0.629]
 [0.629]
 [0.772]
 [0.629]
 [0.629]] [[35.421]
 [35.421]
 [39.65 ]
 [35.421]
 [35.421]] [[1.904]
 [1.904]
 [2.337]
 [1.904]
 [1.904]]
from probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
maxi score, test score, baseline:  0.0241 0.4 0.4
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.056607103620979454, 0.1279970904614252, 0.14584458717153667, 0.056607103620979454, 0.21723457401198246, 0.39570954111309686]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.948694288776096
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.05561608137163448, 0.12575329313691797, 0.14328759607823882, 0.05561608137163448, 0.23095911078484316, 0.38876783725673103]
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.05561608137163448, 0.12575329313691797, 0.14328759607823882, 0.05561608137163448, 0.23095911078484316, 0.38876783725673103]
siam score:  -0.9165361
maxi score, test score, baseline:  0.0241 0.4 0.4
probs:  [0.05561608137163448, 0.12575329313691797, 0.14328759607823882, 0.05561608137163448, 0.23095911078484316, 0.38876783725673103]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 58.134187568898604
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05561608137163448, 0.12575329313691797, 0.14328759607823882, 0.05561608137163448, 0.23095911078484316, 0.38876783725673103]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05561608137163448, 0.12575329313691797, 0.14328759607823882, 0.05561608137163448, 0.23095911078484316, 0.38876783725673103]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.80291748046875
printing an ep nov before normalisation:  38.44891548156738
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.071]
 [0.071]
 [0.071]
 [0.071]] [[63.494]
 [63.494]
 [63.494]
 [63.494]
 [63.494]] [[1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]]
printing an ep nov before normalisation:  33.09296131134033
from probs:  [0.05655664699367648, 0.124316659100132, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
Printing some Q and Qe and total Qs values:  [[0.082]
 [0.068]
 [0.086]
 [0.089]
 [0.086]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [0.068]
 [0.086]
 [0.089]
 [0.086]]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05655664699367648, 0.12431665910013198, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
printing an ep nov before normalisation:  41.40869176645859
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05655664699367648, 0.12431665910013198, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05655664699367648, 0.12431665910013198, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05655664699367648, 0.12431665910013198, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
printing an ep nov before normalisation:  45.61380131317117
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05655664699367648, 0.12431665910013198, 0.14125666212674587, 0.05655664699367648, 0.22595667725981528, 0.39535670752595403]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  31.953383057949313
printing an ep nov before normalisation:  55.34745658574959
printing an ep nov before normalisation:  41.4673131540168
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05561601646510973, 0.13890400411627743, 0.13890400411627743, 0.05561601646510973, 0.22219199176744517, 0.3887679670697806]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05470623402970901, 0.13662850181284875, 0.13662850181284875, 0.05470623402970901, 0.2349352231526164, 0.382395305162268]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05470623402970901, 0.13662850181284875, 0.13662850181284875, 0.05470623402970901, 0.2349352231526164, 0.382395305162268]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  61.123148779468615
actions average: 
K:  3  action  0 :  tensor([    0.8090,     0.0030,     0.0005,     0.0736,     0.1140],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0033,     0.9913,     0.0003,     0.0002,     0.0049],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0047,     0.9628,     0.0152,     0.0170],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0040,     0.0001,     0.0050,     0.9131,     0.0778],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1562, 0.0008, 0.1265, 0.3111, 0.4053], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05297333618183601, 0.14815845007611286, 0.14815845007611286, 0.05297333618183601, 0.22747937832134354, 0.37025704916275887]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.05297333618183599, 0.14815845007611284, 0.1481584500761128, 0.05297333618183599, 0.2274793783213435, 0.37025704916275876]
using explorer policy with actor:  1
printing an ep nov before normalisation:  88.48047355992082
printing an ep nov before normalisation:  39.38421865411842
from probs:  [0.05297333618183599, 0.14815845007611284, 0.1481584500761128, 0.05297333618183599, 0.2274793783213435, 0.37025704916275876]
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.026099999999999998 0.4 0.4
probs:  [0.06776376017051554, 0.14584500214116117, 0.14584500214116114, 0.052147511776386414, 0.22392624411180675, 0.3644724796589689]
using another actor
Starting evaluation
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06776376017051554, 0.14584500214116117, 0.14584500214116114, 0.052147511776386414, 0.22392624411180675, 0.3644724796589689]
printing an ep nov before normalisation:  44.01550442724979
printing an ep nov before normalisation:  17.979903501801804
maxi score, test score, baseline:  0.0281 0.4 0.4
probs:  [0.06776376017051554, 0.14584500214116117, 0.14584500214116114, 0.052147511776386414, 0.22392624411180675, 0.3644724796589689]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([    0.7369,     0.0454,     0.0004,     0.0881,     0.1293],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9774,     0.0084,     0.0004,     0.0129],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0059,     0.9709,     0.0023,     0.0208],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0246,     0.0002,     0.0230,     0.8359,     0.1162],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0544, 0.0392, 0.0018, 0.3356, 0.5691], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0381 0.4 0.4
probs:  [0.06776376017051554, 0.14584500214116117, 0.14584500214116114, 0.052147511776386414, 0.22392624411180675, 0.3644724796589689]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.040100000000000004 0.3 0.3
probs:  [0.06776348699424249, 0.14584494463036682, 0.14584494463036682, 0.05214719546701762, 0.22392640226649116, 0.36447302601151504]
printing an ep nov before normalisation:  20.991737842559814
printing an ep nov before normalisation:  10.698163509368896
maxi score, test score, baseline:  0.040100000000000004 0.3 0.3
probs:  [0.06776348699424249, 0.14584494463036682, 0.14584494463036682, 0.05214719546701762, 0.22392640226649116, 0.36447302601151504]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  64.09688205044729
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.48 ]
 [1.438]
 [1.077]
 [1.401]] [[19.18 ]
 [13.442]
 [15.822]
 [19.572]
 [17.235]] [[2.445]
 [2.339]
 [2.449]
 [2.328]
 [2.502]]
actions average: 
K:  1  action  0 :  tensor([    0.8925,     0.0009,     0.0001,     0.0390,     0.0675],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9958,     0.0027,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0011,     0.0006,     0.9590,     0.0038,     0.0355],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0437, 0.0035, 0.0019, 0.8011, 0.1498], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0900, 0.0130, 0.2037, 0.2902, 0.4031], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.040100000000000004 0.3 0.3
probs:  [0.06768748932364971, 0.14011127762341824, 0.15459603528337193, 0.05320273166369601, 0.21253506592318672, 0.37186740018267744]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0667221926694798, 0.13811110266747045, 0.15238888466706854, 0.052444410669881675, 0.22377779466505918, 0.3665556146610404]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.3 0.3
printing an ep nov before normalisation:  0.14040448773222636
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  70.19711188038558
maxi score, test score, baseline:  0.040100000000000004 0.3 0.3
probs:  [0.06487205985447338, 0.13427747359005973, 0.148158556337177, 0.0509909771073561, 0.24532613556699792, 0.356374797543936]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  28.153781110302983
maxi score, test score, baseline:  0.042100000000000005 0.3 0.3
maxi score, test score, baseline:  0.042100000000000005 0.3 0.3
printing an ep nov before normalisation:  80.76625279030745
printing an ep nov before normalisation:  64.06432181870653
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.83650088310242
from probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [1.083]
 [0.829]] [[49.567]
 [49.567]
 [49.567]
 [69.396]
 [49.567]] [[1.323]
 [1.323]
 [1.323]
 [1.869]
 [1.323]]
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
printing an ep nov before normalisation:  68.09947449580552
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
printing an ep nov before normalisation:  47.29549572021424
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
maxi score, test score, baseline:  0.0441 0.3 0.3
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
siam score:  -0.9044836
using another actor
from probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06312195732906908, 0.1306511155927197, 0.14415694724544978, 0.04961612567633896, 0.2657094321200209, 0.34674442203640155]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  73.69063844009958
maxi score, test score, baseline:  0.0441 0.3 0.3
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.83648870471089
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06415998072037514, 0.12822665943680733, 0.14103999518009377, 0.0513466449770887, 0.25636001686967175, 0.3588667028159632]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06334932363931792, 0.12660483977851106, 0.13925594300634966, 0.05069822041147928, 0.26576697528473586, 0.3543246978796063]
printing an ep nov before normalisation:  26.341147422790527
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06334932363931792, 0.12660483977851106, 0.13925594300634966, 0.05069822041147928, 0.26576697528473586, 0.3543246978796063]
from probs:  [0.06334932363931792, 0.12660483977851106, 0.13925594300634966, 0.05069822041147928, 0.26576697528473586, 0.3543246978796063]
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06334932363931792, 0.12660483977851106, 0.13925594300634966, 0.05069822041147928, 0.26576697528473586, 0.3543246978796063]
printing an ep nov before normalisation:  25.26170253753662
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0441 0.3 0.3
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06255893772246758, 0.12502357508898704, 0.13751650256229092, 0.0500660102491637, 0.2749387047686337, 0.34989626960845704]
printing an ep nov before normalisation:  39.97325816705872
maxi score, test score, baseline:  0.0441 0.3 0.3
probs:  [0.06255893772246758, 0.12502357508898704, 0.13751650256229092, 0.0500660102491637, 0.2749387047686337, 0.34989626960845704]
line 256 mcts: sample exp_bonus 47.62604940383166
Printing some Q and Qe and total Qs values:  [[0.901]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]] [[23.074]
 [22.013]
 [ 5.521]
 [20.275]
 [20.997]] [[0.901]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.74706741611017
printing an ep nov before normalisation:  55.643143676407966
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.06255893772246758, 0.12502357508898704, 0.13751650256229092, 0.0500660102491637, 0.2749387047686337, 0.34989626960845704]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.06178807201263962, 0.1234813629855967, 0.13582002118018813, 0.0494494138180482, 0.28388391951528513, 0.3455772104882422]
printing an ep nov before normalisation:  27.490351628607414
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.06178807201263962, 0.1234813629855967, 0.13582002118018813, 0.0494494138180482, 0.28388391951528513, 0.3455772104882422]
Printing some Q and Qe and total Qs values:  [[1.342]
 [1.34 ]
 [1.296]
 [1.133]
 [1.342]] [[14.578]
 [17.078]
 [15.237]
 [15.574]
 [14.578]] [[1.879]
 [2.084]
 [1.888]
 [1.752]
 [1.879]]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.06103601219306086, 0.12197677438937189, 0.13416492682863412, 0.04884785975379865, 0.29261090853904287, 0.34136351829609163]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.176263093948364
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.04985710915504398, 0.11475130777261215, 0.1277301474961258, 0.04985710915504398, 0.30943390362531664, 0.34837042279585756]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0461 0.3 0.3
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.05007300191091237, 0.1250260721110401, 0.12502607211104017, 0.05007300191091237, 0.2999165692446716, 0.3498852827114234]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9109486
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([    0.7797,     0.0037,     0.0007,     0.0520,     0.1639],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9923,     0.0049,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0024,     0.9648,     0.0008,     0.0320],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0593,     0.0001,     0.0192,     0.8488,     0.0726],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1033, 0.0102, 0.1141, 0.2959, 0.4765], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.619]
 [1.038]
 [0.678]
 [0.074]
 [0.619]] [[18.64 ]
 [12.302]
 [20.24 ]
 [23.846]
 [18.64 ]] [[1.342]
 [1.038]
 [1.584]
 [1.391]
 [1.342]]
printing an ep nov before normalisation:  19.756371601054887
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05165807266769046, 0.12304271721877916, 0.12304271721877916, 0.05165807266769046, 0.28960688783798605, 0.3609915323890748]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05165807266769046, 0.12304271721877916, 0.12304271721877916, 0.05165807266769046, 0.28960688783798605, 0.3609915323890748]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05165807266769046, 0.12304271721877916, 0.12304271721877916, 0.05165807266769046, 0.28960688783798605, 0.3609915323890748]
printing an ep nov before normalisation:  18.87513819517373
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.50068817597423
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.051051787467619035, 0.12159645952127524, 0.13335390486355125, 0.051051787467619035, 0.2862006943131397, 0.3567453663667958]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9464,     0.0022,     0.0001,     0.0020,     0.0493],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9974,     0.0005,     0.0001,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9906,     0.0004,     0.0088],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0171,     0.0003,     0.0262,     0.9141,     0.0422],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1433, 0.0023, 0.1217, 0.2336, 0.4991], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.122]
 [0.166]
 [0.137]] [[32.872]
 [32.872]
 [48.606]
 [24.124]
 [32.872]] [[0.696]
 [0.696]
 [1.214]
 [0.428]
 [0.696]]
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.837]
 [1.011]
 [0.803]
 [0.837]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.837]
 [0.837]
 [1.011]
 [0.803]
 [0.837]]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
printing an ep nov before normalisation:  15.70776362611852
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05045960446467849, 0.1201838417858714, 0.13180454800607022, 0.05045960446467849, 0.29449443508885365, 0.35259796618984773]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.266655271513685
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  19.68044228954315
printing an ep nov before normalisation:  37.28289019220821
actions average: 
K:  3  action  0 :  tensor([0.8284, 0.0062, 0.0018, 0.0507, 0.1129], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0071,     0.9736,     0.0039,     0.0001,     0.0153],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0004,     0.9852,     0.0016,     0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0663,     0.0003,     0.0011,     0.7829,     0.1494],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0867, 0.0097, 0.1215, 0.2532, 0.5289], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
printing an ep nov before normalisation:  66.25119209289551
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.05007214583411516, 0.11669758630985888, 0.12780182638914953, 0.06117638591340578, 0.2943654275785087, 0.3498866279749619]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.051664640170717716, 0.11114844697896718, 0.12304520834061708, 0.051664640170717716, 0.3014966287653654, 0.3609804355736149]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.147062038117838
printing an ep nov before normalisation:  7.942696586269449
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05240170019313822, 0.11050253060340695, 0.12212269668546066, 0.05240170019313822, 0.29642518791626676, 0.36614618440858926]
actions average: 
K:  0  action  0 :  tensor([    0.7740,     0.0502,     0.0007,     0.0559,     0.1192],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0051,     0.9895,     0.0004,     0.0000,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0018,     0.9927,     0.0004,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0537, 0.0056, 0.0054, 0.7698, 0.1656], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0964, 0.0483, 0.1816, 0.2500, 0.4237], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  34.02863589246118
printing an ep nov before normalisation:  67.6803847028041
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05240170019313822, 0.11050253060340695, 0.12212269668546066, 0.05240170019313822, 0.29642518791626676, 0.36614618440858926]
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.079]
 [0.079]
 [0.079]] [[28.72]
 [28.72]
 [28.72]
 [28.72]
 [28.72]] [[1.158]
 [1.158]
 [1.158]
 [1.158]
 [1.158]]
printing an ep nov before normalisation:  85.54742456485961
printing an ep nov before normalisation:  20.934783574826294
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.3 0.3
siam score:  -0.8984653
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
from probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
printing an ep nov before normalisation:  55.83549568216612
from probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
Printing some Q and Qe and total Qs values:  [[0.901]
 [0.901]
 [1.133]
 [0.901]
 [0.901]] [[43.51 ]
 [43.51 ]
 [55.641]
 [43.51 ]
 [43.51 ]] [[1.828]
 [1.828]
 [2.552]
 [1.828]
 [1.828]]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.053187987390553956, 0.10809702574996334, 0.11907883342184522, 0.053187987390553956, 0.2947877561719552, 0.37166040987512833]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.253]
 [1.253]
 [1.253]
 [1.253]] [[63.763]
 [63.763]
 [63.763]
 [63.763]
 [63.763]] [[2.814]
 [2.814]
 [2.814]
 [2.814]
 [2.814]]
printing an ep nov before normalisation:  63.092481772941476
maxi score, test score, baseline:  0.0521 0.3 0.3
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.887]
 [0.887]
 [1.162]
 [0.887]
 [0.887]] [[89.108]
 [89.108]
 [78.023]
 [89.108]
 [89.108]] [[2.554]
 [2.554]
 [2.567]
 [2.554]
 [2.554]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.053892383321920645, 0.1163519864051646, 0.1163519864051646, 0.053892383321920645, 0.2829109279604818, 0.37660033258534775]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.49447041132785
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05340637466450208, 0.11336770572447155, 0.12336126090113311, 0.05340637466450208, 0.2832581437277184, 0.37320014031767273]
printing an ep nov before normalisation:  91.55533264528626
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05340637466450208, 0.11336770572447155, 0.12336126090113311, 0.05340637466450208, 0.2832581437277184, 0.37320014031767273]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.3 0.3
from probs:  [0.05287883680715698, 0.11224640021211857, 0.12214099411294546, 0.05287883680715698, 0.2903490904270033, 0.36950584163361877]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.187126459590424
printing an ep nov before normalisation:  2.662664986854395
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([0.8382, 0.0019, 0.0011, 0.0302, 0.1285], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0010,     0.9902,     0.0036,     0.0000,     0.0052],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9574,     0.0294,     0.0129],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0204,     0.0001,     0.0015,     0.8955,     0.0824],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0804, 0.0016, 0.0350, 0.1061, 0.7769], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.773492134919042
printing an ep nov before normalisation:  33.5424853296918
printing an ep nov before normalisation:  88.34572084211983
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.081]
 [0.126]
 [0.081]] [[50.219]
 [50.219]
 [50.219]
 [46.138]
 [50.219]] [[2.081]
 [2.081]
 [2.081]
 [1.858]
 [2.081]]
siam score:  -0.9197364
actions average: 
K:  2  action  0 :  tensor([    0.8506,     0.0013,     0.0005,     0.0655,     0.0821],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9906,     0.0010,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.9763,     0.0018,     0.0213],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0147,     0.0001,     0.0025,     0.8834,     0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1707, 0.0010, 0.0090, 0.2060, 0.6133], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0541 0.3 0.3
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.511257505640533
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.529]
 [0.797]
 [0.529]
 [0.529]] [[36.688]
 [36.688]
 [59.354]
 [36.688]
 [36.688]] [[1.127]
 [1.127]
 [1.957]
 [1.127]
 [1.127]]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.815]
 [0.525]
 [0.525]] [[35.161]
 [35.161]
 [45.329]
 [35.161]
 [35.161]] [[1.247]
 [1.247]
 [1.855]
 [1.247]
 [1.247]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.051476462670343004, 0.10751494029017614, 0.12619443283012052, 0.051476462670343004, 0.30364961195959217, 0.3596880895794253]
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.051476462670343004, 0.10751494029017614, 0.12619443283012052, 0.051476462670343004, 0.30364961195959217, 0.3596880895794253]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.05100099058917622, 0.10652051510637163, 0.12502702327877008, 0.06025424467537545, 0.30083885091655554, 0.35635837543375104]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.05053424407098452, 0.10554433898472869, 0.12388103728931009, 0.059702593223275216, 0.307248020335124, 0.35308976609657744]
printing an ep nov before normalisation:  44.983551716483056
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05112567366554164, 0.10514483922450918, 0.12315122774416504, 0.06012886792536956, 0.3032151129407235, 0.357234278499691]
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
printing an ep nov before normalisation:  42.56234462586543
from probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
printing an ep nov before normalisation:  31.20041189241931
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
Printing some Q and Qe and total Qs values:  [[1.377]
 [1.377]
 [1.39 ]
 [1.377]
 [1.377]] [[33.378]
 [33.378]
 [34.02 ]
 [33.378]
 [33.378]] [[3.318]
 [3.318]
 [3.39 ]
 [3.318]
 [3.318]]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05067031445148232, 0.10420709239695201, 0.12205268504544191, 0.06851590709997223, 0.30050861153034086, 0.3540453894758106]
printing an ep nov before normalisation:  0.0011626089226979275
printing an ep nov before normalisation:  16.887761754984446
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9129074
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05124379519322748, 0.10384155940897191, 0.12137414748088671, 0.0687763832651423, 0.29670002820003477, 0.3580640864517367]
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05124379519322749, 0.10384155940897194, 0.12137414748088675, 0.06877638326514231, 0.2967000282000348, 0.35806408645173676]
printing an ep nov before normalisation:  74.10170738914567
from probs:  [0.05124379519322749, 0.10384155940897194, 0.12137414748088675, 0.06877638326514231, 0.2967000282000348, 0.35806408645173676]
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05124379519322749, 0.10384155940897194, 0.12137414748088675, 0.06877638326514231, 0.2967000282000348, 0.35806408645173676]
Printing some Q and Qe and total Qs values:  [[1.175]
 [1.328]
 [1.276]
 [1.175]
 [1.175]] [[15.118]
 [20.384]
 [19.304]
 [15.118]
 [15.118]] [[2.064]
 [2.527]
 [2.412]
 [2.064]
 [2.064]]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05124379519322749, 0.10384155940897194, 0.12137414748088675, 0.06877638326514231, 0.2967000282000348, 0.35806408645173676]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05124379519322749, 0.10384155940897194, 0.12137414748088675, 0.06877638326514231, 0.2967000282000348, 0.35806408645173676]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.259937997806205
maxi score, test score, baseline:  0.0621 0.3 0.3
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8927],
        [0.1846],
        [0.6759],
        [0.0000],
        [0.8374],
        [0.1832],
        [0.2819],
        [0.0000],
        [0.1382],
        [0.2467]], dtype=torch.float64)
0.0 0.8926570079006504
0.0 0.18464774053920882
0.0 0.6759067285034729
0.0 0.0
0.0 0.8373790853629064
0.0 0.18320356875341093
0.0 0.2819461424815355
0.0 0.0
0.0 0.13816313409072892
0.0 0.2466729661379726
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05036244252537689, 0.11066833652456418, 0.1192834642387338, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
from probs:  [0.05036244252537689, 0.11066833652456418, 0.1192834642387338, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
printing an ep nov before normalisation:  29.69691753387451
maxi score, test score, baseline:  0.0621 0.3 0.3
actions average: 
K:  1  action  0 :  tensor([    0.9281,     0.0003,     0.0006,     0.0260,     0.0449],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0025,     0.9729,     0.0147,     0.0000,     0.0099],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0012,     0.9903,     0.0004,     0.0081],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0567,     0.0002,     0.0009,     0.8485,     0.0937],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1262, 0.0040, 0.0885, 0.2507, 0.5307], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05036244252537689, 0.11066833652456418, 0.11928346423873376, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05036244252537689, 0.11066833652456418, 0.11928346423873376, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05036244252537689, 0.11066833652456418, 0.11928346423873376, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05036244252537689, 0.11066833652456418, 0.11928346423873376, 0.06759269795371611, 0.3002011462362956, 0.35189191252131347]
actions average: 
K:  0  action  0 :  tensor([0.8385, 0.0031, 0.0014, 0.0610, 0.0960], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9963,     0.0000,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9949,     0.0000,     0.0049],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0602,     0.0005,     0.0297,     0.7787,     0.1309],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1937, 0.0631, 0.0093, 0.2720, 0.4618], grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.838]
 [1.244]
 [0.41 ]
 [0.378]
 [0.771]] [[71.718]
 [59.655]
 [69.539]
 [69.277]
 [69.641]] [[2.832]
 [2.76 ]
 [2.318]
 [2.275]
 [2.682]]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.6  ]
 [0.827]
 [0.6  ]
 [0.6  ]] [[48.319]
 [48.319]
 [51.803]
 [48.319]
 [48.319]] [[1.779]
 [1.779]
 [2.161]
 [1.779]
 [1.779]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05049483999423226, 0.11767854698551966, 0.11767854698551963, 0.06729076674205411, 0.29403577783764895, 0.3528215214550255]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05049483999423226, 0.11767854698551966, 0.11767854698551963, 0.06729076674205411, 0.29403577783764895, 0.3528215214550255]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05049483999423226, 0.11767854698551966, 0.11767854698551963, 0.06729076674205411, 0.29403577783764895, 0.3528215214550255]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05049483999423226, 0.11767854698551966, 0.11767854698551963, 0.06729076674205411, 0.29403577783764895, 0.3528215214550255]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05007509449016102, 0.1166988500195928, 0.1166988500195928, 0.06673103337251897, 0.29991417772553025, 0.3498819943726041]
printing an ep nov before normalisation:  49.93804608127663
actions average: 
K:  3  action  0 :  tensor([    0.9079,     0.0101,     0.0009,     0.0033,     0.0779],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9982,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0264,     0.9517,     0.0003,     0.0216],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1079, 0.0014, 0.0684, 0.6516, 0.1707], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2007, 0.0029, 0.0017, 0.1791, 0.6157], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  79.44280908327495
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05007509449016102, 0.1166988500195928, 0.1166988500195928, 0.06673103337251897, 0.29991417772553025, 0.3498819943726041]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.735]
 [0.565]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.565]
 [0.565]
 [0.735]
 [0.565]
 [0.565]]
Printing some Q and Qe and total Qs values:  [[1.111]
 [1.111]
 [1.111]
 [1.111]
 [1.111]] [[30.64]
 [30.64]
 [30.64]
 [30.64]
 [30.64]] [[2.773]
 [2.773]
 [2.773]
 [2.773]
 [2.773]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.909]
 [0.909]
 [1.07 ]
 [0.732]
 [0.909]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.909]
 [0.909]
 [1.07 ]
 [0.732]
 [0.909]]
printing an ep nov before normalisation:  17.68545389175415
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.954]
 [0.611]
 [0.611]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.611]
 [0.611]
 [0.954]
 [0.611]
 [0.611]]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05021019426022912, 0.11520915560335707, 0.11520915560335702, 0.0664599345960111, 0.30208116946484986, 0.3508303904721957]
printing an ep nov before normalisation:  57.14665393904653
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05021019426022912, 0.11520915560335707, 0.11520915560335702, 0.0664599345960111, 0.30208116946484986, 0.3508303904721957]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  34.41492798054869
actions average: 
K:  4  action  0 :  tensor([    0.9288,     0.0001,     0.0003,     0.0217,     0.0491],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0019,     0.9904,     0.0009,     0.0000,     0.0068],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0011,     0.9757,     0.0011,     0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0038,     0.0001,     0.0017,     0.8195,     0.1750],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2129, 0.0019, 0.1906, 0.1083, 0.4863], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04980628424752089, 0.11428097799601511, 0.11428097799601511, 0.06592495768464444, 0.3077050592414978, 0.3480017428343066]
printing an ep nov before normalisation:  23.551308431381937
printing an ep nov before normalisation:  15.870392085809033
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04980628424752089, 0.11428097799601511, 0.11428097799601511, 0.06592495768464444, 0.3077050592414978, 0.3480017428343066]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04980628424752089, 0.11428097799601511, 0.11428097799601511, 0.06592495768464444, 0.3077050592414978, 0.3480017428343066]
using another actor
using another actor
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04980628424752089, 0.11428097799601511, 0.11428097799601511, 0.06592495768464444, 0.3077050592414978, 0.3480017428343066]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  66.63148207997466
maxi score, test score, baseline:  0.0641 0.3 0.3
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049408837803053836, 0.11336765354684267, 0.12136250551481625, 0.06539854173900105, 0.3052441007782091, 0.34521836061807715]
maxi score, test score, baseline:  0.0641 0.3 0.3
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049408837803053836, 0.11336765354684267, 0.12136250551481625, 0.06539854173900105, 0.3052441007782091, 0.34521836061807715]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.458 0.042 0.417]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049408837803053836, 0.11336765354684267, 0.12136250551481625, 0.06539854173900105, 0.3052441007782091, 0.34521836061807715]
printing an ep nov before normalisation:  21.847000501590266
printing an ep nov before normalisation:  71.71812536003519
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049408837803053836, 0.11336765354684267, 0.12136250551481625, 0.06539854173900105, 0.3052441007782091, 0.34521836061807715]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049408837803053836, 0.11336765354684267, 0.12136250551481625, 0.06539854173900105, 0.3052441007782091, 0.34521836061807715]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
from probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
from probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.01 ]
 [0.015]
 [0.011]
 [0.02 ]] [[17.341]
 [23.406]
 [25.081]
 [15.874]
 [16.142]] [[1.148]
 [1.658]
 [1.809]
 [1.001]
 [1.033]]
printing an ep nov before normalisation:  39.480939417292255
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
maxi score, test score, baseline:  0.0641 0.3 0.3
printing an ep nov before normalisation:  13.405591249465942
printing an ep nov before normalisation:  27.02964244527166
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
siam score:  -0.8963263
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04885681498404683, 0.11385535384342328, 0.12198017120084534, 0.06510644969889096, 0.3088509704215527, 0.3413502398512409]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.03 ]
 [0.014]
 [0.014]] [[15.687]
 [15.687]
 [ 1.481]
 [15.687]
 [15.687]] [[1.713]
 [1.713]
 [0.131]
 [1.713]
 [1.713]]
using another actor
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.048463823570304806, 0.11293810162286581, 0.12905667113600605, 0.06458239308344507, 0.30636093578054885, 0.33859807480682935]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.048463823570304806, 0.11293810162286581, 0.12905667113600605, 0.06458239308344507, 0.30636093578054885, 0.33859807480682935]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.048463823570304806, 0.11293810162286581, 0.12905667113600605, 0.06458239308344507, 0.30636093578054885, 0.33859807480682935]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  38.85328099323164
printing an ep nov before normalisation:  34.58176701657495
printing an ep nov before normalisation:  72.6535505530721
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.048463823570304806, 0.11293810162286581, 0.12905667113600605, 0.06458239308344507, 0.30636093578054885, 0.33859807480682935]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.547]
 [0.192]
 [0.266]
 [0.295]] [[52.349]
 [59.157]
 [71.826]
 [62.976]
 [64.861]] [[0.998]
 [1.427]
 [1.484]
 [1.27 ]
 [1.36 ]]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.048463823570304806, 0.11293810162286581, 0.12905667113600605, 0.06458239308344507, 0.30636093578054885, 0.33859807480682935]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04807712095974854, 0.1120355276331426, 0.1280251293014911, 0.06406672262809705, 0.311905548487499, 0.3358899509900217]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04807712095974854, 0.1120355276331426, 0.1280251293014911, 0.06406672262809705, 0.311905548487499, 0.3358899509900217]
printing an ep nov before normalisation:  40.5322265625
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04807712095974854, 0.1120355276331426, 0.1280251293014911, 0.06406672262809705, 0.311905548487499, 0.3358899509900217]
printing an ep nov before normalisation:  22.35778288284122
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.333 0.167 0.167 0.167 0.167]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.334822530040626
maxi score, test score, baseline:  0.0641 0.3 0.3
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.0473219878430837, 0.11027302722255605, 0.13387966698985815, 0.06305974768795179, 0.3148639052058411, 0.33060166505070926]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.047896460686866794, 0.10523380150470121, 0.12980694756948738, 0.056087509375128866, 0.32639211608777685, 0.33458316477603894]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  67.31833458008693
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.047508100350702, 0.10437923427423058, 0.1368770250876755, 0.05563254805406322, 0.32373932226498375, 0.33186376996834493]
actions average: 
K:  1  action  0 :  tensor([    0.8725,     0.0001,     0.0001,     0.0777,     0.0495],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0349,     0.9405,     0.0001,     0.0003,     0.0241],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0011,     0.9763,     0.0024,     0.0201],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0368,     0.0004,     0.0034,     0.7842,     0.1753],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1612, 0.0027, 0.0578, 0.2294, 0.5489], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.91274121165641
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]] [[33.721]
 [33.721]
 [33.721]
 [33.721]
 [33.721]] [[2.178]
 [2.178]
 [2.178]
 [2.178]
 [2.178]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
maxi score, test score, baseline:  0.0641 0.3 0.3
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
using another actor
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.04863718286949726, 0.10371760864150964, 0.13519213765408813, 0.05650581512264189, 0.3161706794764146, 0.33977657623584845]
printing an ep nov before normalisation:  42.53048451027178
using explorer policy with actor:  0
printing an ep nov before normalisation:  22.12738496808378
printing an ep nov before normalisation:  0.006412384065868082
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049671564269856396, 0.09922243116732897, 0.1322563424323107, 0.049671564269856396, 0.3222013322059556, 0.3469767656546918]
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.049671564269856396, 0.09922243116732897, 0.1322563424323107, 0.049671564269856396, 0.3222013322059556, 0.3469767656546918]
printing an ep nov before normalisation:  16.00744359100037
printing an ep nov before normalisation:  28.30730676651001
printing an ep nov before normalisation:  35.05602957823599
using another actor
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.8648,     0.0009,     0.0001,     0.0034,     0.1308],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0084,     0.9814,     0.0014,     0.0001,     0.0087],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9758,     0.0110,     0.0130],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0263,     0.0000,     0.0007,     0.8943,     0.0787],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2451, 0.1145, 0.0009, 0.1829, 0.4566], grad_fn=<DivBackward0>)
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.04981532499288312, 0.09816760430617286, 0.13040245718169935, 0.05787403821176475, 0.3157528612159768, 0.34798771409150325]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0661 0.3 0.3
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.050347718124356726, 0.09793274252802897, 0.12965609213047716, 0.058278555524968766, 0.31206535234455407, 0.3517195393476142]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  2.2932230475271353
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.05008294465120472, 0.10389081635064866, 0.12695133279326754, 0.06545662227961728, 0.3037486255200121, 0.3498696584052497]
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.05008294465120472, 0.10389081635064866, 0.12695133279326754, 0.06545662227961728, 0.3037486255200121, 0.3498696584052497]
printing an ep nov before normalisation:  25.053143498871375
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.91064286
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.04970161561635165, 0.10309870413932154, 0.12598317064916575, 0.06495792662291447, 0.3090589027279197, 0.3471996802443269]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
actions average: 
K:  4  action  0 :  tensor([    0.8859,     0.0012,     0.0001,     0.0250,     0.0879],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0012,     0.9428,     0.0000,     0.0005,     0.0555],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0012,     0.9752,     0.0044,     0.0191],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0331,     0.0002,     0.0012,     0.8072,     0.1582],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1619, 0.1254, 0.0008, 0.1046, 0.6074], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
printing an ep nov before normalisation:  8.621335516660709
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.050207701255301405, 0.10280207273140184, 0.12534251764973062, 0.06523466453418723, 0.3056660769963607, 0.35074696683301826]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.17595672607422
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.6879439017365
from probs:  [0.04874554152457084, 0.09980417302939583, 0.1289805338892958, 0.06333372195452083, 0.3186268794786459, 0.34050915012357086]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
from probs:  [0.04972740496258696, 0.09933800083704501, 0.12768691276530675, 0.06390186092671782, 0.31195484029900816, 0.3473909802093354]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.049378115997744686, 0.09863930727869191, 0.1267885594392332, 0.06345274207801532, 0.3167960115228868, 0.3449452636834281]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.049378115997744686, 0.09863930727869191, 0.1267885594392332, 0.06345274207801532, 0.3167960115228868, 0.3449452636834281]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.7  ]
 [0.62 ]
 [0.775]
 [0.7  ]] [[46.462]
 [46.462]
 [37.54 ]
 [52.768]
 [46.462]] [[1.383]
 [1.383]
 [1.065]
 [1.627]
 [1.383]]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.04937811599774469, 0.09863930727869191, 0.1267885594392332, 0.06345274207801532, 0.31679601152288683, 0.34494526368342815]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.04937811599774469, 0.09863930727869191, 0.1267885594392332, 0.06345274207801532, 0.31679601152288683, 0.34494526368342815]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.04937811599774469, 0.09863930727869191, 0.1267885594392332, 0.06345274207801532, 0.31679601152288683, 0.34494526368342815]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.883329279456795
Printing some Q and Qe and total Qs values:  [[1.426]
 [1.411]
 [1.427]
 [1.422]
 [1.426]] [[25.087]
 [26.231]
 [25.837]
 [24.01 ]
 [25.087]] [[2.666]
 [2.707]
 [2.704]
 [2.609]
 [2.666]]
maxi score, test score, baseline:  0.0721 0.3 0.3
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0010894178725337155
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.04996462816785501, 0.09372789260490938, 0.12290340222961228, 0.06455238298020646, 0.319838092196357, 0.34901360182105995]
actions average: 
K:  3  action  0 :  tensor([0.7820, 0.0010, 0.0022, 0.0886, 0.1262], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9990,     0.0002,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0016,     0.9753,     0.0038,     0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0518,     0.0002,     0.0088,     0.8176,     0.1215],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1791, 0.0033, 0.0108, 0.3144, 0.4924], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.04996462816785501, 0.09372789260490938, 0.12290340222961228, 0.06455238298020646, 0.319838092196357, 0.34901360182105995]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05008567852912453, 0.10004895915949973, 0.1214617937153748, 0.06436090156637458, 0.3141773047182506, 0.3498653623113759]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.568]
 [0.724]
 [0.568]
 [0.568]] [[47.978]
 [47.978]
 [58.407]
 [47.978]
 [47.978]] [[1.364]
 [1.364]
 [1.782]
 [1.364]
 [1.364]]
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05008567852912453, 0.10004895915949973, 0.1214617937153748, 0.06436090156637458, 0.3141773047182506, 0.3498653623113759]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05008567852912453, 0.10004895915949973, 0.1214617937153748, 0.06436090156637458, 0.3141773047182506, 0.3498653623113759]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.04973138294825829, 0.09934029119243153, 0.12060125186850579, 0.06390535673230778, 0.3190368848451989, 0.3473848324132979]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.962512810505505
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0493820777659057, 0.10567868043827097, 0.11975283110636227, 0.06345622843399701, 0.31679094045964074, 0.3449392417958233]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0493820777659057, 0.10567868043827097, 0.11975283110636227, 0.06345622843399701, 0.31679094045964074, 0.3449392417958233]
maxi score, test score, baseline:  0.0741 0.3 0.3
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0493820777659057, 0.10567868043827097, 0.11975283110636227, 0.06345622843399701, 0.31679094045964074, 0.3449392417958233]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.39072363000236
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.186628341674805
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.049037658281726786, 0.1049405533557576, 0.11891627712426532, 0.07000124393448834, 0.3145764098833732, 0.34252785742038866]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.258]
 [0.008]
 [0.169]
 [0.2  ]
 [0.163]] [[31.152]
 [26.67 ]
 [59.125]
 [34.992]
 [58.678]] [[0.539]
 [0.101]
 [1.626]
 [0.643]
 [1.602]]
printing an ep nov before normalisation:  76.78667388918342
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0761 0.3 0.3
maxi score, test score, baseline:  0.0761 0.3 0.3
probs:  [0.04960757793795606, 0.10029295656275859, 0.11477449331270217, 0.06408911468789963, 0.3247567761868842, 0.34647908131179955]
printing an ep nov before normalisation:  63.87599104638625
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0761 0.3 0.3
probs:  [0.050089672519346436, 0.10005124143962653, 0.11432597541684941, 0.06436440649656931, 0.32130961808658126, 0.349859086041027]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0761 0.3 0.3
probs:  [0.05055819191124841, 0.09981633271657737, 0.11389008723238561, 0.06463194642705668, 0.3179595277116056, 0.3531439140011263]
printing an ep nov before normalisation:  29.694986570336972
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0761 0.3 0.3
from probs:  [0.05055819191124841, 0.09981633271657737, 0.11389008723238561, 0.06463194642705668, 0.3179595277116056, 0.3531439140011263]
maxi score, test score, baseline:  0.0761 0.3 0.3
probs:  [0.05055819191124841, 0.09981633271657737, 0.11389008723238561, 0.06463194642705668, 0.3179595277116056, 0.3531439140011263]
maxi score, test score, baseline:  0.0761 0.3 0.3
probs:  [0.05055819191124841, 0.09981633271657737, 0.11389008723238561, 0.06463194642705668, 0.3179595277116056, 0.3531439140011263]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.05020555718569419, 0.09911922316770264, 0.1130945563054193, 0.07116855689226922, 0.31573688680231143, 0.3506752196466032]
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.05020555718569419, 0.09911922316770264, 0.1130945563054193, 0.07116855689226922, 0.31573688680231143, 0.3506752196466032]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
deleting a thread, now have 1 threads
Frames:  129844 train batches done:  15213 episodes:  8862
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.04963849870313683, 0.11040312437650808, 0.11040312437650808, 0.0698933739275939, 0.312951876621079, 0.34671000199517416]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.04930623538685849, 0.1096630286164741, 0.1163693389753203, 0.06942516646339703, 0.3108523393818596, 0.34438389117609053]
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.04930623538685849, 0.1096630286164741, 0.1163693389753203, 0.06942516646339703, 0.3108523393818596, 0.34438389117609053]
using another actor
from probs:  [0.04930623538685849, 0.1096630286164741, 0.1163693389753203, 0.06942516646339703, 0.3108523393818596, 0.34438389117609053]
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.04930623538685849, 0.1096630286164741, 0.1163693389753203, 0.06942516646339703, 0.3108523393818596, 0.34438389117609053]
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.04930623538685849, 0.1096630286164741, 0.1163693389753203, 0.06942516646339703, 0.3108523393818596, 0.34438389117609053]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.61178753417778
Printing some Q and Qe and total Qs values:  [[0.522]
 [0.042]
 [0.637]
 [0.522]
 [0.522]] [[41.337]
 [34.164]
 [48.899]
 [41.337]
 [41.337]] [[1.041]
 [0.396]
 [1.329]
 [1.041]
 [1.041]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.3 0.3
probs:  [0.048978402422552865, 0.10893280118842216, 0.1222560009141709, 0.06896320201117595, 0.30878079707465306, 0.34208879638902495]
maxi score, test score, baseline:  0.0801 0.3 0.3
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.278]
 [1.284]
 [1.103]
 [1.344]] [[30.338]
 [30.849]
 [30.497]
 [30.158]
 [29.442]] [[2.828]
 [3.126]
 [3.093]
 [2.874]
 [3.037]]
Starting evaluation
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0801 0.3 0.3
probs:  [0.048978402422552865, 0.10893280118842216, 0.1222560009141709, 0.06896320201117595, 0.30878079707465306, 0.34208879638902495]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.476]
 [0.052]
 [0.255]
 [0.408]] [[39.169]
 [50.12 ]
 [48.654]
 [36.956]
 [39.325]] [[0.396]
 [0.476]
 [0.052]
 [0.255]
 [0.408]]
printing an ep nov before normalisation:  35.93874818814283
printing an ep nov before normalisation:  69.84164266695112
printing an ep nov before normalisation:  44.25317567775177
maxi score, test score, baseline:  0.0801 0.3 0.3
probs:  [0.048978402422552865, 0.10893280118842216, 0.1222560009141709, 0.06896320201117595, 0.30878079707465306, 0.34208879638902495]
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.345]
 [0.746]
 [0.345]
 [0.345]] [[47.183]
 [47.183]
 [35.467]
 [47.183]
 [47.183]] [[0.345]
 [0.345]
 [0.746]
 [0.345]
 [0.345]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.162678718566895
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1181 0.3 0.3
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.11655062436682
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.9079004
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.909425
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.425103317523565
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9082118
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.18639138268088
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([    0.8723,     0.0003,     0.0000,     0.0538,     0.0736],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0044,     0.9901,     0.0016,     0.0000,     0.0039],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9774,     0.0030,     0.0194],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0114, 0.0013, 0.0458, 0.8417, 0.0999], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1299, 0.0039, 0.1156, 0.0631, 0.6875], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.8  ]
 [0.771]
 [0.771]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.771]
 [0.771]
 [0.8  ]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.990621089935303
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.929722785949707
siam score:  -0.9020554
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.9070555
printing an ep nov before normalisation:  29.88129855753357
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.23861285534862
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.7481, 0.0030, 0.0008, 0.0987, 0.1494], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9962,     0.0000,     0.0000,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0047,     0.9768,     0.0030,     0.0151],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0995,     0.0003,     0.0013,     0.7647,     0.1343],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1717, 0.0017, 0.0019, 0.3007, 0.5240], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.227620088064114
printing an ep nov before normalisation:  27.961872965581257
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.91081697
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.97715988212637
siam score:  -0.91311157
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.91293496
printing an ep nov before normalisation:  48.444052166966536
maxi score, test score, baseline:  0.1241 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]] [[34.766]
 [34.766]
 [34.766]
 [34.766]
 [34.766]] [[0.899]
 [0.899]
 [0.899]
 [0.899]
 [0.899]]
printing an ep nov before normalisation:  21.95448875427246
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.3  ]
 [1.353]
 [1.279]
 [1.155]
 [1.3  ]] [[18.33 ]
 [23.876]
 [19.461]
 [17.72 ]
 [18.33 ]] [[2.187]
 [2.508]
 [2.221]
 [2.012]
 [2.187]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [1.017]
 [0.754]
 [0.754]] [[45.658]
 [45.658]
 [58.657]
 [45.658]
 [45.658]] [[1.835]
 [1.835]
 [2.684]
 [1.835]
 [1.835]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.119]
 [1.069]
 [1.119]
 [1.119]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.119]
 [1.119]
 [1.069]
 [1.119]
 [1.119]]
maxi score, test score, baseline:  0.1261 1.0 1.0
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.907]
 [1.103]
 [0.275]
 [0.907]] [[17.797]
 [17.797]
 [23.986]
 [27.144]
 [17.797]] [[1.184]
 [1.184]
 [1.799]
 [1.185]
 [1.184]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.61966251674451
printing an ep nov before normalisation:  9.467240269066792
siam score:  -0.9038868
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.387023448944092
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.942313955803655
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 49.64880933620701
printing an ep nov before normalisation:  75.76916248828519
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.458 0.333 0.042 0.167]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.47927604516841
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.06863818961961
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  14.879923296111688
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.79033452336936
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.41595022827805
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.97267461136903
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.197]
 [0.653]
 [0.391]
 [0.215]
 [0.376]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.197]
 [0.653]
 [0.391]
 [0.215]
 [0.376]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.8415, 0.0223, 0.0009, 0.0020, 0.1334], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9922,     0.0053,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9798,     0.0053,     0.0147],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0752,     0.0004,     0.0009,     0.8134,     0.1102],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1175, 0.0468, 0.2015, 0.1401, 0.4941], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.8417, 0.0023, 0.0013, 0.0160, 0.1387], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9856,     0.0021,     0.0000,     0.0115],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0007,     0.0049,     0.9468,     0.0086,     0.0390],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0037,     0.0001,     0.0023,     0.9056,     0.0883],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1010, 0.0066, 0.0763, 0.3438, 0.4722], grad_fn=<DivBackward0>)
siam score:  -0.8855893
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.824]
 [0.774]
 [0.829]
 [0.829]] [[56.485]
 [43.696]
 [56.454]
 [56.485]
 [56.485]] [[0.829]
 [0.824]
 [0.774]
 [0.829]
 [0.829]]
printing an ep nov before normalisation:  40.2590617335565
printing an ep nov before normalisation:  23.350775708972467
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8798691
siam score:  -0.8796387
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.26345870478069
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8661,     0.0005,     0.0017,     0.0386,     0.0932],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9976,     0.0001,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9775,     0.0002,     0.0221],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0395,     0.0003,     0.0040,     0.8309,     0.1253],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1007, 0.0060, 0.0901, 0.1291, 0.6741], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.2994894529821
siam score:  -0.88192964
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.12 ]
 [1.12 ]
 [1.309]
 [1.12 ]
 [1.12 ]] [[20.559]
 [20.559]
 [22.167]
 [20.559]
 [20.559]] [[2.556]
 [2.556]
 [2.976]
 [2.556]
 [2.556]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.705]
 [0.509]
 [0.991]
 [0.906]
 [0.991]] [[24.642]
 [22.945]
 [18.367]
 [18.203]
 [19.848]] [[1.513]
 [1.209]
 [1.396]
 [1.301]
 [1.492]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1301 1.0 1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.89649063
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.034]
 [1.449]
 [1.41 ]
 [1.295]
 [1.26 ]] [[13.042]
 [11.51 ]
 [10.848]
 [ 9.587]
 [11.568]] [[1.367]
 [1.708]
 [1.636]
 [1.46 ]
 [1.522]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  44.562048168348866
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  79.78180427398966
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.688]
 [0.869]
 [0.347]
 [0.513]] [[40.363]
 [37.605]
 [55.06 ]
 [50.1  ]
 [39.902]] [[0.626]
 [1.22 ]
 [1.904]
 [1.239]
 [1.111]]
printing an ep nov before normalisation:  50.963887015190345
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.55361680310812
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.9088628
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.004360143821884321
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.044960021972656
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
actions average: 
K:  2  action  0 :  tensor([    0.8752,     0.0024,     0.0003,     0.0402,     0.0819],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0024,     0.9880,     0.0002,     0.0094],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0319, 0.0064, 0.0107, 0.9101, 0.0410], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1956, 0.0926, 0.0028, 0.1608, 0.5482], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.79251956939697
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.83977679018728
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  28.211820125579834
printing an ep nov before normalisation:  27.735233265073077
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.84015938106524
printing an ep nov before normalisation:  18.348886966705322
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89772904
maxi score, test score, baseline:  0.1341 1.0 1.0
printing an ep nov before normalisation:  68.83986773805701
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89534086
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1341 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.00568569236068
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8933535
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.292 0.292 0.167 0.125 0.125]
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.343661036282136
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
printing an ep nov before normalisation:  39.85113536965116
printing an ep nov before normalisation:  53.412939796272475
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  57.78714801082093
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.05889717772713
Printing some Q and Qe and total Qs values:  [[0.366]
 [0.366]
 [0.46 ]
 [0.366]
 [0.366]] [[32.406]
 [32.406]
 [35.39 ]
 [32.406]
 [32.406]] [[0.366]
 [0.366]
 [0.46 ]
 [0.366]
 [0.366]]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1401 1.0 1.0
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1401 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8851602
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.035]
 [1.035]
 [0.951]
 [1.035]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.035]
 [1.035]
 [1.035]
 [0.951]
 [1.035]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.21867885134616
printing an ep nov before normalisation:  34.87979653193719
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.34195358189872
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.88993895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.717]
 [1.135]
 [0.288]
 [0.053]
 [0.587]] [[53.053]
 [54.615]
 [71.95 ]
 [68.053]
 [69.504]] [[1.338]
 [1.787]
 [1.288]
 [0.975]
 [1.538]]
printing an ep nov before normalisation:  59.653394333852084
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.471]
 [1.471]
 [1.5  ]
 [1.471]
 [1.471]] [[30.475]
 [30.475]
 [26.559]
 [30.475]
 [30.475]] [[2.385]
 [2.385]
 [2.237]
 [2.385]
 [2.385]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.90679434678023
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.883030891418457
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.690374279841432
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.15688354718317
printing an ep nov before normalisation:  52.920304475305194
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  58.846413938197415
siam score:  -0.8739893
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.924202919006348
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([    0.8891,     0.0040,     0.0005,     0.0257,     0.0808],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0009,     0.9985,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9165,     0.0226,     0.0606],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0730, 0.0040, 0.0176, 0.7100, 0.1954], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3055, 0.0138, 0.2104, 0.0527, 0.4175], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.09993618250786
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9042,     0.0200,     0.0003,     0.0094,     0.0661],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9975,     0.0002,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.9805,     0.0062,     0.0129],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0211,     0.0000,     0.0070,     0.9253,     0.0466],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1225, 0.0779, 0.1404, 0.2336, 0.4257], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.59078598022461
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
printing an ep nov before normalisation:  29.95818113595333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8758115
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.88789440443793
printing an ep nov before normalisation:  53.51384450662697
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.420656074341611
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.286]
 [0.622]
 [0.286]
 [0.268]
 [0.286]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.286]
 [0.622]
 [0.286]
 [0.268]
 [0.286]]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.694]
 [0.791]
 [0.694]] [[28.473]
 [28.473]
 [28.473]
 [33.8  ]
 [28.473]] [[2.009]
 [2.009]
 [2.009]
 [2.53 ]
 [2.009]]
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.18659944325346
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8808086
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8814,     0.0027,     0.0002,     0.0515,     0.0642],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0018,     0.9445,     0.0003,     0.0003,     0.0531],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0189,     0.9419,     0.0059,     0.0329],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0260,     0.0004,     0.0324,     0.8157,     0.1256],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1838, 0.0949, 0.0148, 0.2065, 0.5001], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([    0.7957,     0.0013,     0.0003,     0.1064,     0.0963],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0038,     0.9923,     0.0002,     0.0000,     0.0037],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9707,     0.0004,     0.0287],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0661,     0.0002,     0.0032,     0.8159,     0.1146],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2236, 0.0256, 0.0092, 0.2988, 0.4428], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.50072715358012
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.906]
 [1.192]
 [0.906]
 [0.906]
 [0.909]] [[70.352]
 [47.854]
 [70.352]
 [70.352]
 [70.091]] [[1.906]
 [1.697]
 [1.906]
 [1.906]
 [1.903]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.782]
 [0.809]
 [0.852]
 [0.852]] [[30.736]
 [29.993]
 [31.457]
 [30.736]
 [30.736]] [[0.852]
 [0.782]
 [0.809]
 [0.852]
 [0.852]]
printing an ep nov before normalisation:  21.403282213776087
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.70721000212258
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.92]
 [0.92]
 [0.92]
 [0.92]
 [0.92]] [[49.456]
 [49.456]
 [49.456]
 [49.456]
 [49.456]] [[1.92]
 [1.92]
 [1.92]
 [1.92]
 [1.92]]
maxi score, test score, baseline:  0.1601 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.895]
 [0.96 ]
 [0.895]
 [0.895]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.895]
 [0.895]
 [0.96 ]
 [0.895]
 [0.895]]
printing an ep nov before normalisation:  21.38560591289802
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.964485478735734
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.41875551071725
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.88447106
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([    0.8194,     0.0008,     0.0003,     0.0542,     0.1252],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0163,     0.9825,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0002,     0.9429,     0.0154,     0.0413],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0253,     0.0005,     0.0022,     0.8489,     0.1231],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1454, 0.0496, 0.0711, 0.1564, 0.5775], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.28201466882114
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  58.46935807770114
printing an ep nov before normalisation:  65.56759182878216
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.543]
 [1.232]
 [0.543]
 [0.543]] [[54.067]
 [54.067]
 [49.126]
 [54.067]
 [54.067]] [[0.755]
 [0.755]
 [1.406]
 [0.755]
 [0.755]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 27.36862924038362
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]] [[19.223]
 [19.223]
 [19.223]
 [19.223]
 [19.223]] [[2.113]
 [2.113]
 [2.113]
 [2.113]
 [2.113]]
siam score:  -0.87813735
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.849569495622333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8968307
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.376]
 [1.415]
 [1.379]
 [1.376]
 [1.376]] [[14.315]
 [18.333]
 [14.98 ]
 [14.315]
 [14.315]] [[2.059]
 [2.446]
 [2.119]
 [2.059]
 [2.059]]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.11507498725111
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  22.450964082700242
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.136]
 [0.145]
 [0.137]
 [0.136]] [[42.159]
 [42.159]
 [42.178]
 [42.685]
 [42.159]] [[1.367]
 [1.367]
 [1.377]
 [1.397]
 [1.367]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  13.849560155751481
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8925209
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([    0.6394,     0.0009,     0.0004,     0.1125,     0.2467],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0487,     0.9395,     0.0000,     0.0000,     0.0117],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0001,     0.9863,     0.0023,     0.0111],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0686, 0.0012, 0.0010, 0.7726, 0.1567], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.1793,     0.0005,     0.0608,     0.2248,     0.5345],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.50927543640137
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.208420943243873
printing an ep nov before normalisation:  14.905418240952415
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.253]
 [1.091]
 [1.091]
 [1.091]] [[72.874]
 [61.271]
 [72.874]
 [72.874]
 [72.874]] [[2.044]
 [1.971]
 [2.044]
 [2.044]
 [2.044]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.086848820605608
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.877559741159665
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.272483888097394
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
printing an ep nov before normalisation:  23.575349950730857
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.29787826538086
printing an ep nov before normalisation:  45.20876841849788
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.47939310397306
siam score:  -0.8928864
siam score:  -0.8931261
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.16975204982034
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.7631,     0.0006,     0.0010,     0.0824,     0.1529],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9960,     0.0001,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0001,     0.9482,     0.0041,     0.0471],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0000,     0.0003,     0.9427,     0.0566],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1313, 0.0006, 0.0337, 0.3893, 0.4451], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.53271838217255
printing an ep nov before normalisation:  38.790622305574146
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.307]
 [0.307]
 [0.761]
 [0.307]
 [0.307]] [[33.307]
 [33.307]
 [36.349]
 [33.307]
 [33.307]] [[0.307]
 [0.307]
 [0.761]
 [0.307]
 [0.307]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.12285532582958
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.80108758195738
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7864,     0.0004,     0.0007,     0.0879,     0.1245],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9953,     0.0034,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9828,     0.0003,     0.0168],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0334,     0.0000,     0.0013,     0.9134,     0.0518],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2181, 0.0074, 0.0059, 0.2393, 0.5293], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.005]
 [1.069]
 [1.171]
 [1.112]
 [1.051]] [[19.559]
 [16.12 ]
 [12.268]
 [16.473]
 [18.644]] [[2.376]
 [2.199]
 [2.031]
 [2.267]
 [2.358]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.212]
 [0.212]
 [0.212]] [[13.741]
 [13.741]
 [13.741]
 [13.741]
 [13.741]] [[1.221]
 [1.221]
 [1.221]
 [1.221]
 [1.221]]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.89369035
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.039]
 [0.991]
 [1.039]
 [1.039]] [[48.275]
 [48.275]
 [53.373]
 [48.275]
 [48.275]] [[2.002]
 [2.002]
 [2.121]
 [2.002]
 [2.002]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 22.059704896059433
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  68.04924071784096
using explorer policy with actor:  1
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.154]
 [0.154]
 [0.154]
 [0.154]
 [0.154]]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.440752529833716
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.74436196946997
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  9.937117099761963
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.1824716630637
siam score:  -0.8885029
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  18.203382751628173
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  23.355392433726003
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.803104543885265
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.891017126922097
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.89330095
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.784197499995933
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.33342957528848
Printing some Q and Qe and total Qs values:  [[1.174]
 [1.253]
 [1.174]
 [1.174]
 [1.174]] [[24.3  ]
 [26.719]
 [26.923]
 [24.3  ]
 [24.3  ]] [[1.903]
 [2.12 ]
 [2.053]
 [1.903]
 [1.903]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.76423557275155
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.918]
 [0.628]
 [0.628]
 [0.628]] [[30.127]
 [56.356]
 [30.127]
 [30.127]
 [30.127]] [[0.747]
 [1.161]
 [0.747]
 [0.747]
 [0.747]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.803681773500166
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2261 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2261 1.0 1.0
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.532119212690703
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.76922699595061
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.733350877982694
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.194]
 [1.194]
 [1.194]
 [1.194]
 [1.194]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.029]
 [0.918]
 [1.244]
 [1.029]
 [1.029]] [[41.747]
 [29.237]
 [46.225]
 [41.747]
 [41.747]] [[2.194]
 [1.472]
 [2.628]
 [2.194]
 [2.194]]
printing an ep nov before normalisation:  49.21386488982538
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.396]
 [1.429]
 [1.409]
 [1.373]
 [1.41 ]] [[22.675]
 [28.393]
 [36.015]
 [23.058]
 [23.666]] [[1.902]
 [2.225]
 [2.591]
 [1.898]
 [1.967]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.923]
 [1.007]
 [0.923]
 [0.923]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.923]
 [0.923]
 [1.007]
 [0.923]
 [0.923]]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.863933428978974
actions average: 
K:  4  action  0 :  tensor([0.6613, 0.0097, 0.0017, 0.1805, 0.1468], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0019,     0.9827,     0.0003,     0.0077,     0.0074],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0005,     0.9747,     0.0130,     0.0118],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0236,     0.0000,     0.0007,     0.9464,     0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1814, 0.0049, 0.0738, 0.2087, 0.5313], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.48754610098755
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8975067
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]] [[35.497]
 [35.497]
 [35.497]
 [35.497]
 [35.497]] [[1.366]
 [1.366]
 [1.366]
 [1.366]
 [1.366]]
printing an ep nov before normalisation:  41.274197069546005
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.454]
 [1.485]
 [1.402]
 [1.454]
 [1.454]] [[11.953]
 [12.313]
 [12.069]
 [11.953]
 [11.953]] [[1.722]
 [1.76 ]
 [1.672]
 [1.722]
 [1.722]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.772043704986572
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.]
 [0.]
 [0.]
 [0.]
 [1.]] [[9.045]
 [0.003]
 [0.003]
 [0.003]
 [9.045]] [[1.]
 [0.]
 [0.]
 [0.]
 [1.]]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.15489149093628
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8862199
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.667 0.042 0.083]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8831476
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.984764099121094
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.218208931268551
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.18620248019686
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.54067897796631
printing an ep nov before normalisation:  11.835914850234985
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8915251
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
printing an ep nov before normalisation:  12.028756111215161
Printing some Q and Qe and total Qs values:  [[1.36 ]
 [1.322]
 [1.215]
 [1.36 ]
 [1.36 ]] [[10.375]
 [23.93 ]
 [22.907]
 [10.375]
 [10.375]] [[1.554]
 [2.275]
 [2.11 ]
 [1.554]
 [1.554]]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.6386,     0.0041,     0.0002,     0.1888,     0.1683],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9960,     0.0008,     0.0000,     0.0031],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9832,     0.0002,     0.0165],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0413,     0.0002,     0.0366,     0.8652,     0.0567],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1031, 0.2437, 0.0126, 0.2296, 0.4110], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.216]
 [1.29 ]
 [1.216]
 [1.216]
 [1.216]] [[51.26 ]
 [46.809]
 [51.26 ]
 [51.26 ]
 [51.26 ]] [[1.897]
 [1.883]
 [1.897]
 [1.897]
 [1.897]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.0440673828125
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.733]
 [0.447]
 [0.447]
 [0.447]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.447]
 [0.733]
 [0.447]
 [0.447]
 [0.447]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.191]
 [1.125]
 [1.241]
 [1.037]
 [1.191]] [[26.599]
 [24.338]
 [27.026]
 [30.635]
 [26.599]] [[2.232]
 [2.003]
 [2.313]
 [2.37 ]
 [2.232]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8769776
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.5625032810726
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.975]
 [0.981]
 [0.975]
 [0.975]
 [0.741]] [[64.781]
 [47.439]
 [64.781]
 [64.781]
 [65.214]] [[2.293]
 [1.712]
 [2.293]
 [2.293]
 [2.074]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  53.35809478972255
printing an ep nov before normalisation:  73.72294571501712
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.994419742163544
printing an ep nov before normalisation:  56.857656598234556
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
siam score:  -0.8733597
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.869142
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.273958411307575
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.12353094495057
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  15.807636837187882
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([    0.8795,     0.0001,     0.0001,     0.0497,     0.0706],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0013,     0.9970,     0.0002,     0.0001,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0001,     0.9779,     0.0007,     0.0211],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0265,     0.0008,     0.0365,     0.8250,     0.1113],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3466, 0.0009, 0.1462, 0.0074, 0.4989], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.8329,     0.0606,     0.0003,     0.0242,     0.0820],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0085,     0.9633,     0.0005,     0.0021,     0.0256],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0014,     0.9782,     0.0006,     0.0195],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0400,     0.0001,     0.0881,     0.7617,     0.1100],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0006, 0.0019, 0.3349, 0.3459, 0.3167], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8682753
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.939]
 [0.911]
 [0.863]
 [0.863]] [[15.612]
 [17.464]
 [17.089]
 [18.188]
 [18.188]] [[0.73 ]
 [0.939]
 [0.911]
 [0.863]
 [0.863]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2521 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.182]
 [1.151]
 [1.151]
 [1.151]] [[70.268]
 [50.88 ]
 [70.268]
 [70.268]
 [70.268]] [[2.484]
 [1.943]
 [2.484]
 [2.484]
 [2.484]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.06682505396852
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [1.266]
 [0.97 ]
 [0.97 ]
 [0.92 ]] [[58.22 ]
 [47.373]
 [58.22 ]
 [58.22 ]
 [58.265]] [[2.967]
 [2.639]
 [2.967]
 [2.967]
 [2.92 ]]
actions average: 
K:  4  action  0 :  tensor([    0.7321,     0.0467,     0.0005,     0.1091,     0.1116],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9985,     0.0000,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0123,     0.9574,     0.0008,     0.0292],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0011,     0.0000,     0.0561,     0.9174,     0.0254],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1294, 0.0034, 0.1548, 0.1820, 0.5304], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.269]
 [1.251]
 [1.269]
 [1.269]] [[42.696]
 [42.696]
 [50.872]
 [42.696]
 [42.696]] [[2.361]
 [2.361]
 [2.802]
 [2.361]
 [2.361]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.402]
 [1.402]
 [1.363]
 [1.402]
 [1.402]] [[41.445]
 [41.445]
 [45.233]
 [41.445]
 [41.445]] [[3.043]
 [3.043]
 [3.239]
 [3.043]
 [3.043]]
siam score:  -0.8659613
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.9096,     0.0022,     0.0000,     0.0315,     0.0567],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0063,     0.9694,     0.0005,     0.0008,     0.0230],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0018,     0.9399,     0.0039,     0.0541],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0000,     0.0499,     0.8958,     0.0540],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1320, 0.0235, 0.2091, 0.1393, 0.4960], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.279]
 [1.341]
 [1.279]
 [1.279]
 [1.279]] [[24.04 ]
 [26.651]
 [24.04 ]
 [24.04 ]
 [24.04 ]] [[2.472]
 [2.765]
 [2.472]
 [2.472]
 [2.472]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8631306
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.800484476776113
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.217123384615086
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.79817464863147
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.714]
 [0.714]
 [0.888]
 [0.714]
 [0.714]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.714]
 [0.714]
 [0.888]
 [0.714]
 [0.714]]
printing an ep nov before normalisation:  53.944912060651426
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.811]
 [0.817]
 [0.778]
 [0.811]] [[32.872]
 [50.94 ]
 [55.43 ]
 [59.916]
 [50.94 ]] [[1.163]
 [1.856]
 [2.006]
 [2.11 ]
 [1.856]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.2408392406404
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.71084926038908
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86815625
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.389933641302662
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.99606227874756
printing an ep nov before normalisation:  39.45840402495773
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.77 ]
 [0.738]
 [0.77 ]
 [0.77 ]] [[54.935]
 [54.935]
 [56.102]
 [54.935]
 [54.935]] [[0.77 ]
 [0.77 ]
 [0.738]
 [0.77 ]
 [0.77 ]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.646516075765106
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8772585
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8768673
printing an ep nov before normalisation:  50.20256096459252
printing an ep nov before normalisation:  34.08344268798828
actions average: 
K:  2  action  0 :  tensor([    0.8117,     0.0048,     0.0007,     0.0339,     0.1489],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0068,     0.9865,     0.0002,     0.0004,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0003,     0.9743,     0.0007,     0.0247],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0357,     0.0001,     0.0012,     0.8573,     0.1056],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1882, 0.0063, 0.1006, 0.1497, 0.5552], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.846557436492944
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8772412
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.85404019625027
printing an ep nov before normalisation:  31.61020326968437
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.87526476
maxi score, test score, baseline:  0.2681 1.0 1.0
siam score:  -0.87644035
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.87301666
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.19689241001107
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.72889932235904
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2701 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.129]
 [0.923]
 [0.969]
 [0.989]
 [1.081]] [[59.586]
 [58.943]
 [63.484]
 [59.333]
 [59.419]] [[2.003]
 [1.781]
 [1.936]
 [1.857]
 [1.95 ]]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  63.8194542555864
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.175]
 [1.175]
 [1.175]
 [1.175]
 [1.175]] [[24.94]
 [24.94]
 [24.94]
 [24.94]
 [24.94]] [[2.151]
 [2.151]
 [2.151]
 [2.151]
 [2.151]]
printing an ep nov before normalisation:  21.375351739026225
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2701 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.882706
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.919]
 [0.855]
 [0.919]
 [0.919]] [[39.856]
 [39.856]
 [54.582]
 [39.856]
 [39.856]] [[1.446]
 [1.446]
 [1.752]
 [1.446]
 [1.446]]
siam score:  -0.88402236
printing an ep nov before normalisation:  20.779213939832765
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.5533192952474
maxi score, test score, baseline:  0.2741 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.25028097796045756
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.658]
 [0.839]
 [0.754]] [[47.047]
 [47.047]
 [40.532]
 [50.232]
 [47.047]] [[1.548]
 [1.548]
 [1.248]
 [1.732]
 [1.548]]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.712]
 [1.098]
 [1.054]
 [1.054]
 [0.87 ]] [[34.029]
 [35.924]
 [37.668]
 [37.668]
 [45.688]] [[1.519]
 [1.99 ]
 [2.025]
 [2.025]
 [2.203]]
maxi score, test score, baseline:  0.2781 1.0 1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
printing an ep nov before normalisation:  34.89801367294975
printing an ep nov before normalisation:  13.754361867904663
printing an ep nov before normalisation:  16.986348925876563
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  23.679719163236765
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8663647
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.816]
 [0.874]
 [0.927]
 [0.756]
 [0.756]] [[35.195]
 [37.573]
 [55.944]
 [57.203]
 [51.409]] [[1.27 ]
 [1.421]
 [2.184]
 [2.062]
 [1.838]]
printing an ep nov before normalisation:  22.170041141655474
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8708,     0.0004,     0.0000,     0.0423,     0.0864],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9885,     0.0036,     0.0004,     0.0070],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0017,     0.9647,     0.0187,     0.0148],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0409,     0.0013,     0.0004,     0.7744,     0.1830],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1703, 0.0005, 0.1532, 0.1892, 0.4867], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.04040556183327
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.416]
 [1.416]
 [1.416]
 [1.416]
 [1.416]] [[21.481]
 [21.481]
 [21.481]
 [21.481]
 [21.481]] [[2.083]
 [2.083]
 [2.083]
 [2.083]
 [2.083]]
maxi score, test score, baseline:  0.2801 1.0 1.0
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.75509581525958
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86686873
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.86475843
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.31815436573144
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.30527165778363
printing an ep nov before normalisation:  83.32217253819891
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8670027
actions average: 
K:  3  action  0 :  tensor([0.7795, 0.0222, 0.0397, 0.0605, 0.0980], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9869,     0.0001,     0.0000,     0.0128],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0008,     0.9790,     0.0005,     0.0197],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0171,     0.0003,     0.0009,     0.8395,     0.1422],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1234, 0.0008, 0.0980, 0.2755, 0.5023], grad_fn=<DivBackward0>)
siam score:  -0.8674686
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.339]
 [1.309]
 [1.309]
 [1.309]] [[36.736]
 [33.329]
 [36.736]
 [36.736]
 [36.736]] [[1.912]
 [1.859]
 [1.912]
 [1.912]
 [1.912]]
siam score:  -0.8690517
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.44653919107336
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.012011896489525498
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.865752
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.808725635943876
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.8880,     0.0004,     0.0002,     0.0576,     0.0538],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9956,     0.0004,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9858,     0.0003,     0.0138],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0362,     0.0006,     0.0015,     0.8462,     0.1154],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1270, 0.0015, 0.2234, 0.1541, 0.4939], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.9210,     0.0002,     0.0002,     0.0220,     0.0565],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0023,     0.9905,     0.0030,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9621,     0.0073,     0.0305],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0226,     0.0004,     0.0078,     0.8242,     0.1450],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2527, 0.0009, 0.1372, 0.2133, 0.3959], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
printing an ep nov before normalisation:  26.77180957187536
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([0.8208, 0.0010, 0.0016, 0.0622, 0.1144], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0070,     0.9907,     0.0002,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9528,     0.0320,     0.0151],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0643,     0.0003,     0.0008,     0.7711,     0.1635],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1010,     0.0004,     0.1546,     0.1215,     0.6226],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.752457092175966
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.84885883331299
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8727977
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.87082183
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.366234302520752
printing an ep nov before normalisation:  17.27979302406311
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.56421321342536
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.003]
 [0.319]
 [0.371]
 [0.404]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.467]
 [0.003]
 [0.319]
 [0.371]
 [0.404]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.83455774465352
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2981 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  79.988754836493
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.768410516654434
printing an ep nov before normalisation:  52.383565300952974
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2981 1.0 1.0
printing an ep nov before normalisation:  35.98358392715454
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.0635341995337
Printing some Q and Qe and total Qs values:  [[0.668]
 [0.744]
 [0.668]
 [0.668]
 [0.668]] [[71.658]
 [57.581]
 [71.658]
 [71.658]
 [71.658]] [[0.668]
 [0.744]
 [0.668]
 [0.668]
 [0.668]]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.524]
 [0.524]
 [0.784]
 [0.524]
 [0.524]] [[59.463]
 [59.463]
 [59.909]
 [59.463]
 [59.463]] [[1.036]
 [1.036]
 [1.302]
 [1.036]
 [1.036]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  75.60716622347587
Printing some Q and Qe and total Qs values:  [[1.135]
 [1.248]
 [1.135]
 [1.135]
 [1.135]] [[37.304]
 [43.494]
 [37.304]
 [37.304]
 [37.304]] [[2.655]
 [3.248]
 [2.655]
 [2.655]
 [2.655]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([    0.8939,     0.0051,     0.0002,     0.0049,     0.0959],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9900,     0.0000,     0.0000,     0.0091],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0003,     0.9548,     0.0005,     0.0440],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0411,     0.0002,     0.0065,     0.8111,     0.1410],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0859, 0.0066, 0.1325, 0.1350, 0.6400], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.639749315450196
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3021 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.460134375949664
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3041 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.741521341084244
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.05540752410889
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.00847473744165
printing an ep nov before normalisation:  18.878421783447266
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  58.166506805967146
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  10.057333757448568
siam score:  -0.86328346
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3101 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.458 0.292 0.042]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.961]
 [0.873]
 [1.031]
 [0.961]] [[35.91 ]
 [35.91 ]
 [34.505]
 [49.595]
 [35.91 ]] [[1.422]
 [1.422]
 [1.306]
 [1.762]
 [1.422]]
Printing some Q and Qe and total Qs values:  [[1.148]
 [1.225]
 [1.148]
 [1.148]
 [1.035]] [[41.864]
 [39.145]
 [41.864]
 [41.864]
 [43.567]] [[1.911]
 [1.904]
 [1.911]
 [1.911]
 [1.851]]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
printing an ep nov before normalisation:  39.7682484056113
printing an ep nov before normalisation:  19.97912283396263
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.209]
 [1.218]
 [1.2  ]
 [1.209]
 [1.209]] [[50.458]
 [54.707]
 [55.181]
 [50.458]
 [50.458]] [[2.655]
 [2.841]
 [2.843]
 [2.655]
 [2.655]]
printing an ep nov before normalisation:  67.65258378091525
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.236]
 [1.189]
 [1.179]
 [1.147]] [[73.37 ]
 [49.982]
 [62.167]
 [73.37 ]
 [68.088]] [[1.502]
 [1.422]
 [1.446]
 [1.502]
 [1.439]]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3101 1.0 1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0009000412183013395
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3101 1.0 1.0
printing an ep nov before normalisation:  12.69667387008667
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.760970567367345
printing an ep nov before normalisation:  36.177796407525385
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.774]
 [0.869]
 [0.795]
 [0.795]] [[24.21 ]
 [27.612]
 [28.685]
 [24.21 ]
 [24.21 ]] [[0.795]
 [0.774]
 [0.869]
 [0.795]
 [0.795]]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.125 0.333 0.042 0.458]
printing an ep nov before normalisation:  11.359222695584464
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.748]
 [0.748]
 [1.014]
 [0.748]] [[16.805]
 [16.805]
 [16.805]
 [29.238]
 [16.805]] [[0.923]
 [0.923]
 [0.923]
 [1.408]
 [0.923]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.43687856356995
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.37181821475114
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.903]
 [0.939]
 [0.903]
 [0.903]] [[46.421]
 [46.421]
 [64.878]
 [46.421]
 [46.421]] [[1.543]
 [1.543]
 [1.929]
 [1.543]
 [1.543]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
siam score:  -0.8625799
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  86.68255418773052
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.632]
 [0.705]
 [0.632]
 [0.632]
 [0.632]] [[83.845]
 [54.758]
 [83.845]
 [83.845]
 [83.845]] [[0.632]
 [0.705]
 [0.632]
 [0.632]
 [0.632]]
printing an ep nov before normalisation:  43.21011050424872
printing an ep nov before normalisation:  55.87117479821342
printing an ep nov before normalisation:  31.195521135784773
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.00012318385671505894
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0035934280913352268
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0023494974129789625
printing an ep nov before normalisation:  54.466457615095514
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  71.99493057588393
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.6476, 0.0030, 0.0012, 0.1260, 0.2222], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9990,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0002,     0.9718,     0.0057,     0.0218],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0503,     0.0004,     0.0002,     0.8941,     0.0550],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1329, 0.0025, 0.0473, 0.1825, 0.6349], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.459638867559356
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8718474
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
siam score:  -0.87058145
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  68.54380591336809
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.344]
 [1.468]
 [1.332]
 [1.344]
 [1.344]] [[25.533]
 [46.814]
 [42.4  ]
 [25.533]
 [25.533]] [[1.666]
 [2.057]
 [1.866]
 [1.666]
 [1.666]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 83.29159146408782
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.88609981536865
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.573208332061768
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.319]
 [1.319]
 [1.319]
 [1.319]
 [1.319]] [[30.054]
 [30.054]
 [30.054]
 [30.054]
 [30.054]] [[2.319]
 [2.319]
 [2.319]
 [2.319]
 [2.319]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3681 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.103]
 [1.483]
 [1.174]
 [1.101]
 [1.161]] [[21.805]
 [15.733]
 [21.675]
 [19.967]
 [20.393]] [[1.137]
 [2.228]
 [2.201]
 [2.048]
 [2.127]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3701 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.34250851055131
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  59.30873550577363
siam score:  -0.86453325
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.99140232334382
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.837933094022496
line 256 mcts: sample exp_bonus 37.20280697516582
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.97154855601315
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.576]
 [0.718]
 [0.576]] [[35.286]
 [35.286]
 [35.286]
 [51.56 ]
 [35.286]] [[0.576]
 [0.576]
 [0.576]
 [0.718]
 [0.576]]
printing an ep nov before normalisation:  69.26483399790307
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  66.86046384017662
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.6742,     0.0003,     0.0015,     0.1493,     0.1747],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0017,     0.9930,     0.0003,     0.0006,     0.0044],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0002,     0.9726,     0.0004,     0.0266],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0379,     0.0001,     0.0912,     0.8013,     0.0695],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1086, 0.0020, 0.0666, 0.1377, 0.6851], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.879]
 [0.879]
 [1.053]
 [0.879]] [[51.399]
 [51.399]
 [51.399]
 [63.74 ]
 [51.399]] [[1.452]
 [1.452]
 [1.452]
 [1.822]
 [1.452]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.101]
 [1.246]
 [1.101]
 [1.101]] [[30.52 ]
 [30.52 ]
 [31.658]
 [30.52 ]
 [30.52 ]] [[2.377]
 [2.377]
 [2.613]
 [2.377]
 [2.377]]
maxi score, test score, baseline:  0.3761 1.0 1.0
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.063]
 [1.063]
 [1.118]
 [1.063]
 [1.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.063]
 [1.063]
 [1.118]
 [1.063]
 [1.063]]
Printing some Q and Qe and total Qs values:  [[1.207]
 [1.254]
 [1.207]
 [1.207]
 [1.207]] [[60.96 ]
 [43.685]
 [60.96 ]
 [60.96 ]
 [60.96 ]] [[1.426]
 [1.379]
 [1.426]
 [1.426]
 [1.426]]
maxi score, test score, baseline:  0.3781 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  0.016978544976780086
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8534848
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.109]
 [1.097]
 [1.109]
 [1.109]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.109]
 [1.109]
 [1.097]
 [1.109]
 [1.109]]
printing an ep nov before normalisation:  57.569558704131985
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.003031223062955
printing an ep nov before normalisation:  8.187314830382775
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.75612513054691
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.709]
 [0.844]
 [0.709]
 [0.709]
 [0.829]] [[ 0.   ]
 [42.614]
 [ 0.   ]
 [ 0.   ]
 [17.563]] [[0.669]
 [0.996]
 [0.669]
 [0.669]
 [0.868]]
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.363166991054825
printing an ep nov before normalisation:  0.014213784760386261
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.725635332987515
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.99099821154329
printing an ep nov before normalisation:  21.388199887145365
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.833175659179688
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.293789863586426
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([    0.7595,     0.0187,     0.0006,     0.0788,     0.1425],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0075,     0.9800,     0.0063,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9726,     0.0032,     0.0239],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0323, 0.0027, 0.0194, 0.8499, 0.0957], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0004,     0.0000,     0.0030,     0.4879,     0.5086],
       grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([    0.8133,     0.0004,     0.0003,     0.0525,     0.1335],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9998,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9778,     0.0010,     0.0208],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1176,     0.0008,     0.0005,     0.7608,     0.1202],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2869, 0.0035, 0.0659, 0.1648, 0.4790], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9120,     0.0011,     0.0000,     0.0301,     0.0568],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9974,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0002,     0.9442,     0.0038,     0.0514],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0000,     0.0004,     0.9056,     0.0935],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0513, 0.0008, 0.2309, 0.0471, 0.6698], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.009863036844848239
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.8629, 0.0048, 0.0048, 0.0138, 0.1138], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9938,     0.0001,     0.0000,     0.0061],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9501,     0.0266,     0.0231],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0046,     0.0161,     0.8964,     0.0822],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1319, 0.0048, 0.0029, 0.1059, 0.7545], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.160514808275138
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
siam score:  -0.8570835
maxi score, test score, baseline:  0.3881 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8559166
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]] [[77.557]
 [77.557]
 [77.557]
 [77.557]
 [77.557]] [[2.022]
 [2.022]
 [2.022]
 [2.022]
 [2.022]]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  51.849450336752504
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.20915969639295
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  73.01647685057398
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  15.405274629592896
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  19.783244132995605
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.91209286901437
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.8801,     0.0002,     0.0006,     0.0055,     0.1135],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0127,     0.9829,     0.0009,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0001,     0.9569,     0.0020,     0.0404],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0614,     0.0002,     0.0007,     0.8677,     0.0700],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0469, 0.0011, 0.3391, 0.0632, 0.5498], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.856]
 [1.023]
 [0.856]
 [0.856]
 [0.856]] [[37.553]
 [40.735]
 [37.553]
 [37.553]
 [37.553]] [[1.013]
 [1.199]
 [1.013]
 [1.013]
 [1.013]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.635547637939453
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8234,     0.0005,     0.0014,     0.0475,     0.1272],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9903,     0.0011,     0.0001,     0.0077],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9810,     0.0006,     0.0183],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0025,     0.0005,     0.0618,     0.8135,     0.1217],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2303, 0.0011, 0.0028, 0.1735, 0.5923], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([0.7861, 0.0020, 0.0011, 0.0455, 0.1653], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0022,     0.9917,     0.0003,     0.0000,     0.0058],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0008,     0.9728,     0.0045,     0.0217],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0001,     0.0281,     0.8810,     0.0901],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1341, 0.0084, 0.0769, 0.2521, 0.5285], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.33737387401035
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.84693728861266
actions average: 
K:  3  action  0 :  tensor([    0.8532,     0.0014,     0.0004,     0.0239,     0.1212],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9981,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9853,     0.0007,     0.0139],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0328,     0.0000,     0.0017,     0.9401,     0.0254],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1382, 0.0037, 0.1128, 0.1616, 0.5836], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.961]
 [0.961]
 [0.961]
 [0.961]
 [0.961]]
siam score:  -0.85984606
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.659328990506324
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.014]
 [1.014]
 [1.186]
 [1.014]
 [1.014]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.014]
 [1.014]
 [1.186]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.951]
 [0.951]
 [0.951]
 [1.056]
 [0.951]] [[32.296]
 [32.296]
 [32.296]
 [43.145]
 [32.296]] [[2.17 ]
 [2.17 ]
 [2.17 ]
 [3.056]
 [2.17 ]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8599588
printing an ep nov before normalisation:  49.52094040855826
printing an ep nov before normalisation:  30.101454932578335
printing an ep nov before normalisation:  25.84945998177009
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [1.1  ]
 [0.977]
 [1.1  ]
 [1.1  ]] [[47.025]
 [47.025]
 [49.086]
 [47.025]
 [47.025]] [[2.398]
 [2.398]
 [2.361]
 [2.398]
 [2.398]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.190793324940564
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8626678
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.006101131439209
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.303325176239014
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
printing an ep nov before normalisation:  57.013129037587376
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.04774249584496
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.215]
 [1.348]
 [1.345]
 [0.027]
 [1.303]] [[20.996]
 [27.724]
 [24.88 ]
 [31.666]
 [26.63 ]] [[1.563]
 [1.928]
 [1.827]
 [0.743]
 [1.845]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.23539897186872
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.021889510681454283
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]] [[60.494]
 [60.494]
 [60.494]
 [60.494]
 [60.494]] [[0.833]
 [0.833]
 [0.833]
 [0.833]
 [0.833]]
printing an ep nov before normalisation:  0.04120216241858543
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.786]
 [0.783]
 [0.783]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.783]
 [0.783]
 [0.786]
 [0.783]
 [0.783]]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8689535
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9629,     0.0001,     0.0004,     0.0023,     0.0343],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0027,     0.9548,     0.0194,     0.0000,     0.0231],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9629,     0.0023,     0.0347],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0000,     0.0038,     0.9953,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0875, 0.0051, 0.1656, 0.1656, 0.5761], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.005356455011167327
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  67.38224471439268
printing an ep nov before normalisation:  22.429986000061035
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.768512964248657
printing an ep nov before normalisation:  62.84548887382549
Printing some Q and Qe and total Qs values:  [[0.96 ]
 [0.96 ]
 [1.183]
 [0.96 ]
 [0.96 ]] [[49.201]
 [49.201]
 [69.341]
 [49.201]
 [49.201]] [[1.094]
 [1.094]
 [1.399]
 [1.094]
 [1.094]]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.1020031248368
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.8434, 0.0012, 0.0037, 0.0702, 0.0815], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0040,     0.9841,     0.0023,     0.0000,     0.0096],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0014,     0.9863,     0.0011,     0.0112],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0469,     0.0001,     0.0021,     0.8744,     0.0765],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1024, 0.0011, 0.2030, 0.0815, 0.6120], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.491]
 [0.491]
 [0.272]
 [0.491]
 [0.491]] [[53.296]
 [53.296]
 [70.05 ]
 [53.296]
 [53.296]] [[1.821]
 [1.821]
 [2.127]
 [1.821]
 [1.821]]
printing an ep nov before normalisation:  78.72461461080586
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  64.67454349422302
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.11472641184152
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.588975019815273
Printing some Q and Qe and total Qs values:  [[0.939]
 [1.207]
 [1.189]
 [1.189]
 [1.027]] [[60.901]
 [45.913]
 [60.943]
 [60.943]
 [62.569]] [[2.219]
 [2.114]
 [2.47 ]
 [2.47 ]
 [2.348]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.269]
 [1.094]
 [1.115]
 [1.117]] [[25.3  ]
 [23.149]
 [29.329]
 [34.663]
 [30.349]] [[1.728]
 [1.783]
 [1.913]
 [2.198]
 [1.987]]
printing an ep nov before normalisation:  68.1890678328212
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.993]
 [0.993]
 [0.998]
 [0.993]
 [0.993]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.993]
 [0.993]
 [0.998]
 [0.993]
 [0.993]]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7201,     0.0006,     0.0009,     0.0629,     0.2155],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0212,     0.9724,     0.0024,     0.0001,     0.0039],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0002,     0.9613,     0.0008,     0.0374],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0641,     0.0004,     0.0004,     0.8187,     0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1496, 0.0019, 0.1069, 0.2009, 0.5407], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.9094762802124
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.185]
 [1.364]
 [1.185]
 [1.185]
 [1.185]] [[26.881]
 [24.149]
 [26.881]
 [26.881]
 [26.881]] [[3.616]
 [3.364]
 [3.616]
 [3.616]
 [3.616]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.896]
 [1.013]
 [0.896]
 [0.896]] [[23.914]
 [23.914]
 [42.109]
 [23.914]
 [23.914]] [[1.385]
 [1.385]
 [2.21 ]
 [1.385]
 [1.385]]
printing an ep nov before normalisation:  63.57418241522674
maxi score, test score, baseline:  0.4021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.   ]
 [1.023]
 [1.   ]
 [1.   ]] [[29.075]
 [29.075]
 [26.608]
 [29.075]
 [29.075]] [[2.333]
 [2.333]
 [2.243]
 [2.333]
 [2.333]]
printing an ep nov before normalisation:  16.39560045692559
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4041 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.417]
 [1.422]
 [1.419]
 [1.417]
 [1.417]] [[39.986]
 [41.749]
 [42.24 ]
 [39.986]
 [39.986]] [[2.702]
 [2.812]
 [2.839]
 [2.702]
 [2.702]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.556193932013986
printing an ep nov before normalisation:  17.084357714133613
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  81.38947707916833
siam score:  -0.8363231
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.96008868535179
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  35.753562978492724
printing an ep nov before normalisation:  23.586306538327804
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.176]
 [1.176]
 [1.056]
 [1.176]
 [1.176]] [[44.665]
 [44.665]
 [56.547]
 [44.665]
 [44.665]] [[2.027]
 [2.027]
 [2.338]
 [2.027]
 [2.027]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.695043434570692
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84067243
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4041 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.278]
 [1.359]
 [1.162]
 [1.278]
 [1.252]] [[19.444]
 [17.123]
 [18.607]
 [19.444]
 [19.324]] [[2.941]
 [2.644]
 [2.688]
 [2.941]
 [2.895]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.322]
 [1.326]
 [1.322]
 [1.322]
 [1.322]] [[28.341]
 [38.787]
 [28.341]
 [28.341]
 [28.341]] [[2.075]
 [2.516]
 [2.075]
 [2.075]
 [2.075]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.975017547607422
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86137545
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.711186745790926
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.7324084003732
siam score:  -0.85744345
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  17.163054402377178
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  13.929158449172974
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.245]
 [1.245]
 [1.229]
 [1.245]
 [1.245]] [[21.562]
 [21.562]
 [24.767]
 [21.562]
 [21.562]] [[2.085]
 [2.085]
 [2.308]
 [2.085]
 [2.085]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.34968264261668
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.45221474088249
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[36.684]
 [36.684]
 [36.684]
 [36.684]
 [36.684]] [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.941]
 [0.881]
 [0.881]
 [0.881]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.881]
 [0.941]
 [0.881]
 [0.881]
 [0.881]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.49570894241333
printing an ep nov before normalisation:  45.19637285016864
printing an ep nov before normalisation:  27.53378712967603
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.982]
 [0.886]
 [0.886]
 [0.886]] [[17.619]
 [20.931]
 [17.619]
 [17.619]
 [17.619]] [[0.886]
 [0.982]
 [0.886]
 [0.886]
 [0.886]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.746335709522064
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.85191363
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.145973315563804
printing an ep nov before normalisation:  15.459590341260686
siam score:  -0.85044634
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.165]
 [1.165]
 [1.165]
 [1.063]
 [1.165]] [[16.383]
 [16.383]
 [16.383]
 [25.637]
 [16.383]] [[2.101]
 [2.101]
 [2.101]
 [2.73 ]
 [2.101]]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.66341619627902
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.16948175430298
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.76985302653085
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  18.82779780576228
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  80.49806644357726
Printing some Q and Qe and total Qs values:  [[0.886]
 [0.988]
 [0.933]
 [0.855]
 [0.922]] [[22.655]
 [17.715]
 [24.116]
 [18.711]
 [23.419]] [[0.886]
 [0.988]
 [0.933]
 [0.855]
 [0.922]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.865]
 [0.764]
 [0.764]
 [0.764]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.764]
 [0.865]
 [0.764]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.90685123955771
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.35520304588455
actions average: 
K:  0  action  0 :  tensor([0.7953, 0.0009, 0.0008, 0.0663, 0.1367], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9991,     0.0002,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9700,     0.0003,     0.0295],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0146,     0.0001,     0.0025,     0.9163,     0.0664],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1604, 0.0023, 0.0156, 0.0365, 0.7852], grad_fn=<DivBackward0>)
siam score:  -0.84652436
siam score:  -0.8472677
maxi score, test score, baseline:  0.4301 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.909299542283797
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.847529208412425
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.236953258514404
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.932]
 [0.924]
 [0.932]
 [0.932]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.932]
 [0.932]
 [0.924]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8430683
Printing some Q and Qe and total Qs values:  [[0.879]
 [1.167]
 [0.165]
 [0.879]
 [1.065]] [[48.874]
 [45.521]
 [59.182]
 [48.874]
 [52.57 ]] [[1.825]
 [1.991]
 [1.485]
 [1.825]
 [2.145]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6661,     0.0002,     0.0008,     0.1954,     0.1375],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0601,     0.9159,     0.0001,     0.0008,     0.0231],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0007,     0.9478,     0.0144,     0.0369],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0000,     0.0013,     0.9178,     0.0807],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1605, 0.0010, 0.2022, 0.1416, 0.4947], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.425520372999195
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.65494606331219
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.186862667665466
printing an ep nov before normalisation:  18.969012850803956
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]] [[15.55]
 [15.55]
 [15.55]
 [15.55]
 [15.55]] [[0.944]
 [0.944]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.73175632778544
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18691675928787
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.47664331865805
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.698412797626762
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.83938056
printing an ep nov before normalisation:  30.050513744354248
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.144]
 [1.273]
 [1.019]
 [1.019]
 [1.036]] [[37.65 ]
 [21.89 ]
 [33.2  ]
 [33.2  ]
 [33.762]] [[2.006]
 [1.609]
 [1.732]
 [1.732]
 [1.768]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.417]
 [1.417]
 [1.5  ]
 [1.417]
 [1.417]] [[23.258]
 [23.258]
 [25.782]
 [23.258]
 [23.258]] [[3.083]
 [3.083]
 [3.5  ]
 [3.083]
 [3.083]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.95]
 [0.95]
 [0.95]
 [0.95]
 [0.95]]
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.15465083959576
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.762699604034424
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [1.085]
 [0.973]
 [0.973]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.973]
 [0.973]
 [1.085]
 [0.973]
 [0.973]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.705215454101562
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.27035648095192
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.62697949133605
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.81818881563464
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  12.486652768339754
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8401982
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.25  0.458 0.042 0.042 0.208]
siam score:  -0.83631897
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.23]
 [1.23]
 [1.23]
 [1.23]] [[25.108]
 [25.108]
 [25.108]
 [25.108]
 [25.108]] [[1.975]
 [1.975]
 [1.975]
 [1.975]
 [1.975]]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4401 1.0 1.0
maxi score, test score, baseline:  0.4401 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.798316021868203
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.507379741640825
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.72035217285156
maxi score, test score, baseline:  0.4421 1.0 1.0
printing an ep nov before normalisation:  15.903178138815411
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.55441102272877
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.768]
 [0.759]
 [0.759]] [[56.552]
 [56.552]
 [54.806]
 [56.552]
 [56.552]] [[0.759]
 [0.759]
 [0.768]
 [0.759]
 [0.759]]
printing an ep nov before normalisation:  49.55249309539795
printing an ep nov before normalisation:  58.81891537272307
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.844]
 [0.795]
 [0.019]
 [0.793]] [[31.966]
 [31.977]
 [38.869]
 [53.466]
 [44.687]] [[0.775]
 [0.844]
 [0.795]
 [0.019]
 [0.793]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.360414627970506
line 256 mcts: sample exp_bonus 36.73203353088428
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.761]
 [0.698]
 [0.698]
 [0.601]] [[24.099]
 [28.092]
 [24.099]
 [24.099]
 [24.732]] [[0.698]
 [0.761]
 [0.698]
 [0.698]
 [0.601]]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.807328701019287
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.431619274912654
printing an ep nov before normalisation:  25.66798314444746
printing an ep nov before normalisation:  19.188692547522237
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  25.66798314444746
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.391]
 [0.496]
 [0.496]
 [0.703]] [[13.186]
 [15.099]
 [15.128]
 [15.128]
 [14.441]] [[0.607]
 [0.391]
 [0.496]
 [0.496]
 [0.703]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.3275455886928
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  19.188692547522237
printing an ep nov before normalisation:  20.893680613287458
Printing some Q and Qe and total Qs values:  [[0.809]
 [0.822]
 [0.809]
 [0.809]
 [0.809]] [[25.344]
 [34.505]
 [25.344]
 [25.344]
 [25.344]] [[0.809]
 [0.822]
 [0.809]
 [0.809]
 [0.809]]
line 256 mcts: sample exp_bonus 31.55930862701814
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]] [[38.341]
 [38.341]
 [38.341]
 [38.341]
 [38.341]] [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4821 1.0 1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.241]
 [1.324]
 [1.241]
 [1.241]
 [1.241]] [[78.602]
 [73.36 ]
 [78.602]
 [78.602]
 [78.602]] [[1.787]
 [1.811]
 [1.787]
 [1.787]
 [1.787]]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.611039129361274
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.4821 1.0 1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.41615104675293
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 77.33648111627464
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.69599069867815
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.771]
 [0.771]
 [0.887]
 [0.771]
 [0.771]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.771]
 [0.771]
 [0.887]
 [0.771]
 [0.771]]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.397]
 [1.469]
 [1.4  ]
 [1.397]
 [1.35 ]] [[21.71 ]
 [21.778]
 [25.454]
 [21.71 ]
 [24.907]] [[2.158]
 [2.235]
 [2.409]
 [2.158]
 [2.322]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.559245287162845
printing an ep nov before normalisation:  30.296640396118164
maxi score, test score, baseline:  0.4841 1.0 1.0
siam score:  -0.82431066
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.377]
 [1.377]
 [1.377]
 [1.377]
 [1.377]] [[22.638]
 [22.638]
 [22.638]
 [22.638]
 [22.638]] [[2.294]
 [2.294]
 [2.294]
 [2.294]
 [2.294]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.00889088272718
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4881 1.0 1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
printing an ep nov before normalisation:  50.826061804300394
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.092]
 [1.202]
 [1.092]
 [1.092]] [[44.542]
 [44.542]
 [58.272]
 [44.542]
 [44.542]] [[2.299]
 [2.299]
 [2.869]
 [2.299]
 [2.299]]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.625 0.083 0.    0.208]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8301726
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4901 1.0 1.0
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.51414656089723
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8370154
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.960984793875575
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.84 ]
 [0.842]
 [0.839]
 [0.797]] [[69.832]
 [65.195]
 [70.664]
 [69.832]
 [72.614]] [[0.839]
 [0.84 ]
 [0.842]
 [0.839]
 [0.797]]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.938]
 [0.938]
 [0.965]
 [0.938]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.938]
 [0.938]
 [0.938]
 [0.965]
 [0.938]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.80389842290334
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([    0.8341,     0.0006,     0.0003,     0.0487,     0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9991,     0.0002,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9466,     0.0005,     0.0528],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0351,     0.0003,     0.0004,     0.8998,     0.0645],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1962, 0.0171, 0.1168, 0.0431, 0.6268], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.962]
 [1.027]
 [1.182]
 [0.095]
 [1.12 ]] [[27.206]
 [42.874]
 [34.997]
 [35.431]
 [35.743]] [[1.058]
 [1.316]
 [1.374]
 [0.292]
 [1.321]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8364887
printing an ep nov before normalisation:  38.82165370942854
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.77552228357058
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.299]
 [1.246]
 [1.147]
 [1.234]] [[16.561]
 [20.147]
 [12.15 ]
 [13.128]
 [12.654]] [[1.707]
 [2.031]
 [1.571]
 [1.522]
 [1.585]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.937]
 [0.937]
 [0.937]
 [0.937]
 [0.937]]
printing an ep nov before normalisation:  52.18273162841797
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
printing an ep nov before normalisation:  19.16640901392127
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.802]
 [0.802]
 [0.802]
 [0.802]
 [0.802]]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8235496
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.692489872042835
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.374178725824045
maxi score, test score, baseline:  0.5001 1.0 1.0
printing an ep nov before normalisation:  48.69658743610274
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.894]
 [0.897]
 [0.897]] [[28.098]
 [28.098]
 [32.789]
 [28.098]
 [28.098]] [[0.987]
 [0.987]
 [1.012]
 [0.987]
 [0.987]]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8300966
printing an ep nov before normalisation:  58.343303195153005
maxi score, test score, baseline:  0.5001 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.95 ]
 [0.762]
 [0.762]
 [0.762]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.762]
 [0.95 ]
 [0.762]
 [0.762]
 [0.762]]
printing an ep nov before normalisation:  35.67002058029175
printing an ep nov before normalisation:  51.47151806691038
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.243]
 [1.204]
 [1.213]
 [1.213]] [[49.736]
 [43.899]
 [48.587]
 [49.736]
 [49.736]] [[1.805]
 [1.706]
 [1.771]
 [1.805]
 [1.805]]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.4806039973896
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.531]
 [0.646]
 [0.564]
 [0.564]] [[19.26 ]
 [21.568]
 [51.556]
 [29.991]
 [39.172]] [[0.583]
 [0.531]
 [0.646]
 [0.564]
 [0.564]]
siam score:  -0.8247705
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5041 1.0 1.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.67590149855079
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82281584
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [1.16 ]
 [0.994]
 [1.026]] [[70.283]
 [70.283]
 [45.86 ]
 [70.283]
 [72.155]] [[1.285]
 [1.285]
 [1.291]
 [1.285]
 [1.33 ]]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.610715452509695
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.15639891008777
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.5061 1.0 1.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  65.21572091796611
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5101 1.0 1.0
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.729128836753
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.264848880313872
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5141 1.0 1.0
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8139418
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.324989184854
siam score:  -0.8113374
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.023298101192072
actions average: 
K:  4  action  0 :  tensor([    0.8229,     0.0026,     0.0005,     0.0514,     0.1226],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0090,     0.9901,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0006,     0.9506,     0.0002,     0.0486],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0045,     0.9941,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0784, 0.0043, 0.2116, 0.0040, 0.7018], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  17.54539758827288
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.75  0.    0.208]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.64375512406377
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5161 1.0 1.0
printing an ep nov before normalisation:  22.000460624694824
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
printing an ep nov before normalisation:  40.19844907005661
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.997777700424194
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.03830570006212
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5201 1.0 1.0
printing an ep nov before normalisation:  24.85186223563504
printing an ep nov before normalisation:  28.970894423092528
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.82634974
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 27.714085084269858
printing an ep nov before normalisation:  53.37389716062595
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5201 1.0 1.0
printing an ep nov before normalisation:  63.28712932875394
printing an ep nov before normalisation:  27.781781192525006
printing an ep nov before normalisation:  29.37212675842025
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.741]
 [0.86 ]
 [0.004]
 [0.611]
 [0.741]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.741]
 [0.86 ]
 [0.004]
 [0.611]
 [0.741]]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.554993921512096
printing an ep nov before normalisation:  22.057028115490716
maxi score, test score, baseline:  0.5221 1.0 1.0
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8181387
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.035]
 [0.191]
 [0.121]
 [1.132]
 [1.035]] [[43.476]
 [28.85 ]
 [38.895]
 [52.701]
 [43.476]] [[1.317]
 [0.283]
 [0.344]
 [1.535]
 [1.317]]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81009203
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.774]
 [0.952]
 [0.774]
 [0.774]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.774]
 [0.774]
 [0.952]
 [0.774]
 [0.774]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.656420707702637
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5241 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5261 1.0 1.0
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8884],
        [0.9336],
        [0.7612],
        [0.6398],
        [0.0000],
        [0.7312],
        [0.5732],
        [0.0000],
        [0.6996],
        [0.5710]], dtype=torch.float64)
0.0 0.8884035704989982
0.0 0.9336287530167418
0.0 0.7611784634365669
0.0 0.6397517962643352
0.0 0.0
0.0 0.7312445846924234
0.0 0.5732050997860839
0.9801 0.9801
0.0 0.6995826614960097
0.0 0.5710102288710753
printing an ep nov before normalisation:  20.141888500739565
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.80657768249512
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.823166983468195
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.122]
 [1.246]
 [1.311]
 [1.267]
 [1.266]] [[30.508]
 [31.927]
 [34.6  ]
 [27.943]
 [31.45 ]] [[2.169]
 [2.393]
 [2.646]
 [2.134]
 [2.38 ]]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.901215132330142
printing an ep nov before normalisation:  85.83447963635403
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.078]
 [0.975]
 [1.078]
 [1.078]] [[44.025]
 [44.025]
 [57.952]
 [44.025]
 [44.025]] [[1.862]
 [1.862]
 [2.138]
 [1.862]
 [1.862]]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.92003156509796
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5281 1.0 1.0
printing an ep nov before normalisation:  9.620679542764208
maxi score, test score, baseline:  0.5281 1.0 1.0
printing an ep nov before normalisation:  18.06727430807399
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8089,     0.0195,     0.0003,     0.0567,     0.1146],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0292,     0.9580,     0.0001,     0.0018,     0.0109],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9743,     0.0003,     0.0251],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0591,     0.0002,     0.0003,     0.8515,     0.0890],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0166, 0.0042, 0.0910, 0.1096, 0.7787], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5281 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.01 ]
 [1.119]
 [1.01 ]
 [1.01 ]] [[28.118]
 [39.526]
 [39.363]
 [39.526]
 [39.526]] [[1.954]
 [3.025]
 [3.119]
 [3.025]
 [3.025]]
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 22.04256661878968
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5281 1.0 1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.063325924045028
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
siam score:  -0.82812315
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.235841536860995
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9976,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9810,     0.0007,     0.0180],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0014,     0.9968,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0736, 0.0020, 0.1948, 0.1441, 0.5854], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  38.43635635365722
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5321 1.0 1.0
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.0559586485214822e-05
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.8641556328375
maxi score, test score, baseline:  0.5321 1.0 1.0
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5321 1.0 1.0
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.3  ]
 [1.411]
 [1.342]
 [1.275]
 [1.278]] [[16.351]
 [14.246]
 [14.682]
 [13.222]
 [13.954]] [[1.543]
 [1.623]
 [1.561]
 [1.472]
 [1.486]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.302]
 [0.039]
 [1.008]
 [1.23 ]] [[45.424]
 [41.694]
 [51.039]
 [45.424]
 [45.85 ]] [[1.984]
 [2.149]
 [1.208]
 [1.984]
 [2.221]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8312568
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5401 1.0 1.0
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.374]
 [1.37 ]
 [1.246]
 [1.186]] [[19.822]
 [11.507]
 [15.369]
 [22.745]
 [19.822]] [[1.849]
 [1.633]
 [1.816]
 [2.051]
 [1.849]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5421 1.0 1.0
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.96560960059474
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.33247695284558
maxi score, test score, baseline:  0.5421 1.0 1.0
printing an ep nov before normalisation:  46.01486480004432
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8290991
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.046]
 [1.046]
 [1.065]
 [1.102]
 [1.046]] [[50.043]
 [50.043]
 [63.937]
 [67.059]
 [50.043]] [[2.095]
 [2.095]
 [2.524]
 [2.652]
 [2.095]]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.29461314248947
Printing some Q and Qe and total Qs values:  [[1.325]
 [1.333]
 [1.325]
 [1.325]
 [1.325]] [[55.608]
 [39.976]
 [55.608]
 [55.608]
 [55.608]] [[1.922]
 [1.599]
 [1.922]
 [1.922]
 [1.922]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20054
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.015429283658815
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([    0.8500,     0.0029,     0.0001,     0.0018,     0.1452],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0057,     0.9888,     0.0000,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0004,     0.9725,     0.0035,     0.0235],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0141,     0.0000,     0.0028,     0.9452,     0.0378],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0512, 0.0018, 0.3593, 0.1410, 0.4467], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.96]
 [0.96]
 [0.96]
 [0.96]
 [0.96]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.56864192583841
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.23891592025757
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.981]
 [0.981]
 [1.022]
 [1.013]
 [0.981]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.981]
 [0.981]
 [1.022]
 [1.013]
 [0.981]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]] [[45.892]
 [45.892]
 [45.892]
 [45.892]
 [45.892]] [[0.744]
 [0.744]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.06416119014446
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.91124439239502
printing an ep nov before normalisation:  17.340396248836285
actions average: 
K:  0  action  0 :  tensor([    0.8238,     0.0002,     0.0005,     0.0793,     0.0961],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0024,     0.9964,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0002,     0.9323,     0.0067,     0.0600],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0000,     0.0037,     0.9945,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2453, 0.0015, 0.0215, 0.0737, 0.6580], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5481 1.0 1.0
maxi score, test score, baseline:  0.5481 1.0 1.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.329]
 [1.384]
 [1.377]
 [1.329]
 [1.329]] [[10.418]
 [16.722]
 [ 9.221]
 [10.418]
 [10.418]] [[1.55 ]
 [1.857]
 [1.551]
 [1.55 ]
 [1.55 ]]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  65.92552518815803
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.194]
 [1.209]
 [0.899]
 [1.119]] [[21.469]
 [16.462]
 [19.753]
 [44.81 ]
 [21.469]] [[1.473]
 [1.38 ]
 [1.505]
 [2.036]
 [1.473]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.868]
 [0.008]
 [0.785]
 [0.785]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.785]
 [0.868]
 [0.008]
 [0.785]
 [0.785]]
siam score:  -0.8160367
printing an ep nov before normalisation:  36.6482250314991
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8444,     0.0001,     0.0009,     0.0847,     0.0699],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9964,     0.0000,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9496,     0.0008,     0.0494],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0415,     0.0006,     0.0031,     0.8184,     0.1365],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1076,     0.0002,     0.2416,     0.1719,     0.4786],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5481 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.273]
 [1.38 ]
 [1.273]
 [1.273]
 [1.273]] [[23.334]
 [26.111]
 [23.334]
 [23.334]
 [23.334]] [[2.397]
 [2.638]
 [2.397]
 [2.397]
 [2.397]]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.654]
 [0.745]
 [0.654]
 [0.654]
 [0.654]] [[46.996]
 [37.658]
 [46.996]
 [46.996]
 [46.996]] [[0.654]
 [0.745]
 [0.654]
 [0.654]
 [0.654]]
printing an ep nov before normalisation:  46.53855258239393
printing an ep nov before normalisation:  34.85480028044559
Printing some Q and Qe and total Qs values:  [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.844]
 [0.844]
 [0.844]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  0.5481 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.01582400704023712
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.57283805047231
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.3715313820337
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.742336523402685
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5501 1.0 1.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.93228446163379
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.001969333059719247
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81321305
siam score:  -0.813043
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.8332, 0.0010, 0.0009, 0.0011, 0.1638], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9873,     0.0000,     0.0000,     0.0112],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0012,     0.9629,     0.0001,     0.0356],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0325,     0.0002,     0.0127,     0.8803,     0.0743],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2032, 0.0037, 0.1303, 0.0049, 0.6579], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.58343194812217
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5521 1.0 1.0
printing an ep nov before normalisation:  62.87974977001984
maxi score, test score, baseline:  0.5521 1.0 1.0
printing an ep nov before normalisation:  32.702059745788574
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.105]
 [1.185]
 [1.105]
 [1.105]
 [1.105]] [[72.925]
 [66.072]
 [72.925]
 [72.925]
 [72.925]] [[1.66 ]
 [1.661]
 [1.66 ]
 [1.66 ]
 [1.66 ]]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.09503867526856
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9175,     0.0005,     0.0001,     0.0272,     0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0027,     0.9947,     0.0003,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9548,     0.0203,     0.0246],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0688,     0.0000,     0.0249,     0.8256,     0.0807],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1053, 0.0276, 0.2350, 0.0165, 0.6156], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5541 1.0 1.0
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.313]
 [1.313]
 [1.313]
 [1.313]
 [1.313]] [[36.356]
 [36.356]
 [36.356]
 [36.356]
 [36.356]] [[1.954]
 [1.954]
 [1.954]
 [1.954]
 [1.954]]
printing an ep nov before normalisation:  40.47202642141658
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  76.65211752489954
maxi score, test score, baseline:  0.5541 1.0 1.0
printing an ep nov before normalisation:  53.63612594574889
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8043034
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.812519
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5541 1.0 1.0
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80969644
maxi score, test score, baseline:  0.5541 1.0 1.0
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5541 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.098]
 [1.24 ]
 [1.098]
 [1.098]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.098]
 [1.098]
 [1.24 ]
 [1.098]
 [1.098]]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.892637112810394
Printing some Q and Qe and total Qs values:  [[1.261]
 [1.315]
 [1.261]
 [1.261]
 [1.261]] [[81.512]
 [56.204]
 [81.512]
 [81.512]
 [81.512]] [[1.594]
 [1.516]
 [1.594]
 [1.594]
 [1.594]]
siam score:  -0.814337
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5561 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5561 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.846335689481194
printing an ep nov before normalisation:  69.06366349232985
maxi score, test score, baseline:  0.5581 1.0 1.0
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.25  0.    0.25 ]
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5581 1.0 1.0
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.045]
 [1.096]
 [1.045]
 [1.045]] [[56.632]
 [56.632]
 [53.71 ]
 [56.632]
 [56.632]] [[1.532]
 [1.532]
 [1.535]
 [1.532]
 [1.532]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.644]
 [0.644]
 [0.644]
 [0.644]
 [0.644]]
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.751550388409306
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5901 1.0 1.0
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.231]
 [1.262]
 [1.204]
 [1.231]
 [1.231]] [[25.546]
 [18.258]
 [25.79 ]
 [25.546]
 [25.546]] [[2.182]
 [1.699]
 [2.172]
 [2.182]
 [2.182]]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8092755
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.928889398497063
maxi score, test score, baseline:  0.5921 1.0 1.0
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.79204106206971
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.86022417185226
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.657657623291016
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.768]
 [0.973]
 [0.012]
 [0.833]] [[19.162]
 [22.089]
 [51.411]
 [25.86 ]
 [22.46 ]] [[0.872]
 [0.851]
 [1.353]
 [0.133]
 [0.92 ]]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.161]
 [1.127]
 [1.127]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.127]
 [1.127]
 [1.161]
 [1.127]
 [1.127]]
printing an ep nov before normalisation:  33.32415113021651
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5941 1.0 1.0
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([    0.8392,     0.0054,     0.0006,     0.0854,     0.0695],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0011,     0.9970,     0.0000,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0181,     0.9306,     0.0080,     0.0429],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0239,     0.0001,     0.0208,     0.9042,     0.0510],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3084, 0.0044, 0.0147, 0.0014, 0.6712], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.916]
 [0.963]
 [1.067]
 [0.963]
 [0.963]] [[24.198]
 [ 0.   ]
 [39.138]
 [ 0.   ]
 [ 0.   ]] [[1.148]
 [0.709]
 [1.598]
 [0.709]
 [0.709]]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.76164336731164
printing an ep nov before normalisation:  21.597788333892822
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 33.49687778789035
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.32321739196777
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.23168293333483803
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]] [[47.482]
 [47.482]
 [47.482]
 [47.482]
 [47.482]] [[0.756]
 [0.756]
 [0.756]
 [0.756]
 [0.756]]
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.956]
 [0.923]
 [0.923]
 [0.923]] [[11.935]
 [20.371]
 [11.935]
 [11.935]
 [11.935]] [[0.923]
 [0.956]
 [0.923]
 [0.923]
 [0.923]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.190184593200684
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.20762967515408
printing an ep nov before normalisation:  15.692908060592003
printing an ep nov before normalisation:  38.077993392944336
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
printing an ep nov before normalisation:  64.92893919378142
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.00862077217401
printing an ep nov before normalisation:  40.8924748182592
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.211781978607178
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.6592],
        [0.8467],
        [0.0000],
        [0.8913],
        [0.0000],
        [0.8923],
        [0.0000],
        [0.8456],
        [0.0000],
        [0.4217]], dtype=torch.float64)
0.0 0.659173974696036
0.0 0.8466961168865624
0.0 0.0
0.0 0.8912920594494267
0.0 0.0
0.0 0.8922986970823686
0.0 0.0
0.0 0.8456121762482592
0.0 0.0
0.0 0.4217240525701207
Printing some Q and Qe and total Qs values:  [[1.171]
 [1.171]
 [1.224]
 [1.171]
 [1.202]] [[58.398]
 [58.398]
 [59.82 ]
 [58.398]
 [64.477]] [[2.146]
 [2.146]
 [2.229]
 [2.146]
 [2.309]]
printing an ep nov before normalisation:  66.87915274059472
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.991]
 [0.005]
 [0.823]
 [0.869]] [[23.27 ]
 [30.152]
 [15.477]
 [20.567]
 [22.133]] [[1.275]
 [1.677]
 [0.117]
 [1.135]
 [1.242]]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8137352
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.3]
 [1.3]
 [1.3]
 [1.3]
 [1.3]] [[30.644]
 [30.644]
 [30.644]
 [30.644]
 [30.644]] [[2.633]
 [2.633]
 [2.633]
 [2.633]
 [2.633]]
printing an ep nov before normalisation:  30.392717382554913
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
printing an ep nov before normalisation:  28.635848669196758
printing an ep nov before normalisation:  23.105427312455173
actions average: 
K:  3  action  0 :  tensor([    0.8501,     0.0545,     0.0004,     0.0385,     0.0565],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0040,     0.9953,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0006,     0.9686,     0.0004,     0.0301],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0357,     0.0000,     0.0014,     0.9254,     0.0374],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0606, 0.0068, 0.0782, 0.0459, 0.8085], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  13.493734887784894
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8079715
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.89757655830372
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.318]
 [1.292]
 [1.291]
 [1.14 ]
 [1.318]] [[22.277]
 [25.033]
 [21.627]
 [35.749]
 [22.277]] [[1.716]
 [1.793]
 [1.664]
 [2.041]
 [1.716]]
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.254]
 [1.254]
 [1.254]
 [1.254]
 [1.254]] [[46.814]
 [46.814]
 [46.814]
 [46.814]
 [46.814]] [[2.921]
 [2.921]
 [2.921]
 [2.921]
 [2.921]]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  85.04261745433901
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8067197
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.09214401245117
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.123464584350586
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.72617675537343
maxi score, test score, baseline:  0.6141 1.0 1.0
printing an ep nov before normalisation:  32.09379307847326
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.2544089617289
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.676072249092414
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.17213866828667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.079]
 [1.079]
 [1.079]
 [1.079]
 [1.077]] [[31.678]
 [31.678]
 [31.678]
 [31.678]
 [39.116]] [[2.044]
 [2.044]
 [2.044]
 [2.044]
 [2.41 ]]
maxi score, test score, baseline:  0.6161 1.0 1.0
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.737825368665625
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.93306950696622
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.81613845
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  85.73880639502471
siam score:  -0.8094333
maxi score, test score, baseline:  0.6181 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8101382
maxi score, test score, baseline:  0.6181 1.0 1.0
siam score:  -0.8107539
maxi score, test score, baseline:  0.6181 1.0 1.0
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.57813734162785
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.111]
 [1.111]
 [1.208]
 [1.111]
 [1.111]] [[31.434]
 [31.434]
 [36.936]
 [31.434]
 [31.434]] [[1.269]
 [1.269]
 [1.419]
 [1.269]
 [1.269]]
siam score:  -0.8103147
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6181 1.0 1.0
printing an ep nov before normalisation:  45.86872379282259
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [1.014]
 [0.221]
 [0.999]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.999]
 [0.999]
 [1.014]
 [0.221]
 [0.999]]
Printing some Q and Qe and total Qs values:  [[1.136]
 [1.136]
 [1.136]
 [1.136]
 [1.136]] [[56.274]
 [56.274]
 [56.274]
 [56.274]
 [56.274]] [[2.018]
 [2.018]
 [2.018]
 [2.018]
 [2.018]]
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.299]
 [1.217]
 [1.217]
 [1.09 ]] [[52.563]
 [48.696]
 [67.007]
 [67.007]
 [65.506]] [[1.571]
 [1.72 ]
 [1.884]
 [1.884]
 [1.737]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8833,     0.1013,     0.0005,     0.0008,     0.0142],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0018, 0.9733, 0.0169, 0.0021, 0.0059], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0027,     0.9431,     0.0008,     0.0533],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0014,     0.0061,     0.9899,     0.0025],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1250,     0.0005,     0.0085,     0.2893,     0.5767],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6181 1.0 1.0
printing an ep nov before normalisation:  38.47369364444702
printing an ep nov before normalisation:  39.25906586596891
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.821]
 [1.006]
 [0.821]
 [0.821]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.821]
 [0.821]
 [1.006]
 [0.821]
 [0.821]]
Printing some Q and Qe and total Qs values:  [[1.145]
 [1.145]
 [1.1  ]
 [1.145]
 [1.145]] [[45.312]
 [45.312]
 [53.493]
 [45.312]
 [45.312]] [[1.696]
 [1.696]
 [1.796]
 [1.696]
 [1.696]]
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80486315
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.07145595550537
actions average: 
K:  0  action  0 :  tensor([    0.7370,     0.0002,     0.0048,     0.0646,     0.1935],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9986,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0031,     0.9141,     0.0187,     0.0633],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0257,     0.0001,     0.0017,     0.8919,     0.0806],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0520, 0.0203, 0.0917, 0.1444, 0.6916], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9522,     0.0044,     0.0006,     0.0150,     0.0277],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9979,     0.0000,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9789,     0.0008,     0.0201],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0000,     0.0086,     0.9539,     0.0372],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0010, 0.0028, 0.2904, 0.0069, 0.6989], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.084]
 [1.171]
 [1.084]
 [1.109]] [[35.881]
 [36.917]
 [31.856]
 [36.917]
 [33.798]] [[2.821]
 [2.923]
 [2.529]
 [2.923]
 [2.651]]
printing an ep nov before normalisation:  34.258688590914026
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.817946403849824
maxi score, test score, baseline:  0.6221 1.0 1.0
printing an ep nov before normalisation:  24.243135453910487
Printing some Q and Qe and total Qs values:  [[1.086]
 [1.287]
 [1.234]
 [1.228]
 [1.148]] [[17.542]
 [14.781]
 [17.806]
 [21.786]
 [20.405]] [[1.834]
 [1.748]
 [2.01 ]
 [2.417]
 [2.193]]
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  29.94925535011646
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.526448304694668
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8852,     0.0003,     0.0004,     0.0008,     0.1135],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0194,     0.9771,     0.0010,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0041,     0.9858,     0.0002,     0.0098],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0367, 0.0327, 0.0059, 0.7338, 0.1908], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0632, 0.0046, 0.3615, 0.0548, 0.5160], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6241 1.0 1.0
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.30182198657804
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6261 1.0 1.0
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.536124757742808
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.877]
 [0.756]
 [0.891]
 [0.877]
 [0.877]] [[55.687]
 [47.914]
 [61.617]
 [55.687]
 [55.687]] [[0.877]
 [0.756]
 [0.891]
 [0.877]
 [0.877]]
printing an ep nov before normalisation:  54.258782852039126
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.38988455577672
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.79794395
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7955772
line 256 mcts: sample exp_bonus 0.06180722746407241
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  74.47508173150395
maxi score, test score, baseline:  0.6301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.854]
 [0.76 ]
 [0.786]
 [0.724]] [[27.213]
 [30.773]
 [39.004]
 [27.213]
 [38.728]] [[0.786]
 [0.854]
 [0.76 ]
 [0.786]
 [0.724]]
maxi score, test score, baseline:  0.6301 1.0 1.0
maxi score, test score, baseline:  0.6301 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7896312
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  22.01910098119526
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.875897766012713e-05
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6321 1.0 1.0
maxi score, test score, baseline:  0.6321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.12225532531738
printing an ep nov before normalisation:  46.02060268794407
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.244]
 [1.244]
 [1.244]
 [1.22 ]
 [1.244]] [[20.194]
 [20.194]
 [20.194]
 [30.248]
 [20.194]] [[1.945]
 [1.945]
 [1.945]
 [2.424]
 [1.945]]
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.445030319419665
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
