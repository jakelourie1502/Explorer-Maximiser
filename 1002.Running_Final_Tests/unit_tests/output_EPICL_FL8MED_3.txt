append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  5.802936213357108
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  7.249385118484497
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
printing an ep nov before normalisation:  5.540951840345336
siam score:  -0.003320044499229301
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.03687101470042066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 2 threads
Frames:  1057 train batches done:  28 episodes:  92
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.10755809435001785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3507, 0.1833, 0.0614, 0.2053, 0.1992], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1578, 0.5398, 0.1011, 0.0415, 0.1598], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1089, 0.1586, 0.4734, 0.1437, 0.1154], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3392, 0.0412, 0.1691, 0.2658, 0.1847], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2459, 0.1059, 0.2570, 0.2038, 0.1874], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.3328889
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  1057 train batches done:  73 episodes:  92
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3461, 0.1147, 0.1052, 0.2723, 0.1617], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1037, 0.5041, 0.2183, 0.0464, 0.1274], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0723, 0.0286, 0.6461, 0.1790, 0.0741], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2395, 0.0331, 0.2579, 0.3105, 0.1589], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2266, 0.1206, 0.2410, 0.2412, 0.1706], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4110, 0.0514, 0.0590, 0.2656, 0.2131], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1613, 0.5876, 0.0450, 0.0463, 0.1598], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0558, 0.0782, 0.5684, 0.1394, 0.1583], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2701, 0.0325, 0.1593, 0.3369, 0.2011], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3398, 0.1031, 0.1163, 0.2053, 0.2356], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  13.437696039248337
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.51360434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.820409079585584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4940, 0.1198, 0.0146, 0.1856, 0.1860], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1563, 0.6750, 0.0420, 0.0290, 0.0977], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0180, 0.0811, 0.7219, 0.0984, 0.0807], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3527, 0.0121, 0.0843, 0.3448, 0.2062], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3513, 0.1051, 0.0188, 0.2775, 0.2473], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.49502944946289
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.4 0.2 0.2 0.2 0. ]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[32.725]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.334]
 [-1.2  ]
 [-1.2  ]
 [-1.2  ]
 [-1.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.49228394
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.045183181762695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[38.052]
 [64.503]
 [38.052]
 [38.052]
 [38.052]] [[0.109]
 [1.022]
 [0.109]
 [0.109]
 [0.109]]
actions average: 
K:  0  action  0 :  tensor([0.6029, 0.0106, 0.0051, 0.2048, 0.1767], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0654, 0.8380, 0.0048, 0.0350, 0.0567], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0082, 0.0078, 0.8651, 0.0796, 0.0393], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2396, 0.0022, 0.0474, 0.5353, 0.1754], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4576, 0.0192, 0.0100, 0.2646, 0.2487], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.4810, 0.0318, 0.0085, 0.2782, 0.2006], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1523, 0.6435, 0.0361, 0.0189, 0.1492], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0122, 0.0223, 0.7730, 0.1142, 0.0784], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2893, 0.0029, 0.0802, 0.4090, 0.2186], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3051, 0.0904, 0.0430, 0.2567, 0.3048], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.663851230410046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6384006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5914289
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5970537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5056, 0.0694, 0.0038, 0.1779, 0.2432], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0869, 0.8509, 0.0024, 0.0029, 0.0569], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0012, 0.0023, 0.9240, 0.0541, 0.0185], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1689, 0.0035, 0.0993, 0.5144, 0.2140], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2317, 0.0213, 0.0565, 0.3525, 0.3380], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.76324037993902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6397108
actions average: 
K:  0  action  0 :  tensor([    0.6845,     0.0008,     0.0002,     0.1380,     0.1764],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.1094,     0.8453,     0.0004,     0.0022,     0.0427],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0028, 0.0027, 0.8106, 0.1361, 0.0478], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2925, 0.0014, 0.0061, 0.4686, 0.2314], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4616, 0.0041, 0.0093, 0.2623, 0.2627], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5381, 0.0242, 0.0042, 0.1739, 0.2596], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0485, 0.8588, 0.0023, 0.0011, 0.0893], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0034,     0.8774,     0.0745,     0.0439],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2157, 0.0006, 0.0915, 0.4319, 0.2602], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2762, 0.0013, 0.0131, 0.2802, 0.4293], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.553186427771834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.6588672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.899828910827637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.6704, 0.0395, 0.0048, 0.0782, 0.2071], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0109, 0.9554, 0.0179, 0.0014, 0.0144], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0022,     0.9367,     0.0439,     0.0171],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.3270, 0.0016, 0.0268, 0.4112, 0.2334], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4208, 0.0946, 0.0021, 0.1651, 0.3174], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.65770334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.17225223388965
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6754987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.7389193615868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.2 0.  0.2]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.849738488024315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.29118898179796
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.798]
 [71.798]
 [71.798]
 [71.798]
 [71.798]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.681683406961795
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6686,     0.0025,     0.0006,     0.1389,     0.1894],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0617, 0.9071, 0.0076, 0.0069, 0.0167], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0017, 0.0117, 0.9538, 0.0149, 0.0179], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.2532,     0.0002,     0.0193,     0.4812,     0.2461],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3347, 0.0876, 0.0061, 0.2088, 0.3628], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([0.5968, 0.0140, 0.0162, 0.1093, 0.2638], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0092,     0.9335,     0.0004,     0.0016,     0.0554],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9970,     0.0013,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1656, 0.0182, 0.0427, 0.5339, 0.2397], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3323, 0.0025, 0.0395, 0.2533, 0.3725], grad_fn=<DivBackward0>)
siam score:  -0.69259524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.087]
 [47.087]
 [76.823]
 [47.087]
 [47.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.03872314396853
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.34131744243261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.22588169807676
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  0.4640891220404342
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.02755689620972
siam score:  -0.68038404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.06034386430805
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.557]
 [49.747]
 [55.572]
 [48.715]
 [36.557]] [[0.347]
 [0.685]
 [0.834]
 [0.659]
 [0.347]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.448]
 [55.536]
 [96.892]
 [76.469]
 [72.957]] [[0.45 ]
 [0.345]
 [0.603]
 [0.475]
 [0.453]]
printing an ep nov before normalisation:  60.798455621759416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.73503949558849
siam score:  -0.7300861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.208 0.125 0.167 0.125 0.375]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.05974306699513
printing an ep nov before normalisation:  0.4784579398187816
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.28758297143118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.67778456
printing an ep nov before normalisation:  36.832937068528324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.94068863773668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  102.80822283280023
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  78.10880398290266
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.379857716339835
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.31290449799532
printing an ep nov before normalisation:  116.7630683585248
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  1.4063405070442059
printing an ep nov before normalisation:  82.91390676328484
siam score:  -0.6650761
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.66288900375366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.71172961576302
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.25  0.167 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  1  action  0 :  tensor([0.5868, 0.0285, 0.0340, 0.1052, 0.2455], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0137, 0.8992, 0.0016, 0.0054, 0.0801], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0009,     0.0022,     0.9619,     0.0168,     0.0182],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1740,     0.0005,     0.0227,     0.5118,     0.2910],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3113, 0.0283, 0.0060, 0.2594, 0.3950], grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.925]
 [62.983]
 [62.983]
 [62.983]
 [62.983]] [[1.015]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.423]
 [21.423]
 [40.499]
 [47.407]
 [38.594]] [[-0.114]
 [-0.114]
 [ 0.657]
 [ 0.936]
 [ 0.58 ]]
printing an ep nov before normalisation:  3.794746190415026
printing an ep nov before normalisation:  35.32053057889608
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  9396 train batches done:  1098 episodes:  805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.79052140468715
printing an ep nov before normalisation:  32.90986516020376
actions average: 
K:  1  action  0 :  tensor([0.6295, 0.0498, 0.0108, 0.1314, 0.1786], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0350, 0.9344, 0.0093, 0.0081, 0.0132], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0032,     0.9475,     0.0305,     0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1348, 0.0020, 0.0193, 0.6543, 0.1896], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3257, 0.0022, 0.0050, 0.3591, 0.3079], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.348999261856079
printing an ep nov before normalisation:  39.060216478424685
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6753, 0.0071, 0.0035, 0.1676, 0.1466], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0580,     0.9132,     0.0083,     0.0009,     0.0196],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0031,     0.0001,     0.9475,     0.0320,     0.0173],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.2052,     0.0002,     0.0020,     0.6068,     0.1858],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.5214, 0.0025, 0.0040, 0.1953, 0.2769], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7411274
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.729]
 [49.729]
 [49.729]
 [49.729]
 [49.729]] [[1.567]
 [1.567]
 [1.567]
 [1.567]
 [1.567]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74323034
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.692010039821461
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74925524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7495929
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  81.46943469549251
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.24535357835702598
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.65452463219972
printing an ep nov before normalisation:  45.66194216410319
printing an ep nov before normalisation:  48.12764772568649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.93453091811036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.486707992758966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.65025566216943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.24139779418286
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([0.6306, 0.0063, 0.0015, 0.1302, 0.2314], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0289, 0.9041, 0.0013, 0.0172, 0.0485], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0057, 0.0082, 0.9362, 0.0314, 0.0185], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1619,     0.0004,     0.0110,     0.6324,     0.1944],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2600, 0.0679, 0.0492, 0.2445, 0.3783], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.70534027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.44683244493272
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  37.5627291912227
printing an ep nov before normalisation:  31.330721568747823
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.186032809620414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.117]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 0.36]
 [-0.15]
 [-0.15]
 [-0.15]
 [-0.15]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6591, 0.0092, 0.0045, 0.1330, 0.1942], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0235,     0.9074,     0.0546,     0.0001,     0.0145],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0062, 0.0183, 0.9579, 0.0068, 0.0109], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1300, 0.0014, 0.1619, 0.4900, 0.2166], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2677, 0.0170, 0.0692, 0.2724, 0.3738], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.70372094048394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.5854, 0.0138, 0.0014, 0.1760, 0.2233], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0183, 0.9431, 0.0066, 0.0033, 0.0287], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0012,     0.9365,     0.0406,     0.0215],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1539, 0.0008, 0.0262, 0.6156, 0.2036], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3904, 0.0027, 0.0138, 0.2365, 0.3567], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
actions average: 
K:  1  action  0 :  tensor([0.6094, 0.0482, 0.0052, 0.1591, 0.1781], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0045,     0.9934,     0.0000,     0.0006,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0006,     0.0004,     0.9725,     0.0154,     0.0111],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1023, 0.0013, 0.0979, 0.6252, 0.1732], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2285, 0.0061, 0.0283, 0.4168, 0.3203], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7539697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.4849244920314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.0407334043033534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7466676
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.22978181857471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.054476194003087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  20.16492700442526
printing an ep nov before normalisation:  12.55743616720757
actions average: 
K:  0  action  0 :  tensor([0.6115, 0.0033, 0.0057, 0.1236, 0.2559], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0200,     0.9671,     0.0043,     0.0003,     0.0084],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0164, 0.0204, 0.9111, 0.0300, 0.0221], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1731, 0.0027, 0.0114, 0.5275, 0.2853], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3743, 0.0006, 0.0016, 0.2281, 0.3954], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7467855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.74485831645453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.744321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.994004705422242
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.09324500540606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7413852
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.324358911149766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.66652274458174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.420488795204922
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.971055420553824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.652701139450073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[10.779]
 [32.087]
 [ 4.253]
 [ 6.366]
 [ 8.771]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.1193199234824
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.3812129927434
siam score:  -0.7490231
printing an ep nov before normalisation:  0.06822036376433971
from probs:  [0.08337029099707673, 0.08337029099707673, 0.08337029099707673, 0.5831485450146164, 0.08337029099707673, 0.08337029099707673]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337031098333324, 0.08337031098333324, 0.08337031098333324, 0.5831484450833339, 0.08337031098333324, 0.08337031098333324]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337037094204522, 0.08337037094204522, 0.08337037094204522, 0.583148145289774, 0.08337037094204522, 0.08337037094204522]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337037094204522, 0.08337037094204522, 0.08337037094204522, 0.583148145289774, 0.08337037094204522, 0.08337037094204522]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337037094204522, 0.08337037094204522, 0.08337037094204522, 0.583148145289774, 0.08337037094204522, 0.08337037094204522]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337039092826339, 0.08337039092826339, 0.08337039092826339, 0.5831480453586833, 0.08337039092826339, 0.08337039092826339]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337039092826339, 0.08337039092826339, 0.08337039092826339, 0.5831480453586833, 0.08337039092826339, 0.08337039092826339]
from probs:  [0.08337039092826339, 0.08337039092826339, 0.08337039092826339, 0.5831480453586833, 0.08337039092826339, 0.08337039092826339]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337041091447193, 0.08337041091447193, 0.08337041091447193, 0.5831479454276405, 0.08337041091447193, 0.08337041091447193]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.936]
 [45.833]
 [27.39 ]
 [26.613]
 [29.177]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337045088686025, 0.08337045088686025, 0.08337045088686025, 0.5831477455656987, 0.08337045088686025, 0.08337045088686025]
printing an ep nov before normalisation:  93.2226772339354
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337051084537085, 0.08337051084537085, 0.08337051084537085, 0.5831474457731458, 0.08337051084537085, 0.08337051084537085]
from probs:  [0.08337061077602999, 0.08337061077602999, 0.08337061077602999, 0.5831469461198501, 0.08337061077602999, 0.08337061077602999]
printing an ep nov before normalisation:  0.031175991576252015
using another actor
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.728]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 0.744]
 [-1.206]
 [-1.206]
 [-1.206]
 [-1.206]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337069072038465, 0.08337069072038465, 0.08337069072038465, 0.5831465463980767, 0.08337069072038465, 0.08337069072038465]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337073069250445, 0.08337073069250445, 0.08337073069250445, 0.5831463465374778, 0.08337073069250445, 0.08337073069250445]
using another actor
printing an ep nov before normalisation:  49.29004479666865
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.34639472764945
printing an ep nov before normalisation:  41.2880511444899
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.336]
 [51.336]
 [51.336]
 [51.336]
 [51.336]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337075067854999, 0.08337075067854999, 0.08337075067854999, 0.5831462466072502, 0.08337075067854999, 0.08337075067854999]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833707706645859, 0.0833707706645859, 0.0833707706645859, 0.5831461466770707, 0.0833707706645859, 0.0833707706645859]
printing an ep nov before normalisation:  73.77390675421071
UNIT TEST: sample policy line 217 mcts : [0.25  0.208 0.333 0.083 0.125]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.57649361532792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833708306226361, 0.0833708306226361, 0.0833708306226361, 0.5831458468868196, 0.0833708306226361, 0.0833708306226361]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337089058059999, 0.08337089058059999, 0.08337089058059999, 0.5831455470970001, 0.08337089058059999, 0.08337089058059999]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  90.26652669934151
printing an ep nov before normalisation:  45.826063077998484
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.266279440251296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337093055252796, 0.08337093055252796, 0.08337093055252796, 0.5831453472373601, 0.08337093055252796, 0.08337093055252796]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  0.15908625535985266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337095053847758, 0.08337095053847758, 0.08337095053847758, 0.5831452473076122, 0.08337095053847758, 0.08337095053847758]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[25.483]
 [45.799]
 [25.483]
 [25.483]
 [25.483]] [[0.466]
 [1.377]
 [0.466]
 [0.466]
 [0.466]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.33109855651855
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337099051034799, 0.08337099051034799, 0.08337099051034799, 0.58314504744826, 0.08337099051034799, 0.08337099051034799]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337101049626881, 0.08337101049626881, 0.08337101049626881, 0.5831449475186559, 0.08337101049626881, 0.08337101049626881]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337103048218006, 0.08337103048218006, 0.08337103048218006, 0.5831448475890997, 0.08337103048218006, 0.08337103048218006]
from probs:  [0.08337105046808173, 0.08337105046808173, 0.08337105046808173, 0.5831447476595915, 0.08337105046808173, 0.08337105046808173]
deleting a thread, now have 1 threads
Frames:  15274 train batches done:  1788 episodes:  1313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833711503974461, 0.0833711503974461, 0.0833711503974461, 0.5831442480127697, 0.0833711503974461, 0.0833711503974461]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833711503974461, 0.0833711503974461, 0.0833711503974461, 0.5831442480127697, 0.0833711503974461, 0.0833711503974461]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833711503974461, 0.0833711503974461, 0.0833711503974461, 0.5831442480127697, 0.0833711503974461, 0.0833711503974461]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337123034076495, 0.08337123034076495, 0.08337123034076495, 0.5831438482961753, 0.08337123034076495, 0.08337123034076495]
siam score:  -0.67957085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337127031236685, 0.08337127031236685, 0.08337127031236685, 0.5831436484381658, 0.08337127031236685, 0.08337127031236685]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337133026969774, 0.08337133026969774, 0.08337133026969774, 0.5831433486515113, 0.08337133026969774, 0.08337133026969774]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337135025545553, 0.08337135025545553, 0.08337135025545553, 0.5831432487227223, 0.08337135025545553, 0.08337135025545553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337135025545553, 0.08337135025545553, 0.08337135025545553, 0.5831432487227223, 0.08337135025545553, 0.08337135025545553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337139022694233, 0.08337139022694233, 0.08337139022694233, 0.5831430488652883, 0.08337139022694233, 0.08337139022694233]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337139022694233, 0.08337139022694233, 0.08337139022694233, 0.5831430488652883, 0.08337139022694233, 0.08337139022694233]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337143019839076, 0.08337143019839076, 0.08337143019839076, 0.5831428490080461, 0.08337143019839076, 0.08337143019839076]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337145018410061, 0.08337145018410061, 0.08337145018410061, 0.583142749079497, 0.08337145018410061, 0.08337145018410061]
using another actor
from probs:  [0.08337145018410061, 0.08337145018410061, 0.08337145018410061, 0.583142749079497, 0.08337145018410061, 0.08337145018410061]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337147016980084, 0.08337147016980084, 0.08337147016980084, 0.5831426491509958, 0.08337147016980084, 0.08337147016980084]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337147016980084, 0.08337147016980084, 0.08337147016980084, 0.5831426491509958, 0.08337147016980084, 0.08337147016980084]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337147016980084, 0.08337147016980084, 0.08337147016980084, 0.5831426491509958, 0.08337147016980084, 0.08337147016980084]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337147016980084, 0.08337147016980084, 0.08337147016980084, 0.5831426491509958, 0.08337147016980084, 0.08337147016980084]
printing an ep nov before normalisation:  92.64698028564453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337151014117256, 0.08337151014117256, 0.08337151014117256, 0.5831424492941373, 0.08337151014117256, 0.08337151014117256]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337151014117256, 0.08337151014117256, 0.08337151014117256, 0.5831424492941373, 0.08337151014117256, 0.08337151014117256]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337151014117256, 0.08337151014117256, 0.08337151014117256, 0.5831424492941373, 0.08337151014117256, 0.08337151014117256]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  117.67202925461312
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337153012684402, 0.08337153012684402, 0.08337153012684402, 0.5831423493657799, 0.08337153012684402, 0.08337153012684402]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833715700981582, 0.0833715700981582, 0.0833715700981582, 0.5831421495092091, 0.0833715700981582, 0.0833715700981582]
from probs:  [0.0833715900838009, 0.0833715900838009, 0.0833715900838009, 0.5831420495809957, 0.0833715900838009, 0.0833715900838009]
printing an ep nov before normalisation:  86.5553092956543
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337161006943401, 0.08337161006943401, 0.08337161006943401, 0.5831419496528301, 0.08337161006943401, 0.08337161006943401]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337161006943401, 0.08337161006943401, 0.08337161006943401, 0.5831419496528301, 0.08337161006943401, 0.08337161006943401]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337170999745568, 0.08337170999745568, 0.08337170999745568, 0.5831414500127217, 0.08337170999745568, 0.08337170999745568]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337174996859721, 0.08337174996859721, 0.08337174996859721, 0.583141250157014, 0.08337174996859721, 0.08337174996859721]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[84.808]
 [84.808]
 [84.808]
 [84.808]
 [84.808]] [[0.882]
 [0.882]
 [0.882]
 [0.882]
 [0.882]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337180992523759, 0.08337180992523759, 0.08337180992523759, 0.5831409503738121, 0.08337180992523759, 0.08337180992523759]
printing an ep nov before normalisation:  33.84065866470337
printing an ep nov before normalisation:  51.39453982086734
printing an ep nov before normalisation:  100.21497938368056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337182991076521, 0.08337182991076521, 0.08337182991076521, 0.583140850446174, 0.08337182991076521, 0.08337182991076521]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337184989628323, 0.08337184989628323, 0.08337184989628323, 0.5831407505185839, 0.08337184989628323, 0.08337184989628323]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337198979464082, 0.08337198979464082, 0.08337198979464082, 0.5831400510267959, 0.08337198979464082, 0.08337198979464082]
printing an ep nov before normalisation:  100.70857947008945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337202976551383, 0.08337202976551383, 0.08337202976551383, 0.583139851172431, 0.08337202976551383, 0.08337202976551383]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337206973634846, 0.08337206973634846, 0.08337206973634846, 0.5831396513182576, 0.08337206973634846, 0.08337206973634846]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.6999,     0.0449,     0.0003,     0.0567,     0.1982],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0150, 0.8799, 0.0424, 0.0142, 0.0486], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0012, 0.0114, 0.8709, 0.0378, 0.0787], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1633, 0.0511, 0.1331, 0.4080, 0.2445], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1862, 0.0150, 0.0743, 0.2683, 0.4562], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  6.9001458760649825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337210970714475, 0.08337210970714475, 0.08337210970714475, 0.5831394514642763, 0.08337210970714475, 0.08337210970714475]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337212969252851, 0.08337212969252851, 0.08337212969252851, 0.5831393515373575, 0.08337212969252851, 0.08337212969252851]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337216966326723, 0.08337216966326723, 0.08337216966326723, 0.5831391516836638, 0.08337216966326723, 0.08337216966326723]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.08177175985114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833722296193034, 0.0833722296193034, 0.0833722296193034, 0.5831388519034829, 0.0833722296193034, 0.0833722296193034]
from probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337224960462962, 0.08337224960462962, 0.08337224960462962, 0.5831387519768519, 0.08337224960462962, 0.08337224960462962]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337228957525328, 0.08337228957525328, 0.08337228957525328, 0.5831385521237337, 0.08337228957525328, 0.08337228957525328]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337228957525328, 0.08337228957525328, 0.08337228957525328, 0.5831385521237337, 0.08337228957525328, 0.08337228957525328]
printing an ep nov before normalisation:  114.1075027105778
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337230956055071, 0.08337230956055071, 0.08337230956055071, 0.5831384521972464, 0.08337230956055071, 0.08337230956055071]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.002]
 [0.004]
 [0.003]] [[72.001]
 [77.03 ]
 [73.908]
 [72.001]
 [72.439]] [[0.797]
 [0.851]
 [0.817]
 [0.797]
 [0.801]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337232954583856, 0.08337232954583856, 0.08337232954583856, 0.5831383522708072, 0.08337232954583856, 0.08337232954583856]
printing an ep nov before normalisation:  97.41486647736268
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337234953111683, 0.08337234953111683, 0.08337234953111683, 0.5831382523444159, 0.08337234953111683, 0.08337234953111683]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833723695163855, 0.0833723695163855, 0.0833723695163855, 0.5831381524180727, 0.0833723695163855, 0.0833723695163855]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337238950164458, 0.08337238950164458, 0.08337238950164458, 0.5831380524917772, 0.08337238950164458, 0.08337238950164458]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337238950164458, 0.08337238950164458, 0.08337238950164458, 0.5831380524917772, 0.08337238950164458, 0.08337238950164458]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337242947213397, 0.08337242947213397, 0.08337242947213397, 0.5831378526393303, 0.08337242947213397, 0.08337242947213397]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.70600736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337244945736427, 0.08337244945736427, 0.08337244945736427, 0.5831377527131787, 0.08337244945736427, 0.08337244945736427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337246944258499, 0.08337246944258499, 0.08337246944258499, 0.5831376527870751, 0.08337246944258499, 0.08337246944258499]
printing an ep nov before normalisation:  18.071176910279974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337248942779611, 0.08337248942779611, 0.08337248942779611, 0.5831375528610195, 0.08337248942779611, 0.08337248942779611]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337248942779611, 0.08337248942779611, 0.08337248942779611, 0.5831375528610195, 0.08337248942779611, 0.08337248942779611]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337250941299765, 0.08337250941299765, 0.08337250941299765, 0.5831374529350118, 0.08337250941299765, 0.08337250941299765]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7024094
printing an ep nov before normalisation:  78.665167814749
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337260933886147, 0.08337260933886147, 0.08337260933886147, 0.5831369533056926, 0.08337260933886147, 0.08337260933886147]
printing an ep nov before normalisation:  55.88337182998657
siam score:  -0.7272254
actions average: 
K:  0  action  0 :  tensor([    0.7028,     0.0028,     0.0003,     0.1356,     0.1586],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0433, 0.8599, 0.0296, 0.0070, 0.0602], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0033, 0.0302, 0.8844, 0.0401, 0.0420], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1171,     0.0006,     0.0236,     0.6339,     0.2247],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3349, 0.0053, 0.0743, 0.2634, 0.3222], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
from probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337270926448553, 0.08337270926448553, 0.08337270926448553, 0.5831364536775723, 0.08337270926448553, 0.08337270926448553]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833727292495816, 0.0833727292495816, 0.0833727292495816, 0.5831363537520922, 0.0833727292495816, 0.0833727292495816]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337274923466804, 0.08337274923466804, 0.08337274923466804, 0.5831362538266599, 0.08337274923466804, 0.08337274923466804]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337274923466804, 0.08337274923466804, 0.08337274923466804, 0.5831362538266599, 0.08337274923466804, 0.08337274923466804]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337276921974492, 0.08337276921974492, 0.08337276921974492, 0.5831361539012756, 0.08337276921974492, 0.08337276921974492]
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337276921974492, 0.08337276921974492, 0.08337276921974492, 0.5831361539012756, 0.08337276921974492, 0.08337276921974492]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337278920481217, 0.08337278920481217, 0.08337278920481217, 0.5831360539759392, 0.08337278920481217, 0.08337278920481217]
from probs:  [0.08337280918986986, 0.08337280918986986, 0.08337280918986986, 0.5831359540506508, 0.08337280918986986, 0.08337280918986986]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
UNIT TEST: sample policy line 217 mcts : [0.125 0.333 0.125 0.292 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337282917491796, 0.08337282917491796, 0.08337282917491796, 0.5831358541254104, 0.08337282917491796, 0.08337282917491796]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337284915995644, 0.08337284915995644, 0.08337284915995644, 0.5831357542002177, 0.08337284915995644, 0.08337284915995644]
siam score:  -0.7294046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337290911501444, 0.08337290911501444, 0.08337290911501444, 0.583135454424928, 0.08337290911501444, 0.08337290911501444]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]] [[48.625]
 [59.   ]
 [48.753]
 [40.266]
 [48.35 ]] [[0.625]
 [0.764]
 [0.626]
 [0.513]
 [0.621]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337292910001458, 0.08337292910001458, 0.08337292910001458, 0.5831353544999273, 0.08337292910001458, 0.08337292910001458]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337294908500512, 0.08337294908500512, 0.08337294908500512, 0.5831352545749745, 0.08337294908500512, 0.08337294908500512]
printing an ep nov before normalisation:  25.534720176104504
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337302902487143, 0.08337302902487143, 0.08337302902487143, 0.5831348548756429, 0.08337302902487143, 0.08337302902487143]
UNIT TEST: sample policy line 217 mcts : [0.25  0.25  0.167 0.167 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337302902487143, 0.08337302902487143, 0.08337302902487143, 0.5831348548756429, 0.08337302902487143, 0.08337302902487143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337302902487143, 0.08337302902487143, 0.08337302902487143, 0.5831348548756429, 0.08337302902487143, 0.08337302902487143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337306899474703, 0.08337306899474703, 0.08337306899474703, 0.5831346550262649, 0.08337306899474703, 0.08337306899474703]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337308897967044, 0.08337308897967044, 0.08337308897967044, 0.5831345551016477, 0.08337308897967044, 0.08337308897967044]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.6158,     0.0344,     0.0002,     0.1720,     0.1776],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0458, 0.8859, 0.0048, 0.0300, 0.0335], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0034, 0.1267, 0.6872, 0.0958, 0.0869], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0393, 0.0592, 0.0699, 0.6306, 0.2009], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3019, 0.0325, 0.1350, 0.1408, 0.3898], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73799133
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833732888283772, 0.0833732888283772, 0.0833732888283772, 0.583133555858114, 0.0833732888283772, 0.0833732888283772]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[63.403]
 [64.406]
 [92.089]
 [58.969]
 [68.822]] [[0.697]
 [0.719]
 [1.329]
 [0.6  ]
 [0.816]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.014]
 [0.001]
 [0.014]
 [0.014]] [[78.643]
 [78.643]
 [86.993]
 [78.643]
 [78.643]] [[1.546]
 [1.546]
 [1.786]
 [1.546]
 [1.546]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337330881319512, 0.08337330881319512, 0.08337330881319512, 0.5831334559340243, 0.08337330881319512, 0.08337330881319512]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337332879800348, 0.08337332879800348, 0.08337332879800348, 0.5831333560099827, 0.08337332879800348, 0.08337332879800348]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337334878280223, 0.08337334878280223, 0.08337334878280223, 0.583133256085989, 0.08337334878280223, 0.08337334878280223]
actions average: 
K:  0  action  0 :  tensor([    0.6371,     0.0122,     0.0005,     0.1028,     0.2474],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0063,     0.9866,     0.0003,     0.0001,     0.0067],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0019, 0.0337, 0.9284, 0.0165, 0.0195], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0960, 0.0020, 0.0031, 0.6697, 0.2292], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2595, 0.0261, 0.0907, 0.1911, 0.4327], grad_fn=<DivBackward0>)
from probs:  [0.08337334878280223, 0.08337334878280223, 0.08337334878280223, 0.583133256085989, 0.08337334878280223, 0.08337334878280223]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833733687675914, 0.0833733687675914, 0.0833733687675914, 0.5831331561620432, 0.0833733687675914, 0.0833733687675914]
actions average: 
K:  0  action  0 :  tensor([0.5049, 0.0036, 0.0063, 0.1512, 0.3339], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0037,     0.9858,     0.0006,     0.0006,     0.0093],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0001,     0.9838,     0.0009,     0.0148],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1422,     0.0004,     0.0536,     0.6293,     0.1745],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3542, 0.0139, 0.0528, 0.2345, 0.3446], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337342872190133, 0.08337342872190133, 0.08337342872190133, 0.5831328563904935, 0.08337342872190133, 0.08337342872190133]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337344870665211, 0.08337344870665211, 0.08337344870665211, 0.5831327564667395, 0.08337344870665211, 0.08337344870665211]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
actions average: 
K:  4  action  0 :  tensor([0.6458, 0.0077, 0.0050, 0.1364, 0.2052], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0673, 0.8871, 0.0071, 0.0023, 0.0361], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0009,     0.0008,     0.9173,     0.0420,     0.0390],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0502, 0.0022, 0.0482, 0.5803, 0.3191], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1858, 0.0088, 0.0539, 0.3298, 0.4218], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337346869139334, 0.08337346869139334, 0.08337346869139334, 0.5831326565430334, 0.08337346869139334, 0.08337346869139334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337346869139334, 0.08337346869139334, 0.08337346869139334, 0.5831326565430334, 0.08337346869139334, 0.08337346869139334]
actions average: 
K:  0  action  0 :  tensor([0.7947, 0.0016, 0.0014, 0.0636, 0.1387], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0111,     0.9678,     0.0007,     0.0011,     0.0193],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0013, 0.0092, 0.9444, 0.0163, 0.0288], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1984,     0.0002,     0.0128,     0.6439,     0.1447],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.4344, 0.0132, 0.0390, 0.2242, 0.2891], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337352864555943, 0.08337352864555943, 0.08337352864555943, 0.583132356772203, 0.08337352864555943, 0.08337352864555943]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337352864555943, 0.08337352864555943, 0.08337352864555943, 0.583132356772203, 0.08337352864555943, 0.08337352864555943]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[67.055]
 [67.055]
 [67.055]
 [67.055]
 [67.055]] [[2.001]
 [2.001]
 [2.001]
 [2.001]
 [2.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337354863026228, 0.08337354863026228, 0.08337354863026228, 0.5831322568486887, 0.08337354863026228, 0.08337354863026228]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[68.404]
 [68.404]
 [68.404]
 [68.404]
 [68.404]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337354863026228, 0.08337354863026228, 0.08337354863026228, 0.5831322568486887, 0.08337354863026228, 0.08337354863026228]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833735885996392, 0.0833735885996392, 0.0833735885996392, 0.583132057001804, 0.0833735885996392, 0.0833735885996392]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833736085843133, 0.0833736085843133, 0.0833736085843133, 0.5831319570784337, 0.0833736085843133, 0.0833736085843133]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337362856897777, 0.08337362856897777, 0.08337362856897777, 0.5831318571551112, 0.08337362856897777, 0.08337362856897777]
printing an ep nov before normalisation:  98.80793922368814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833736885229137, 0.0833736885229137, 0.0833736885229137, 0.5831315573854315, 0.0833736885229137, 0.0833736885229137]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337370850753983, 0.08337370850753983, 0.08337370850753983, 0.5831314574623009, 0.08337370850753983, 0.08337370850753983]
actions average: 
K:  2  action  0 :  tensor([0.6666, 0.0072, 0.0015, 0.0700, 0.2546], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0216,     0.9613,     0.0007,     0.0001,     0.0163],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0302,     0.9057,     0.0233,     0.0407],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1108, 0.0526, 0.0392, 0.5764, 0.2209], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1738, 0.0143, 0.0429, 0.2118, 0.5572], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337372849215638, 0.08337372849215638, 0.08337372849215638, 0.5831313575392181, 0.08337372849215638, 0.08337372849215638]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337374847676333, 0.08337374847676333, 0.08337374847676333, 0.5831312576161835, 0.08337374847676333, 0.08337374847676333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337376846136069, 0.08337376846136069, 0.08337376846136069, 0.5831311576931966, 0.08337376846136069, 0.08337376846136069]
actions average: 
K:  2  action  0 :  tensor([    0.6297,     0.0066,     0.0006,     0.1299,     0.2332],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0167,     0.9449,     0.0055,     0.0001,     0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0013,     0.0007,     0.9180,     0.0584,     0.0216],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1111, 0.0029, 0.1488, 0.5724, 0.1649], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2361, 0.0023, 0.0552, 0.3062, 0.4002], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
from probs:  [0.08337380843052662, 0.08337380843052662, 0.08337380843052662, 0.5831309578473668, 0.08337380843052662, 0.08337380843052662]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.38890435989653
printing an ep nov before normalisation:  60.68885148370951
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  64.07609180691011
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337384839965423, 0.08337384839965423, 0.08337384839965423, 0.5831307580017289, 0.08337384839965423, 0.08337384839965423]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337386838420364, 0.08337386838420364, 0.08337386838420364, 0.5831306580789819, 0.08337386838420364, 0.08337386838420364]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337386838420364, 0.08337386838420364, 0.08337386838420364, 0.5831306580789819, 0.08337386838420364, 0.08337386838420364]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337386838420364, 0.08337386838420364, 0.08337386838420364, 0.5831306580789819, 0.08337386838420364, 0.08337386838420364]
from probs:  [0.08337388836874347, 0.08337388836874347, 0.08337388836874347, 0.5831305581562828, 0.08337388836874347, 0.08337388836874347]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337390835327368, 0.08337390835327368, 0.08337390835327368, 0.5831304582336315, 0.08337390835327368, 0.08337390835327368]
using another actor
from probs:  [0.08337392833779433, 0.08337392833779433, 0.08337392833779433, 0.5831303583110283, 0.08337392833779433, 0.08337392833779433]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337392833779433, 0.08337392833779433, 0.08337392833779433, 0.5831303583110283, 0.08337392833779433, 0.08337392833779433]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337392833779433, 0.08337392833779433, 0.08337392833779433, 0.5831303583110283, 0.08337392833779433, 0.08337392833779433]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833739483223054, 0.0833739483223054, 0.0833739483223054, 0.5831302583884731, 0.0833739483223054, 0.0833739483223054]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  126.73100619653037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337406822917032, 0.08337406822917032, 0.08337406822917032, 0.5831296588541485, 0.08337406822917032, 0.08337406822917032]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
using another actor
from probs:  [0.08337408821361425, 0.08337408821361425, 0.08337408821361425, 0.5831295589319289, 0.08337408821361425, 0.08337408821361425]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337410819804858, 0.08337410819804858, 0.08337410819804858, 0.5831294590097572, 0.08337410819804858, 0.08337410819804858]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337410819804858, 0.08337410819804858, 0.08337410819804858, 0.5831294590097572, 0.08337410819804858, 0.08337410819804858]
from probs:  [0.08337410819804858, 0.08337410819804858, 0.08337410819804858, 0.5831294590097572, 0.08337410819804858, 0.08337410819804858]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337412818247333, 0.08337412818247333, 0.08337412818247333, 0.5831293590876334, 0.08337412818247333, 0.08337412818247333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337412818247333, 0.08337412818247333, 0.08337412818247333, 0.5831293590876334, 0.08337412818247333, 0.08337412818247333]
from probs:  [0.08337414816688848, 0.08337414816688848, 0.08337414816688848, 0.5831292591655577, 0.08337414816688848, 0.08337414816688848]
printing an ep nov before normalisation:  8.512547055787577
printing an ep nov before normalisation:  32.47983931617325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337414816688848, 0.08337414816688848, 0.08337414816688848, 0.5831292591655577, 0.08337414816688848, 0.08337414816688848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337418813569002, 0.08337418813569002, 0.08337418813569002, 0.5831290593215499, 0.08337418813569002, 0.08337418813569002]
actions average: 
K:  4  action  0 :  tensor([0.7078, 0.0340, 0.0145, 0.0711, 0.1726], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0285, 0.9495, 0.0035, 0.0011, 0.0173], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0073,     0.9496,     0.0236,     0.0194],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0753, 0.0009, 0.0109, 0.7293, 0.1836], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.4729, 0.1016, 0.1202, 0.0803, 0.2250], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.74436027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337422810445319, 0.08337422810445319, 0.08337422810445319, 0.583128859477734, 0.08337422810445319, 0.08337422810445319]
siam score:  -0.74499804
from probs:  [0.08337426807317802, 0.08337426807317802, 0.08337426807317802, 0.58312865963411, 0.08337426807317802, 0.08337426807317802]
line 256 mcts: sample exp_bonus 77.78808584087223
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7388517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337432802619334, 0.08337432802619334, 0.08337432802619334, 0.5831283598690333, 0.08337432802619334, 0.08337432802619334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337432802619334, 0.08337432802619334, 0.08337432802619334, 0.5831283598690333, 0.08337432802619334, 0.08337432802619334]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337434801051259, 0.08337434801051259, 0.08337434801051259, 0.5831282599474371, 0.08337434801051259, 0.08337434801051259]
from probs:  [0.08337434801051259, 0.08337434801051259, 0.08337434801051259, 0.5831282599474371, 0.08337434801051259, 0.08337434801051259]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337436799482226, 0.08337436799482226, 0.08337436799482226, 0.5831281600258887, 0.08337436799482226, 0.08337436799482226]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337438797912235, 0.08337438797912235, 0.08337438797912235, 0.5831280601043883, 0.08337438797912235, 0.08337438797912235]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.003]
 [0.001]
 [0.001]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337446791622674, 0.08337446791622674, 0.08337446791622674, 0.5831276604188662, 0.08337446791622674, 0.08337446791622674]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337448790047888, 0.08337448790047888, 0.08337448790047888, 0.5831275604976056, 0.08337448790047888, 0.08337448790047888]
printing an ep nov before normalisation:  45.65950291051261
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7134909
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337448790047888, 0.08337448790047888, 0.08337448790047888, 0.5831275604976056, 0.08337448790047888, 0.08337448790047888]
printing an ep nov before normalisation:  72.41503277049341
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337448790047888, 0.08337448790047888, 0.08337448790047888, 0.5831275604976056, 0.08337448790047888, 0.08337448790047888]
from probs:  [0.08337450788472142, 0.08337450788472142, 0.08337450788472142, 0.5831274605763929, 0.08337450788472142, 0.08337450788472142]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337452786895437, 0.08337452786895437, 0.08337452786895437, 0.5831273606552282, 0.08337452786895437, 0.08337452786895437]
printing an ep nov before normalisation:  97.42819726239189
siam score:  -0.7138411
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[ 95.393]
 [102.772]
 [ 93.155]
 [ 85.539]
 [102.191]] [[0.955]
 [1.095]
 [0.912]
 [0.767]
 [1.084]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337458782159568, 0.08337458782159568, 0.08337458782159568, 0.5831270608920215, 0.08337458782159568, 0.08337458782159568]
from probs:  [0.08337460780579029, 0.08337460780579029, 0.08337460780579029, 0.5831269609710487, 0.08337460780579029, 0.08337460780579029]
STARTED EXPV TRAINING ON FRAME NO.  20001
line 256 mcts: sample exp_bonus 98.60263013155395
actions average: 
K:  4  action  0 :  tensor([0.6392, 0.0037, 0.0019, 0.0917, 0.2636], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0020,     0.9761,     0.0025,     0.0001,     0.0193],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0023,     0.9665,     0.0025,     0.0286],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1019, 0.0015, 0.1507, 0.5460, 0.1999], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2832, 0.1094, 0.1625, 0.0818, 0.3631], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337462778997527, 0.08337462778997527, 0.08337462778997527, 0.5831268610501236, 0.08337462778997527, 0.08337462778997527]
line 256 mcts: sample exp_bonus 99.2160073656283
printing an ep nov before normalisation:  115.20148096554854
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833746477741507, 0.0833746477741507, 0.0833746477741507, 0.5831267611292466, 0.0833746477741507, 0.0833746477741507]
printing an ep nov before normalisation:  21.809782959925883
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833746677583165, 0.0833746677583165, 0.0833746677583165, 0.5831266612084174, 0.0833746677583165, 0.0833746677583165]
printing an ep nov before normalisation:  80.9952002630524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337468774247275, 0.08337468774247275, 0.08337468774247275, 0.5831265612876363, 0.08337468774247275, 0.08337468774247275]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337470772661938, 0.08337470772661938, 0.08337470772661938, 0.583126461366903, 0.08337470772661938, 0.08337470772661938]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337470772661938, 0.08337470772661938, 0.08337470772661938, 0.583126461366903, 0.08337470772661938, 0.08337470772661938]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337470772661938, 0.08337470772661938, 0.08337470772661938, 0.583126461366903, 0.08337470772661938, 0.08337470772661938]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337470772661938, 0.08337470772661938, 0.08337470772661938, 0.583126461366903, 0.08337470772661938, 0.08337470772661938]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337470772661938, 0.08337470772661938, 0.08337470772661938, 0.583126461366903, 0.08337470772661938, 0.08337470772661938]
using another actor
from probs:  [0.08337474769488393, 0.08337474769488393, 0.08337474769488393, 0.5831262615255804, 0.08337474769488393, 0.08337474769488393]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.002]
 [0.002]] [[58.045]
 [61.629]
 [54.944]
 [49.838]
 [57.386]] [[1.021]
 [1.107]
 [0.946]
 [0.824]
 [1.005]]
using another actor
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[43.232]
 [55.097]
 [83.528]
 [55.097]
 [55.097]] [[0.311]
 [0.526]
 [1.04 ]
 [0.526]
 [0.526]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337484761537739, 0.08337484761537739, 0.08337484761537739, 0.5831257619231129, 0.08337484761537739, 0.08337484761537739]
printing an ep nov before normalisation:  38.15344174702962
printing an ep nov before normalisation:  73.97715563502342
printing an ep nov before normalisation:  7.2600979660819585
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337488758350767, 0.08337488758350767, 0.08337488758350767, 0.5831255620824617, 0.08337488758350767, 0.08337488758350767]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337488758350767, 0.08337488758350767, 0.08337488758350767, 0.5831255620824617, 0.08337488758350767, 0.08337488758350767]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337490756755843, 0.08337490756755843, 0.08337490756755843, 0.583125462162208, 0.08337490756755843, 0.08337490756755843]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337492755159959, 0.08337492755159959, 0.08337492755159959, 0.5831253622420022, 0.08337492755159959, 0.08337492755159959]
actions average: 
K:  2  action  0 :  tensor([    0.8097,     0.0057,     0.0006,     0.0223,     0.1618],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0050,     0.9886,     0.0009,     0.0000,     0.0054],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0010,     0.0004,     0.9100,     0.0023,     0.0863],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1645, 0.0023, 0.0478, 0.6022, 0.1832], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2632, 0.0247, 0.0081, 0.0836, 0.6205], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  102.58722303039153
siam score:  -0.76028246
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([0.7587, 0.0393, 0.0096, 0.0594, 0.1330], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0280,     0.8888,     0.0116,     0.0003,     0.0713],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0045,     0.9525,     0.0064,     0.0362],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0656, 0.0009, 0.0066, 0.7395, 0.1875], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1458, 0.0333, 0.1386, 0.1287, 0.5537], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337496751965312, 0.08337496751965312, 0.08337496751965312, 0.5831251624017343, 0.08337496751965312, 0.08337496751965312]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337496751965312, 0.08337496751965312, 0.08337496751965312, 0.5831251624017343, 0.08337496751965312, 0.08337496751965312]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337498750366551, 0.08337498750366551, 0.08337498750366551, 0.5831250624816724, 0.08337498750366551, 0.08337498750366551]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337500748766834, 0.08337500748766834, 0.08337500748766834, 0.5831249625616585, 0.08337500748766834, 0.08337500748766834]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337510740753849, 0.08337510740753849, 0.08337510740753849, 0.5831244629623075, 0.08337510740753849, 0.08337510740753849]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337512739148376, 0.08337512739148376, 0.08337512739148376, 0.5831243630425812, 0.08337512739148376, 0.08337512739148376]
printing an ep nov before normalisation:  38.43010187149048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337514737541946, 0.08337514737541946, 0.08337514737541946, 0.5831242631229029, 0.08337514737541946, 0.08337514737541946]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337514737541946, 0.08337514737541946, 0.08337514737541946, 0.5831242631229029, 0.08337514737541946, 0.08337514737541946]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.6464, 0.0096, 0.0010, 0.0930, 0.2501], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0093,     0.9873,     0.0004,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9584,     0.0133,     0.0282],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1749, 0.0007, 0.0009, 0.6132, 0.2103], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2597, 0.0138, 0.0955, 0.1510, 0.4800], grad_fn=<DivBackward0>)
siam score:  -0.76607865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337516735934551, 0.08337516735934551, 0.08337516735934551, 0.5831241632032723, 0.08337516735934551, 0.08337516735934551]
siam score:  -0.7656809
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337520732716892, 0.08337520732716892, 0.08337520732716892, 0.5831239633641553, 0.08337520732716892, 0.08337520732716892]
printing an ep nov before normalisation:  29.746474910701316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337520732716892, 0.08337520732716892, 0.08337520732716892, 0.5831239633641553, 0.08337520732716892, 0.08337520732716892]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337524729495399, 0.08337524729495399, 0.08337524729495399, 0.5831237635252302, 0.08337524729495399, 0.08337524729495399]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337526727883213, 0.08337526727883213, 0.08337526727883213, 0.5831236636058394, 0.08337526727883213, 0.08337526727883213]
actions average: 
K:  4  action  0 :  tensor([    0.4945,     0.0717,     0.0004,     0.2215,     0.2119],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0082,     0.9817,     0.0014,     0.0004,     0.0083],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0015,     0.8371,     0.0985,     0.0625],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0415, 0.0015, 0.1261, 0.7014, 0.1295], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2778, 0.0231, 0.0522, 0.1987, 0.4481], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.584010124206543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337532723040898, 0.08337532723040898, 0.08337532723040898, 0.583123363847955, 0.08337532723040898, 0.08337532723040898]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337536719807898, 0.08337536719807898, 0.08337536719807898, 0.5831231640096051, 0.08337536719807898, 0.08337536719807898]
printing an ep nov before normalisation:  44.12108033921788
printing an ep nov before normalisation:  41.67205114477575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337538718189957, 0.08337538718189957, 0.08337538718189957, 0.5831230640905021, 0.08337538718189957, 0.08337538718189957]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833754071657106, 0.0833754071657106, 0.0833754071657106, 0.583122964171447, 0.0833754071657106, 0.0833754071657106]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833754071657106, 0.0833754071657106, 0.0833754071657106, 0.583122964171447, 0.0833754071657106, 0.0833754071657106]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833754071657106, 0.0833754071657106, 0.0833754071657106, 0.583122964171447, 0.0833754071657106, 0.0833754071657106]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833754071657106, 0.0833754071657106, 0.0833754071657106, 0.583122964171447, 0.0833754071657106, 0.0833754071657106]
using another actor
siam score:  -0.74583143
actions average: 
K:  3  action  0 :  tensor([0.7424, 0.0111, 0.0015, 0.0677, 0.1772], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0128,     0.9827,     0.0003,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0371,     0.8583,     0.0489,     0.0552],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1257, 0.0079, 0.0185, 0.6247, 0.2233], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3748, 0.0082, 0.0043, 0.2075, 0.4053], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.99893463457032
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337550708462184, 0.08337550708462184, 0.08337550708462184, 0.5831224645768909, 0.08337550708462184, 0.08337550708462184]
printing an ep nov before normalisation:  46.05441709851834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833755670358535, 0.0833755670358535, 0.0833755670358535, 0.5831221648207324, 0.0833755670358535, 0.0833755670358535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337558701957822, 0.08337558701957822, 0.08337558701957822, 0.5831220649021089, 0.08337558701957822, 0.08337558701957822]
printing an ep nov before normalisation:  50.43678879737854
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337568693805794, 0.08337568693805794, 0.08337568693805794, 0.5831215653097105, 0.08337568693805794, 0.08337568693805794]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337568693805794, 0.08337568693805794, 0.08337568693805794, 0.5831215653097105, 0.08337568693805794, 0.08337568693805794]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757069217251, 0.0833757069217251, 0.0833757069217251, 0.5831214653913744, 0.0833757069217251, 0.0833757069217251]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757468890307, 0.0833757468890307, 0.0833757468890307, 0.5831212655548467, 0.0833757468890307, 0.0833757468890307]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757468890307, 0.0833757468890307, 0.0833757468890307, 0.5831212655548467, 0.0833757468890307, 0.0833757468890307]
from probs:  [0.0833757468890307, 0.0833757468890307, 0.0833757468890307, 0.5831212655548467, 0.0833757468890307, 0.0833757468890307]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[112.618]
 [112.618]
 [112.618]
 [112.618]
 [112.618]] [[1.92]
 [1.92]
 [1.92]
 [1.92]
 [1.92]]
printing an ep nov before normalisation:  85.14130108766638
siam score:  -0.7322777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337578685629791, 0.08337578685629791, 0.08337578685629791, 0.5831210657185104, 0.08337578685629791, 0.08337578685629791]
from probs:  [0.08337578685629791, 0.08337578685629791, 0.08337578685629791, 0.5831210657185104, 0.08337578685629791, 0.08337578685629791]
line 256 mcts: sample exp_bonus 110.89534726633283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337582682352679, 0.08337582682352679, 0.08337582682352679, 0.583120865882366, 0.08337582682352679, 0.08337582682352679]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833758667907173, 0.0833758667907173, 0.0833758667907173, 0.5831206660464135, 0.0833758667907173, 0.0833758667907173]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337594672498325, 0.08337594672498325, 0.08337594672498325, 0.5831202663750837, 0.08337594672498325, 0.08337594672498325]
printing an ep nov before normalisation:  11.872212671339355
printing an ep nov before normalisation:  31.323932974825063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337600667558205, 0.08337600667558205, 0.08337600667558205, 0.5831199666220898, 0.08337600667558205, 0.08337600667558205]
printing an ep nov before normalisation:  56.91899440666133
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337606662609452, 0.08337606662609452, 0.08337606662609452, 0.5831196668695273, 0.08337606662609452, 0.08337606662609452]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337608660957951, 0.08337608660957951, 0.08337608660957951, 0.5831195669521024, 0.08337608660957951, 0.08337608660957951]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833761265765207, 0.0833761265765207, 0.0833761265765207, 0.5831193671173965, 0.0833761265765207, 0.0833761265765207]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337614655997692, 0.08337614655997692, 0.08337614655997692, 0.5831192672001155, 0.08337614655997692, 0.08337614655997692]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337614655997692, 0.08337614655997692, 0.08337614655997692, 0.5831192672001155, 0.08337614655997692, 0.08337614655997692]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337620651028804, 0.08337620651028804, 0.08337620651028804, 0.58311896744856, 0.08337620651028804, 0.08337620651028804]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337634639401169, 0.08337634639401169, 0.08337634639401169, 0.5831182680299417, 0.08337634639401169, 0.08337634639401169]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337636637736241, 0.08337636637736241, 0.08337636637736241, 0.583118168113188, 0.08337636637736241, 0.08337636637736241]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337638636070356, 0.08337638636070356, 0.08337638636070356, 0.5831180681964823, 0.08337638636070356, 0.08337638636070356]
siam score:  -0.7227192
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337640634403513, 0.08337640634403513, 0.08337640634403513, 0.5831179682798245, 0.08337640634403513, 0.08337640634403513]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.032]
 [0.042]
 [0.004]
 [0.002]] [[38.054]
 [33.408]
 [43.035]
 [42.677]
 [41.989]] [[0.951]
 [0.787]
 [1.201]
 [1.149]
 [1.117]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337654622708754, 0.08337654622708754, 0.08337654622708754, 0.5831172688645624, 0.08337654622708754, 0.08337654622708754]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337654622708754, 0.08337654622708754, 0.08337654622708754, 0.5831172688645624, 0.08337654622708754, 0.08337654622708754]
actions average: 
K:  0  action  0 :  tensor([0.5588, 0.0317, 0.0011, 0.1082, 0.3002], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0147,     0.9396,     0.0047,     0.0001,     0.0409],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9759,     0.0113,     0.0126],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0903,     0.0003,     0.0048,     0.6813,     0.2233],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3471, 0.0380, 0.0022, 0.2053, 0.4074], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  74.96757189620205
printing an ep nov before normalisation:  105.40589050279635
siam score:  -0.72112215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337656621034238, 0.08337656621034238, 0.08337656621034238, 0.583117168948288, 0.08337656621034238, 0.08337656621034238]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337660617682331, 0.08337660617682331, 0.08337660617682331, 0.5831169691158835, 0.08337660617682331, 0.08337660617682331]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337660617682331, 0.08337660617682331, 0.08337660617682331, 0.5831169691158835, 0.08337660617682331, 0.08337660617682331]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([0.7931, 0.0008, 0.0011, 0.0595, 0.1455], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0108,     0.9340,     0.0368,     0.0002,     0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0019,     0.0004,     0.9324,     0.0170,     0.0483],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1605, 0.0014, 0.0067, 0.5943, 0.2371], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2576, 0.0007, 0.0396, 0.3229, 0.3792], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[19.479]
 [23.49 ]
 [57.029]
 [18.002]
 [57.029]] [[0.276]
 [0.405]
 [1.485]
 [0.228]
 [1.485]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 53.640517478411226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  67.86584854125977
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  61.92967250013792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  8.559717777555989
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  1.7807525654951917
printing an ep nov before normalisation:  63.07679915788261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.039846533438364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  28.088754834139724
printing an ep nov before normalisation:  20.148300376737023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.73136955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.003]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.003]
 [0.003]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  63.53423497982302
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([0.6989, 0.0345, 0.0011, 0.0853, 0.1803], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0069,     0.9375,     0.0006,     0.0003,     0.0547],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0304,     0.9402,     0.0040,     0.0252],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0588,     0.0005,     0.0239,     0.7473,     0.1695],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1608, 0.0028, 0.2150, 0.2764, 0.3450], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
siam score:  -0.7344072
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  1
siam score:  -0.7339679
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.001]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.001]
 [0.002]
 [0.002]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
siam score:  -0.7561093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
siam score:  -0.7568327
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  93.04658865054665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[39.926]
 [48.291]
 [67.665]
 [51.791]
 [45.936]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using another actor
printing an ep nov before normalisation:  48.5859333367098
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  42.16631615036587
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  43.511128425598145
siam score:  -0.7512555
main train batch thing paused
add a thread
Adding thread: now have 2 threads
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  61.43930268345655
using another actor
printing an ep nov before normalisation:  28.182194232940674
printing an ep nov before normalisation:  97.66597948439697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
siam score:  -0.7219431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  44.86440337772377
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  67.13159902693029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  46.3437731290783
printing an ep nov before normalisation:  26.119852417475954
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  58.35657247982205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using another actor
printing an ep nov before normalisation:  66.03710627290029
printing an ep nov before normalisation:  43.85822950828136
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  26.272814762645375
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  74.28606553394613
printing an ep nov before normalisation:  23.34125849955279
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  70.10160446166992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  91.91831616089274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  46.87400880610085
printing an ep nov before normalisation:  69.05196329088344
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  24.47231875132296
siam score:  -0.6990607
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
actions average: 
K:  4  action  0 :  tensor([    0.7784,     0.1015,     0.0003,     0.0044,     0.1154],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0686,     0.9266,     0.0004,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0012, 0.0060, 0.8287, 0.0109, 0.1531], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0672, 0.0027, 0.0057, 0.6646, 0.2597], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2541, 0.0332, 0.0072, 0.2524, 0.4531], grad_fn=<DivBackward0>)
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.003]
 [0.002]
 [0.009]] [[44.103]
 [44.103]
 [51.584]
 [54.05 ]
 [44.103]] [[0.99 ]
 [0.99 ]
 [1.241]
 [1.325]
 [0.99 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using another actor
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using another actor
using explorer policy with actor:  1
siam score:  -0.68889946
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.13732371286713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  74.0865629705117
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
printing an ep nov before normalisation:  33.72213363647461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  111.74318572724488
printing an ep nov before normalisation:  100.81173089923482
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  94.36611627497325
printing an ep nov before normalisation:  76.85618115961704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
line 256 mcts: sample exp_bonus 70.06929690851096
actions average: 
K:  3  action  0 :  tensor([0.7420, 0.0618, 0.0011, 0.0490, 0.1460], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0080,     0.9820,     0.0012,     0.0000,     0.0088],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0210,     0.9358,     0.0276,     0.0156],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1059,     0.0004,     0.0075,     0.7290,     0.1572],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2347, 0.0080, 0.0964, 0.1834, 0.4775], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
using another actor
actions average: 
K:  4  action  0 :  tensor([    0.8289,     0.0172,     0.0007,     0.0377,     0.1155],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0197,     0.9630,     0.0003,     0.0000,     0.0170],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0011,     0.0006,     0.8838,     0.0741,     0.0404],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1011, 0.0016, 0.0130, 0.7162, 0.1681], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2964, 0.0438, 0.1081, 0.1126, 0.4392], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
siam score:  -0.751985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06197763831960879, 0.06197763831960879, 0.3187781984513942, 0.43331124827017053, 0.06197763831960879, 0.06197763831960879]
printing an ep nov before normalisation:  22.759521007537842
from probs:  [0.06197763831960879, 0.06197763831960879, 0.3187781984513942, 0.43331124827017053, 0.06197763831960879, 0.06197763831960879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06197763831960879, 0.06197763831960879, 0.3187781984513942, 0.43331124827017053, 0.06197763831960879, 0.06197763831960879]
printing an ep nov before normalisation:  59.8889836306236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06197763831960879, 0.06197763831960879, 0.3187781984513942, 0.43331124827017053, 0.06197763831960879, 0.06197763831960879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062043387422043024, 0.062043387422043024, 0.31805467431753315, 0.43377177599429473, 0.062043387422043024, 0.062043387422043024]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062173680560274086, 0.062173680560274086, 0.3166208869881781, 0.4346843907707256, 0.062173680560274086, 0.062173680560274086]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062238231935908, 0.062238231935908, 0.3159105430227711, 0.43513652923359686, 0.062238231935908, 0.062238231935908]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062302391334829184, 0.062302391334829184, 0.31520451249437076, 0.43558592216631253, 0.062302391334829184, 0.062302391334829184]
from probs:  [0.062302391334829184, 0.062302391334829184, 0.31520451249437076, 0.43558592216631253, 0.062302391334829184, 0.062302391334829184]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062429548397588304, 0.062429548397588304, 0.3138052355416679, 0.4364765708679788, 0.062429548397588304, 0.062429548397588304]
printing an ep nov before normalisation:  61.511200087342374
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.09836858389043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06261743177105819, 0.06261743177105819, 0.31173770686030433, 0.43779256605546296, 0.06261743177105819, 0.06261743177105819]
siam score:  -0.7224355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06261743177105819, 0.06261743177105819, 0.31173770686030433, 0.43779256605546296, 0.06261743177105819, 0.06261743177105819]
printing an ep nov before normalisation:  86.39717978468572
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  16.698928790302716
printing an ep nov before normalisation:  43.06076897515191
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06274082544431264, 0.06274082544431264, 0.31037984344833336, 0.43865685477441607, 0.06274082544431264, 0.06274082544431264]
printing an ep nov before normalisation:  44.683425406067116
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[41.761]
 [41.761]
 [41.761]
 [41.761]
 [41.761]] [[1.738]
 [1.738]
 [1.738]
 [1.738]
 [1.738]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06286276040539411, 0.06286276040539411, 0.3090380321712579, 0.43951092620716575, 0.06286276040539411, 0.06286276040539411]
actions average: 
K:  2  action  0 :  tensor([0.7207, 0.0063, 0.0013, 0.0631, 0.2086], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0268,     0.9626,     0.0013,     0.0000,     0.0093],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0011,     0.9436,     0.0195,     0.0358],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.2109,     0.0004,     0.0009,     0.6066,     0.1813],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2606, 0.0059, 0.0386, 0.2412, 0.4538], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.22414620717367
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.004]] [[88.074]
 [88.074]
 [88.63 ]
 [86.953]
 [81.75 ]] [[1.244]
 [1.244]
 [1.254]
 [1.224]
 [1.13 ]]
deleting a thread, now have 1 threads
Frames:  28479 train batches done:  3334 episodes:  2429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06304298384738174, 0.06304298384738174, 0.3070547958237199, 0.4407732687867532, 0.06304298384738174, 0.06304298384738174]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06304298384738174, 0.06304298384738174, 0.3070547958237199, 0.4407732687867532, 0.06304298384738174, 0.06304298384738174]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06304298384738174, 0.06304298384738174, 0.3070547958237199, 0.4407732687867532, 0.06304298384738174, 0.06304298384738174]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06304298384738174, 0.06304298384738174, 0.3070547958237199, 0.4407732687867532, 0.06304298384738174, 0.06304298384738174]
siam score:  -0.7061563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06310235644817287, 0.06310235644817287, 0.30640144075160386, 0.44118913345570476, 0.06310235644817287, 0.06310235644817287]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06310235644817287, 0.06310235644817287, 0.30640144075160386, 0.44118913345570476, 0.06310235644817287, 0.06310235644817287]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06310235644817287, 0.06310235644817287, 0.30640144075160386, 0.44118913345570476, 0.06310235644817287, 0.06310235644817287]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0631613832193577, 0.0631613832193577, 0.305751891298988, 0.44160257582358103, 0.0631613832193577, 0.0631613832193577]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06327841128911019, 0.06327841128911019, 0.30446407702213457, 0.4424222778214245, 0.06327841128911019, 0.06327841128911019]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
from probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.167 0.292 0.208]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[64.01]
 [64.01]
 [64.01]
 [64.01]
 [64.01]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
using another actor
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0004],
        [0.0000],
        [0.0005],
        [0.0000],
        [0.0009],
        [0.0009],
        [0.0182],
        [0.0007],
        [0.0007],
        [0.0003]], dtype=torch.float64)
0.0 0.00044925885882661325
0.0 0.0
0.0 0.0005213553449758256
0.0 0.0
0.0 0.0008720395919024807
0.0 0.0008907520753732008
0.0 0.018185375729191156
0.0 0.0007142623883882938
0.0 0.0007119544737265909
0.0 0.0003104199394358771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06333641850904743, 0.06333641850904743, 0.30382574703725346, 0.44282857892655664, 0.06333641850904743, 0.06333641850904743]
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.114126352923677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06339409174311637, 0.06339409174311637, 0.3031910923395953, 0.4432325406879391, 0.06339409174311637, 0.06339409174311637]
siam score:  -0.699846
siam score:  -0.7021332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06339409174311637, 0.06339409174311637, 0.3031910923395953, 0.4432325406879391, 0.06339409174311637, 0.06339409174311637]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.06339409174311637, 0.06339409174311637, 0.3031910923395953, 0.4432325406879391, 0.06339409174311637, 0.06339409174311637]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06743101736779845, 0.06743101736779845, 0.2587593900519916, 0.47151654047681446, 0.06743101736779845, 0.06743101736779845]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06743101736779845, 0.06743101736779845, 0.2587593900519916, 0.47151654047681446, 0.06743101736779845, 0.06743101736779845]
printing an ep nov before normalisation:  90.67362238682875
Printing some Q and Qe and total Qs values:  [[1.539]
 [1.539]
 [1.539]
 [1.539]
 [1.539]] [[35.685]
 [35.685]
 [35.685]
 [35.685]
 [35.685]] [[2.253]
 [2.253]
 [2.253]
 [2.253]
 [2.253]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  63.62700701860045
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07011041558260872, 0.07011041558260872, 0.22926907121567122, 0.4902892664538937, 0.07011041558260872, 0.07011041558260872]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07011041558260872, 0.07011041558260872, 0.22926907121567122, 0.4902892664538937, 0.07011041558260872, 0.07011041558260872]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
from probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  83.23211159751001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
printing an ep nov before normalisation:  101.3816747974314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701525027115491, 0.0701525027115491, 0.2288059229117424, 0.49058406624206125, 0.0701525027115491, 0.0701525027115491]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701943234820954, 0.0701943234820954, 0.2283457057518844, 0.4908770003197342, 0.0701943234820954, 0.0701943234820954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701943234820954, 0.0701943234820954, 0.2283457057518844, 0.4908770003197342, 0.0701943234820954, 0.0701943234820954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701943234820954, 0.0701943234820954, 0.2283457057518844, 0.4908770003197342, 0.0701943234820954, 0.0701943234820954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0701943234820954, 0.0701943234820954, 0.2283457057518844, 0.4908770003197342, 0.0701943234820954, 0.0701943234820954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07023588041484008, 0.07023588041484008, 0.22788839199820776, 0.49116808634243186, 0.07023588041484008, 0.07023588041484008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07023588041484008, 0.07023588041484008, 0.22788839199820776, 0.49116808634243186, 0.07023588041484008, 0.07023588041484008]
printing an ep nov before normalisation:  53.50461806369743
printing an ep nov before normalisation:  14.844804174940691
from probs:  [0.07023588041484008, 0.07023588041484008, 0.22788839199820776, 0.49116808634243186, 0.07023588041484008, 0.07023588041484008]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07027717599867196, 0.07027717599867196, 0.22743395426170682, 0.49145734174360545, 0.07027717599867196, 0.07027717599867196]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07027717599867196, 0.07027717599867196, 0.22743395426170682, 0.49145734174360545, 0.07027717599867196, 0.07027717599867196]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07031821269127302, 0.07031821269127302, 0.22698236549679113, 0.491744783738117, 0.07031821269127302, 0.07031821269127302]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07031821269127302, 0.07031821269127302, 0.22698236549679113, 0.491744783738117, 0.07031821269127302, 0.07031821269127302]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.070358992919606, 0.070358992919606, 0.22653359899592065, 0.4920304293256555, 0.070358992919606, 0.070358992919606]
siam score:  -0.6599366
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
printing an ep nov before normalisation:  54.16801245357304
siam score:  -0.6634511
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.070358992919606, 0.070358992919606, 0.22653359899592065, 0.4920304293256555, 0.070358992919606, 0.070358992919606]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.070358992919606, 0.070358992919606, 0.22653359899592065, 0.4920304293256555, 0.070358992919606, 0.070358992919606]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.070358992919606, 0.070358992919606, 0.22653359899592065, 0.4920304293256555, 0.070358992919606, 0.070358992919606]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.004]
 [0.007]
 [0.007]
 [0.003]] [[74.362]
 [83.7  ]
 [74.362]
 [74.362]
 [77.384]] [[0.982]
 [1.219]
 [0.982]
 [0.982]
 [1.056]]
siam score:  -0.6606007
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  84.70968140496147
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07039951908039292, 0.07039951908039292, 0.22608762838433966, 0.49231429529408866, 0.07039951908039292, 0.07039951908039292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07039951908039292, 0.07039951908039292, 0.22608762838433966, 0.49231429529408866, 0.07039951908039292, 0.07039951908039292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07039951908039292, 0.07039951908039292, 0.22608762838433966, 0.49231429529408866, 0.07039951908039292, 0.07039951908039292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07043979354058462, 0.07043979354058462, 0.22564442761491046, 0.49259639822275103, 0.07043979354058462, 0.07043979354058462]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07047981863782149, 0.07047981863782149, 0.22520397096304154, 0.4928767544856723, 0.07047981863782149, 0.07047981863782149]
printing an ep nov before normalisation:  28.218711252683253
siam score:  -0.662688
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07047981863782149, 0.07047981863782149, 0.22520397096304154, 0.4928767544856723, 0.07047981863782149, 0.07047981863782149]
actions average: 
K:  4  action  0 :  tensor([    0.8691,     0.0113,     0.0008,     0.0335,     0.0854],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0330,     0.9555,     0.0013,     0.0001,     0.0101],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0266,     0.8361,     0.0212,     0.1159],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0745, 0.0032, 0.1229, 0.4752, 0.3242], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1802, 0.0017, 0.0802, 0.0471, 0.6908], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07051959668088588, 0.07051959668088588, 0.22476623302171067, 0.49315538025474576, 0.07051959668088588, 0.07051959668088588]
actions average: 
K:  0  action  0 :  tensor([    0.6946,     0.0058,     0.0007,     0.1142,     0.1847],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9986,     0.0001,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0007,     0.9315,     0.0010,     0.0666],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0905,     0.0007,     0.0218,     0.7557,     0.1313],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1338, 0.0090, 0.3006, 0.2115, 0.3450], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07051959668088588, 0.07051959668088588, 0.22476623302171067, 0.49315538025474576, 0.07051959668088588, 0.07051959668088588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  20.537038601958102
printing an ep nov before normalisation:  25.71230290109576
siam score:  -0.65675473
printing an ep nov before normalisation:  66.8537712097168
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07051959668088588, 0.07051959668088588, 0.22476623302171067, 0.49315538025474576, 0.07051959668088588, 0.07051959668088588]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07055912995014603, 0.07055912995014603, 0.2243311886965791, 0.493432291502837, 0.07055912995014603, 0.07055912995014603]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07059842069799167, 0.07059842069799167, 0.2238988132011964, 0.4937075040068368, 0.07059842069799167, 0.07059842069799167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07059842069799167, 0.07059842069799167, 0.2238988132011964, 0.4937075040068368, 0.07059842069799167, 0.07059842069799167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07063747114926217, 0.07063747114926217, 0.22346908205229318, 0.493981033350658, 0.07063747114926217, 0.07063747114926217]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07067628350166608, 0.07067628350166608, 0.22304197106515905, 0.4942528949281766, 0.07067628350166608, 0.07067628350166608]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07067628350166608, 0.07067628350166608, 0.22304197106515905, 0.4942528949281766, 0.07067628350166608, 0.07067628350166608]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07067628350166608, 0.07067628350166608, 0.22304197106515905, 0.4942528949281766, 0.07067628350166608, 0.07067628350166608]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  105.3594747918995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07071485992619368, 0.07071485992619368, 0.22261745634910599, 0.49452310394611904, 0.07071485992619368, 0.07071485992619368]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07079131354441096, 0.07079131354441096, 0.22177612161095536, 0.49505862421140073, 0.07079131354441096, 0.07079131354441096]
siam score:  -0.65352464
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2842],
        [0.0006],
        [0.0000],
        [0.0008],
        [0.0000],
        [0.0008],
        [0.0000],
        [0.0004],
        [0.0004],
        [0.0000]], dtype=torch.float64)
0.0 0.28415463046866474
0.0 0.0005845196745734041
0.0 0.0
0.0 0.0008052270053123224
0.0 0.0
0.0 0.0008052270053123224
0.0 0.0
0.0 0.0004263024022025001
0.0 0.0004316577186653906
0.0 0.0
printing an ep nov before normalisation:  49.29741244807155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[77.393]
 [77.393]
 [77.393]
 [77.393]
 [77.393]] [[0.945]
 [0.945]
 [0.945]
 [0.945]
 [0.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
line 256 mcts: sample exp_bonus 45.97422249981378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
printing an ep nov before normalisation:  22.00523547957087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.43687092375679
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.07082919495009617, 0.07082919495009617, 0.22135925523790317, 0.4953239649617119, 0.07082919495009617, 0.07082919495009617]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06082829461499446, 0.06082829461499446, 0.18960718036802926, 0.4252725412960829, 0.20263539449090442, 0.06082829461499446]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06082829461499446, 0.06082829461499446, 0.18960718036802926, 0.4252725412960829, 0.20263539449090442, 0.06082829461499446]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.060861700111389604, 0.060861700111389604, 0.18971146349509943, 0.4255065304872884, 0.20219690568344342, 0.060861700111389604]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05396839129476371, 0.05396839129476371, 0.2816119664757567, 0.37722226805177383, 0.17926059158817848, 0.05396839129476371]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05396839129476371, 0.05396839129476371, 0.2816119664757567, 0.37722226805177383, 0.17926059158817848, 0.05396839129476371]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054011460929429274, 0.054011460929429274, 0.2810377689177694, 0.377523949812814, 0.17940389848112878, 0.054011460929429274]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054054297624689183, 0.054054297624689183, 0.2804666768710995, 0.3778239999470559, 0.1795464303077771, 0.054054297624689183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05408045735049063, 0.05408045735049063, 0.28060268021950097, 0.3780072360531755, 0.17914871167585156, 0.05408045735049063]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054165521984068384, 0.054165521984068384, 0.27946937635382674, 0.37860307227652046, 0.17943098541744756, 0.054165521984068384]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054165521984068384, 0.054165521984068384, 0.27946937635382674, 0.37860307227652046, 0.17943098541744756, 0.054165521984068384]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05420771207582793, 0.05420771207582793, 0.2789072838357824, 0.3788985932689622, 0.17957098666777163, 0.05420771207582793]
from probs:  [0.05420771207582793, 0.05420771207582793, 0.2789072838357824, 0.3788985932689622, 0.17957098666777163, 0.05420771207582793]
printing an ep nov before normalisation:  26.07576653624755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05429141693905855, 0.05429141693905855, 0.27779209599466526, 0.3794849049649663, 0.1798487482231928, 0.05429141693905855]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05429141693905855, 0.05429141693905855, 0.27779209599466526, 0.3794849049649663, 0.1798487482231928, 0.05429141693905855]
printing an ep nov before normalisation:  80.80217571720993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04964788249644288, 0.04964788249644288, 0.34695945139798867, 0.34200425858296296, 0.16209264252971983, 0.04964788249644288]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04964788249644288, 0.04964788249644288, 0.34695945139798867, 0.34200425858296296, 0.16209264252971983, 0.04964788249644288]
printing an ep nov before normalisation:  75.01231146042666
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04964788249644288, 0.04964788249644288, 0.34695945139798867, 0.34200425858296296, 0.16209264252971983, 0.04964788249644288]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04964788249644288, 0.04964788249644288, 0.34695945139798867, 0.34200425858296296, 0.16209264252971983, 0.04964788249644288]
printing an ep nov before normalisation:  74.27286411368537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04964788249644288, 0.04964788249644288, 0.34695945139798867, 0.34200425858296296, 0.16209264252971983, 0.04964788249644288]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.004]
 [0.001]
 [0.001]
 [0.001]] [[ 6.233]
 [ 5.408]
 [41.653]
 [41.653]
 [41.653]] [[0.083]
 [0.063]
 [1.084]
 [1.084]
 [1.084]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049579982589986894, 0.049579982589986894, 0.3464837978689537, 0.3425250803319009, 0.16225117402918457, 0.049579982589986894]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049533654198271354, 0.049533654198271354, 0.34615924246054447, 0.34319298657792174, 0.16204680836671978, 0.049533654198271354]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[67.236]
 [68.88 ]
 [67.236]
 [67.236]
 [67.236]] [[1.505]
 [1.575]
 [1.505]
 [1.505]
 [1.505]]
siam score:  -0.67220205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04957599193082083, 0.04957599193082083, 0.34645579772667334, 0.3434869996687149, 0.16132922681214934, 0.04957599193082083]
siam score:  -0.6747354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04950850319417707, 0.04950850319417707, 0.34598302444179335, 0.34400652763347594, 0.16148493834219946, 0.04950850319417707]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04950850319417707, 0.04950850319417707, 0.34598302444179335, 0.34400652763347594, 0.16148493834219946, 0.04950850319417707]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04950850319417707, 0.04950850319417707, 0.34598302444179335, 0.34400652763347594, 0.16148493834219946, 0.04950850319417707]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.009]
 [0.003]
 [0.003]
 [0.002]] [[ 79.451]
 [ 76.93 ]
 [111.751]
 [112.379]
 [ 79.451]] [[0.652]
 [0.639]
 [0.917]
 [0.922]
 [0.652]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04949183705830709, 0.04949183705830709, 0.34390344734283645, 0.34586619141139996, 0.16175485007084225, 0.04949183705830709]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04949183705830709, 0.04949183705830709, 0.34390344734283645, 0.34586619141139996, 0.16175485007084225, 0.04949183705830709]
line 256 mcts: sample exp_bonus 91.55491624283091
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04963530241894513, 0.04963530241894513, 0.34199837828119556, 0.34687109621223305, 0.162224618249736, 0.04963530241894513]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.4649647278492
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  8.385695219039917
from probs:  [0.04970377940202481, 0.04970377940202481, 0.34151452905019763, 0.34735074404316113, 0.1620233887005668, 0.04970377940202481]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04970377940202481, 0.04970377940202481, 0.34151452905019763, 0.34735074404316113, 0.1620233887005668, 0.04970377940202481]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04981899985880919, 0.04981899985880919, 0.3404087471558369, 0.3481578070837576, 0.16197644618397777, 0.04981899985880919]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04981899985880919, 0.04981899985880919, 0.3404087471558369, 0.3481578070837576, 0.16197644618397777, 0.04981899985880919]
actions average: 
K:  1  action  0 :  tensor([0.7664, 0.0070, 0.0012, 0.0938, 0.1316], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0043,     0.9576,     0.0187,     0.0000,     0.0194],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9768,     0.0023,     0.0207],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0793, 0.0614, 0.0136, 0.7466, 0.0991], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1996, 0.0425, 0.0719, 0.2048, 0.4812], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04981899985880919, 0.04981899985880919, 0.3404087471558369, 0.3481578070837576, 0.16197644618397777, 0.04981899985880919]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04981899985880919, 0.04981899985880919, 0.3404087471558369, 0.3481578070837576, 0.16197644618397777, 0.04981899985880919]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04981899985880919, 0.04981899985880919, 0.3404087471558369, 0.3481578070837576, 0.16197644618397777, 0.04981899985880919]
printing an ep nov before normalisation:  107.26249796065322
printing an ep nov before normalisation:  59.92015361785889
siam score:  -0.670253
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.7825,     0.0232,     0.0004,     0.0458,     0.1481],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0019,     0.9270,     0.0189,     0.0005,     0.0517],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0005,     0.9330,     0.0287,     0.0377],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0926,     0.0004,     0.0632,     0.7933,     0.0505],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1669, 0.0901, 0.0735, 0.0836, 0.5859], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049954210913252056, 0.049954210913252056, 0.33945487244887124, 0.34910489450005844, 0.16157760031131413, 0.049954210913252056]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049954210913252056, 0.049954210913252056, 0.33945487244887124, 0.34910489450005844, 0.16157760031131413, 0.049954210913252056]
using another actor
actions average: 
K:  4  action  0 :  tensor([0.6006, 0.0008, 0.0028, 0.1726, 0.2232], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0124,     0.9212,     0.0488,     0.0001,     0.0176],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0075,     0.9531,     0.0271,     0.0122],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1441, 0.0015, 0.0085, 0.7611, 0.0848], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2424, 0.0018, 0.2409, 0.1546, 0.3603], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.898780896876435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05004691351179649, 0.05004691351179649, 0.3382270264221592, 0.3497542309385737, 0.16187800210387754, 0.05004691351179649]
printing an ep nov before normalisation:  42.8603140826119
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050092948387644824, 0.050092948387644824, 0.33761729443742466, 0.3500766827662485, 0.16202717763339242, 0.050092948387644824]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[85.477]
 [85.477]
 [85.477]
 [85.477]
 [85.477]] [[1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
printing an ep nov before normalisation:  67.2808809804193
printing an ep nov before normalisation:  67.33489245581754
printing an ep nov before normalisation:  50.38037558120462
siam score:  -0.6892879
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05013877423027999, 0.05013877423027999, 0.3370103310978441, 0.3503976704183304, 0.16217567579298536, 0.05013877423027999]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05018439246023059, 0.05018439246023059, 0.33640611758851474, 0.350717203844929, 0.16232350118586433, 0.05018439246023059]
printing an ep nov before normalisation:  61.27805357351434
siam score:  -0.6897101
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05040941824731338, 0.05040941824731338, 0.33342565077403935, 0.3522933996091544, 0.163052694874866, 0.05040941824731338]
siam score:  -0.70177317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05040941824731338, 0.05040941824731338, 0.33342565077403935, 0.3522933996091544, 0.163052694874866, 0.05040941824731338]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05045381992311892, 0.05045381992311892, 0.3328375505257918, 0.3526044116679789, 0.16319657803687262, 0.05045381992311892]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050516752638699446, 0.050516752638699446, 0.3332536423335581, 0.35304522461219817, 0.16215087513814547, 0.050516752638699446]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05060518422073634, 0.05060518422073634, 0.33208455924683755, 0.35366464466550535, 0.1624352434254481, 0.05060518422073634]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  78.50200486778346
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05060518422073634, 0.05060518422073634, 0.33208455924683755, 0.35366464466550535, 0.1624352434254481, 0.05060518422073634]
siam score:  -0.7116345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05060518422073634, 0.05060518422073634, 0.33208455924683755, 0.35366464466550535, 0.1624352434254481, 0.05060518422073634]
siam score:  -0.7152768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.050649105681762535, 0.050649105681762535, 0.331503908804461, 0.3539722930542769, 0.16257648109597447, 0.050649105681762535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05069283266050082, 0.05069283266050082, 0.33092582945657606, 0.3542785791895824, 0.16271709337233906, 0.05069283266050082]
printing an ep nov before normalisation:  101.4062212886
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05103461324505563, 0.05103461324505563, 0.32804788097014925, 0.3566725853017423, 0.1621756939929417, 0.05103461324505563]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05103461324505563, 0.05103461324505563, 0.32804788097014925, 0.3566725853017423, 0.1621756939929417, 0.05103461324505563]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05107715370707833, 0.05107715370707833, 0.32748685861044174, 0.35697056046680053, 0.1623111198015228, 0.05107715370707833]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051118134143633594, 0.051118134143633594, 0.3277501892082837, 0.3572576084151798, 0.16163779994563582, 0.051118134143633594]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051118134143633594, 0.051118134143633594, 0.3277501892082837, 0.3572576084151798, 0.16163779994563582, 0.051118134143633594]
from probs:  [0.051202797510666566, 0.051202797510666566, 0.32663498594028334, 0.35785063396230665, 0.16190598756541025, 0.051202797510666566]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0512231584569765, 0.0512231584569765, 0.3267651590490479, 0.3579932524494827, 0.16157211313053987, 0.0512231584569765]
actions average: 
K:  3  action  0 :  tensor([    0.8900,     0.0013,     0.0001,     0.0109,     0.0976],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0020,     0.9669,     0.0184,     0.0004,     0.0122],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0005,     0.9610,     0.0036,     0.0349],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0386,     0.0003,     0.0050,     0.8546,     0.1015],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1840, 0.1109, 0.1184, 0.0999, 0.4867], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051265248877533344, 0.051265248877533344, 0.32621106357868224, 0.3582880752938163, 0.16170511449490133, 0.051265248877533344]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051265248877533344, 0.051265248877533344, 0.32621106357868224, 0.3582880752938163, 0.16170511449490133, 0.051265248877533344]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051265248877533344, 0.051265248877533344, 0.32621106357868224, 0.3582880752938163, 0.16170511449490133, 0.051265248877533344]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051265248877533344, 0.051265248877533344, 0.32621106357868224, 0.3582880752938163, 0.16170511449490133, 0.051265248877533344]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05130715755057313, 0.05130715755057313, 0.3256593607067896, 0.35858162508553565, 0.16183754155595534, 0.05130715755057313]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05130715755057313, 0.05130715755057313, 0.3256593607067896, 0.35858162508553565, 0.16183754155595534, 0.05130715755057313]
printing an ep nov before normalisation:  18.074861511157394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05130715755057313, 0.05130715755057313, 0.3256593607067896, 0.35858162508553565, 0.16183754155595534, 0.05130715755057313]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05130715755057313, 0.05130715755057313, 0.3256593607067896, 0.35858162508553565, 0.16183754155595534, 0.05130715755057313]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05134888565074545, 0.05134888565074545, 0.3251100349698035, 0.3588739100524873, 0.16196939802547272, 0.05134888565074545]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051410798122076905, 0.051410798122076905, 0.32469196252606264, 0.35930757668390084, 0.1617680664238059, 0.051410798122076905]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051431032459849625, 0.051431032459849625, 0.324820034720421, 0.3594493083400934, 0.16143755955993666, 0.051431032459849625]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051431032459849625, 0.051431032459849625, 0.324820034720421, 0.3594493083400934, 0.16143755955993666, 0.051431032459849625]
actions average: 
K:  2  action  0 :  tensor([    0.8309,     0.0141,     0.0005,     0.0340,     0.1204],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0019,     0.9949,     0.0003,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9608,     0.0029,     0.0360],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0971, 0.0014, 0.0019, 0.7394, 0.1602], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0861, 0.0033, 0.1793, 0.1713, 0.5600], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051431032459849625, 0.051431032459849625, 0.324820034720421, 0.3594493083400934, 0.16143755955993666, 0.051431032459849625]
siam score:  -0.7746631
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05090960468790447, 0.05090960468790447, 0.3498211695884893, 0.35579940088650097, 0.14165061546129626, 0.05090960468790447]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.05090960468790447, 0.05090960468790447, 0.3498211695884893, 0.35579940088650097, 0.14165061546129626, 0.05090960468790447]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046272380968615585, 0.046272380968615585, 0.317885429214968, 0.32331769017989503, 0.12872634204340114, 0.13752577662450471]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046272380968615585, 0.046272380968615585, 0.317885429214968, 0.32331769017989503, 0.12872634204340114, 0.13752577662450471]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046272380968615585, 0.046272380968615585, 0.317885429214968, 0.32331769017989503, 0.12872634204340114, 0.13752577662450471]
printing an ep nov before normalisation:  27.88407802581787
actions average: 
K:  4  action  0 :  tensor([    0.9015,     0.0115,     0.0003,     0.0042,     0.0825],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0359,     0.9497,     0.0003,     0.0000,     0.0141],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0007,     0.9849,     0.0048,     0.0096],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1016,     0.0008,     0.0022,     0.8015,     0.0940],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1125, 0.0057, 0.3384, 0.0924, 0.4511], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04630253959297942, 0.04630253959297942, 0.3180931263320382, 0.32352893806681937, 0.12851677401582637, 0.13725608239935735]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04630253959297942, 0.04630253959297942, 0.3180931263320382, 0.32352893806681937, 0.12851677401582637, 0.13725608239935735]
siam score:  -0.77065086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04642618122094676, 0.04642618122094676, 0.31656108114220644, 0.32439499323992305, 0.12856826515090425, 0.1376232980250728]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04296372440106962, 0.04296372440106962, 0.2921674870895233, 0.3001420074955538, 0.19442325508097782, 0.1273398015318059]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04653080479448117, 0.04653080479448117, 0.32512975238152136, 0.27721073339655045, 0.18238470856484, 0.12221319606812597]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046507398774839566, 0.046507398774839566, 0.32496577581441227, 0.27773923506850073, 0.18220683321947642, 0.12207335834793147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046475440528620265, 0.046475440528620265, 0.32474189434856915, 0.27821574326987364, 0.1824732707758347, 0.12161821054848196]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.046475440528620265, 0.046475440528620265, 0.32474189434856915, 0.27821574326987364, 0.1824732707758347, 0.12161821054848196]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05011428861627475, 0.05011428861627475, 0.35023221194498866, 0.25899636325305964, 0.1726976657505382, 0.11784518181886389]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05011428861627475, 0.05011428861627475, 0.35023221194498866, 0.25899636325305964, 0.1726976657505382, 0.11784518181886389]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05011428861627475, 0.05011428861627475, 0.35023221194498866, 0.25899636325305964, 0.1726976657505382, 0.11784518181886389]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05011428861627475, 0.05011428861627475, 0.35023221194498866, 0.25899636325305964, 0.1726976657505382, 0.11784518181886389]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05011428861627475, 0.05011428861627475, 0.35023221194498866, 0.25899636325305964, 0.1726976657505382, 0.11784518181886389]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049942748009399934, 0.049942748009399934, 0.3490305611905214, 0.25990239286254724, 0.17315850203119998, 0.11802304789693148]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.049942748009399934, 0.049942748009399934, 0.3490305611905214, 0.25990239286254724, 0.17315850203119998, 0.11802304789693148]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.849721908569336
actions average: 
K:  2  action  0 :  tensor([    0.7891,     0.0019,     0.0007,     0.0399,     0.1684],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0038,     0.9856,     0.0022,     0.0000,     0.0084],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0131,     0.8940,     0.0489,     0.0439],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1386,     0.0002,     0.0010,     0.8132,     0.0469],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0270, 0.0982, 0.2462, 0.0055, 0.6230], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05008327932943937, 0.05008327932943937, 0.3500160935066166, 0.23158547945151406, 0.2092957355768733, 0.10893613280611728]
actions average: 
K:  3  action  0 :  tensor([    0.9396,     0.0026,     0.0002,     0.0050,     0.0525],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9900,     0.0047,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9326,     0.0111,     0.0562],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0650,     0.0003,     0.0118,     0.8079,     0.1150],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0933, 0.0039, 0.0391, 0.0121, 0.8517], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.244]
 [0.244]
 [0.261]
 [0.244]
 [0.244]] [[57.121]
 [57.121]
 [63.99 ]
 [57.121]
 [57.121]] [[1.683]
 [1.683]
 [1.928]
 [1.683]
 [1.683]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04759755208839788, 0.04759755208839788, 0.3326044326937221, 0.22006743012327692, 0.24861139595189324, 0.10352163705431199]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.047539840379025346, 0.047539840379025346, 0.33220016257881163, 0.22028799019398138, 0.2488780103265265, 0.10355415614262976]
printing an ep nov before normalisation:  77.61058495955336
siam score:  -0.7886922
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.047539840379025346, 0.047539840379025346, 0.33220016257881163, 0.22028799019398138, 0.2488780103265265, 0.10355415614262976]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.047492575788316525, 0.047492575788316525, 0.33186907096088786, 0.22055598570762422, 0.2491981817548756, 0.10339160999997922]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04743514790735551, 0.04743514790735551, 0.3314667890563923, 0.2207756009057391, 0.24946364790547151, 0.10342366631768614]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04746849537695821, 0.04746849537695821, 0.33170037652234313, 0.22093115198454166, 0.24893499550190992, 0.10349648523728879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04746849537695821, 0.04746849537695821, 0.33170037652234313, 0.22093115198454166, 0.24893499550190992, 0.10349648523728879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.78958464
printing an ep nov before normalisation:  56.363870632108046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04746849537695821, 0.04746849537695821, 0.33170037652234313, 0.22093115198454166, 0.24893499550190992, 0.10349648523728879]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04741121936290452, 0.04741121936290452, 0.3312991584264015, 0.22115063806976468, 0.24919916211187096, 0.10352860266615393]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.018]
 [0.002]
 [0.008]
 [0.006]] [[32.825]
 [62.401]
 [61.118]
 [59.047]
 [44.927]] [[0.141]
 [0.881]
 [0.834]
 [0.79 ]
 [0.44 ]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04605346185776065, 0.04605346185776065, 0.3217885193655038, 0.24347976303330476, 0.2419128876271496, 0.10071190625852036]
printing an ep nov before normalisation:  42.494089670876356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0459960328218086, 0.0459960328218086, 0.32138623003431127, 0.24372619442038554, 0.24215690742357138, 0.1007386024781145]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04588160463164874, 0.04588160463164874, 0.32058466268556784, 0.2442172125465783, 0.2426431204202693, 0.10079179508428704]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.045922520040553215, 0.045922520040553215, 0.3208712608589441, 0.2444355109114314, 0.24217856637854443, 0.10066962176997354]
line 256 mcts: sample exp_bonus 96.87559339049523
printing an ep nov before normalisation:  73.79682556100343
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.04593217144582955, 0.04593217144582955, 0.3209388655877436, 0.24448700461629147, 0.24222958436769002, 0.10048020253661578]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.04593217144582955, 0.04593217144582955, 0.3209388655877436, 0.24448700461629147, 0.24222958436769002, 0.10048020253661578]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.1912985480998941
from probs:  [0.04593217474435504, 0.04593217474435504, 0.32093886137295163, 0.24448700249020175, 0.24222958230327418, 0.10048020434486238]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.045941756215746476, 0.045941756215746476, 0.3210059762335558, 0.24453812306860484, 0.24228023061303602, 0.10029215765331036]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.045941756215746476, 0.045941756215746476, 0.3210059762335558, 0.24453812306860484, 0.24228023061303602, 0.10029215765331036]
using another actor
from probs:  [0.045941756215746476, 0.045941756215746476, 0.3210059762335558, 0.24453812306860484, 0.24228023061303602, 0.10029215765331036]
printing an ep nov before normalisation:  61.75032093366841
printing an ep nov before normalisation:  24.11830948707008
siam score:  -0.7822035
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.016]
 [0.019]
 [0.019]] [[18.243]
 [18.243]
 [19.364]
 [18.243]
 [18.243]] [[0.019]
 [0.019]
 [0.016]
 [0.019]
 [0.019]]
printing an ep nov before normalisation:  63.214198257822915
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.045941756215746476, 0.045941756215746476, 0.3210059762335558, 0.24453812306860484, 0.24228023061303602, 0.10029215765331036]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04588477252358667, 0.04588477252358667, 0.3206068064555236, 0.24478352509030896, 0.2425221947348791, 0.10031792867211495]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04461971736154192, 0.04461971736154192, 0.311745537410617, 0.2656472530364338, 0.23582000773774592, 0.09754776709211949]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04262216314868932, 0.04262216314868932, 0.2977533686554911, 0.2537250120480316, 0.2701036813591875, 0.09317361163991114]
siam score:  -0.7851799
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
UNIT TEST: sample policy line 217 mcts : [0.167 0.583 0.083 0.125 0.042]
from probs:  [0.042877355397274726, 0.042877355397274726, 0.28226584104800884, 0.24204857545868555, 0.2995412987753815, 0.09038957392337464]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0428060224133035, 0.0428060224133035, 0.28261625642417315, 0.24232813711034706, 0.29904161491806835, 0.09040194672080433]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0428060224133035, 0.0428060224133035, 0.28261625642417315, 0.24232813711034706, 0.29904161491806835, 0.09040194672080433]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04273492713031878, 0.04273492713031878, 0.28296550412059157, 0.24260676718622573, 0.2985435961438516, 0.09041427828869353]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04269215049385785, 0.04269215049385785, 0.2828408183252503, 0.24304475337033385, 0.29824393987711806, 0.09048618743958209]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04254457767589481, 0.04254457767589481, 0.28298499095922497, 0.2442397357901398, 0.2972101813554971, 0.09047593654334848]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.192]
 [0.316]
 [0.114]
 [0.152]] [[33.281]
 [39.948]
 [54.99 ]
 [39.89 ]
 [37.285]] [[0.477]
 [0.861]
 [1.515]
 [0.781]
 [0.727]]
printing an ep nov before normalisation:  46.478197678652926
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.040617281726694154, 0.040617281726694154, 0.27013137871852266, 0.23314682137469656, 0.2837101348176063, 0.1317771016357862]
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.232]
 [0.277]
 [0.116]
 [0.232]] [[62.089]
 [62.089]
 [63.639]
 [64.525]
 [62.089]] [[1.311]
 [1.311]
 [1.399]
 [1.262]
 [1.311]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.040617281726694154, 0.040617281726694154, 0.27013137871852266, 0.23314682137469656, 0.2837101348176063, 0.1317771016357862]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.040617281726694154, 0.040617281726694154, 0.27013137871852266, 0.23314682137469656, 0.2837101348176063, 0.1317771016357862]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
siam score:  -0.8013071
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.007]
 [0.006]
 [0.003]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.007]
 [0.006]
 [0.003]
 [0.006]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04375306158084935, 0.04375306158084935, 0.2564316904838306, 0.22216004856917876, 0.3056760710016351, 0.12822606678365686]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actions average: 
K:  2  action  0 :  tensor([0.6987, 0.0041, 0.0007, 0.0454, 0.2510], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0140,     0.9732,     0.0011,     0.0000,     0.0116],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0003,     0.9502,     0.0117,     0.0376],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0044,     0.0002,     0.2062,     0.6896,     0.0996],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0569, 0.0281, 0.1459, 0.0787, 0.6905], grad_fn=<DivBackward0>)
from probs:  [0.043697431466100244, 0.043697431466100244, 0.2568175846387049, 0.22247479424174804, 0.3052863820157521, 0.12802637617159449]
actions average: 
K:  4  action  0 :  tensor([0.8292, 0.0015, 0.0010, 0.0565, 0.1117], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0025,     0.9828,     0.0002,     0.0000,     0.0145],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0029,     0.9083,     0.0295,     0.0593],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0000,     0.0385,     0.9404,     0.0209],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1224, 0.0389, 0.1400, 0.0855, 0.6132], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  80.77493340011628
printing an ep nov before normalisation:  37.17101933739899
maxi score, test score, baseline:  0.0021 0.0 0.0021
siam score:  -0.8106856
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464222312805561, 0.0464222312805561, 0.24427463751358072, 0.2137488376947712, 0.32437340472609616, 0.1247586575044396]
actions average: 
K:  2  action  0 :  tensor([    0.8006,     0.0373,     0.0002,     0.0578,     0.1041],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0027,     0.9910,     0.0029,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0012,     0.9248,     0.0162,     0.0577],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0032,     0.0001,     0.0040,     0.8981,     0.0946],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1085, 0.0050, 0.3289, 0.0751, 0.4825], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0464469499118069, 0.0464469499118069, 0.2438714309456826, 0.21386290982853345, 0.3245465511023826, 0.12482520829978752]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8280492
from probs:  [0.04521502846387731, 0.04521502846387731, 0.26425102396099565, 0.20817780911373335, 0.3159173219021764, 0.12122378809534003]
actions average: 
K:  4  action  0 :  tensor([    0.7832,     0.0005,     0.0010,     0.0067,     0.2087],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9952,     0.0021,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9541,     0.0016,     0.0440],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0708,     0.8685,     0.0607],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1594, 0.0331, 0.2811, 0.0604, 0.4660], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.8942,     0.0048,     0.0002,     0.0222,     0.0785],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9557,     0.0024,     0.0001,     0.0412],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9524,     0.0050,     0.0426],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0000,     0.0033,     0.9923,     0.0035],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1792, 0.0054, 0.0878, 0.1768, 0.5508], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04751008916281634, 0.04751008916281634, 0.25250711735280873, 0.20125786030531062, 0.33199404070114874, 0.11922080331509911]
printing an ep nov before normalisation:  93.8508620948323
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  10.887228591101511
maxi score, test score, baseline:  0.0021 0.0 0.0021
actions average: 
K:  2  action  0 :  tensor([    0.7209,     0.0345,     0.0001,     0.0609,     0.1837],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0099,     0.9847,     0.0008,     0.0000,     0.0046],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.9199,     0.0243,     0.0554],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0284,     0.0002,     0.0016,     0.9370,     0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0745, 0.0026, 0.2418, 0.0668, 0.6143], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04947121114767893, 0.04947121114767893, 0.24298594688446176, 0.19499429242173963, 0.34573155576099474, 0.11734578263744605]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0494312261191683, 0.0494312261191683, 0.2429043961858724, 0.1953099963494632, 0.34545145791765636, 0.11747169730867152]
printing an ep nov before normalisation:  30.45639762308864
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
using another actor
siam score:  -0.85379547
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047251726124760994, 0.047251726124760994, 0.25675681956425017, 0.20563757676501476, 0.3301846497407914, 0.11291750168042176]
printing an ep nov before normalisation:  63.56877326965332
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04614063090644072, 0.04614063090644072, 0.27356402139125635, 0.2012888678951819, 0.3224017366556238, 0.11046411224505646]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04789585642679682, 0.04789585642679682, 0.26459612064606447, 0.1957287766771812, 0.33469693627763075, 0.10918645354552999]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04789585642679682, 0.04789585642679682, 0.26459612064606447, 0.1957287766771812, 0.33469693627763075, 0.10918645354552999]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04959090912149942, 0.04959090912149942, 0.2559356584161859, 0.1903592970903345, 0.3465706305747381, 0.10795259567574282]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04959090912149942, 0.04959090912149942, 0.2559356584161859, 0.1903592970903345, 0.3465706305747381, 0.10795259567574282]
printing an ep nov before normalisation:  97.83837540013234
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05105983262977822, 0.05105983262977822, 0.24843054863299918, 0.18570613508717557, 0.35686030990898254, 0.10688334111128624]
printing an ep nov before normalisation:  72.14390457410025
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05105983262977822, 0.05105983262977822, 0.24843054863299918, 0.18570613508717557, 0.35686030990898254, 0.10688334111128624]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.45790672302246
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050876039250564775, 0.050876039250564775, 0.24896279564678703, 0.18672393678709398, 0.35557284186872895, 0.10698834719626045]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05012091098575045, 0.05012091098575045, 0.2452598428276241, 0.19881677704925818, 0.35028337353630956, 0.10539818461530723]
actions average: 
K:  3  action  0 :  tensor([    0.8534,     0.0091,     0.0002,     0.0279,     0.1095],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9549,     0.0141,     0.0000,     0.0301],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9507,     0.0039,     0.0453],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0648, 0.0026, 0.0055, 0.8344, 0.0927], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0115, 0.1841, 0.1476, 0.0450, 0.6118], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.397738522398527
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04829856446844343, 0.04829856446844343, 0.2727502975550342, 0.1915735874220505, 0.337518331300259, 0.10156065478576948]
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.035]
 [0.048]
 [0.028]
 [0.028]] [[49.325]
 [46.924]
 [90.237]
 [43.991]
 [43.991]] [[0.341]
 [0.326]
 [0.773]
 [0.29 ]
 [0.29 ]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048263831291910385, 0.048263831291910385, 0.2726643487997694, 0.19188016249694015, 0.3372750241193697, 0.10165280200009989]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04829192345277102, 0.04829192345277102, 0.2722402170613139, 0.1919920785182527, 0.3374718020675175, 0.1017120554473739]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04829192345277102, 0.04829192345277102, 0.2722402170613139, 0.1919920785182527, 0.3374718020675175, 0.1017120554473739]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.619]
 [0.672]
 [0.619]
 [0.619]] [[62.903]
 [62.903]
 [80.982]
 [62.903]
 [62.903]] [[1.597]
 [1.597]
 [2.137]
 [1.597]
 [1.597]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  83.51523103541493
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04807320566221724, 0.04807320566221724, 0.2727049428843234, 0.19333506239917916, 0.3359396900494895, 0.10187389334257352]
maxi score, test score, baseline:  0.0021 0.0 0.0021
from probs:  [0.048039045248168065, 0.048039045248168065, 0.2726162666955085, 0.19363994381986047, 0.33570039500987037, 0.10196530397842452]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047977322735042946, 0.047977322735042946, 0.2729469135288412, 0.19383260743302216, 0.3352680350189414, 0.10199779854910931]
from probs:  [0.047977322735042946, 0.047977322735042946, 0.2729469135288412, 0.19383260743302216, 0.3352680350189414, 0.10199779854910931]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04800495940133014, 0.04800495940133014, 0.27252732332873475, 0.19394449595414312, 0.33546162230838605, 0.1020566396060757]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04803248640535, 0.04803248640535, 0.27210939806217066, 0.19405594050171146, 0.33565444144363776, 0.10211524718178017]
using explorer policy with actor:  1
siam score:  -0.88684404
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04803248640535, 0.04803248640535, 0.27210939806217066, 0.19405594050171146, 0.33565444144363776, 0.10211524718178017]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.046783953280924225, 0.046783953280924225, 0.2650201181569623, 0.18900118739180904, 0.3269088083478186, 0.12550197954156161]
printing an ep nov before normalisation:  91.12922083561337
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.046783953280924225, 0.046783953280924225, 0.2650201181569623, 0.18900118739180904, 0.3269088083478186, 0.12550197954156161]
maxi score, test score, baseline:  0.0021 0.0 0.0021
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.046809960170034276, 0.046809960170034276, 0.26461075226623987, 0.1891064776728886, 0.32709097949383825, 0.1255718702269647]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.084]
 [0.033]
 [0.033]] [[35.722]
 [35.722]
 [91.83 ]
 [35.722]
 [35.722]] [[0.233]
 [0.233]
 [0.799]
 [0.233]
 [0.233]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04741337610555169, 0.04741337610555169, 0.2701200708550903, 0.18206527000796507, 0.3313179716829293, 0.12166993524291202]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04735194167885215, 0.04735194167885215, 0.27043918193542144, 0.1822339192493625, 0.3308876302655106, 0.12173538519200124]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04735194167885215, 0.04735194167885215, 0.27043918193542144, 0.1822339192493625, 0.3308876302655106, 0.12173538519200124]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04735194167885215, 0.04735194167885215, 0.27043918193542144, 0.1822339192493625, 0.3308876302655106, 0.12173538519200124]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048693831261984476, 0.048693831261984476, 0.2634689559959388, 0.17855017590882144, 0.3402874195408101, 0.12030578603046076]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  23.86375665664673
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04865986125937823, 0.04865986125937823, 0.26338096099424907, 0.1788138817140538, 0.34004945929223396, 0.12043597548070666]
Printing some Q and Qe and total Qs values:  [[0.262]
 [0.896]
 [0.101]
 [0.262]
 [0.121]] [[57.476]
 [64.884]
 [66.13 ]
 [57.476]
 [63.009]] [[1.108]
 [1.851]
 [1.075]
 [1.108]
 [1.049]]
from probs:  [0.04865986125937823, 0.04865986125937823, 0.26338096099424907, 0.1788138817140538, 0.34004945929223396, 0.12043597548070666]
printing an ep nov before normalisation:  67.43978056895976
printing an ep nov before normalisation:  18.54128156389509
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.66 ]
 [0.613]
 [0.66 ]
 [0.66 ]] [[49.365]
 [49.365]
 [59.388]
 [49.365]
 [49.365]] [[1.407]
 [1.407]
 [1.649]
 [1.407]
 [1.407]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
actions average: 
K:  0  action  0 :  tensor([    0.8393,     0.0020,     0.0004,     0.0270,     0.1313],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0018,     0.9930,     0.0002,     0.0000,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9584,     0.0028,     0.0388],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0391,     0.0006,     0.0006,     0.8039,     0.1558],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0617, 0.0028, 0.0741, 0.1367, 0.7246], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.77011008133013
line 256 mcts: sample exp_bonus 84.29783377417557
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04986986260257795, 0.04986986260257795, 0.2569867044889443, 0.17605181550565654, 0.34852537243234977, 0.11869638236789355]
maxi score, test score, baseline:  0.0021 0.0 0.0021
printing an ep nov before normalisation:  110.94282044470995
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0497706075431618, 0.0497706075431618, 0.2567407962184508, 0.17681846182230074, 0.3478300897229599, 0.11906943714996487]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0497706075431618, 0.0497706075431618, 0.2567407962184508, 0.17681846182230074, 0.3478300897229599, 0.11906943714996487]
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04965337627401621, 0.04965337627401621, 0.25734577428433825, 0.17714455597573694, 0.3470088974442104, 0.11919401974768205]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04965337627401621, 0.04965337627401621, 0.25734577428433825, 0.17714455597573694, 0.3470088974442104, 0.11919401974768205]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04965337627401621, 0.04965337627401621, 0.25734577428433825, 0.17714455597573694, 0.3470088974442104, 0.11919401974768205]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.0496791226194388, 0.0496791226194388, 0.2569600023935159, 0.17723658709579393, 0.34718924384708927, 0.11925592142472341]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04962072860746564, 0.04962072860746564, 0.25726090934296414, 0.17739930136777238, 0.3467802001430629, 0.11931813193126932]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.979]
 [0.821]
 [0.821]
 [0.821]] [[37.77 ]
 [41.554]
 [37.77 ]
 [37.77 ]
 [37.77 ]] [[2.012]
 [2.385]
 [2.012]
 [2.012]
 [2.012]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050772247559151544, 0.050772247559151544, 0.2513270808000129, 0.17419060647660475, 0.35484646518186236, 0.1180913524232169]
actions average: 
K:  3  action  0 :  tensor([0.7254, 0.0015, 0.0007, 0.1070, 0.1654], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0406,     0.9335,     0.0117,     0.0000,     0.0141],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0066,     0.9691,     0.0027,     0.0215],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0270,     0.0001,     0.0054,     0.8411,     0.1264],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1096, 0.0068, 0.3280, 0.0818, 0.4739], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050772247559151544, 0.050772247559151544, 0.2513270808000129, 0.17419060647660475, 0.35484646518186236, 0.1180913524232169]
from probs:  [0.05071483398774311, 0.05071483398774311, 0.2516229354954805, 0.17435058876173534, 0.35444428935737726, 0.11815251840992069]
from probs:  [0.050740209266202464, 0.050740209266202464, 0.2512478313002678, 0.1744379883979874, 0.35462203661307296, 0.11821172515626696]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05075259367820336, 0.05075259367820336, 0.25130928342737874, 0.17448064381577152, 0.35470878621269175, 0.1179960991877513]
printing an ep nov before normalisation:  43.221930975918625
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.40083057098446
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05187236000924098, 0.05187236000924098, 0.24507316713064325, 0.1716568604245104, 0.3625526204523029, 0.11697263197406133]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05187236000924098, 0.05187236000924098, 0.24507316713064325, 0.1716568604245104, 0.3625526204523029, 0.11697263197406133]
siam score:  -0.87737745
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  65.99713443579594
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050788150067966935, 0.050788150067966935, 0.23994037302191795, 0.16806252829941656, 0.35495800715241843, 0.13546279139031323]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050788150067966935, 0.050788150067966935, 0.23994037302191795, 0.16806252829941656, 0.35495800715241843, 0.13546279139031323]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.050788150067966935, 0.050788150067966935, 0.23994037302191795, 0.16806252829941656, 0.35495800715241843, 0.13546279139031323]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05020051927903406, 0.05020051927903406, 0.23715845068684321, 0.17770582849915992, 0.3508418037907272, 0.13389287846520134]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05020051927903406, 0.05020051927903406, 0.23715845068684321, 0.17770582849915992, 0.3508418037907272, 0.13389287846520134]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049191719069127046, 0.049191719069127046, 0.2323826562791596, 0.1741279382463692, 0.34377541636941, 0.15133055096680703]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.111]
 [0.071]
 [0.073]
 [0.073]] [[57.522]
 [67.992]
 [85.896]
 [57.522]
 [57.522]] [[0.684]
 [0.879]
 [1.108]
 [0.684]
 [0.684]]
printing an ep nov before normalisation:  66.74196960804123
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049530551153433126, 0.049530551153433126, 0.23787072475118592, 0.16939561877886006, 0.3461489784630336, 0.14752357570005423]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049530551153433126, 0.049530551153433126, 0.23787072475118592, 0.16939561877886006, 0.3461489784630336, 0.14752357570005423]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049530551153433126, 0.049530551153433126, 0.23787072475118592, 0.16939561877886006, 0.3461489784630336, 0.14752357570005423]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049547920935427245, 0.049547920935427245, 0.237954321354081, 0.16945513720187047, 0.34627064938846475, 0.14722405018472928]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04901465502810748, 0.04901465502810748, 0.23538784019117315, 0.17841089501275018, 0.34253525671246904, 0.14563669802739265]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04901465502810748, 0.04901465502810748, 0.23538784019117315, 0.17841089501275018, 0.34253525671246904, 0.14563669802739265]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  55.27442635712442
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05000416196551461, 0.05000416196551461, 0.23116046836900425, 0.17577839755422311, 0.349466627652916, 0.14358618249282748]
maxi score, test score, baseline:  0.0021 0.0 0.0021
printing an ep nov before normalisation:  46.80863246177572
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04987795417972187, 0.04987795417972187, 0.231229074962509, 0.17640922188016936, 0.34858255215056116, 0.14402324264731675]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04987795417972187, 0.04987795417972187, 0.231229074962509, 0.17640922188016936, 0.34858255215056116, 0.14402324264731675]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04987795417972187, 0.04987795417972187, 0.231229074962509, 0.17640922188016936, 0.34858255215056116, 0.14402324264731675]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048977185835105, 0.048977185835105, 0.2270444339883948, 0.17321724868948604, 0.3422728960673549, 0.15951104958455434]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04899852963672972, 0.04899852963672972, 0.22670695945217254, 0.17329288283335942, 0.3424224040385941, 0.1595806944024145]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04899852963672972, 0.04899852963672972, 0.22670695945217254, 0.17329288283335942, 0.3424224040385941, 0.1595806944024145]
printing an ep nov before normalisation:  1.2098248589609284e-05
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04899852963672972, 0.04899852963672972, 0.22670695945217254, 0.17329288283335942, 0.3424224040385941, 0.1595806944024145]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049922028635058596, 0.049922028635058596, 0.2228552906714143, 0.17087649305362965, 0.3488913969013007, 0.1575327621035382]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049922028635058596, 0.049922028635058596, 0.2228552906714143, 0.17087649305362965, 0.3488913969013007, 0.1575327621035382]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.049809257325619555, 0.049809257325619555, 0.223325629819283, 0.17117156585833043, 0.3481014482416927, 0.15778284142945484]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.093]
 [0.607]
 [0.093]
 [0.093]] [[61.314]
 [61.314]
 [62.824]
 [61.314]
 [61.314]] [[1.308]
 [1.308]
 [1.852]
 [1.308]
 [1.308]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 69.6579664108123
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04830421259760381, 0.04830421259760381, 0.21721209325426605, 0.1772291849331104, 0.337558958222138, 0.1713913383952779]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04832398519642711, 0.04832398519642711, 0.21689105809634418, 0.17730187697527794, 0.3376974603412848, 0.17146163419423888]
printing an ep nov before normalisation:  104.28947560062817
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04832398519642711, 0.04832398519642711, 0.21689105809634418, 0.17730187697527794, 0.3376974603412848, 0.17146163419423888]
printing an ep nov before normalisation:  29.100691839152002
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04832398519642711, 0.04832398519642711, 0.21689105809634418, 0.17730187697527794, 0.3376974603412848, 0.17146163419423888]
Printing some Q and Qe and total Qs values:  [[1.006]
 [1.006]
 [1.103]
 [1.006]
 [1.006]] [[19.931]
 [19.931]
 [19.169]
 [19.931]
 [19.931]] [[2.673]
 [2.673]
 [2.648]
 [2.673]
 [2.673]]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.04924875548363971, 0.04924875548363971, 0.21329883330067947, 0.17507516516930924, 0.34417534967549246, 0.16895314088723942]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.048333183389073026, 0.048333183389073026, 0.2102170871765589, 0.17249813759407467, 0.3377619810697296, 0.18285642738149083]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048333183389073026, 0.048333183389073026, 0.2102170871765589, 0.17249813759407467, 0.3377619810697296, 0.18285642738149083]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048333183389073026, 0.048333183389073026, 0.2102170871765589, 0.17249813759407467, 0.3377619810697296, 0.18285642738149083]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.218]
 [0.162]
 [0.162]
 [0.162]] [[53.884]
 [64.675]
 [53.884]
 [53.884]
 [53.884]] [[1.223]
 [1.551]
 [1.223]
 [1.223]
 [1.223]]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048277359683496605, 0.048277359683496605, 0.2104192156348873, 0.17264016319821326, 0.3373709435089159, 0.18301495829099027]
using another actor
siam score:  -0.89921296
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.048296237672315015, 0.048296237672315015, 0.21010987115102306, 0.17270780558408738, 0.33750317914093225, 0.18308666877932733]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047765176409469146, 0.047765176409469146, 0.21881211553921603, 0.1708049412901337, 0.3337832266392475, 0.18106936371246435]
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047653496701579964, 0.047653496701579964, 0.21923699986869372, 0.17107922997979047, 0.33300092546583293, 0.18137585128252304]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047673120709259345, 0.047673120709259345, 0.21891485045934544, 0.1711498239743881, 0.3331383867698765, 0.1814506973778712]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.047673120709259345, 0.047673120709259345, 0.21891485045934544, 0.1711498239743881, 0.3331383867698765, 0.1814506973778712]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.947174072265625
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.047692666695087704, 0.047692666695087704, 0.21859398186346005, 0.17122013729878724, 0.3332753015502149, 0.18152524589736238]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04769266779053017, 0.04769266779053017, 0.218593981385344, 0.1712201372568616, 0.3332753000161808, 0.18152524576055334]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.039]
 [0.14 ]
 [0.038]
 [0.053]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.071]
 [0.039]
 [0.14 ]
 [0.038]
 [0.053]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04763703924151017, 0.04763703924151017, 0.21880484933103334, 0.1713571323742175, 0.3328856295705357, 0.18167831024119308]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04844973689187213, 0.04844973689187213, 0.21548349039909453, 0.16947125909963834, 0.33857846430261046, 0.17956731241491242]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04849129568154052, 0.04849129568154052, 0.215269729533629, 0.16961691290718398, 0.33886957337486956, 0.1792611928212365]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04851343461911477, 0.04851343461911477, 0.21536822175448164, 0.16969450468929387, 0.33902465117259817, 0.1788857531453967]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04851343461911477, 0.04851343461911477, 0.21536822175448164, 0.16969450468929387, 0.33902465117259817, 0.1788857531453967]
printing an ep nov before normalisation:  23.587322800030865
siam score:  -0.9057106
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04756305784783624, 0.04756305784783624, 0.2216298699051196, 0.17550216470993948, 0.3323674962748954, 0.17537435341437294]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.325]
 [1.302]
 [1.302]
 [1.302]] [[35.813]
 [36.372]
 [35.813]
 [35.813]
 [35.813]] [[2.513]
 [2.554]
 [2.513]
 [2.513]
 [2.513]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.167 0.458 0.083]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.048719794613934664, 0.048719794613934664, 0.21365609274461667, 0.17860712939184673, 0.3404703219677354, 0.1698268666679319]
Printing some Q and Qe and total Qs values:  [[0.164]
 [0.801]
 [0.801]
 [0.832]
 [0.801]] [[39.497]
 [38.45 ]
 [38.45 ]
 [49.933]
 [38.45 ]] [[0.916]
 [1.515]
 [1.515]
 [1.962]
 [1.515]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049476637135872595, 0.049476637135872595, 0.21097383326092, 0.17665567908434743, 0.34577189615561676, 0.16764531722737067]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049476637135872595, 0.049476637135872595, 0.21097383326092, 0.17665567908434743, 0.34577189615561676, 0.16764531722737067]
printing an ep nov before normalisation:  90.34830711265417
printing an ep nov before normalisation:  25.49945831298828
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049476637135872595, 0.049476637135872595, 0.21097383326092, 0.17665567908434743, 0.34577189615561676, 0.16764531722737067]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04973101708817359, 0.04973101708817359, 0.20651572055044137, 0.1817437374034031, 0.34755383197045936, 0.16472467589934905]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04973101708817359, 0.04973101708817359, 0.20651572055044137, 0.1817437374034031, 0.34755383197045936, 0.16472467589934905]
actions average: 
K:  3  action  0 :  tensor([    0.8918,     0.0188,     0.0001,     0.0057,     0.0837],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0054,     0.9665,     0.0030,     0.0001,     0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9338,     0.0031,     0.0628],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0751, 0.0016, 0.1034, 0.6882, 0.1317], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1287, 0.0076, 0.2298, 0.0650, 0.5690], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.823]
 [0.823]
 [0.823]
 [0.823]
 [0.823]] [[13.399]
 [13.399]
 [13.399]
 [13.399]
 [13.399]] [[1.042]
 [1.042]
 [1.042]
 [1.042]
 [1.042]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.05106709819616956, 0.05106709819616956, 0.2017729942928409, 0.17796146270956684, 0.3569128903245373, 0.16121845628071582]
printing an ep nov before normalisation:  77.23086618865361
printing an ep nov before normalisation:  39.94820137126068
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.050998571655887046, 0.050998571655887046, 0.20158686743370882, 0.17839626988392426, 0.3564328680028678, 0.1615868513677249]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.05059644410252236, 0.05059644410252236, 0.1999942298570496, 0.184886378772623, 0.35361606014927927, 0.1603104430160033]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.05059644410252236, 0.05059644410252236, 0.1999942298570496, 0.184886378772623, 0.35361606014927927, 0.1603104430160033]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049843577277606854, 0.06475077210981253, 0.1970124794508715, 0.18213002421860008, 0.3483424071320111, 0.15792073981109803]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049843577277606854, 0.06475077210981253, 0.1970124794508715, 0.18213002421860008, 0.3483424071320111, 0.15792073981109803]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04920082118872737, 0.06391526561492729, 0.19446682573674356, 0.17977680102682542, 0.3438400536276351, 0.1688002328051412]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04920082118872737, 0.06391526561492729, 0.19446682573674356, 0.17977680102682542, 0.3438400536276351, 0.1688002328051412]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.145]
 [0.25 ]
 [0.145]
 [0.145]] [[88.111]
 [88.111]
 [92.513]
 [88.111]
 [88.111]] [[2.008]
 [2.008]
 [2.25 ]
 [2.008]
 [2.008]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04877655043075229, 0.06336376407334557, 0.2014262365915692, 0.17822348429512505, 0.3408681378931558, 0.16734182671605197]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.151]
 [0.509]
 [0.151]
 [0.151]] [[57.442]
 [57.442]
 [58.302]
 [57.442]
 [57.442]] [[1.528]
 [1.528]
 [1.926]
 [1.528]
 [1.528]]
printing an ep nov before normalisation:  32.501323610263725
printing an ep nov before normalisation:  25.684468269899895
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04879646210578893, 0.06338964688348923, 0.2015086339422424, 0.17829638382310148, 0.34100761444021654, 0.16700125880516145]
siam score:  -0.90162855
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.048760564666273654, 0.06338156206635269, 0.20140377752309876, 0.17850729559457504, 0.3407561565427861, 0.16719064360691385]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.048760564666273654, 0.06338156206635269, 0.20140377752309876, 0.17850729559457504, 0.3407561565427861, 0.16719064360691385]
printing an ep nov before normalisation:  22.201509296418823
siam score:  -0.90110964
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.048760564666273654, 0.06338156206635269, 0.20140377752309876, 0.17850729559457504, 0.3407561565427861, 0.16719064360691385]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049408339366212986, 0.06376803863978726, 0.1989716860254831, 0.17683631071991118, 0.3452937217664409, 0.16572190348216465]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.049408339366212986, 0.06376803863978726, 0.1989716860254831, 0.17683631071991118, 0.3452937217664409, 0.16572190348216465]
printing an ep nov before normalisation:  14.759750366210938
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04880050459959571, 0.06298306933214982, 0.19651890769429672, 0.17465658403628098, 0.34103598316775036, 0.17600495116992648]
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04880050459959571, 0.06298306933214982, 0.19651890769429672, 0.17465658403628098, 0.34103598316775036, 0.17600495116992648]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04940850303107988, 0.06334160470725163, 0.19452862541881238, 0.173050847305428, 0.34529492209119955, 0.17437549744622857]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04935579425283599, 0.06331052245341637, 0.19470116755324718, 0.17319005230478632, 0.3449257049096315, 0.17451675852608273]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04935579425283599, 0.06331052245341637, 0.19470116755324718, 0.17319005230478632, 0.3449257049096315, 0.17451675852608273]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04935579425283599, 0.06331052245341637, 0.19470116755324718, 0.17319005230478632, 0.3449257049096315, 0.17451675852608273]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04935579425283599, 0.06331052245341637, 0.19470116755324718, 0.17319005230478632, 0.3449257049096315, 0.17451675852608273]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.048999549203171025, 0.06285328098903198, 0.1932929950573143, 0.1791692130631205, 0.342430292342964, 0.17325466934439812]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04836866798331453, 0.06207934116193071, 0.1911720879211548, 0.17719415316959208, 0.3380111136443679, 0.18317463611964]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.3  ]
 [0.221]
 [0.215]
 [0.24 ]] [[78.974]
 [88.815]
 [93.803]
 [92.596]
 [78.974]] [[1.509]
 [1.806]
 [1.848]
 [1.813]
 [1.509]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04836866798331453, 0.06207934116193071, 0.1911720879211548, 0.17719415316959208, 0.3380111136443679, 0.18317463611964]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
printing an ep nov before normalisation:  102.07989378999817
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04745459854144562, 0.060905452237538404, 0.1954454900267944, 0.1738388198699335, 0.3316082728367113, 0.19074736648757695]
printing an ep nov before normalisation:  45.60250759124756
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([    0.7388,     0.0003,     0.0011,     0.0337,     0.2260],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9927,     0.0001,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9452,     0.0055,     0.0491],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0360,     0.0001,     0.0004,     0.8192,     0.1443],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1182,     0.0002,     0.2600,     0.1162,     0.5054],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.046920106190148594, 0.060219033216597155, 0.19323944850924782, 0.17187682453065933, 0.32786428000475787, 0.1998803075485892]
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04688269456547705, 0.06020568260618823, 0.19312427824599362, 0.1720654901959992, 0.32760221601702416, 0.2001196383693177]
printing an ep nov before normalisation:  24.45518294929201
actions average: 
K:  3  action  0 :  tensor([    0.8240,     0.0006,     0.0003,     0.0447,     0.1303],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9972,     0.0006,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9300,     0.0158,     0.0541],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0292,     0.0000,     0.0012,     0.8726,     0.0969],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0427, 0.0010, 0.2295, 0.1924, 0.5345], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04685363308958588, 0.06020301249392374, 0.19338490514935056, 0.17228440197274444, 0.32739864285711223, 0.19987540443728313]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8940761
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04635064052508286, 0.05955630031935174, 0.19096646736302048, 0.1704310199520333, 0.32387529869502985, 0.2088202731454819]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.045944730378060385, 0.0590684087960491, 0.19729431473807432, 0.1692548127934823, 0.32103198965253354, 0.20740574364180026]
printing an ep nov before normalisation:  47.64366626739502
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  48.937106529835795
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
printing an ep nov before normalisation:  88.18171868475616
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04506698640862968, 0.05800591150836966, 0.1935950641160615, 0.17339524554785077, 0.3148835920323311, 0.21505320038675724]
actions average: 
K:  1  action  0 :  tensor([    0.7857,     0.0018,     0.0004,     0.0546,     0.1574],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9973,     0.0005,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9329,     0.0040,     0.0630],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0542, 0.0018, 0.0017, 0.8135, 0.1287], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1643, 0.0016, 0.1377, 0.1324, 0.5639], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0451180058067096, 0.05807162222485768, 0.1938147276067011, 0.17359197344190228, 0.3152409708562873, 0.2141627000635421]
printing an ep nov before normalisation:  45.64214050769806
printing an ep nov before normalisation:  56.41383807154974
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04512023180137653, 0.0580250454764687, 0.19382431160371175, 0.17360055675059416, 0.3152565634227647, 0.21417329094508397]
siam score:  -0.8917012
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.045616807456636085, 0.05835407038974672, 0.19239017523678595, 0.17242899721868554, 0.3187349978177603, 0.2124749518803854]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.045632242528717525, 0.05837382791332488, 0.1921163350104158, 0.17248746661786826, 0.31884311686259986, 0.21254701106707377]
maxi score, test score, baseline:  0.0041 0.0 0.0041
maxi score, test score, baseline:  0.0041 0.0 0.0041
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.045157480810485366, 0.05776611349563488, 0.19011307782359, 0.17068902782383397, 0.3155175228368116, 0.2207567772096441]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04515964117435532, 0.057720934980261845, 0.19012219347947049, 0.17069721147058506, 0.3155326556769366, 0.22076736321839063]
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04518585945075429, 0.057754467529341776, 0.1902328215031831, 0.17079652858815764, 0.315716308528484, 0.2203140144000793]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.04518585945075429, 0.057754467529341776, 0.1902328215031831, 0.17079652858815764, 0.315716308528484, 0.2203140144000793]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8984133
maxi score, test score, baseline:  0.0041 0.0 0.0041
Starting evaluation
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0457943034246775, 0.058178235943123234, 0.18871003274262968, 0.16955932501402407, 0.3199783603828164, 0.21777974249272902]
printing an ep nov before normalisation:  42.34907967703683
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.04 ]
 [0.027]
 [0.024]
 [0.024]] [[29.24 ]
 [47.604]
 [34.07 ]
 [31.353]
 [24.855]] [[0.025]
 [0.04 ]
 [0.027]
 [0.024]
 [0.024]]
printing an ep nov before normalisation:  53.28804308389067
maxi score, test score, baseline:  0.0041 0.0 0.0041
probs:  [0.0457943034246775, 0.058178235943123234, 0.18871003274262968, 0.16955932501402407, 0.3199783603828164, 0.21777974249272902]
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 32.66823842760807
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.0 0.0381
probs:  [0.04579431713155256, 0.05817824824566555, 0.18871003024292124, 0.16955932468599785, 0.31997834299733496, 0.21777973669652778]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04585395486135644, 0.05824707065389263, 0.1885461198735761, 0.16971075409196318, 0.3202413323190646, 0.21740076820014692]
printing an ep nov before normalisation:  56.580562591552734
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04555760648371231, 0.05787039372701184, 0.18732488264677163, 0.17508812828322343, 0.31816648527094854, 0.21599250358833216]
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04555760648371231, 0.05787039372701184, 0.18732488264677163, 0.17508812828322343, 0.31816648527094854, 0.21599250358833216]
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04545263356289242, 0.05779960599063527, 0.18761351195328657, 0.17534278350274735, 0.317430739401421, 0.2163607255890173]
siam score:  -0.89555144
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04546746094302439, 0.057818472992345445, 0.18734793728890334, 0.175400107701882, 0.3175345512375866, 0.21643146983625805]
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04546746094302439, 0.057818472992345445, 0.18734793728890334, 0.175400107701882, 0.3175345512375866, 0.21643146983625805]
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.04546746094302439, 0.057818472992345445, 0.18734793728890334, 0.175400107701882, 0.3175345512375866, 0.21643146983625805]
printing an ep nov before normalisation:  54.00410676637247
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.9 0.9
probs:  [0.046030330040225845, 0.058197908022426116, 0.18580363389552415, 0.17403325041297274, 0.3214796523613267, 0.2144552252675244]
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.042100000000000005 0.9 0.9
probs:  [0.04604506027521789, 0.058216543324843356, 0.18554253917195745, 0.17408906195727783, 0.3215827854138279, 0.21452400985687567]
maxi score, test score, baseline:  0.042100000000000005 0.9 0.9
probs:  [0.04604506027521789, 0.058216543324843356, 0.18554253917195745, 0.17408906195727783, 0.3215827854138279, 0.21452400985687567]
maxi score, test score, baseline:  0.042100000000000005 0.9 0.9
probs:  [0.04599303016285782, 0.058181481502919984, 0.18568498279196474, 0.17421553826031178, 0.32121811067506956, 0.21470685660687616]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.042100000000000005 0.9 0.9
actions average: 
K:  0  action  0 :  tensor([0.6809, 0.0016, 0.0012, 0.0790, 0.2373], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9988,     0.0002,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0000,     0.9448,     0.0058,     0.0489],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0743, 0.0010, 0.0015, 0.8101, 0.1131], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1051, 0.0020, 0.1006, 0.1733, 0.6190], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.042100000000000005 0.9 0.9
probs:  [0.04562128744259875, 0.057741450678135395, 0.19150700510603763, 0.17312540468044435, 0.3186149893372303, 0.2133898627555535]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0441 0.9 0.9
probs:  [0.04562128744259875, 0.057741450678135395, 0.19150700510603763, 0.17312540468044435, 0.3186149893372303, 0.2133898627555535]
maxi score, test score, baseline:  0.0441 0.9 0.9
probs:  [0.04562128744259875, 0.057741450678135395, 0.19150700510603763, 0.17312540468044435, 0.3186149893372303, 0.2133898627555535]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0461 0.9 0.9
probs:  [0.04616635132852677, 0.05811149272722564, 0.18994539882773975, 0.17182923884283896, 0.32243526232104713, 0.2115122559526217]
using another actor
maxi score, test score, baseline:  0.0461 0.9 0.9
probs:  [0.04616635132852677, 0.05811149272722564, 0.18994539882773975, 0.17182923884283896, 0.32243526232104713, 0.2115122559526217]
maxi score, test score, baseline:  0.0461 0.9 0.9
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.9 0.9
probs:  [0.04616635132852677, 0.05811149272722564, 0.18994539882773975, 0.17182923884283896, 0.32243526232104713, 0.2115122559526217]
maxi score, test score, baseline:  0.048100000000000004 0.9 0.9
probs:  [0.04616635132852677, 0.05811149272722564, 0.18994539882773975, 0.17182923884283896, 0.32243526232104713, 0.2115122559526217]
actions average: 
K:  3  action  0 :  tensor([    0.7306,     0.0041,     0.0002,     0.0730,     0.1921],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9816,     0.0001,     0.0000,     0.0169],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0023,     0.9680,     0.0035,     0.0256],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0018,     0.0000,     0.0980,     0.8441,     0.0561],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1156, 0.0277, 0.0869, 0.0514, 0.7184], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.048100000000000004 0.9 0.9
probs:  [0.04616635132852677, 0.05811149272722564, 0.18994539882773975, 0.17182923884283896, 0.32243526232104713, 0.2115122559526217]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.9 0.9
probs:  [0.04564138710516482, 0.06916976233253581, 0.18745625148931871, 0.1698712083056837, 0.31875971260881797, 0.20910167815847908]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.9 0.9
probs:  [0.04533595445593337, 0.06870641659866135, 0.1929052507193547, 0.1687319945695371, 0.31662121852478575, 0.20769916513172773]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.050100000000000006 0.9 0.9
probs:  [0.04533595445593337, 0.06870641659866135, 0.1929052507193547, 0.1687319945695371, 0.31662121852478575, 0.20769916513172773]
maxi score, test score, baseline:  0.050100000000000006 0.9 0.9
probs:  [0.04528414816540043, 0.06868644608783335, 0.19305446670978318, 0.1688482811958462, 0.31625812410936044, 0.20786853373177644]
maxi score, test score, baseline:  0.050100000000000006 0.9 0.9
probs:  [0.04528414816540043, 0.06868644608783335, 0.19305446670978318, 0.1688482811958462, 0.31625812410936044, 0.20786853373177644]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04528414816540043, 0.06868644608783335, 0.19305446670978318, 0.1688482811958462, 0.31625812410936044, 0.20786853373177644]
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04532344559400564, 0.06874611187444796, 0.1928862431607923, 0.16899512355474117, 0.31653326568333784, 0.20751581013267512]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.9 0.9
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04587664063495022, 0.0689855323644747, 0.19146265853095448, 0.16789158896683953, 0.320410274381701, 0.20537330512107999]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04638878874598771, 0.06918199172552483, 0.1899859675170716, 0.16673690047794376, 0.3239997696738164, 0.2037065818596557]
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04633762567853792, 0.06916236563821938, 0.19013348742453104, 0.166852252665656, 0.32364118280063786, 0.20387308579241772]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04602444117013281, 0.0687508332006731, 0.18887604821924317, 0.1720195585874482, 0.32144801108864085, 0.20288110773386187]
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.742]
 [0.556]
 [0.556]
 [0.556]] [[60.229]
 [72.567]
 [69.95 ]
 [69.95 ]
 [69.95 ]] [[0.48 ]
 [0.979]
 [0.781]
 [0.781]
 [0.781]]
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04602444117013281, 0.0687508332006731, 0.18887604821924317, 0.1720195585874482, 0.32144801108864085, 0.20288110773386187]
printing an ep nov before normalisation:  57.06601810847948
printing an ep nov before normalisation:  8.581579635347225e-05
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9113976
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.079]
 [0.045]
 [0.079]
 [0.079]] [[33.271]
 [33.271]
 [35.951]
 [33.271]
 [33.271]] [[0.079]
 [0.079]
 [0.045]
 [0.079]
 [0.079]]
siam score:  -0.9134129
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.04667930693116977, 0.0715280529178554, 0.18627414477125936, 0.16851122253236855, 0.3259749673591627, 0.20103230548818427]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04657379768315203, 0.07149400625189847, 0.1865700967301677, 0.16875609005068515, 0.3252353087989585, 0.20137070048513828]
maxi score, test score, baseline:  0.0521 0.9 0.9
probs:  [0.047088802030711946, 0.0716601924518493, 0.18512551676803002, 0.1675608598800747, 0.32884567832932404, 0.19971895054001013]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
using another actor
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.072]
 [0.006]
 [0.046]
 [0.056]] [[41.536]
 [52.132]
 [33.189]
 [31.721]
 [31.894]] [[0.062]
 [0.072]
 [0.006]
 [0.046]
 [0.056]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0541 0.9 0.9
siam score:  -0.9176213
maxi score, test score, baseline:  0.0541 0.9 0.9
probs:  [0.04756676068096997, 0.07185189823970864, 0.18333117551694902, 0.16663533786410342, 0.33219603590999924, 0.19841879178826974]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.0541 0.9 0.9
probs:  [0.047118290976220986, 0.07117381991066762, 0.18159909436701452, 0.16506111080519886, 0.3290564825677577, 0.2059912013731402]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0541 0.9 0.9
probs:  [0.04760252615892144, 0.07133670172208871, 0.1802868253173716, 0.16396976962566165, 0.33245109450807836, 0.20435308266787816]
maxi score, test score, baseline:  0.0541 0.9 0.9
probs:  [0.047628329582661166, 0.07137540659996447, 0.18038475344565855, 0.16405882810751488, 0.33263173628845255, 0.20392094597574836]
maxi score, test score, baseline:  0.0541 0.9 0.9
probs:  [0.047628329582661166, 0.07137540659996447, 0.18038475344565855, 0.16405882810751488, 0.33263173628845255, 0.20392094597574836]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.047628329582661166, 0.07137540659996447, 0.18038475344565855, 0.16405882810751488, 0.33263173628845255, 0.20392094597574836]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.814]
 [1.063]
 [0.814]
 [0.814]] [[42.684]
 [42.684]
 [49.481]
 [42.684]
 [42.684]] [[1.871]
 [1.871]
 [2.441]
 [1.871]
 [1.871]]
printing an ep nov before normalisation:  98.95625781975232
printing an ep nov before normalisation:  0.00017154884801584558
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 69.42347799039328
siam score:  -0.92071897
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.036]
 [0.096]
 [0.054]
 [0.058]] [[41.678]
 [38.706]
 [32.608]
 [36.746]
 [33.703]] [[0.051]
 [0.036]
 [0.096]
 [0.054]
 [0.058]]
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.04702703308222836, 0.07053286128314333, 0.19054593626885943, 0.1622747030871696, 0.3284217574832231, 0.20119770879537616]
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.047049134932957655, 0.07043871463728987, 0.19029206791735626, 0.16235112313039976, 0.32857648546694146, 0.201292473915055]
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.047049134932957655, 0.07043871463728987, 0.19029206791735626, 0.16235112313039976, 0.32857648546694146, 0.201292473915055]
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.04706994061623138, 0.07050265073929424, 0.19039403650448902, 0.1624442253526124, 0.32872233517971666, 0.2008668116076563]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.04655507042602033, 0.08069142334876167, 0.1883069790340785, 0.16066376208957936, 0.3251178916039133, 0.19866487349764675]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.9 0.9
probs:  [0.04657933084777891, 0.08073352369788331, 0.1884053201216559, 0.16074765661388488, 0.32528773115379706, 0.19824643756500002]
printing an ep nov before normalisation:  29.16935443878174
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04657933084777891, 0.08073352369788331, 0.1884053201216559, 0.16074765661388488, 0.32528773115379706, 0.19824643756500002]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04657933084777891, 0.08073352369788331, 0.1884053201216559, 0.16074765661388488, 0.32528773115379706, 0.19824643756500002]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04660346088113654, 0.08077539777735245, 0.18850313267196042, 0.16083110024442684, 0.32545665789566997, 0.19783025052945388]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.917]
 [0.917]
 [0.917]
 [0.917]
 [0.917]] [[42.966]
 [42.966]
 [42.966]
 [42.966]
 [42.966]] [[1.691]
 [1.691]
 [1.691]
 [1.691]
 [1.691]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  19.58917619338744
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.046465758718721016, 0.07984404599463131, 0.1911846427812805, 0.1638287506373955, 0.3244965239510673, 0.19418027791690443]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
printing an ep nov before normalisation:  28.681039810180664
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9137799
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04682343068425637, 0.07987818982516784, 0.1901395875705278, 0.16304884985752713, 0.3270037553653338, 0.19310618669718704]
Printing some Q and Qe and total Qs values:  [[0.599]
 [1.222]
 [1.215]
 [1.027]
 [0.967]] [[37.481]
 [36.609]
 [48.221]
 [48.191]
 [39.918]] [[1.439]
 [2.01 ]
 [2.689]
 [2.499]
 [1.951]]
printing an ep nov before normalisation:  91.36636248125095
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.046846836972861475, 0.07975787054896469, 0.1898942262592485, 0.16313052116695678, 0.32716761898735436, 0.19320292606461426]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04687763282843457, 0.0796511535810708, 0.19001932582459138, 0.16323797681240723, 0.3273832157666032, 0.19283069518689278]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04687763282843457, 0.0796511535810708, 0.19001932582459138, 0.16323797681240723, 0.3273832157666032, 0.19283069518689278]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.04704538683831185, 0.07922575872870113, 0.1935104805989326, 0.1612997927338597, 0.3285616513646017, 0.19035692973559304]
maxi score, test score, baseline:  0.058100000000000006 0.9 0.9
probs:  [0.04699456215837516, 0.07921996433705455, 0.19366460586236334, 0.16140884528758007, 0.3282053801481276, 0.19050664220649924]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.29472064971924
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.156]
 [0.152]
 [0.17 ]
 [0.159]
 [0.152]] [[57.44 ]
 [43.433]
 [76.801]
 [58.028]
 [58.336]] [[0.913]
 [0.621]
 [1.325]
 [0.928]
 [0.928]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.393]
 [0.713]
 [0.688]
 [0.49 ]
 [0.624]] [[81.787]
 [71.672]
 [64.007]
 [66.314]
 [75.579]] [[1.476]
 [1.534]
 [1.31 ]
 [1.172]
 [1.546]]
printing an ep nov before normalisation:  85.9016942274766
maxi score, test score, baseline:  0.0601 0.9 0.9
probs:  [0.047067742166260744, 0.07872175230062274, 0.19056600453405842, 0.15928673610101796, 0.328721903530994, 0.19563586136704625]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0601 0.9 0.9
probs:  [0.04634522306175322, 0.08744759778358409, 0.18763444512927085, 0.16228293321829637, 0.3236635452119212, 0.19262625559517432]
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.042 0.417 0.125]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.8067,     0.0006,     0.0025,     0.0213,     0.1689],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9697,     0.0203,     0.0000,     0.0087],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9876,     0.0013,     0.0111],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0592,     0.0004,     0.0036,     0.7984,     0.1384],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2255, 0.0006, 0.3266, 0.0580, 0.3893], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9094919
maxi score, test score, baseline:  0.0621 0.9 0.9
siam score:  -0.9086928
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04672197444543502, 0.0872784707565603, 0.1866147516697675, 0.16152506927941704, 0.32630472695553175, 0.19155500689328842]
maxi score, test score, baseline:  0.0621 0.9 0.9
printing an ep nov before normalisation:  38.03956985473633
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.046379695603409994, 0.08665377663763223, 0.1859273306458585, 0.16627623996203802, 0.3239075598342344, 0.19085539731682696]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.0465377058870343, 0.0861397109297821, 0.18926937597620544, 0.16443346992107308, 0.3250172760802456, 0.18860246120565946]
Printing some Q and Qe and total Qs values:  [[1.061]
 [1.061]
 [1.044]
 [0.582]
 [1.061]] [[56.095]
 [56.095]
 [68.117]
 [59.496]
 [56.095]] [[1.857]
 [1.857]
 [2.159]
 [1.469]
 [1.857]]
maxi score, test score, baseline:  0.0621 0.9 0.9
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.046487678165760264, 0.08614347779692993, 0.1894132321615398, 0.1645435894977969, 0.32466661091094834, 0.1887454114670247]
printing an ep nov before normalisation:  17.533109084068972
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04581349600662881, 0.08489253910628981, 0.18567533446857346, 0.16215239447857324, 0.3199465762439586, 0.20151965969597613]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04581349600662881, 0.08489253910628981, 0.18567533446857346, 0.16215239447857324, 0.3199465762439586, 0.20151965969597613]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Sims:  25 1 epoch:  42542 pick best:  False frame count:  42542
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.045568739929157984, 0.08443839649374721, 0.19003493816369957, 0.16128429154015228, 0.318233007952405, 0.2004406259208379]
maxi score, test score, baseline:  0.0621 0.9 0.9
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04597590913126661, 0.084426851390699, 0.18888587895633124, 0.16044494196460912, 0.3210869446868342, 0.19917947387025986]
line 256 mcts: sample exp_bonus 24.07744753595039
printing an ep nov before normalisation:  92.35081955013267
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04597590913126661, 0.084426851390699, 0.18888587895633124, 0.16044494196460912, 0.3210869446868342, 0.19917947387025986]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.045110842691878254, 0.08292773719343674, 0.19601365005925467, 0.1576923058234361, 0.3150300498149492, 0.203225414417045]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04513437788085994, 0.08297106062019394, 0.19611614732781835, 0.15777475089359455, 0.3151948241384797, 0.2028088391390535]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04515000653830744, 0.08299982975379923, 0.19583719413951708, 0.15782949894599999, 0.31530424333458673, 0.20287922728778962]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.314]
 [1.464]
 [1.266]
 [1.193]
 [1.367]] [[16.409]
 [14.993]
 [21.376]
 [24.185]
 [15.863]] [[1.801]
 [1.909]
 [1.901]
 [1.911]
 [1.838]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.29148737589519
printing an ep nov before normalisation:  46.982346974424644
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.523]
 [0.523]] [[59.598]
 [59.598]
 [59.598]
 [59.598]
 [59.598]] [[1.856]
 [1.856]
 [1.856]
 [1.856]
 [1.856]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.6982, 0.0008, 0.0009, 0.0545, 0.2455], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0019,     0.9944,     0.0005,     0.0000,     0.0033],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9580,     0.0005,     0.0413],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0039,     0.0007,     0.0040,     0.9326,     0.0587],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1649, 0.0025, 0.1098, 0.0922, 0.6305], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.044859219428533166, 0.08183525413437412, 0.20172121753434402, 0.15987382354192683, 0.31327108972795314, 0.1984393956328686]
printing an ep nov before normalisation:  84.16073472855696
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.967]
 [1.212]
 [0.627]
 [0.967]] [[58.088]
 [58.088]
 [71.149]
 [57.608]
 [58.088]] [[1.654]
 [1.654]
 [2.194]
 [1.303]
 [1.654]]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04487516736835293, 0.08186438661278031, 0.2014368129239642, 0.1599307822201344, 0.31338274524402276, 0.19851010563074545]
Printing some Q and Qe and total Qs values:  [[1.146]
 [1.146]
 [1.112]
 [1.146]
 [1.146]] [[45.453]
 [45.453]
 [57.644]
 [45.453]
 [45.453]] [[1.963]
 [1.963]
 [2.347]
 [1.963]
 [1.963]]
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.04487516736835293, 0.08186438661278031, 0.2014368129239642, 0.1599307822201344, 0.31338274524402276, 0.19851010563074545]
Printing some Q and Qe and total Qs values:  [[1.412]
 [1.412]
 [1.412]
 [1.412]
 [1.412]] [[33.201]
 [33.201]
 [33.201]
 [33.201]
 [33.201]] [[3.079]
 [3.079]
 [3.079]
 [3.079]
 [3.079]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0621 0.9 0.9
probs:  [0.045641718680338346, 0.08191325622306837, 0.1990755105209514, 0.15840609973587474, 0.31875561918186585, 0.19620779565790114]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04602827005290322, 0.0819557241698182, 0.19766223544743042, 0.1577229320622648, 0.32146480928172577, 0.19516602898585758]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04602827005290322, 0.0819557241698182, 0.19766223544743042, 0.1577229320622648, 0.32146480928172577, 0.19516602898585758]
printing an ep nov before normalisation:  31.22069703090213
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04597916899852907, 0.08195386875713717, 0.1978125374449459, 0.15782071272022008, 0.32112066367951736, 0.19531304839965055]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04597916899852907, 0.08195386875713717, 0.1978125374449459, 0.15782071272022008, 0.32112066367951736, 0.19531304839965055]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  82.18851451428388
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04553468828622128, 0.08983288743935118, 0.1962664581524468, 0.1565719707143661, 0.31800844272004786, 0.19378555268756675]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04553468828622128, 0.08983288743935118, 0.1962664581524468, 0.1565719707143661, 0.31800844272004786, 0.19378555268756675]
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.274]
 [1.374]
 [1.274]
 [1.274]] [[69.434]
 [69.434]
 [66.013]
 [69.434]
 [69.434]] [[1.909]
 [1.909]
 [1.967]
 [1.909]
 [1.909]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.04510260498645515, 0.0975237840368623, 0.19475467246168865, 0.15534451946109334, 0.31498288115474904, 0.19229153789915146]
printing an ep nov before normalisation:  64.67441004784804
siam score:  -0.9244481
siam score:  -0.9251257
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04467961166330274, 0.10505060846770904, 0.19327552751170207, 0.1541435069679955, 0.3120209691615703, 0.1908297762277204]
maxi score, test score, baseline:  0.0641 0.9 0.9
probs:  [0.04467961166330274, 0.10505060846770904, 0.19327552751170207, 0.1541435069679955, 0.3120209691615703, 0.1908297762277204]
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([    0.7640,     0.0153,     0.0002,     0.0315,     0.1890],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9993,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9617,     0.0011,     0.0369],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0368,     0.0002,     0.0412,     0.8156,     0.1063],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0157, 0.0549, 0.0760, 0.1624, 0.6910], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.04443291272310464, 0.1043649262498568, 0.1972430960239407, 0.15354813888022398, 0.3102933726791226, 0.19011755344375128]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  -0.9262706
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.04403823873760014, 0.11172696887969912, 0.19550263334427098, 0.15243662157848312, 0.30752974771921104, 0.18876578974073552]
using another actor
maxi score, test score, baseline:  0.0661 0.9 0.9
probs:  [0.04398942985946499, 0.11176216570111076, 0.19564180069790518, 0.152522341475835, 0.3071876659908844, 0.18889659627479957]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.281679153442383
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.043694014360759384, 0.11101023287672122, 0.1943248544330438, 0.15149584800094193, 0.3051193464859986, 0.19435570384253514]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.918]
 [0.767]
 [0.767]
 [0.767]] [[ 0.   ]
 [65.276]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.203]
 [1.611]
 [0.203]
 [0.203]
 [0.203]]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.04364527292368704, 0.11104419552269638, 0.19446117665197435, 0.1515795508989586, 0.3047777400399188, 0.1944920639627647]
printing an ep nov before normalisation:  51.58402919769287
printing an ep nov before normalisation:  22.893923782036428
using explorer policy with actor:  1
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.04364527292368704, 0.11104419552269638, 0.19446117665197435, 0.1515795508989586, 0.3047777400399188, 0.1944920639627647]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.04364527292368704, 0.11104419552269638, 0.19446117665197435, 0.1515795508989586, 0.3047777400399188, 0.1944920639627647]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.04364527292368704, 0.11104419552269638, 0.19446117665197435, 0.1515795508989586, 0.3047777400399188, 0.1944920639627647]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.043444571259617794, 0.11053261268634901, 0.19356482848286843, 0.15548986348548452, 0.30337255076141145, 0.19359557332426883]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  25.21884044011434
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.0431182514696963, 0.11001311976137144, 0.19246981940464072, 0.1548409207674339, 0.30108717199595614, 0.19847071660090138]
maxi score, test score, baseline:  0.06810000000000001 0.9 0.9
probs:  [0.0431182514696963, 0.11001311976137144, 0.19246981940464072, 0.1548409207674339, 0.30108717199595614, 0.19847071660090138]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.60157708862781
maxi score, test score, baseline:  0.07010000000000001 0.9 0.9
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.043436796386862715, 0.10980222700426194, 0.19160632526864432, 0.15427523955922717, 0.303319683259002, 0.19755972852200185]
siam score:  -0.9135912
printing an ep nov before normalisation:  44.42959503689486
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.04345838615153423, 0.10985690443448697, 0.19170178758671078, 0.1543520898006867, 0.3034708422581968, 0.1971599897683845]
from probs:  [0.04345838615153423, 0.10985690443448697, 0.19170178758671078, 0.1543520898006867, 0.3034708422581968, 0.1971599897683845]
maxi score, test score, baseline:  0.0721 0.9 0.9
probs:  [0.04347986480445392, 0.10991130046724655, 0.1917967586078337, 0.15442854453227906, 0.30362122331740654, 0.19676230827078028]
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.718]
 [1.06 ]
 [0.718]
 [0.718]] [[39.691]
 [39.691]
 [61.825]
 [39.691]
 [39.691]] [[0.951]
 [0.951]
 [1.54 ]
 [0.951]
 [0.951]]
printing an ep nov before normalisation:  40.25277041402004
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 48.31281399726868
maxi score, test score, baseline:  0.0721 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.965]
 [0.651]
 [0.65 ]
 [0.776]] [[29.614]
 [31.701]
 [31.127]
 [28.619]
 [28.733]] [[0.143]
 [0.965]
 [0.651]
 [0.65 ]
 [0.776]]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04380858497277134, 0.1113994511053898, 0.19220961216875757, 0.15404732441489397, 0.3058830376949912, 0.19265198964319605]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04380858497277134, 0.1113994511053898, 0.19220961216875757, 0.15404732441489397, 0.3058830376949912, 0.19265198964319605]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.04344883421463155, 0.11859909347657525, 0.1906307366781101, 0.15303980441976486, 0.3033641563952382, 0.19091737481568005]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04339954341150281, 0.11864259874085803, 0.19076318712081183, 0.15312583731556578, 0.30301865421009627, 0.19105017920116524]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04319943018150479, 0.1180944750491572, 0.18988149484725958, 0.1570396768792796, 0.3016177634967756, 0.19016715954602334]
printing an ep nov before normalisation:  62.81441856847577
printing an ep nov before normalisation:  32.87634811297548
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04316507395247865, 0.11817776484788392, 0.18973254352680088, 0.1571841422645013, 0.3013768134330898, 0.1903636619752455]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.0431789142187821, 0.11789435997703689, 0.18979352406112202, 0.1572346543069838, 0.3014737019408605, 0.19042484549521485]
from probs:  [0.0431789142187821, 0.11789435997703689, 0.18979352406112202, 0.1572346543069838, 0.3014737019408605, 0.19042484549521485]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.042983309167057185, 0.11735923848377648, 0.19347217783166096, 0.1565207655373683, 0.300104372683858, 0.18956013629627913]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04299836550053821, 0.11740042846115359, 0.1931890203291217, 0.156575715791919, 0.30020977424561196, 0.1896266956716556]
maxi score, test score, baseline:  0.0741 0.9 0.9
printing an ep nov before normalisation:  27.476136869326098
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0741 0.9 0.9
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04239094271987118, 0.12345624733745372, 0.1904533127032517, 0.15435883898989933, 0.29595752320402396, 0.1933831350455002]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.528]
 [0.535]
 [0.122]
 [0.528]] [[65.163]
 [60.346]
 [72.879]
 [60.9  ]
 [60.346]] [[0.769]
 [1.052]
 [1.292]
 [0.656]
 [1.052]]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04217561540400449, 0.12301862702612855, 0.19426988951555027, 0.15383647922439883, 0.29444969900727824, 0.1922496898226396]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04217561540400449, 0.12301862702612855, 0.19426988951555027, 0.15383647922439883, 0.29444969900727824, 0.1922496898226396]
printing an ep nov before normalisation:  42.51742813876429
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04217561540400449, 0.12301862702612855, 0.19426988951555027, 0.15383647922439883, 0.29444969900727824, 0.1922496898226396]
printing an ep nov before normalisation:  24.459510930051444
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04172661533850432, 0.1217062479120696, 0.19656859932113382, 0.1521949747181052, 0.2913064876066597, 0.19649707510352737]
printing an ep nov before normalisation:  73.52338682274298
Printing some Q and Qe and total Qs values:  [[0.89]
 [0.89]
 [0.89]
 [0.89]
 [0.89]] [[19.299]
 [19.299]
 [19.299]
 [19.299]
 [19.299]] [[1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.8401,     0.0005,     0.0009,     0.0230,     0.1355],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9942,     0.0000,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0137,     0.9649,     0.0005,     0.0207],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1046, 0.0020, 0.0111, 0.7611, 0.1213], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2315, 0.0028, 0.1092, 0.1765, 0.4800], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04156195507349701, 0.12122496358871372, 0.1957909492761818, 0.1560598062561892, 0.29015378839352424, 0.195208537411894]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.0419249044417984, 0.12089627685347988, 0.19481487890884108, 0.15542868260200332, 0.2926977336358936, 0.19423752355798377]
printing an ep nov before normalisation:  28.306761718084932
actions average: 
K:  3  action  0 :  tensor([    0.8777,     0.0004,     0.0002,     0.0552,     0.0665],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9940,     0.0006,     0.0000,     0.0040],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0004,     0.9647,     0.0076,     0.0271],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0214,     0.0002,     0.0009,     0.8729,     0.1045],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2481, 0.0543, 0.0928, 0.0605, 0.5444], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0741 0.9 0.9
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04228003751700461, 0.12057466854869243, 0.19385982869675877, 0.15481115050601046, 0.29518689376670454, 0.1932874209648291]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.13052490779332
Printing some Q and Qe and total Qs values:  [[0.129]
 [0.359]
 [0.129]
 [0.129]
 [0.129]] [[56.58 ]
 [76.175]
 [56.58 ]
 [56.58 ]
 [56.58 ]] [[0.504]
 [0.899]
 [0.504]
 [0.504]
 [0.504]]
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.042627604100885896, 0.12025991245401467, 0.1929251268544824, 0.1542067755941084, 0.2976230196693597, 0.19235756132714893]
printing an ep nov before normalisation:  102.07840998108932
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.042627604100885896, 0.12025991245401467, 0.1929251268544824, 0.1542067755941084, 0.2976230196693597, 0.19235756132714893]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 25.936923775463303
printing an ep nov before normalisation:  22.427122592926025
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0741 0.9 0.9
probs:  [0.04203502731858044, 0.13300668251500808, 0.190236676971356, 0.15205824645902918, 0.2934745756809207, 0.18918879105510547]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
printing an ep nov before normalisation:  75.70852582920048
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0761 0.9 0.9
probs:  [0.04197797710140806, 0.1313514551499333, 0.18690603617580553, 0.15428516786991017, 0.2930779787317401, 0.1924013849712028]
UNIT TEST: sample policy line 217 mcts : [0.042 0.833 0.042 0.042 0.042]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0761 0.9 0.9
probs:  [0.042311457621982455, 0.13096482774015697, 0.18607178945840508, 0.15371375691178552, 0.29541530762657797, 0.191522860641092]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0761 0.9 0.9
probs:  [0.042176476185227525, 0.13054613219247627, 0.1854767371334369, 0.15739174014992918, 0.2944703272729153, 0.18993858706601477]
printing an ep nov before normalisation:  1.6758429057711055
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.90686417
using explorer policy with actor:  1
printing an ep nov before normalisation:  77.27452229736203
maxi score, test score, baseline:  0.0761 0.9 0.9
maxi score, test score, baseline:  0.0761 0.9 0.9
probs:  [0.04192051161440098, 0.1366411958969992, 0.18402118567274, 0.15643433061490733, 0.2926783661227022, 0.18830441007825036]
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.712]
 [0.409]
 [0.409]
 [0.409]] [[47.922]
 [79.215]
 [47.922]
 [47.922]
 [47.922]] [[0.931]
 [1.884]
 [0.931]
 [0.931]
 [0.931]]
maxi score, test score, baseline:  0.0761 0.9 0.9
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.322]
 [0.789]
 [0.523]
 [0.523]
 [0.523]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.322]
 [0.789]
 [0.523]
 [0.523]
 [0.523]]
printing an ep nov before normalisation:  105.51567066747445
maxi score, test score, baseline:  0.0761 0.9 0.9
probs:  [0.04171935964092262, 0.136203151937384, 0.18346464658224468, 0.16008051778209428, 0.2912697530215885, 0.18726257103576588]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0781 0.9 0.9
probs:  [0.041887377045035055, 0.13511565871389516, 0.18626325912546457, 0.15914448562454556, 0.2924482299114284, 0.18514098957963127]
maxi score, test score, baseline:  0.0781 0.9 0.9
probs:  [0.041887377045035055, 0.13511565871389516, 0.18626325912546457, 0.15914448562454556, 0.2924482299114284, 0.18514098957963127]
maxi score, test score, baseline:  0.0781 0.9 0.9
probs:  [0.041887377045035055, 0.13511565871389516, 0.18626325912546457, 0.15914448562454556, 0.2924482299114284, 0.18514098957963127]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0801 0.9 0.9
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.041887377045035055, 0.13511565871389516, 0.18626325912546457, 0.15914448562454556, 0.2924482299114284, 0.18514098957963127]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.04172353425698325, 0.134585989259831, 0.18945373849619854, 0.1585205273401012, 0.2913011893233473, 0.18441502132353854]
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.04172353425698325, 0.134585989259831, 0.18945373849619854, 0.1585205273401012, 0.2913011893233473, 0.18441502132353854]
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.04172353425698325, 0.134585989259831, 0.18945373849619854, 0.1585205273401012, 0.2913011893233473, 0.18441502132353854]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.04112401360613631, 0.1397157046612316, 0.18739668628356648, 0.15676866750027846, 0.2871032833784836, 0.18789164457030355]
printing an ep nov before normalisation:  83.27418663194715
Printing some Q and Qe and total Qs values:  [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.109]
 [0.109]
 [0.109]
 [0.109]
 [0.109]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.040958992159158394, 0.13915377101113405, 0.18664279768006364, 0.16016068163174293, 0.28594799416307326, 0.18713576335482765]
actions average: 
K:  4  action  0 :  tensor([    0.7243,     0.0005,     0.0012,     0.0567,     0.2173],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0141, 0.9435, 0.0036, 0.0011, 0.0377], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0000,     0.9226,     0.0151,     0.0617],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0356,     0.0004,     0.0038,     0.8439,     0.1164],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1896, 0.0010, 0.3139, 0.1020, 0.3936], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.54857063293457
maxi score, test score, baseline:  0.0801 0.9 0.9
probs:  [0.040958992159158394, 0.13915377101113405, 0.18664279768006364, 0.16016068163174293, 0.28594799416307326, 0.18713576335482765]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([    0.7730,     0.0671,     0.0007,     0.0571,     0.1020],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0054,     0.9553,     0.0232,     0.0000,     0.0161],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0036,     0.9266,     0.0165,     0.0527],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0444,     0.0002,     0.0076,     0.7973,     0.1505],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2101, 0.0658, 0.0490, 0.1177, 0.5574], grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([    0.7522,     0.0023,     0.0005,     0.0625,     0.1826],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9902,     0.0007,     0.0000,     0.0082],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9644,     0.0047,     0.0308],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1095,     0.0004,     0.0003,     0.7645,     0.1253],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.2082,     0.0004,     0.1305,     0.1204,     0.5405],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.63642877035491
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.021]
 [0.025]
 [0.03 ]
 [0.032]] [[27.066]
 [23.604]
 [25.893]
 [26.514]
 [24.897]] [[0.675]
 [0.458]
 [0.595]
 [0.636]
 [0.544]]
printing an ep nov before normalisation:  32.243576782065105
Printing some Q and Qe and total Qs values:  [[0.257]
 [0.417]
 [0.257]
 [0.257]
 [0.257]] [[37.989]
 [56.162]
 [37.989]
 [37.989]
 [37.989]] [[0.257]
 [0.417]
 [0.257]
 [0.257]
 [0.257]]
printing an ep nov before normalisation:  22.502442622924264
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.06127523743815
maxi score, test score, baseline:  0.0821 0.9 0.9
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.040754538623025534, 0.13868206977870476, 0.18988183210707013, 0.15963180782318181, 0.2845162777201382, 0.18653347394787945]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
siam score:  -0.91333956
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04107548481862461, 0.1382654447746917, 0.18907958040875844, 0.15905739344943104, 0.28676565511372837, 0.18575644143476588]
maxi score, test score, baseline:  0.0821 0.9 0.9
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04139027412834409, 0.13785681211531287, 0.18829271874223685, 0.15849399837983544, 0.28897188147420977, 0.1849943151600609]
Printing some Q and Qe and total Qs values:  [[0.417]
 [0.417]
 [0.855]
 [0.461]
 [0.339]] [[62.657]
 [62.657]
 [70.667]
 [71.001]
 [60.675]] [[1.528]
 [1.528]
 [2.18 ]
 [1.795]
 [1.397]]
using another actor
using explorer policy with actor:  0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.91310525
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.041782403084727755, 0.13655851282600726, 0.18545884445306707, 0.15683406402721067, 0.2917214475503846, 0.1876447280586026]
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04180172633993976, 0.13662180773006768, 0.18554482680670467, 0.15690676583159716, 0.29185673074451746, 0.1872681425471733]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04153871615700812, 0.14206721753770604, 0.18437451495650103, 0.15591721646937223, 0.29001538169405555, 0.18608695318535703]
printing an ep nov before normalisation:  54.3182312858959
using another actor
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04149170526728448, 0.1421331917629778, 0.18448803882986617, 0.1559987568644415, 0.2896859055880945, 0.1862024016873355]
maxi score, test score, baseline:  0.0821 0.9 0.9
probs:  [0.04149170526728448, 0.1421331917629778, 0.18448803882986617, 0.1559987568644415, 0.2896859055880945, 0.1862024016873355]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.120282665090556
printing an ep nov before normalisation:  46.872050292544905
printing an ep nov before normalisation:  32.37459897994995
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.53030443191528
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.10611003051236
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
probs:  [0.04247257322602794, 0.13884645380526064, 0.18217798459100457, 0.15698503259217303, 0.2965214867241319, 0.18299646906140196]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
probs:  [0.04232826124926947, 0.13837365529357695, 0.1849632278793669, 0.15645040945686342, 0.29551125914041715, 0.1823731869805062]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08410000000000001 0.9 0.9
probs:  [0.04235437730879773, 0.1434162993718444, 0.18307248216936195, 0.15494284560518554, 0.295696746888029, 0.18051724865678137]
printing an ep nov before normalisation:  6.744072931940082
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.037685049976524
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.47001616869216
printing an ep nov before normalisation:  43.88055843511981
printing an ep nov before normalisation:  9.51243063561833
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0861 0.9 0.9
probs:  [0.0429107474912444, 0.14653881711245761, 0.18083679454963134, 0.15194417835655616, 0.2995572088226281, 0.17821225366748233]
printing an ep nov before normalisation:  5.280911726836166
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0881 0.9 0.9
probs:  [0.0429107474912444, 0.14653881711245761, 0.18083679454963134, 0.15194417835655616, 0.2995572088226281, 0.17821225366748233]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91527283
printing an ep nov before normalisation:  111.9098390671669
maxi score, test score, baseline:  0.0881 0.9 0.9
probs:  [0.042741173310632326, 0.1461997280729672, 0.18420397947257736, 0.15159624722387777, 0.29836974201376665, 0.17688912990617858]
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04270743434593328, 0.14632525680071792, 0.18404225929580492, 0.15173008354063747, 0.2981330630519126, 0.17706190296499377]
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04272211100415167, 0.1463756537565401, 0.18376120283134745, 0.15178234370893673, 0.2982357928968981, 0.17712289580212595]
printing an ep nov before normalisation:  70.6888959218855
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04272211100415167, 0.1463756537565401, 0.18376120283134745, 0.15178234370893673, 0.2982357928968981, 0.17712289580212595]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04301351912183852, 0.145923899765611, 0.1830414066419132, 0.15129182551372708, 0.3002786548470264, 0.17645069410988373]
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.0430523016433679, 0.1456121050110367, 0.1832068355611773, 0.15142853893816596, 0.3005501174673516, 0.1761501013789006]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.91489315032959
siam score:  -0.91559035
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04335726978643963, 0.14479629663870083, 0.18257832248137446, 0.15101168111034313, 0.3026878412148491, 0.1755685887682929]
printing an ep nov before normalisation:  45.09130779132292
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04291387458695969, 0.14331233294102877, 0.1807067881700072, 0.14946395957226155, 0.29958421521840395, 0.1840188295113387]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
printing an ep nov before normalisation:  62.48692625057193
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.91407657
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04274748816768778, 0.14275546762928257, 0.18000448491327786, 0.15276935156077853, 0.29841956296931915, 0.1833036447596542]
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04274748816768778, 0.14275546762928257, 0.18000448491327786, 0.15276935156077853, 0.29841956296931915, 0.1833036447596542]
printing an ep nov before normalisation:  19.637380118055063
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04302860115223328, 0.1423444555672773, 0.1793356836432469, 0.1522890364299626, 0.30039021220736933, 0.18261201099991042]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.7761,     0.1221,     0.0002,     0.0043,     0.0973],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0021,     0.9681,     0.0083,     0.0000,     0.0214],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0004,     0.9534,     0.0005,     0.0454],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0069,     0.0001,     0.0434,     0.8824,     0.0672],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1293, 0.0879, 0.1025, 0.1150, 0.5653], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.042863560574746974, 0.14179729579406508, 0.17864619958148525, 0.1555480537441399, 0.29923496908461633, 0.18190992122094646]
printing an ep nov before normalisation:  37.11610555648804
siam score:  -0.90620965
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.042710133070667365, 0.14128863689565763, 0.18159296009233233, 0.15499002138470985, 0.2981610145964007, 0.1812572339602321]
from probs:  [0.042710133070667365, 0.14128863689565763, 0.18159296009233233, 0.15499002138470985, 0.2981610145964007, 0.1812572339602321]
printing an ep nov before normalisation:  65.88430643329278
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04266224385795299, 0.14135686305404782, 0.18170866061139462, 0.15507438636934373, 0.2978253070789309, 0.1813725390283299]
maxi score, test score, baseline:  0.0921 0.9 0.9
probs:  [0.04266224385795299, 0.14135686305404782, 0.18170866061139462, 0.15507438636934373, 0.2978253070789309, 0.1813725390283299]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0941 0.9 0.9
probs:  [0.04321018294959805, 0.14057623257098406, 0.18038483769336144, 0.15410909856386962, 0.3016664074385867, 0.1800532407836001]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0941 0.9 0.9
probs:  [0.04282097795897954, 0.14483794892716476, 0.18225349714643388, 0.1527179097930323, 0.29894202231207495, 0.17842764386231452]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0941 0.9 0.9
probs:  [0.04282097795897954, 0.14483794892716476, 0.18225349714643388, 0.1527179097930323, 0.29894202231207495, 0.17842764386231452]
printing an ep nov before normalisation:  48.24601293064995
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.164]
 [1.164]
 [1.368]
 [1.164]
 [1.164]] [[43.881]
 [43.881]
 [52.92 ]
 [43.881]
 [43.881]] [[1.62 ]
 [1.62 ]
 [1.948]
 [1.62 ]
 [1.62 ]]
maxi score, test score, baseline:  0.0941 0.9 0.9
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.154844066827636
maxi score, test score, baseline:  0.0941 0.9 0.9
probs:  [0.04270998310283146, 0.1489769099719487, 0.18343517627519254, 0.15092262716012653, 0.2981677495980126, 0.17578755389188802]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.964993476867676
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [1.306]
 [1.292]
 [1.159]
 [1.167]] [[28.88 ]
 [34.328]
 [32.842]
 [34.639]
 [31.057]] [[1.444]
 [2.382]
 [2.272]
 [2.255]
 [2.031]]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0961 0.9 0.9
printing an ep nov before normalisation:  47.60250430871172
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.04251759131832262, 0.1485453732308969, 0.1863340919229339, 0.15048671174297937, 0.29682054977746675, 0.17529568200740048]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.0427803461693605, 0.14812612187017793, 0.18567177086465628, 0.1500549730426171, 0.2986624243537059, 0.1747043636994824]
maxi score, test score, baseline:  0.0961 0.9 0.9
maxi score, test score, baseline:  0.0961 0.9 0.9
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.04281800110708306, 0.14781927873834122, 0.18583559065879796, 0.1501873452660392, 0.29892600835416394, 0.1744137758755746]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.9 0.9
maxi score, test score, baseline:  0.0961 0.9 0.9
Printing some Q and Qe and total Qs values:  [[0.774]
 [0.944]
 [1.082]
 [0.774]
 [0.774]] [[88.652]
 [87.178]
 [69.106]
 [88.652]
 [88.652]] [[2.577]
 [2.71 ]
 [2.396]
 [2.577]
 [2.577]]
siam score:  -0.9186833
Printing some Q and Qe and total Qs values:  [[0.588]
 [1.036]
 [0.588]
 [0.588]
 [0.588]] [[45.536]
 [67.184]
 [45.536]
 [45.536]
 [45.536]] [[0.968]
 [1.665]
 [0.968]
 [0.968]
 [0.968]]
printing an ep nov before normalisation:  90.45741070934403
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.043110347231822894, 0.14732823099277112, 0.18463896718990602, 0.15012152431761935, 0.3009745200901586, 0.17382641017772202]
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.043110347231822894, 0.14732823099277112, 0.18463896718990602, 0.15012152431761935, 0.3009745200901586, 0.17382641017772202]
maxi score, test score, baseline:  0.0961 0.9 0.9
probs:  [0.043110347231822894, 0.14732823099277112, 0.18463896718990602, 0.15012152431761935, 0.3009745200901586, 0.17382641017772202]
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.613]
 [0.793]
 [0.79 ]
 [0.676]] [[45.874]
 [45.983]
 [53.594]
 [46.945]
 [45.874]] [[0.676]
 [0.613]
 [0.793]
 [0.79 ]
 [0.676]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.614]
 [0.784]
 [0.679]
 [0.75 ]] [[42.286]
 [32.073]
 [45.428]
 [42.349]
 [37.112]] [[0.658]
 [0.614]
 [0.784]
 [0.679]
 [0.75 ]]
printing an ep nov before normalisation:  52.28408384148041
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0981 0.9 0.9
probs:  [0.043082368692491914, 0.14746734360402733, 0.1848378996095814, 0.15026511537901027, 0.3007782076002786, 0.17356906511461037]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0981 0.9 0.9
probs:  [0.04291069068008787, 0.14711250335429182, 0.18776272095105975, 0.1499053659348406, 0.29957599746427827, 0.17273272161544168]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.733675595882687
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.935]
 [1.252]
 [1.101]
 [0.972]
 [1.123]] [[36.806]
 [43.725]
 [47.861]
 [58.809]
 [43.556]] [[1.766]
 [2.451]
 [2.519]
 [2.972]
 [2.313]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1021 0.9 0.9
probs:  [0.04289482259797211, 0.14592166083590574, 0.18664352527237502, 0.14911655826956005, 0.29946740407205924, 0.17595602895212797]
printing an ep nov before normalisation:  52.36334096039047
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.73136962040165
maxi score, test score, baseline:  0.1021 0.9 0.9
probs:  [0.04262843212524536, 0.15082654558292372, 0.18580029412954013, 0.14842391687390408, 0.29760220368280965, 0.17471860760557703]
maxi score, test score, baseline:  0.1021 0.9 0.9
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
deleting a thread, now have 1 threads
Frames:  48557 train batches done:  5687 episodes:  3630
maxi score, test score, baseline:  0.1021 0.9 0.9
probs:  [0.042494070474022365, 0.150350082345047, 0.18521325063765828, 0.1515475983750425, 0.29666166409541544, 0.17373333407281455]
maxi score, test score, baseline:  0.1021 0.9 0.9
probs:  [0.042494070474022365, 0.150350082345047, 0.18521325063765828, 0.1515475983750425, 0.29666166409541544, 0.17373333407281455]
printing an ep nov before normalisation:  73.27398253666853
printing an ep nov before normalisation:  79.22702132505243
maxi score, test score, baseline:  0.1021 0.9 0.9
probs:  [0.042494070474022365, 0.150350082345047, 0.18521325063765828, 0.1515475983750425, 0.29666166409541544, 0.17373333407281455]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  83.49481617967561
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.042494070474022365, 0.150350082345047, 0.18521325063765828, 0.1515475983750425, 0.29666166409541544, 0.17373333407281455]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.92782927
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04303179541194202, 0.1487407895185411, 0.18487623687810076, 0.14998200710097473, 0.30039179312382297, 0.17297737796661836]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1041 0.9 0.9
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04288637378082493, 0.14823702552713316, 0.18763716249024193, 0.14947403551264582, 0.2993739481969972, 0.1723914544921571]
Printing some Q and Qe and total Qs values:  [[0.715]
 [1.072]
 [1.124]
 [0.581]
 [0.968]] [[16.51 ]
 [48.095]
 [27.004]
 [24.886]
 [19.95 ]] [[1.061]
 [4.388]
 [2.457]
 [1.715]
 [1.638]]
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04288637378082493, 0.14823702552713316, 0.18763716249024193, 0.14947403551264582, 0.2993739481969972, 0.1723914544921571]
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04288637378082493, 0.14823702552713316, 0.18763716249024193, 0.14947403551264582, 0.2993739481969972, 0.1723914544921571]
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04288637378082493, 0.14823702552713316, 0.18763716249024193, 0.14947403551264582, 0.2993739481969972, 0.1723914544921571]
actions average: 
K:  3  action  0 :  tensor([    0.7117,     0.0135,     0.0003,     0.0264,     0.2482],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0100,     0.9820,     0.0000,     0.0000,     0.0080],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0024,     0.9774,     0.0042,     0.0160],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0263,     0.0002,     0.0204,     0.7271,     0.2260],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2017, 0.0516, 0.0244, 0.1280, 0.5943], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.04288637378082493, 0.14823702552713316, 0.18763716249024193, 0.14947403551264582, 0.2993739481969972, 0.1723914544921571]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 0.9 0.9
probs:  [0.042905425671622824, 0.14830302435054404, 0.1877207190177988, 0.14954058557920008, 0.2995072974810487, 0.1720229478997854]
maxi score, test score, baseline:  0.1041 0.9 0.9
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1061 0.9 0.9
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1081 0.9 0.9
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.042905425671622824, 0.14830302435054404, 0.1877207190177988, 0.14954058557920008, 0.2995072974810487, 0.1720229478997854]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.042924388858412516, 0.14836871588895964, 0.1878038865131601, 0.14960682579421722, 0.299640025902, 0.17165615704325057]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.04294377464691812, 0.14798320099246506, 0.1878889074272869, 0.1496745421972211, 0.29977571222549526, 0.1717338625106136]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.04294377464691812, 0.14798320099246506, 0.1878889074272869, 0.1496745421972211, 0.29977571222549526, 0.1717338625106136]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.04294377464691812, 0.14798320099246506, 0.1878889074272869, 0.1496745421972211, 0.29977571222549526, 0.1717338625106136]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.0427995278312015, 0.14748503198887522, 0.19062300818500674, 0.14917067435082093, 0.2987660901461657, 0.1711556674979299]
Printing some Q and Qe and total Qs values:  [[0.432]
 [0.474]
 [0.417]
 [0.417]
 [0.417]] [[107.302]
 [100.025]
 [103.693]
 [103.693]
 [103.693]] [[1.409]
 [1.362]
 [1.35 ]
 [1.35 ]
 [1.35 ]]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.0427995278312015, 0.14748503198887522, 0.19062300818500674, 0.14917067435082093, 0.2987660901461657, 0.1711556674979299]
printing an ep nov before normalisation:  68.3639980821525
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.0427995278312015, 0.14748503198887522, 0.19062300818500674, 0.14917067435082093, 0.2987660901461657, 0.1711556674979299]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0427995278312015, 0.14748503198887522, 0.19062300818500674, 0.14917067435082093, 0.2987660901461657, 0.1711556674979299]
maxi score, test score, baseline:  0.1081 0.9 0.9
probs:  [0.042656822147983234, 0.14699218542125933, 0.19332789781934376, 0.14867218982681232, 0.29776725486576666, 0.17058364991883462]
maxi score, test score, baseline:  0.1081 0.9 0.9
printing an ep nov before normalisation:  21.255481195915333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9257261
siam score:  -0.92617905
maxi score, test score, baseline:  0.1101 0.9 0.9
probs:  [0.042385015726556535, 0.15193080584136168, 0.19207325204120906, 0.14796281388831423, 0.29586427623159106, 0.16978383627096744]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  58.208238482288394
maxi score, test score, baseline:  0.1101 0.9 0.9
probs:  [0.04214937611035345, 0.15665680056886677, 0.19100279620985156, 0.14713836342073291, 0.2942149744460365, 0.16883768924415885]
maxi score, test score, baseline:  0.1101 0.9 0.9
probs:  [0.0421020954295135, 0.15674087230511244, 0.191126266522792, 0.14721151647871647, 0.2938835154849519, 0.16893573377891355]
actions average: 
K:  2  action  0 :  tensor([    0.6774,     0.0565,     0.0004,     0.0997,     0.1660],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0083,     0.9423,     0.0007,     0.0000,     0.0487],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0000,     0.9711,     0.0013,     0.0271],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0156,     0.0001,     0.0448,     0.7760,     0.1635],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2043, 0.0021, 0.1522, 0.1249, 0.5166], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.467]
 [0.467]
 [0.931]
 [0.467]] [[58.218]
 [58.218]
 [58.218]
 [71.772]
 [58.218]] [[1.307]
 [1.307]
 [1.307]
 [2.057]
 [1.307]]
maxi score, test score, baseline:  0.1101 0.9 0.9
probs:  [0.0421020954295135, 0.15674087230511244, 0.191126266522792, 0.14721151647871647, 0.2938835154849519, 0.16893573377891355]
maxi score, test score, baseline:  0.1101 0.9 0.9
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04202602422530043, 0.1569768598395102, 0.19145585469598456, 0.14742156411005522, 0.29335001700298774, 0.16876968012616184]
siam score:  -0.9254399
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04202602422530043, 0.1569768598395102, 0.19145585469598456, 0.14742156411005522, 0.29335001700298774, 0.16876968012616184]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1121 0.9 0.9
siam score:  -0.9254662
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04187273754837353, 0.15640296550055083, 0.19075580094862832, 0.1505387301553574, 0.29227713025138485, 0.16815263559570504]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04213013283733026, 0.15594990624318272, 0.1900896439915558, 0.15012204795526235, 0.2940815782531434, 0.16762669071952543]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04213013283733026, 0.15594990624318272, 0.1900896439915558, 0.15012204795526235, 0.2940815782531434, 0.16762669071952543]
maxi score, test score, baseline:  0.1121 0.9 0.9
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04201367020684588, 0.15551780726117692, 0.19318621318470217, 0.14970611034726158, 0.293266423008784, 0.16630977599122929]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04186235036557037, 0.1549563812561101, 0.1960989362802999, 0.14916568280787482, 0.29220729218133895, 0.1657093571088057]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04186235036557037, 0.1549563812561101, 0.1960989362802999, 0.14916568280787482, 0.29220729218133895, 0.1657093571088057]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04167425338651446, 0.15425850492590545, 0.1952156069704846, 0.1484939084608995, 0.29089074765295364, 0.1694669786032422]
maxi score, test score, baseline:  0.1121 0.9 0.9
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04152537196663371, 0.1537061259378774, 0.19809753546120873, 0.1479621895618553, 0.289848684033931, 0.1688600930384938]
printing an ep nov before normalisation:  56.11752078920423
siam score:  -0.92115295
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04152537196663371, 0.1537061259378774, 0.19809753546120873, 0.1479621895618553, 0.289848684033931, 0.1688600930384938]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  75.68578419048859
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.041255048048977505, 0.1583546797772162, 0.19715547768455224, 0.14723522499603287, 0.2879560982689014, 0.16804347122431976]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.041035442325871946, 0.16284581943335869, 0.19610330605111703, 0.14644964596096735, 0.2864190177643872, 0.16714676846429782]
printing an ep nov before normalisation:  46.41055113635894
maxi score, test score, baseline:  0.1121 0.9 0.9
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04089077230287582, 0.1622703094226008, 0.19894431067600574, 0.1459321287694251, 0.28540643271518945, 0.16655604611390304]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04089077230287582, 0.1622703094226008, 0.19894431067600574, 0.1459321287694251, 0.28540643271518945, 0.16655604611390304]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04071188903045699, 0.16155869611182574, 0.19807173660892852, 0.14529222309415912, 0.2841543797780877, 0.170211075376542]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04071188903045699, 0.16155869611182574, 0.19807173660892852, 0.14529222309415912, 0.2841543797780877, 0.170211075376542]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04096516411901451, 0.16109440808025277, 0.19739064176303142, 0.14492452198460729, 0.2859298527304766, 0.16969541132261734]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04096516411901451, 0.16109440808025277, 0.19739064176303142, 0.14492452198460729, 0.2859298527304766, 0.16969541132261734]
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.04096516411901451, 0.16109440808025277, 0.19739064176303142, 0.14492452198460729, 0.2859298527304766, 0.16969541132261734]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1121 0.9 0.9
probs:  [0.0412144519237544, 0.16063742928767982, 0.19672026932364026, 0.1445626095560197, 0.2876773745937472, 0.1691878653151586]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.0412144519237544, 0.16063742928767982, 0.19672026932364026, 0.1445626095560197, 0.2876773745937472, 0.1691878653151586]
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.264111042022705
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.0412144519237544, 0.16063742928767982, 0.19672026932364026, 0.1445626095560197, 0.2876773745937472, 0.1691878653151586]
siam score:  -0.9178266
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.0414598458664143, 0.1601875884792278, 0.19606036806563693, 0.14420635018006916, 0.289397600259659, 0.16868824714899291]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04147741659343095, 0.16025563835986148, 0.19614366987906717, 0.14426760536078406, 0.2895205860119769, 0.16833508379487944]
line 256 mcts: sample exp_bonus 24.872079854697784
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04147741659343095, 0.16025563835986148, 0.19614366987906717, 0.14426760536078406, 0.2895205860119769, 0.16833508379487944]
printing an ep nov before normalisation:  62.87677298305331
printing an ep nov before normalisation:  28.980904716986995
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04126462743337738, 0.1645741835294348, 0.1951348486223283, 0.14352577843234257, 0.2880311749643027, 0.16746938701821423]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04126462743337738, 0.1645741835294348, 0.1951348486223283, 0.14352577843234257, 0.2880311749643027, 0.16746938701821423]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.869]
 [0.995]
 [0.869]
 [0.869]
 [0.869]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.869]
 [0.995]
 [0.869]
 [0.869]
 [0.869]]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04105538248509986, 0.16882079910145928, 0.1941428302712316, 0.14279630735762458, 0.28656657151572434, 0.16661810926886023]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04105538248509986, 0.16882079910145928, 0.1941428302712316, 0.14279630735762458, 0.28656657151572434, 0.16661810926886023]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.04105538248509986, 0.16882079910145928, 0.1941428302712316, 0.14279630735762458, 0.28656657151572434, 0.16661810926886023]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.0413431986692867, 0.16823950986905795, 0.19338929165373453, 0.14239204589484603, 0.2885841504036599, 0.1660518035094148]
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.0413431986692867, 0.16823950986905795, 0.19338929165373453, 0.14239204589484603, 0.2885841504036599, 0.1660518035094148]
Starting evaluation
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.0413431986692867, 0.16823950986905795, 0.19338929165373453, 0.14239204589484603, 0.2885841504036599, 0.1660518035094148]
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.382]
 [0.478]
 [0.292]
 [0.292]] [[54.781]
 [58.935]
 [64.953]
 [54.781]
 [54.781]] [[0.292]
 [0.382]
 [0.478]
 [0.292]
 [0.292]]
printing an ep nov before normalisation:  51.03533708327433
printing an ep nov before normalisation:  48.42975778332513
printing an ep nov before normalisation:  43.1510066986084
maxi score, test score, baseline:  0.11410000000000001 0.9 0.9
probs:  [0.04113736540187213, 0.17239061356902605, 0.19242402704821154, 0.14168149948580094, 0.287143411831378, 0.16522308266371133]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.04116793239219039, 0.17238921976034727, 0.19241775500672423, 0.14168758349451943, 0.28711407516283993, 0.1652234341833788]
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.04116793239219039, 0.17238921976034727, 0.19241775500672423, 0.14168758349451943, 0.28711407516283993, 0.1652234341833788]
siam score:  -0.9228932
printing an ep nov before normalisation:  58.772472944020635
printing an ep nov before normalisation:  47.864108085632324
printing an ep nov before normalisation:  24.428557029197293
printing an ep nov before normalisation:  23.830664448734552
maxi score, test score, baseline:  0.1541 0.95 0.95
probs:  [0.04116793239219039, 0.17238921976034727, 0.19241775500672423, 0.14168758349451943, 0.28711407516283993, 0.1652234341833788]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([    0.7062,     0.0445,     0.0004,     0.0446,     0.2042],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0036,     0.9939,     0.0003,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9756,     0.0005,     0.0239],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0876, 0.0016, 0.0230, 0.6886, 0.1992], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1737, 0.0092, 0.0792, 0.1810, 0.5569], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1561 0.95 0.95
printing an ep nov before normalisation:  43.85197967040953
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04102951004512054, 0.17180814872603528, 0.1917691217268955, 0.1445806811678444, 0.2861460028449978, 0.16466653548910648]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.528417799200156
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04075544821621053, 0.1706576886033658, 0.19718092575949964, 0.1436126828474392, 0.28422932049159794, 0.16356393408188674]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.040556913869637286, 0.17470770550341747, 0.1962178879256227, 0.14291145088533064, 0.28284084811609744, 0.16276519369989445]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.040361572970174744, 0.17869257742986122, 0.19527034066272841, 0.14222149831755346, 0.28147470947363784, 0.16197930114604417]
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.32468474266051
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.7917,     0.0003,     0.0005,     0.0173,     0.1902],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9974,     0.0003,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9389,     0.0258,     0.0353],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0817,     0.0001,     0.0033,     0.7769,     0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1431, 0.0057, 0.1393, 0.0725, 0.6395], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04019718890728822, 0.17796293491928974, 0.1944729568619979, 0.14164088664533192, 0.28032507096408016, 0.1654009617020119]
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04019718890728822, 0.17796293491928974, 0.1944729568619979, 0.14164088664533192, 0.28032507096408016, 0.1654009617020119]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04006522778013573, 0.17737720631261913, 0.19383284818436913, 0.14446592256911917, 0.2794021859174691, 0.16485660923628764]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.1561 0.95 0.95
probs:  [0.04006522778013573, 0.17737720631261913, 0.19383284818436913, 0.14446592256911917, 0.2794021859174691, 0.16485660923628764]
maxi score, test score, baseline:  0.1561 0.95 0.95
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 0.95 0.95
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.04009809779570075, 0.17752310464520832, 0.19399229197399853, 0.14458472998762797, 0.2796320660837076, 0.16416970951375687]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.04009809779570075, 0.17752310464520832, 0.19399229197399853, 0.14458472998762797, 0.2796320660837076, 0.16416970951375687]
maxi score, test score, baseline:  0.1581 0.95 0.95
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[5.58 ]
 [6.246]
 [4.232]
 [7.464]
 [5.172]] [[1.687]
 [1.709]
 [1.642]
 [1.75 ]
 [1.673]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.039966787556508114, 0.1769402650991728, 0.1966383556791179, 0.14411011413259764, 0.2787137330955559, 0.16363074443704775]
siam score:  -0.925038
printing an ep nov before normalisation:  35.038365309327745
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.039836337368241284, 0.17636124301394032, 0.19926708829449652, 0.14363860689886013, 0.27780141497068916, 0.1630953094537727]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.039836337368241284, 0.17636124301394032, 0.19926708829449652, 0.14363860689886013, 0.27780141497068916, 0.1630953094537727]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.92 ]
 [0.2  ]
 [0.702]
 [0.702]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.536]
 [0.92 ]
 [0.2  ]
 [0.702]
 [0.702]]
maxi score, test score, baseline:  0.1581 0.95 0.95
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03970673880878727, 0.1757860010067407, 0.1986170772199529, 0.14643176016118223, 0.27689505280810905, 0.1625633699952279]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.92574126
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03933759295645037, 0.18346814148027565, 0.1967655991390876, 0.14506707216651027, 0.2743133895979536, 0.16104820465972255]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03921122475932744, 0.18287719447903994, 0.19613179018360175, 0.14782064696136699, 0.2734296193391774, 0.1605295242774864]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03921122475932744, 0.18287719447903994, 0.19613179018360175, 0.14782064696136699, 0.2734296193391774, 0.1605295242774864]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03949394323899215, 0.18224004815735334, 0.1954097773060526, 0.14740796078275617, 0.27541280484488, 0.1600354656699657]
maxi score, test score, baseline:  0.1581 0.95 0.95
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.039773064370700925, 0.18161100897114887, 0.1946969514174554, 0.14700052567858105, 0.2773707560315044, 0.15954769353060924]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03979438953216578, 0.1811710961709887, 0.1948016214436952, 0.14707953631665782, 0.27751990233056, 0.15963345420593253]
siam score:  -0.93071055
maxi score, test score, baseline:  0.1581 0.95 0.95
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.0396153167287233, 0.184865155816199, 0.19392268061757106, 0.1464160638976798, 0.2762674829320494, 0.15891330000777734]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.0396153167287233, 0.184865155816199, 0.19392268061757106, 0.1464160638976798, 0.2762674829320494, 0.15891330000777734]
maxi score, test score, baseline:  0.1581 0.95 0.95
maxi score, test score, baseline:  0.1581 0.95 0.95
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  76.65826056949417
siam score:  -0.9390531
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03991234652907148, 0.1837845402337275, 0.19333111319734414, 0.14609806895327218, 0.2783505928366737, 0.15852333824991097]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03991234652907148, 0.1837845402337275, 0.19333111319734414, 0.14609806895327218, 0.2783505928366737, 0.15852333824991097]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.03991234652907148, 0.1837845402337275, 0.19333111319734414, 0.14609806895327218, 0.2783505928366737, 0.15852333824991097]
Printing some Q and Qe and total Qs values:  [[1.294]
 [1.294]
 [1.294]
 [1.294]
 [1.294]] [[40.794]
 [40.794]
 [40.794]
 [40.794]
 [40.794]] [[2.961]
 [2.961]
 [2.961]
 [2.961]
 [2.961]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93719894
printing an ep nov before normalisation:  50.25301607909956
printing an ep nov before normalisation:  54.68652941421588
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
printing an ep nov before normalisation:  44.65791212159152
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.04002354899802677, 0.186819813164422, 0.19186947402103263, 0.14512066062194223, 0.2791339256993348, 0.15703257749524147]
maxi score, test score, baseline:  0.1581 0.95 0.95
probs:  [0.04002354899802677, 0.186819813164422, 0.19186947402103263, 0.14512066062194223, 0.2791339256993348, 0.15703257749524147]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 0.95 0.95
probs:  [0.04002354899802677, 0.186819813164422, 0.19186947402103263, 0.14512066062194223, 0.2791339256993348, 0.15703257749524147]
maxi score, test score, baseline:  0.1601 0.95 0.95
probs:  [0.040045297161582935, 0.18637683132013022, 0.1919740084301439, 0.14519970762502082, 0.279286036599707, 0.15711811886341517]
maxi score, test score, baseline:  0.1601 0.95 0.95
probs:  [0.040045297161582935, 0.18637683132013022, 0.1919740084301439, 0.14519970762502082, 0.279286036599707, 0.15711811886341517]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93537056
from probs:  [0.04039391078845223, 0.1845006427481577, 0.19168123861401157, 0.14510440056522927, 0.2817297921749909, 0.1565900151091584]
maxi score, test score, baseline:  0.1601 0.95 0.95
probs:  [0.040415237319387426, 0.18406901655094582, 0.19178270372068604, 0.14518119340412478, 0.28187895699937127, 0.15667289200548473]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.839]
 [1.051]
 [0.432]
 [0.974]] [[30.555]
 [45.101]
 [26.633]
 [28.538]
 [30.555]] [[1.453]
 [1.753]
 [1.413]
 [0.851]
 [1.453]]
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04043058807609924, 0.18413911120411788, 0.19185573792263996, 0.14523646860057735, 0.2819863252786277, 0.15635176891793778]
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04043058807609924, 0.18413911120411788, 0.19185573792263996, 0.14523646860057735, 0.2819863252786277, 0.15635176891793778]
maxi score, test score, baseline:  0.1621 0.95 0.95
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9357208
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04025792146133533, 0.1876320674201116, 0.19103424301850522, 0.1446147284980499, 0.2807786377580522, 0.1556824018439458]
maxi score, test score, baseline:  0.1621 0.95 0.95
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04025792146133533, 0.1876320674201116, 0.19103424301850522, 0.1446147284980499, 0.2807786377580522, 0.1556824018439458]
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04025792146133533, 0.1876320674201116, 0.19103424301850522, 0.1446147284980499, 0.2807786377580522, 0.1556824018439458]
Printing some Q and Qe and total Qs values:  [[0.999]
 [1.454]
 [1.416]
 [1.253]
 [1.254]] [[22.169]
 [19.426]
 [18.685]
 [19.488]
 [16.837]] [[2.182]
 [2.492]
 [2.414]
 [2.294]
 [2.153]]
printing an ep nov before normalisation:  29.87464366142963
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04025792146133533, 0.1876320674201116, 0.19103424301850522, 0.1446147284980499, 0.2807786377580522, 0.1556824018439458]
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1621 0.95 0.95
probs:  [0.04052319718718919, 0.18699045533371075, 0.19037169515965074, 0.14423782973792654, 0.2826394260030994, 0.15523739657842342]
printing an ep nov before normalisation:  61.58599707798913
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  63.653945972472115
printing an ep nov before normalisation:  38.90574656916068
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.0405383530560249, 0.1870605713456036, 0.19044307994264575, 0.14429190332198572, 0.28274543318396594, 0.1549206591497741]
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.0405383530560249, 0.1870605713456036, 0.19044307994264575, 0.14429190332198572, 0.28274543318396594, 0.1549206591497741]
printing an ep nov before normalisation:  0.007314235659663382
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.0405383530560249, 0.1870605713456036, 0.19044307994264575, 0.14429190332198572, 0.28274543318396594, 0.1549206591497741]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.179158212745314
maxi score, test score, baseline:  0.1641 0.95 0.95
probs:  [0.040429313081407174, 0.1865561167188206, 0.19299793905010657, 0.1439028670814894, 0.2819827569932251, 0.1541310070749513]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.040429313081407174, 0.1865561167188206, 0.19299793905010657, 0.1439028670814894, 0.2819827569932251, 0.1541310070749513]
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.040429313081407174, 0.1865561167188206, 0.19299793905010657, 0.1439028670814894, 0.2819827569932251, 0.1541310070749513]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.284]
 [1.284]
 [1.398]
 [1.389]
 [1.284]] [[50.64 ]
 [50.64 ]
 [47.822]
 [46.869]
 [50.64 ]] [[2.238]
 [2.238]
 [2.258]
 [2.217]
 [2.238]]
printing an ep nov before normalisation:  44.573630252076484
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.04025988870659575, 0.18997343027496605, 0.1921870418568373, 0.1432983894845699, 0.280797724281572, 0.15348352539545898]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.040137556059402134, 0.18939467157664758, 0.19464776006490322, 0.1428619272300526, 0.27994207296936324, 0.15301601209963114]
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.040137556059402134, 0.18939467157664758, 0.19464776006490322, 0.1428619272300526, 0.27994207296936324, 0.15301601209963114]
maxi score, test score, baseline:  0.1661 0.95 0.95
printing an ep nov before normalisation:  33.470001762483726
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.040137556059402134, 0.18939467157664758, 0.19464776006490322, 0.1428619272300526, 0.27994207296936324, 0.15301601209963114]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.94485277
maxi score, test score, baseline:  0.1661 0.95 0.95
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.977]
 [1.155]
 [0.977]
 [0.977]] [[41.053]
 [41.053]
 [49.167]
 [41.053]
 [41.053]] [[1.377]
 [1.377]
 [1.768]
 [1.377]
 [1.377]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1661 0.95 0.95
printing an ep nov before normalisation:  83.60417025453755
from probs:  [0.040172073030103464, 0.1871873007841423, 0.1959190028316672, 0.14475494445106415, 0.28018921663501317, 0.1517774622680097]
maxi score, test score, baseline:  0.1661 0.95 0.95
probs:  [0.04019327216584054, 0.18675729825210402, 0.19602266439108543, 0.14483151647321316, 0.28033749625581617, 0.1518577524619408]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.832]
 [0.834]
 [0.608]
 [0.69 ]
 [0.621]] [[81.444]
 [87.384]
 [79.799]
 [66.956]
 [82.73 ]] [[1.532]
 [1.608]
 [1.287]
 [1.21 ]
 [1.337]]
maxi score, test score, baseline:  0.1681 0.95 0.95
probs:  [0.04019327216584054, 0.18675729825210402, 0.19602266439108543, 0.14483151647321316, 0.28033749625581617, 0.1518577524619408]
maxi score, test score, baseline:  0.1681 0.95 0.95
probs:  [0.04019327216584054, 0.18675729825210402, 0.19602266439108543, 0.14483151647321316, 0.28033749625581617, 0.1518577524619408]
actions average: 
K:  2  action  0 :  tensor([0.7613, 0.0080, 0.0008, 0.0698, 0.1600], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0107,     0.9767,     0.0002,     0.0000,     0.0124],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9887,     0.0007,     0.0104],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1063,     0.0006,     0.0069,     0.7233,     0.1629],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2278, 0.0087, 0.0692, 0.1221, 0.5721], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.451]
 [1.493]
 [1.411]
 [1.451]
 [1.451]] [[22.591]
 [31.407]
 [37.55 ]
 [22.591]
 [22.591]] [[1.897]
 [2.113]
 [2.152]
 [1.897]
 [1.897]]
actions average: 
K:  4  action  0 :  tensor([    0.8092,     0.0063,     0.0007,     0.0635,     0.1202],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9989,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0006,     0.9674,     0.0006,     0.0308],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0336,     0.0006,     0.0072,     0.7981,     0.1605],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2417, 0.0613, 0.1093, 0.1203, 0.4674], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1681 0.95 0.95
probs:  [0.04019327216584054, 0.18675729825210402, 0.19602266439108543, 0.14483151647321316, 0.28033749625581617, 0.1518577524619408]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.55493448889716
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.845]
 [0.87 ]
 [0.845]
 [0.845]] [[60.134]
 [60.134]
 [64.45 ]
 [60.134]
 [60.134]] [[0.845]
 [0.845]
 [0.87 ]
 [0.845]
 [0.845]]
actions average: 
K:  1  action  0 :  tensor([0.6601, 0.0012, 0.0009, 0.0864, 0.2514], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0020,     0.9746,     0.0001,     0.0000,     0.0234],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9706,     0.0063,     0.0230],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1072,     0.0005,     0.0016,     0.7738,     0.1169],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3297, 0.0030, 0.0008, 0.0833, 0.5833], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  0.0022426033444844506
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.798]
 [1.342]
 [1.252]
 [1.09 ]
 [1.263]] [[23.36 ]
 [14.418]
 [12.068]
 [30.406]
 [12.496]] [[1.241]
 [1.435]
 [1.252]
 [1.809]
 [1.279]]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
Printing some Q and Qe and total Qs values:  [[1.28 ]
 [1.313]
 [1.191]
 [1.166]
 [1.28 ]] [[24.789]
 [15.726]
 [23.702]
 [41.567]
 [24.789]] [[1.997]
 [1.517]
 [1.846]
 [2.833]
 [1.997]]
printing an ep nov before normalisation:  32.01658619277647
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
from probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
printing an ep nov before normalisation:  78.20543539828465
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
printing an ep nov before normalisation:  24.38040256500244
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03986875801339044, 0.1933397320346168, 0.19443582408242902, 0.1436593599675299, 0.2780676473304981, 0.15062867857153567]
printing an ep nov before normalisation:  43.13258647918701
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.9  ]
 [0.594]
 [0.594]
 [0.594]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.594]
 [0.9  ]
 [0.594]
 [0.594]
 [0.594]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.89753366109332
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03978606726389619, 0.19239576056495952, 0.19701211006514086, 0.1433606780787811, 0.2774892580446806, 0.14995612598254165]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.040044426069412394, 0.19174940326583656, 0.19633838565946574, 0.14300501547838892, 0.27930140594114095, 0.1495613635857554]
UNIT TEST: sample policy line 217 mcts : [0.    0.875 0.    0.042 0.083]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.87142289711701
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.039940540550449474, 0.19125064235964354, 0.1958276800153138, 0.14558838480268063, 0.27857475448318036, 0.14881799778873228]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  74.9123880880826
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.040153544384930254, 0.19045879452128409, 0.19515376628282477, 0.14361998175063753, 0.2800329408064274, 0.1505809722538959]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.864]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.53 ]
 [0.864]
 [0.53 ]
 [0.53 ]
 [0.53 ]]
printing an ep nov before normalisation:  40.351901054382324
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.040153544384930254, 0.19045879452128409, 0.19515376628282477, 0.14361998175063753, 0.2800329408064274, 0.1505809722538959]
printing an ep nov before normalisation:  33.99782915161481
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.040153544384930254, 0.19045879452128409, 0.19515376628282477, 0.14361998175063753, 0.2800329408064274, 0.1505809722538959]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.040032497012875194, 0.18988312668114743, 0.19758612798098832, 0.14318598482995884, 0.27918634270753256, 0.1501259207874976]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03991217968406999, 0.1893109307249, 0.20000381997415237, 0.14275460536818926, 0.27834485048757557, 0.14967361376111268]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03991217968406999, 0.1893109307249, 0.20000381997415237, 0.14275460536818926, 0.27834485048757557, 0.14967361376111268]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03992683798456191, 0.18938064155001744, 0.20007747106345955, 0.14280716047259073, 0.2784473697667537, 0.14936051916261658]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.039948541908639716, 0.18893888311818086, 0.20018652310884844, 0.14288497658752566, 0.27859916571697446, 0.14944190955983092]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
probs:  [0.03978868875174995, 0.19219251041887742, 0.19938333592213425, 0.1423118474342603, 0.27748116227396175, 0.14884245519901637]
maxi score, test score, baseline:  0.17209999999999998 0.95 0.95
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
probs:  [0.03978868875174995, 0.19219251041887742, 0.19938333592213425, 0.1423118474342603, 0.27748116227396175, 0.14884245519901637]
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
probs:  [0.03978868875174995, 0.19219251041887742, 0.19938333592213425, 0.1423118474342603, 0.27748116227396175, 0.14884245519901637]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.617]
 [0.82 ]
 [0.048]
 [0.354]
 [0.434]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.617]
 [0.82 ]
 [0.048]
 [0.354]
 [0.434]]
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
probs:  [0.039631068387031176, 0.19540069180092168, 0.19859136746952089, 0.1417467236173378, 0.2763787748461925, 0.1482513738789958]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
siam score:  -0.9368808
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  70.91615041134396
maxi score, test score, baseline:  0.17409999999999998 0.95 0.95
probs:  [0.03984536072165076, 0.198218864125068, 0.19802526124515604, 0.13969547928309983, 0.27784496287744354, 0.14637007174758176]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.48052647095806
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039868411621355966, 0.19775385709270438, 0.19814012763443242, 0.13977648816285887, 0.27800616059553307, 0.14645495489311533]
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.039868411621355966, 0.19775385709270438, 0.19814012763443242, 0.13977648816285887, 0.27800616059553307, 0.14645495489311533]
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039746630988248356, 0.19714818819857163, 0.1975332748960247, 0.14241086477488615, 0.27715453395989154, 0.14600650718237762]
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039746630988248356, 0.19714818819857163, 0.1975332748960247, 0.14241086477488615, 0.27715453395989154, 0.14600650718237762]
Printing some Q and Qe and total Qs values:  [[1.17]
 [1.33]
 [1.17]
 [1.17]
 [1.17]] [[36.993]
 [41.615]
 [36.993]
 [36.993]
 [36.993]] [[2.511]
 [2.842]
 [2.511]
 [2.511]
 [2.511]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
UNIT TEST: sample policy line 217 mcts : [0.    0.375 0.5   0.083 0.042]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.7864,     0.0023,     0.0002,     0.0736,     0.1376],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0041,     0.9166,     0.0426,     0.0000,     0.0367],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0000,     0.9305,     0.0427,     0.0265],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0019,     0.0002,     0.0742,     0.9164,     0.0072],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1723, 0.0380, 0.0622, 0.1033, 0.6242], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039905838718033965, 0.19593131971489616, 0.19934862341251267, 0.14167253494156984, 0.2782737971095924, 0.14486788610339493]
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039905838718033965, 0.19593131971489616, 0.19934862341251267, 0.14167253494156984, 0.2782737971095924, 0.14486788610339493]
Printing some Q and Qe and total Qs values:  [[1.034]
 [1.034]
 [1.191]
 [1.034]
 [1.034]] [[61.033]
 [61.033]
 [60.271]
 [61.033]
 [61.033]] [[1.877]
 [1.877]
 [2.018]
 [1.877]
 [1.877]]
printing an ep nov before normalisation:  56.830256417796946
maxi score, test score, baseline:  0.17609999999999998 0.95 0.95
probs:  [0.039905838718033965, 0.19593131971489616, 0.19934862341251267, 0.14167253494156984, 0.2782737971095924, 0.14486788610339493]
line 256 mcts: sample exp_bonus 16.765952476982626
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.447]
 [0.344]
 [0.344]
 [0.344]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.344]
 [0.447]
 [0.344]
 [0.344]
 [0.344]]
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17809999999999998 0.95 0.95
probs:  [0.039785375012990995, 0.1953382843935355, 0.20177162712202237, 0.1412438391976676, 0.2774313620274658, 0.1444295122463178]
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.216383934020996
maxi score, test score, baseline:  0.17809999999999998 0.95 0.95
maxi score, test score, baseline:  0.17809999999999998 0.95 0.95
probs:  [0.039785375012990995, 0.1953382843935355, 0.20177162712202237, 0.1412438391976676, 0.2774313620274658, 0.1444295122463178]
maxi score, test score, baseline:  0.17809999999999998 0.95 0.95
probs:  [0.039785375012990995, 0.1953382843935355, 0.20177162712202237, 0.1412438391976676, 0.2774313620274658, 0.1444295122463178]
actor:  0 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.039785375012990995, 0.1953382843935355, 0.20177162712202237, 0.1412438391976676, 0.2774313620274658, 0.1444295122463178]
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.039785375012990995, 0.1953382843935355, 0.20177162712202237, 0.1412438391976676, 0.2774313620274658, 0.1444295122463178]
printing an ep nov before normalisation:  47.05362928826954
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.03978537501299098, 0.19533828439353548, 0.20177162712202235, 0.1412438391976676, 0.27743136202746577, 0.14442951224631778]
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.03980792850503479, 0.1948809848838328, 0.20188631573935858, 0.14132410060126516, 0.2775890846619754, 0.14451158560853322]
siam score:  -0.929924
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.03980792850503479, 0.1948809848838328, 0.20188631573935858, 0.14132410060126516, 0.2775890846619754, 0.14451158560853322]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.03965205344458755, 0.19804154150978412, 0.2010936626493105, 0.1407693860092609, 0.27649900844937253, 0.1439443479376845]
printing an ep nov before normalisation:  36.60794951335987
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
actions average: 
K:  3  action  0 :  tensor([    0.6427,     0.0092,     0.0002,     0.0818,     0.2661],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9836,     0.0013,     0.0000,     0.0137],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0009,     0.0181,     0.9229,     0.0072,     0.0509],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0277,     0.0003,     0.0009,     0.8310,     0.1401],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2176, 0.0107, 0.0326, 0.1199, 0.6192], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18009999999999998 0.95 0.95
probs:  [0.039533119871179766, 0.1974459281009063, 0.20048886375331232, 0.14335327197401934, 0.2756672739892242, 0.1435115423113581]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03980962171074409, 0.19683478147320926, 0.19986061238789252, 0.14304618721337028, 0.27760666788987026, 0.14284212932491364]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03966898453233292, 0.19613755200783545, 0.19915265754480652, 0.14253961710944452, 0.2766231339300388, 0.14587805487554192]
printing an ep nov before normalisation:  83.46770401764809
Printing some Q and Qe and total Qs values:  [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]] [[57.149]
 [57.149]
 [57.149]
 [57.149]
 [57.149]] [[2.415]
 [2.415]
 [2.415]
 [2.415]
 [2.415]]
printing an ep nov before normalisation:  25.001705516576042
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03955143873775888, 0.1955548014689404, 0.2015317174455111, 0.1421162213470572, 0.2758010875685785, 0.14544473343215383]
siam score:  -0.93003523
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03955143873775888, 0.1955548014689404, 0.2015317174455111, 0.1421162213470572, 0.2758010875685785, 0.14544473343215383]
printing an ep nov before normalisation:  82.86289961057135
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03955143873775888, 0.1955548014689404, 0.2015317174455111, 0.1421162213470572, 0.2758010875685785, 0.14544473343215383]
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03955143873775888, 0.1955548014689404, 0.2015317174455111, 0.1421162213470572, 0.2758010875685785, 0.14544473343215383]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.039810334285050375, 0.1948919036085884, 0.2008335031557643, 0.14176908162170807, 0.2776172511500374, 0.14507792617885154]
Printing some Q and Qe and total Qs values:  [[1.21 ]
 [1.21 ]
 [1.318]
 [1.206]
 [1.21 ]] [[56.93 ]
 [56.93 ]
 [67.692]
 [50.268]
 [56.93 ]] [[2.212]
 [2.212]
 [2.646]
 [2.006]
 [2.212]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.039810334285050375, 0.1948919036085884, 0.2008335031557643, 0.14176908162170807, 0.2776172511500374, 0.14507792617885154]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
siam score:  -0.9327177
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.04006618827782463, 0.19423679359937057, 0.20014349162133405, 0.14142602016039554, 0.279412078093601, 0.1447154282474743]
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.04005806482819631, 0.19423693310872306, 0.20013973229011056, 0.14146101853430584, 0.27935599586044685, 0.14474825537821745]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.37820674517357
actions average: 
K:  3  action  0 :  tensor([    0.7938,     0.0029,     0.0005,     0.0383,     0.1645],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0021,     0.9959,     0.0001,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0039,     0.9675,     0.0006,     0.0279],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0017,     0.0000,     0.0040,     0.9489,     0.0453],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1335, 0.0122, 0.0022, 0.0372, 0.8149], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  13.62674355506897
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.0399411774258825, 0.19366865834271926, 0.1995541759796864, 0.1439726011359025, 0.27853851917874767, 0.14432486793706176]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03983923250954194, 0.1931730298803536, 0.20196132965814634, 0.1436042402245821, 0.2778255459217799, 0.14359662180559601]
actions average: 
K:  0  action  0 :  tensor([    0.7013,     0.0012,     0.0002,     0.1243,     0.1729],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9978,     0.0001,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9800,     0.0006,     0.0194],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1224,     0.0004,     0.0002,     0.7720,     0.1051],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3330, 0.0023, 0.0483, 0.2135, 0.4029], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.68985088066681
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03983923250954194, 0.1931730298803536, 0.20196132965814634, 0.1436042402245821, 0.2778255459217799, 0.14359662180559601]
from probs:  [0.03983923250954194, 0.1931730298803536, 0.20196132965814634, 0.1436042402245821, 0.2778255459217799, 0.14359662180559601]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.73 ]
 [0.807]
 [0.349]
 [0.73 ]] [[35.343]
 [53.403]
 [53.138]
 [35.853]
 [53.403]] [[0.525]
 [0.73 ]
 [0.807]
 [0.349]
 [0.73 ]]
Printing some Q and Qe and total Qs values:  [[0.865]
 [0.857]
 [0.884]
 [0.794]
 [0.865]] [[28.331]
 [33.843]
 [38.698]
 [39.92 ]
 [28.331]] [[0.865]
 [0.857]
 [0.884]
 [0.794]
 [0.865]]
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03983923250954194, 0.1931730298803536, 0.20196132965814634, 0.1436042402245821, 0.2778255459217799, 0.14359662180559601]
maxi score, test score, baseline:  0.18409999999999999 0.95 0.95
probs:  [0.03983923250954194, 0.1931730298803536, 0.20196132965814634, 0.1436042402245821, 0.2778255459217799, 0.14359662180559601]
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.401163473255295
printing an ep nov before normalisation:  80.28836158057582
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.039853428473121746, 0.1932420467951094, 0.20203348862613044, 0.1436555349655956, 0.27792482838482574, 0.14329067275521726]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.03985342847312174, 0.1932420467951094, 0.20203348862613044, 0.1436555349655956, 0.2779248283848257, 0.14329067275521726]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.34579979455546
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1861 0.95 0.95
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.03983536122662914, 0.19130058405753098, 0.1999817868123877, 0.14233585547578953, 0.2778037941167953, 0.1487426183108674]
using another actor
from probs:  [0.03983536122662914, 0.19130058405753098, 0.1999817868123877, 0.14233585547578953, 0.2778037941167953, 0.1487426183108674]
maxi score, test score, baseline:  0.1861 0.95 0.95
probs:  [0.03983536122662914, 0.19130058405753098, 0.1999817868123877, 0.14233585547578953, 0.2778037941167953, 0.1487426183108674]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  84.241120737692
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.20152493311354
siam score:  -0.92806
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.03995924088093075, 0.19322640317426787, 0.1986863490336115, 0.14155131186772477, 0.2786754010658529, 0.14790129397761215]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.04020593842555943, 0.19260203675840853, 0.1980309520826191, 0.14122063073329516, 0.2804059180391387, 0.1475345239609791]
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.04020593842555943, 0.19260203675840853, 0.1980309520826191, 0.14122063073329516, 0.2804059180391387, 0.1475345239609791]
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.04020593842555943, 0.19260203675840853, 0.1980309520826191, 0.14122063073329516, 0.2804059180391387, 0.1475345239609791]
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.04020593842555943, 0.19260203675840853, 0.1980309520826191, 0.14122063073329516, 0.2804059180391387, 0.1475345239609791]
from probs:  [0.04022782292779276, 0.1921614645139397, 0.19813902795348215, 0.1412976813614801, 0.2805589805118851, 0.1476150227314202]
maxi score, test score, baseline:  0.1901 0.95 0.95
probs:  [0.040227822927792777, 0.1921614645139397, 0.19813902795348212, 0.1412976813614801, 0.28055898051188516, 0.14761502273142021]
maxi score, test score, baseline:  0.1901 0.95 0.95
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.04011410368098864, 0.19161681988233797, 0.19757742931618127, 0.14373130678689003, 0.2797636163345733, 0.14719672399902875]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.312]
 [0.312]
 [0.312]
 [0.312]] [[54.131]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.202]
 [-0.045]
 [-0.045]
 [-0.045]
 [-0.045]]
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.264]
 [1.195]
 [1.101]
 [1.264]] [[56.161]
 [56.161]
 [66.477]
 [56.564]
 [56.161]] [[1.826]
 [1.826]
 [1.928]
 [1.67 ]
 [1.826]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1921 0.95 0.95
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.040001027687549276, 0.19107525603522726, 0.1998449985283943, 0.14332517523593005, 0.27897275113784437, 0.1467807913750548]
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.040022561402227604, 0.19063869294697022, 0.19995286844372487, 0.14340251721342887, 0.27912336016613926, 0.14685999982750914]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.03995251928456502, 0.18923613692710617, 0.20242455564865833, 0.1431509491041106, 0.2786334783487911, 0.14660236068676868]
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.03995251928456502, 0.18923613692710617, 0.20242455564865833, 0.1431509491041106, 0.2786334783487911, 0.14660236068676868]
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.03995251928456502, 0.18923613692710617, 0.20242455564865833, 0.1431509491041106, 0.2786334783487911, 0.14660236068676868]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.039840354164803006, 0.18870346170316132, 0.20466933255394204, 0.14274808910998646, 0.27784898389679863, 0.14618977857130852]
maxi score, test score, baseline:  0.1921 0.95 0.95
probs:  [0.039840354164803006, 0.18870346170316132, 0.20466933255394204, 0.14274808910998646, 0.27784898389679863, 0.14618977857130852]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9150285
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1921 0.95 0.95
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.92501235
from probs:  [0.0403284859014566, 0.18696350195168818, 0.20377585225903055, 0.1435274148143605, 0.28123812897360634, 0.14416661609985784]
siam score:  -0.92541456
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.859]
 [0.837]
 [0.837]
 [0.837]] [[41.688]
 [40.833]
 [41.688]
 [41.688]
 [41.688]] [[0.837]
 [0.859]
 [0.837]
 [0.837]
 [0.837]]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.276]
 [1.276]
 [1.337]
 [1.276]
 [1.276]] [[54.034]
 [54.034]
 [73.38 ]
 [54.034]
 [54.034]] [[1.688]
 [1.688]
 [1.995]
 [1.688]
 [1.688]]
siam score:  -0.9282917
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.54312924632558
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040471751223827394, 0.18843019711701847, 0.20257484774966816, 0.14282192246081196, 0.2822454148014765, 0.1434558666471975]
printing an ep nov before normalisation:  28.579758141832873
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040471751223827394, 0.18843019711701847, 0.20257484774966816, 0.14282192246081196, 0.2822454148014765, 0.1434558666471975]
printing an ep nov before normalisation:  0.003335833540063504
printing an ep nov before normalisation:  10.462591648101807
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040471751223827394, 0.18843019711701847, 0.20257484774966816, 0.14282192246081196, 0.2822454148014765, 0.1434558666471975]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04071457085551396, 0.1878354529925151, 0.2019000335308734, 0.1424853578380179, 0.28394887139243574, 0.1431157133906438]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040599987858225356, 0.18730546848472027, 0.20415161885190083, 0.14208342108085603, 0.2831475069241397, 0.14271199680015773]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
printing an ep nov before normalisation:  40.34830678167648
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04032554607951185, 0.18954862975750472, 0.20276798525021203, 0.14112072817548546, 0.28122813061804586, 0.1450089801192402]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04032554607951185, 0.18954862975750472, 0.20276798525021203, 0.14112072817548546, 0.28122813061804586, 0.1450089801192402]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04019535568119301, 0.18893507803474813, 0.2021116135761337, 0.1406640434338384, 0.28031761193905497, 0.14777629733503186]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([    0.7697,     0.0006,     0.0002,     0.1020,     0.1276],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0022,     0.9968,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0024,     0.9190,     0.0045,     0.0738],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0252, 0.0153, 0.0119, 0.8369, 0.1107], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1703, 0.0206, 0.2658, 0.1294, 0.4139], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1941 0.95 0.95
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93118393
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04068739279991084, 0.18783696560184854, 0.2008726331776936, 0.1400819879278456, 0.28376896760930465, 0.14675205288339682]
using another actor
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04068739279991084, 0.18783696560184854, 0.2008726331776936, 0.1400819879278456, 0.28376896760930465, 0.14675205288339682]
printing an ep nov before normalisation:  52.88968483606975
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04115414769192186, 0.1866944427768693, 0.199587547863701, 0.1394617297617627, 0.2870432832847022, 0.14605884862104287]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04115414769192186, 0.1866944427768693, 0.199587547863701, 0.1394617297617627, 0.2870432832847022, 0.14605884862104287]
printing an ep nov before normalisation:  79.55108922819794
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04090365129536297, 0.1889405652915391, 0.1983695696327956, 0.14132721555506178, 0.2852912520369614, 0.14516774618827927]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1941 0.95 0.95
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04079311837139785, 0.18842868866948867, 0.19783212998481176, 0.1436533621415594, 0.2845181585340154, 0.14477454229872697]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04104503852169526, 0.18731189479980018, 0.20025264683198235, 0.14199541609250865, 0.2862514160188246, 0.14314358773518895]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04104503852169526, 0.18731189479980018, 0.20025264683198235, 0.14199541609250865, 0.2862514160188246, 0.14314358773518895]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04090510491360735, 0.190089311868245, 0.19956816400687902, 0.14151019465774564, 0.2852727854270284, 0.14265443912649464]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04090510491360735, 0.190089311868245, 0.19956816400687902, 0.14151019465774564, 0.2852727854270284, 0.14265443912649464]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04090510491360735, 0.190089311868245, 0.19956816400687902, 0.14151019465774564, 0.2852727854270284, 0.14265443912649464]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([0.5863, 0.0408, 0.0020, 0.0779, 0.2931], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0015,     0.9966,     0.0000,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0004,     0.9687,     0.0003,     0.0305],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0040, 0.0019, 0.0243, 0.8858, 0.0839], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0921, 0.1550, 0.0893, 0.0940, 0.5695], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.27227647412092
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04067728396946554, 0.1918619089948239, 0.19845378179666753, 0.1434694399424265, 0.2836795116707411, 0.1418580736258755]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04069199546694083, 0.19193147815534525, 0.19852574283226845, 0.14352144979252995, 0.28378239704386327, 0.1415469367090521]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04069199546694083, 0.1919314781553453, 0.1985257428322685, 0.14352144979252998, 0.28378239704386327, 0.14154693670905213]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04069199546694083, 0.1919314781553453, 0.1985257428322685, 0.14352144979252998, 0.28378239704386327, 0.14154693670905213]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04055633949882768, 0.19463197809857963, 0.19786218400915956, 0.1430418625459527, 0.2828336822771301, 0.14107395357035035]
actions average: 
K:  1  action  0 :  tensor([    0.6561,     0.0014,     0.0003,     0.1174,     0.2248],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9979,     0.0000,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0045,     0.9765,     0.0015,     0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1300,     0.0003,     0.0037,     0.7716,     0.0943],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2778, 0.0149, 0.0020, 0.1323, 0.5731], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040789840319469854, 0.19402510227100087, 0.19723768962980404, 0.1427163753193459, 0.28447179252653715, 0.14075919993384223]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040655806200066154, 0.1966799926210644, 0.19658788826185467, 0.14224631632812434, 0.2835344033558232, 0.1402955932330673]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040655806200066154, 0.1966799926210644, 0.19658788826185467, 0.14224631632812434, 0.2835344033558232, 0.1402955932330673]
printing an ep nov before normalisation:  42.57036716541474
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040655806200066154, 0.1966799926210644, 0.19658788826185467, 0.14224631632812434, 0.2835344033558232, 0.1402955932330673]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040655806200066154, 0.1966799926210644, 0.19658788826185467, 0.14224631632812434, 0.2835344033558232, 0.1402955932330673]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040655806200066154, 0.19667999262106434, 0.1965878882618546, 0.14224631632812432, 0.2835344033558232, 0.14029559323306728]
printing an ep nov before normalisation:  46.92675991493534
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1941 0.95 0.95
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.137]
 [0.05 ]
 [0.135]
 [0.208]] [[34.49 ]
 [35.039]
 [25.857]
 [28.833]
 [44.163]] [[1.126]
 [1.136]
 [0.313]
 [0.636]
 [1.938]]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.040544394213169625, 0.19613961078945877, 0.19604775966017177, 0.1418555933808363, 0.2827552257071085, 0.14265741624925501]
maxi score, test score, baseline:  0.1941 0.95 0.95
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.33 ]
 [1.127]
 [0.693]
 [1.127]] [[55.372]
 [49.999]
 [55.372]
 [61.619]
 [55.372]] [[2.524]
 [2.494]
 [2.524]
 [2.36 ]
 [2.524]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.8223,     0.0006,     0.0005,     0.0179,     0.1587],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9989,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0004,     0.9560,     0.0003,     0.0429],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1024,     0.0005,     0.0011,     0.7556,     0.1404],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3107, 0.0013, 0.0825, 0.1314, 0.4742], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.7316,     0.0020,     0.0004,     0.0642,     0.2018],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0079,     0.9884,     0.0002,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.8935,     0.0163,     0.0899],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0202,     0.0003,     0.0470,     0.8380,     0.0945],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1960, 0.0060, 0.0776, 0.0482, 0.6722], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1941 0.95 0.95
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04125674799124086, 0.19395922410040276, 0.19716698919563713, 0.13954316079811324, 0.28771871953460343, 0.1403551583800025]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1941 0.95 0.95
line 256 mcts: sample exp_bonus 18.5856602744143
printing an ep nov before normalisation:  52.39419851214952
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04112136828279548, 0.19661053850530816, 0.19651834889175118, 0.13908421964578013, 0.28677198056399156, 0.1398935441103735]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04100948397829913, 0.1960742193028802, 0.1987097743246361, 0.13870492859675082, 0.28598954992883296, 0.13951204386860083]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1941 0.95 0.95
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9160338
printing an ep nov before normalisation:  27.078739100528203
Printing some Q and Qe and total Qs values:  [[0.551]
 [0.527]
 [0.089]
 [0.359]
 [0.813]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.551]
 [0.527]
 [0.089]
 [0.359]
 [0.813]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04064070559985101, 0.19753908272344534, 0.196918251759198, 0.14015764518648996, 0.28341060463189616, 0.14133371009911963]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04064070559985101, 0.19753908272344534, 0.196918251759198, 0.14015764518648996, 0.28341060463189616, 0.14133371009911963]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.631]
 [0.947]
 [0.631]
 [0.631]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.631]
 [0.631]
 [0.947]
 [0.631]
 [0.631]]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04054621260490227, 0.19707858823991556, 0.19915578643146473, 0.13983100608081983, 0.2827497951073736, 0.14063861153552404]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04054621260490227, 0.19707858823991556, 0.19915578643146473, 0.13983100608081983, 0.2827497951073736, 0.14063861153552404]
using another actor
from probs:  [0.04054621260490227, 0.19707858823991556, 0.19915578643146473, 0.13983100608081983, 0.2827497951073736, 0.14063861153552404]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04054621260490227, 0.19707858823991556, 0.19915578643146473, 0.13983100608081983, 0.2827497951073736, 0.14063861153552404]
maxi score, test score, baseline:  0.1941 0.95 0.95
probs:  [0.04054621260490227, 0.19707858823991556, 0.19915578643146473, 0.13983100608081983, 0.2827497951073736, 0.14063861153552404]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.04078893735793377, 0.19614651130215738, 0.19884524597602032, 0.13811600514360625, 0.2844182671489676, 0.1416850330713147]
from probs:  [0.04078893735793377, 0.19614651130215738, 0.19884524597602032, 0.13811600514360625, 0.2844182671489676, 0.1416850330713147]
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.04078893735793377, 0.19614651130215738, 0.19884524597602032, 0.13811600514360625, 0.2844182671489676, 0.1416850330713147]
printing an ep nov before normalisation:  56.359593407604656
actions average: 
K:  2  action  0 :  tensor([    0.6600,     0.0015,     0.0005,     0.1072,     0.2307],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0163,     0.9308,     0.0048,     0.0002,     0.0480],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0001,     0.9496,     0.0025,     0.0475],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0247,     0.0002,     0.0085,     0.8819,     0.0846],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1114, 0.0596, 0.0839, 0.1039, 0.6411], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.04078893735793377, 0.19614651130215738, 0.19884524597602032, 0.13811600514360625, 0.2844182671489676, 0.1416850330713147]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.040699974440495085, 0.1951496679992083, 0.2011648035688757, 0.13781407091741646, 0.2837961939838226, 0.14137528909018182]
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.040699974440495085, 0.1951496679992083, 0.2011648035688757, 0.13781407091741646, 0.2837961939838226, 0.14137528909018182]
printing an ep nov before normalisation:  73.71024362348098
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.040699974440495085, 0.1951496679992083, 0.2011648035688757, 0.13781407091741646, 0.2837961939838226, 0.14137528909018182]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  56.52237274687856
maxi score, test score, baseline:  0.1961 0.95 0.95
probs:  [0.0404436746907955, 0.19715771325977735, 0.1998946908347849, 0.1369442066096108, 0.282004018085012, 0.14355569652001945]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.0404436746907955, 0.19715771325977735, 0.1998946908347849, 0.1369442066096108, 0.282004018085012, 0.14355569652001945]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.04046662758419424, 0.19670095168386292, 0.2000084356286215, 0.1370221072035514, 0.2821645161830609, 0.14363736171670896]
Printing some Q and Qe and total Qs values:  [[0.576]
 [0.576]
 [0.409]
 [0.839]
 [0.576]] [[40.434]
 [40.434]
 [50.073]
 [51.015]
 [40.434]] [[0.576]
 [0.576]
 [0.409]
 [0.839]
 [0.576]]
Printing some Q and Qe and total Qs values:  [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]] [[37.878]
 [37.878]
 [37.878]
 [37.878]
 [37.878]] [[0.881]
 [0.881]
 [0.881]
 [0.881]
 [0.881]]
maxi score, test score, baseline:  0.1981 0.95 0.95
probs:  [0.04046662758419424, 0.19670095168386292, 0.2000084356286215, 0.1370221072035514, 0.2821645161830609, 0.14363736171670896]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2001 0.95 0.95
probs:  [0.04046662758419424, 0.19670095168386292, 0.2000084356286215, 0.1370221072035514, 0.2821645161830609, 0.14363736171670896]
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2021 0.95 0.95
Printing some Q and Qe and total Qs values:  [[1.28]
 [1.28]
 [1.28]
 [1.28]
 [1.28]] [[86.327]
 [86.327]
 [86.327]
 [86.327]
 [86.327]] [[2.947]
 [2.947]
 [2.947]
 [2.947]
 [2.947]]
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.04046662758419424, 0.19670095168386292, 0.2000084356286215, 0.1370221072035514, 0.2821645161830609, 0.14363736171670896]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.0407232663137963, 0.1956307246272696, 0.1994850766861369, 0.1368066875413535, 0.2839646446638886, 0.14338960016755511]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.04073868583235505, 0.1957049910564039, 0.19956080732308534, 0.13685860764263647, 0.2840724677619514, 0.1430644403835679]
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.04073868583235505, 0.1957049910564039, 0.19956080732308534, 0.13685860764263647, 0.2840724677619514, 0.1430644403835679]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [1.087]
 [0.911]
 [0.932]
 [0.932]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.77 ]
 [1.087]
 [0.911]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.2021 0.95 0.95
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.040969940657674675, 0.1950955605771823, 0.19893045921233266, 0.13656841617275842, 0.28569504083260994, 0.142740582547442]
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.040969940657674675, 0.1950955605771823, 0.19893045921233266, 0.13656841617275842, 0.28569504083260994, 0.142740582547442]
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.035]
 [1.218]
 [1.035]
 [1.035]] [[53.492]
 [53.492]
 [41.946]
 [53.492]
 [53.492]] [[4.084]
 [4.084]
 [3.218]
 [4.084]
 [4.084]]
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.040969940657674675, 0.1950955605771823, 0.19893045921233266, 0.13656841617275842, 0.28569504083260994, 0.142740582547442]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2021 0.95 0.95
probs:  [0.041200476369125424, 0.19458528242182094, 0.1985087795201957, 0.13470597318998445, 0.28727790137092446, 0.14372158712794905]
actor:  0 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
probs:  [0.041200476369125424, 0.19458528242182094, 0.1985087795201957, 0.13470597318998445, 0.28727790137092446, 0.14372158712794905]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
probs:  [0.041086778894467636, 0.19404692867576373, 0.2007259182585193, 0.1343333993855443, 0.2864829218027786, 0.14332405298292633]
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
probs:  [0.041086778894467636, 0.19404692867576373, 0.2007259182585193, 0.1343333993855443, 0.2864829218027786, 0.14332405298292633]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  69.77117986380247
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20409999999999998 0.95 0.95
probs:  [0.040861262220053714, 0.19297911475259158, 0.20237244712883212, 0.13634552829001503, 0.28490609567735353, 0.1425355519311541]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  77.74537185753192
printing an ep nov before normalisation:  55.115510785920925
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040861262220053714, 0.19297911475259158, 0.20237244712883212, 0.13634552829001503, 0.28490609567735353, 0.1425355519311541]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04110968926494091, 0.19245293321850565, 0.2017984332350156, 0.1361077340898388, 0.28664891963086897, 0.14188229056083004]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04110968926494091, 0.19245293321850565, 0.2017984332350156, 0.1361077340898388, 0.28664891963086897, 0.14188229056083004]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040997748991134404, 0.1919275506139575, 0.20397715558599086, 0.13573627636389976, 0.2858662106525001, 0.14149505779251717]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04102069941638725, 0.1914740932558367, 0.20409163887358614, 0.1358124340548171, 0.28602668465610886, 0.14157444974326386]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04102069941638725, 0.1914740932558367, 0.20409163887358614, 0.1358124340548171, 0.28602668465610886, 0.14157444974326386]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04102069941638725, 0.1914740932558367, 0.20409163887358614, 0.1358124340548171, 0.28602668465610886, 0.14157444974326386]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040890436540353674, 0.19404784337055803, 0.20344185037792042, 0.13538017540419633, 0.2851158603463892, 0.14112383396058245]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040890436540353674, 0.19404784337055803, 0.20344185037792042, 0.13538017540419633, 0.2851158603463892, 0.14112383396058245]
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040890436540353674, 0.19404784337055803, 0.20344185037792034, 0.13538017540419633, 0.2851158603463892, 0.14112383396058245]
printing an ep nov before normalisation:  37.33579023000492
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.040890436540353674, 0.19404784337055803, 0.20344185037792034, 0.13538017540419633, 0.2851158603463892, 0.14112383396058245]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.20609999999999998 0.95 0.95
probs:  [0.04100721101779775, 0.19605366968510912, 0.202230908043351, 0.13472515951765351, 0.2859380362152159, 0.14004501552087267]
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04100721101779775, 0.19605366968510912, 0.202230908043351, 0.13472515951765351, 0.2859380362152159, 0.14004501552087267]
siam score:  -0.9231455
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04100721101779775, 0.19605366968510912, 0.202230908043351, 0.13472515951765351, 0.2859380362152159, 0.14004501552087267]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04087986237093646, 0.19855640211795436, 0.20160124429247064, 0.13430581942091435, 0.2850475711332005, 0.1396091006645236]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04087986237093646, 0.19855640211795436, 0.20160124429247064, 0.13430581942091435, 0.2850475711332005, 0.1396091006645236]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.63535923428005
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.378]
 [1.378]
 [1.309]
 [1.292]
 [1.327]] [[91.069]
 [91.069]
 [91.304]
 [99.711]
 [93.685]] [[1.939]
 [1.939]
 [1.872]
 [1.926]
 [1.909]]
printing an ep nov before normalisation:  33.77292377181391
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04054916246428211, 0.1994437433412786, 0.2026360990081716, 0.13321687396082568, 0.2827352048320323, 0.1414189163934096]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04054916246428211, 0.1994437433412786, 0.2026360990081716, 0.13321687396082568, 0.2827352048320323, 0.1414189163934096]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04054916246428211, 0.1994437433412786, 0.2026360990081716, 0.13321687396082568, 0.2827352048320323, 0.1414189163934096]
Printing some Q and Qe and total Qs values:  [[1.163]
 [1.297]
 [1.285]
 [1.236]
 [1.238]] [[41.608]
 [27.895]
 [40.196]
 [41.027]
 [31.38 ]] [[2.098]
 [1.739]
 [2.169]
 [2.15 ]
 [1.805]]
Printing some Q and Qe and total Qs values:  [[1.28 ]
 [1.28 ]
 [1.255]
 [1.218]
 [1.28 ]] [[61.726]
 [61.726]
 [68.93 ]
 [74.268]
 [61.726]] [[2.148]
 [2.148]
 [2.273]
 [2.347]
 [2.148]]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.04054916246428211, 0.1994437433412786, 0.2026360990081716, 0.13321687396082568, 0.2827352048320323, 0.1414189163934096]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.040441454599936255, 0.19891258379479926, 0.20209643188157433, 0.13552506279445906, 0.28198207478611276, 0.14104239214311828]
Printing some Q and Qe and total Qs values:  [[1.205]
 [1.205]
 [1.205]
 [1.205]
 [1.205]] [[68.896]
 [68.896]
 [68.896]
 [68.896]
 [68.896]] [[2.872]
 [2.872]
 [2.872]
 [2.872]
 [2.872]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.040668941170654206, 0.19830023607708994, 0.2014672110286384, 0.13524864385989752, 0.2835782343178771, 0.14073673354584282]
maxi score, test score, baseline:  0.20809999999999998 0.95 0.95
probs:  [0.040684126666897386, 0.1983744734358927, 0.20154263480013718, 0.13529926082047933, 0.28368441853491294, 0.14041508574168066]
printing an ep nov before normalisation:  100.8356538456671
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.785]
 [0.773]
 [0.662]
 [0.785]] [[82.68 ]
 [82.68 ]
 [82.594]
 [89.517]
 [82.68 ]] [[0.785]
 [0.785]
 [0.773]
 [0.662]
 [0.785]]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.040684126666897386, 0.1983744734358927, 0.20154263480013718, 0.13529926082047933, 0.28368441853491294, 0.14041508574168066]
printing an ep nov before normalisation:  92.57887683670118
Printing some Q and Qe and total Qs values:  [[1.266]
 [1.429]
 [1.276]
 [1.266]
 [1.266]] [[46.022]
 [56.275]
 [62.648]
 [46.022]
 [46.022]] [[1.421]
 [1.654]
 [1.544]
 [1.421]
 [1.421]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.041032372753180675, 0.19800783776120226, 0.20183785497195514, 0.13389187656965604, 0.28609086819080604, 0.13913918975319997]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.040921428298111295, 0.197471076165011, 0.20400117056752942, 0.13352903817409661, 0.2853151694830287, 0.13876211731222285]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.040921428298111295, 0.197471076165011, 0.20400117056752942, 0.13352903817409661, 0.2853151694830287, 0.13876211731222285]
printing an ep nov before normalisation:  40.559892654418945
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.04081108411833874, 0.19693721876737819, 0.20615278134279127, 0.13316816294943615, 0.2845436677652838, 0.13838708505677183]
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.04081108411833874, 0.19693721876737819, 0.20615278134279127, 0.13316816294943615, 0.2845436677652838, 0.13838708505677183]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  111.71257015336735
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.0406861332829809, 0.19940195358580726, 0.20551994772979384, 0.13275951737452485, 0.28367003959286047, 0.1379624084340326]
maxi score, test score, baseline:  0.21009999999999998 0.95 0.95
probs:  [0.04070965120515196, 0.19893804759057232, 0.20563905803117824, 0.1328364315848124, 0.28383447162171926, 0.1380423399665657]
from probs:  [0.04070965120515196, 0.19893804759057232, 0.20563905803117824, 0.1328364315848124, 0.28383447162171926, 0.1380423399665657]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.21209999999999998 0.95 0.95
probs:  [0.0407330697604957, 0.19847610166930105, 0.2057576650732753, 0.13291302082076764, 0.2839982089000428, 0.1381219337761175]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2990],
        [0.5910],
        [0.5963],
        [0.0000],
        [0.0000],
        [0.4201],
        [0.0000],
        [0.6341],
        [0.5007],
        [0.0000]], dtype=torch.float64)
0.0 0.29903564571650926
0.0 0.5910385701525378
0.0 0.596265353959436
0.0 0.0
0.0 0.0
0.0 0.42012197918243904
0.0 0.0
0.0 0.6340698404356852
0.0 0.5007404587631846
0.0 0.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  69.75097524806613
printing an ep nov before normalisation:  100.47193896489901
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.0407484854101, 0.19855141191768286, 0.2058357401114122, 0.13296343694625531, 0.2841059916591733, 0.13779493395537637]
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.0407484854101, 0.19855141191768286, 0.2058357401114122, 0.13296343694625531, 0.2841059916591733, 0.13779493395537637]
printing an ep nov before normalisation:  0.18345981525783372
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04063907371516097, 0.19801690171971945, 0.20797331051669746, 0.13260561136747998, 0.283341009665915, 0.13742409301502695]
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04063907371516097, 0.19801690171971945, 0.20797331051669746, 0.13260561136747998, 0.283341009665915, 0.13742409301502695]
maxi score, test score, baseline:  0.2161 0.95 0.95
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04051637577431306, 0.20044420416823852, 0.20734373349966229, 0.1322043337734996, 0.28248313322582497, 0.13700821955846154]
printing an ep nov before normalisation:  90.01954660618127
printing an ep nov before normalisation:  63.30500373673522
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  95.46369747952556
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04062387628100022, 0.20220781190410703, 0.20606447949050222, 0.13154967697254955, 0.28324052495552465, 0.1363136303963164]
printing an ep nov before normalisation:  65.43656973412922
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.040516321080657065, 0.20167104174252445, 0.20817163876826852, 0.13120059581933868, 0.28248850782240764, 0.13595189476680353]
maxi score, test score, baseline:  0.2161 0.95 0.95
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.040516321080657065, 0.20167104174252445, 0.20817163876826852, 0.13120059581933868, 0.28248850782240764, 0.13595189476680353]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2161 0.95 0.95
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.040623293156444036, 0.20339286911552093, 0.20690272520089187, 0.1305633553440741, 0.2832420950577097, 0.1352756621253592]
printing an ep nov before normalisation:  21.243034106593736
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04051690244450071, 0.2028587890316711, 0.20898489795948882, 0.13022064031611774, 0.28249820509330187, 0.13492056515491974]
maxi score, test score, baseline:  0.2161 0.95 0.95
probs:  [0.04051690244450071, 0.2028587890316711, 0.20898489795948882, 0.13022064031611774, 0.28249820509330187, 0.13492056515491974]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  62.126447769430435
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  67.26544308401117
maxi score, test score, baseline:  0.2181 0.95 0.95
maxi score, test score, baseline:  0.2181 0.95 0.95
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2181 0.95 0.95
probs:  [0.0406381153339639, 0.20461475802793216, 0.20780190967465997, 0.12964740842446443, 0.28335126088318247, 0.133946547655797]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.745]
 [0.839]
 [0.669]
 [0.745]] [[62.016]
 [62.016]
 [67.963]
 [72.155]
 [62.016]] [[0.745]
 [0.745]
 [0.839]
 [0.669]
 [0.745]]
maxi score, test score, baseline:  0.2181 0.95 0.95
probs:  [0.040532784227595535, 0.20408301317749072, 0.2098602522508194, 0.12931061273082034, 0.28261476536565733, 0.13359857224761673]
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040532784227595535, 0.20408301317749072, 0.2098602522508194, 0.12931061273082034, 0.28261476536565733, 0.13359857224761673]
maxi score, test score, baseline:  0.2201 0.95 0.95
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04075431849687911, 0.20345867521497502, 0.2092060347452424, 0.12907299325625116, 0.2841692025897826, 0.13333877569686967]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04075431849687911, 0.20345867521497502, 0.2092060347452424, 0.12907299325625116, 0.2841692025897826, 0.13333877569686967]
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04075431849687911, 0.20345867521497502, 0.2092060347452424, 0.12907299325625116, 0.2841692025897826, 0.13333877569686967]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040649502697676636, 0.2029340285729628, 0.21124482485581553, 0.12874028617983901, 0.28343629619729493, 0.13299506149641108]
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04068732158015914, 0.2025498529015842, 0.2114418815826005, 0.1288603311532082, 0.2837007382083187, 0.13275987457412927]
siam score:  -0.9176567
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
printing an ep nov before normalisation:  51.56579176816376
printing an ep nov before normalisation:  0.00040578067000751616
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040906560509243406, 0.20193762220441996, 0.2107839735337747, 0.12862663524483262, 0.2852390613581285, 0.13250614714960068]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.251]
 [1.298]
 [1.251]
 [1.251]] [[76.702]
 [76.702]
 [78.517]
 [76.702]
 [76.702]] [[2.545]
 [2.545]
 [2.631]
 [2.545]
 [2.545]]
maxi score, test score, baseline:  0.2201 0.95 0.95
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.041123558538117944, 0.20133164928229338, 0.2101327901405144, 0.12839532800535033, 0.2867616608922307, 0.1322550131414933]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 0.95 0.95
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.041019040532559066, 0.2008186293351957, 0.20959732886032517, 0.13061606531129624, 0.28603080971422423, 0.1319181262463995]
siam score:  -0.91893935
siam score:  -0.91804373
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2201 0.95 0.95
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04134241036001625, 0.1991309794077416, 0.20779920218947176, 0.13232760891667078, 0.2883022350137929, 0.13109756411230652]
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04134241036001625, 0.1991309794077416, 0.20779920218947176, 0.13232760891667078, 0.2883022350137929, 0.13109756411230652]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04123891723325737, 0.19863121460200783, 0.20727766792847152, 0.13450501649026683, 0.28757852468786993, 0.13076865905812654]
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04123891723325737, 0.19863121460200783, 0.20727766792847152, 0.13450501649026683, 0.28757852468786993, 0.13076865905812654]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04113594263147108, 0.19813395373581402, 0.20926186210349876, 0.13416839931202237, 0.28685844032135766, 0.13044140189583606]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  71.87193863553588
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  67.17698615808563
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040819802070777525, 0.19934376142411445, 0.2076494312530312, 0.13810258831849365, 0.28464772164484065, 0.12943669528874263]
printing an ep nov before normalisation:  45.02459526062012
actions average: 
K:  0  action  0 :  tensor([    0.7913,     0.0002,     0.0007,     0.0380,     0.1698],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9971,     0.0002,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0319,     0.0003,     0.0007,     0.8580,     0.1091],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2454, 0.0011, 0.0367, 0.2363, 0.4804], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.6792,     0.0031,     0.0004,     0.1156,     0.2017],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9992,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9920,     0.0003,     0.0077],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0248, 0.0014, 0.0009, 0.8093, 0.1637], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2034, 0.0032, 0.0032, 0.1958, 0.5945], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.04081980207077751, 0.1993437614241144, 0.20764943125303117, 0.1381025883184937, 0.28464772164484053, 0.1294366952887426]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040710162187814626, 0.19880694013760233, 0.20709022831702328, 0.1377307962830108, 0.2838810280689658, 0.13178084500558318]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.0406283502155396, 0.20111556149158258, 0.2066729577594446, 0.13745336950292839, 0.28330893047201616, 0.13082083055848867]
printing an ep nov before normalisation:  56.705972193131764
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9151833
maxi score, test score, baseline:  0.2201 0.95 0.95
probs:  [0.040631609224138165, 0.19951644026487964, 0.20501834885949624, 0.13893732697133743, 0.2833365970232401, 0.13255967765690843]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.040631609224138165, 0.19951644026487964, 0.20501834885949624, 0.13893732697133743, 0.2833365970232401, 0.13255967765690843]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063352309928441, 0.20058424661454954, 0.20581320049956683, 0.13796433545495207, 0.28335476055055514, 0.1316499337810919]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063352309928441, 0.20058424661454954, 0.20581320049956683, 0.13796433545495207, 0.28335476055055514, 0.1316499337810919]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063352309928441, 0.20058424661454954, 0.20581320049956683, 0.13796433545495207, 0.28335476055055514, 0.1316499337810919]
printing an ep nov before normalisation:  83.40634346008301
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063352309928441, 0.20058424661454954, 0.20581320049956683, 0.13796433545495207, 0.28335476055055514, 0.1316499337810919]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063352309928441, 0.20058424661454954, 0.20581320049956683, 0.13796433545495207, 0.28335476055055514, 0.1316499337810919]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.7803,     0.0023,     0.0007,     0.0339,     0.1828],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0081,     0.9882,     0.0003,     0.0000,     0.0033],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0154,     0.9342,     0.0073,     0.0427],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0101,     0.0003,     0.0257,     0.8565,     0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2843, 0.0084, 0.0346, 0.0023, 0.6704], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.91695726446152
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 0.95 0.95
maxi score, test score, baseline:  0.2221 0.95 0.95
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063595594747708, 0.20162012027141427, 0.20419586690059727, 0.1394117456917819, 0.28337645948914936, 0.13075985169958]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04063595594747708, 0.20162012027141427, 0.20419586690059727, 0.1394117456917819, 0.28337645948914936, 0.13075985169958]
siam score:  -0.916711
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.091405868530273
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04084011494272163, 0.20105512811198012, 0.20361856832268826, 0.13914397309639337, 0.2848087993483929, 0.1305334161778236]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04084011494272163, 0.20105512811198012, 0.20361856832268826, 0.13914397309639337, 0.2848087993483929, 0.1305334161778236]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04084011494272161, 0.20105512811198015, 0.20361856832268826, 0.13914397309639337, 0.28480879934839287, 0.1305334161778236]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04084011494272161, 0.20105512811198015, 0.20361856832268826, 0.13914397309639337, 0.28480879934839287, 0.1305334161778236]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04074306318684901, 0.20057609804638385, 0.20551568132419443, 0.13881254916257027, 0.2841300870861086, 0.13022252119389394]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04074306318684901, 0.20057609804638385, 0.20551568132419443, 0.13881254916257027, 0.2841300870861086, 0.13022252119389394]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04074306318684901, 0.20057609804638385, 0.20551568132419443, 0.13881254916257027, 0.2841300870861086, 0.13022252119389394]
maxi score, test score, baseline:  0.2221 0.95 0.95
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04074306318684901, 0.20057609804638385, 0.20551568132419443, 0.13881254916257027, 0.2841300870861086, 0.13022252119389394]
maxi score, test score, baseline:  0.2221 0.95 0.95
probs:  [0.04074306318684901, 0.20057609804638385, 0.20551568132419443, 0.13881254916257027, 0.2841300870861086, 0.13022252119389394]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  56.33894800538849
maxi score, test score, baseline:  0.2221 0.95 0.95
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.53113092007397
maxi score, test score, baseline:  0.2241 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
maxi score, test score, baseline:  0.2241 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
printing an ep nov before normalisation:  2.446079386754718
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2261 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
printing an ep nov before normalisation:  32.53957317577576
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04064647310420355, 0.2000993467170587, 0.20502718124631664, 0.14085929035710623, 0.28345460344424045, 0.12991310513107437]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.040848649544539845, 0.1995469005361535, 0.20445141372083322, 0.1405872021628585, 0.2848730134605052, 0.12969282057510984]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.040848649544539845, 0.1995469005361535, 0.20445141372083322, 0.1405872021628585, 0.2848730134605052, 0.12969282057510984]
siam score:  -0.90131927
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04104892136843874, 0.19899965871078637, 0.20388107025114643, 0.1403176771933184, 0.28627806125203475, 0.1294746112242755]
printing an ep nov before normalisation:  45.88245441926551
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04104892136843874, 0.19899965871078637, 0.20388107025114643, 0.1403176771933184, 0.28627806125203475, 0.1294746112242755]
maxi score, test score, baseline:  0.2281 0.95 0.95
probs:  [0.04104892136843874, 0.19899965871078637, 0.20388107025114643, 0.1403176771933184, 0.28627806125203475, 0.1294746112242755]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.041036440148777775, 0.19899538408093237, 0.2038722111587328, 0.14036851476717166, 0.28619181759223805, 0.1295356322521473]
using explorer policy with actor:  1
siam score:  -0.9089473
printing an ep nov before normalisation:  19.7669197298421
line 256 mcts: sample exp_bonus 25.0397070646286
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.041036440148777775, 0.19899538408093237, 0.2038722111587328, 0.14036851476717166, 0.28619181759223805, 0.1295356322521473]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.041036440148777775, 0.19899538408093237, 0.2038722111587328, 0.14036851476717166, 0.28619181759223805, 0.1295356322521473]
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.041036440148777775, 0.19899538408093237, 0.2038722111587328, 0.14036851476717166, 0.28619181759223805, 0.1295356322521473]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.04132865785953689, 0.19909546424811153, 0.2040796254799951, 0.1391782753630504, 0.2882110052612198, 0.1281069717880863]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.152]
 [0.656]
 [1.336]
 [1.336]
 [1.278]] [[27.873]
 [34.577]
 [41.086]
 [36.316]
 [29.9  ]] [[1.732]
 [1.667]
 [2.765]
 [2.459]
 [1.988]]
actions average: 
K:  4  action  0 :  tensor([    0.8156,     0.0035,     0.0002,     0.0423,     0.1385],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0019,     0.9937,     0.0007,     0.0000,     0.0037],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9415,     0.0136,     0.0448],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0903,     0.0000,     0.0047,     0.8490,     0.0559],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1359, 0.0015, 0.0577, 0.1417, 0.6633], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2301 0.95 0.95
probs:  [0.04132865785953689, 0.19909546424811153, 0.2040796254799951, 0.1391782753630504, 0.2882110052612198, 0.1281069717880863]
printing an ep nov before normalisation:  65.07894885634559
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.716]
 [0.674]
 [0.508]
 [0.306]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.428]
 [0.716]
 [0.674]
 [0.508]
 [0.306]]
printing an ep nov before normalisation:  55.887473111058746
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.37600517272949
printing an ep nov before normalisation:  10.486039424778824
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.069]
 [1.426]
 [1.41 ]
 [1.368]
 [1.333]] [[33.469]
 [43.494]
 [41.01 ]
 [39.784]
 [35.023]] [[1.21 ]
 [1.651]
 [1.614]
 [1.562]
 [1.488]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.91251206
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.5145,     0.0107,     0.0002,     0.1801,     0.2945],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9898,     0.0020,     0.0000,     0.0075],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0057,     0.9551,     0.0011,     0.0375],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0269, 0.0010, 0.0082, 0.8665, 0.0974], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1839, 0.0699, 0.0109, 0.1531, 0.5822], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.622042655944824
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2761 1.0 1.0
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.956090749194814
printing an ep nov before normalisation:  1.988157709066627
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.2761 1.0 1.0
printing an ep nov before normalisation:  62.57599097573033
UNIT TEST: sample policy line 217 mcts : [0.    0.5   0.167 0.083 0.25 ]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.00012070320622115105
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.04470634460449
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
printing an ep nov before normalisation:  38.04795800986948
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9214873
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  72.06361325476612
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00539326019080022
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.793258795003034
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.243]
 [1.237]
 [1.243]
 [1.243]] [[23.374]
 [23.374]
 [39.872]
 [23.374]
 [23.374]] [[1.695]
 [1.695]
 [2.163]
 [1.695]
 [1.695]]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.351289954206337
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.218]
 [1.218]
 [1.276]
 [1.218]
 [1.218]] [[46.354]
 [46.354]
 [57.135]
 [46.354]
 [46.354]] [[2.283]
 [2.283]
 [2.812]
 [2.283]
 [2.283]]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.875343322753906
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.235110732512254
line 256 mcts: sample exp_bonus 0.12592144888988344
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.13059114338636
printing an ep nov before normalisation:  50.70717104464387
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5546],
        [0.5081],
        [0.4266],
        [0.8500],
        [0.2927],
        [0.8277],
        [0.5873],
        [0.0000],
        [0.5258],
        [0.5927]], dtype=torch.float64)
0.0 0.5545820135298644
0.0 0.5080845045386982
0.0 0.4265523494355953
0.0 0.8499870038487743
0.0 0.29265827238811726
0.0 0.8277499747279011
0.0 0.5873187893572468
0.9509900498999999 0.9509900498999999
0.0 0.5258144328945038
0.0 0.5927320655375209
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.018789771121419108
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.02448984557759104
printing an ep nov before normalisation:  25.38606882095337
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.948]
 [0.984]
 [0.948]
 [0.948]] [[64.282]
 [64.282]
 [71.711]
 [64.282]
 [64.282]] [[2.344]
 [2.344]
 [2.579]
 [2.344]
 [2.344]]
printing an ep nov before normalisation:  40.42862634504644
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  55.8747714612822
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.92681915
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7472,     0.0005,     0.0005,     0.0419,     0.2099],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9947,     0.0003,     0.0000,     0.0039],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9765,     0.0016,     0.0218],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1176,     0.0004,     0.0004,     0.7669,     0.1147],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2937, 0.0458, 0.0434, 0.1217, 0.4954], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.806115665868454
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.26 ]
 [1.475]
 [1.303]
 [1.313]
 [1.354]] [[24.526]
 [17.815]
 [38.776]
 [36.968]
 [21.203]] [[0.705]
 [1.73 ]
 [2.151]
 [2.11 ]
 [1.706]]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.92554945
printing an ep nov before normalisation:  36.21425352249222
actions average: 
K:  2  action  0 :  tensor([    0.6201,     0.0016,     0.0002,     0.0755,     0.3027],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9974,     0.0016,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9677,     0.0048,     0.0272],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1016,     0.0003,     0.0068,     0.6796,     0.2117],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3224, 0.0100, 0.1185, 0.1180, 0.4311], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.63929495977804
printing an ep nov before normalisation:  47.5315743733623
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]] [[39.941]
 [39.941]
 [39.941]
 [39.941]
 [39.941]] [[3.003]
 [3.003]
 [3.003]
 [3.003]
 [3.003]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7778,     0.0031,     0.0006,     0.0649,     0.1537],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9981,     0.0002,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9646,     0.0186,     0.0167],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0011,     0.0000,     0.0025,     0.9917,     0.0047],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2189, 0.0012, 0.0503, 0.2415, 0.4882], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.97513780886452
siam score:  -0.92400724
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.68588399887085
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.636592320033483
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.602446038956465
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  -0.9257121
siam score:  -0.92644495
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8332,     0.0185,     0.0007,     0.0412,     0.1064],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9962,     0.0000,     0.0000,     0.0032],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0008,     0.9667,     0.0002,     0.0320],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0287,     0.0007,     0.0256,     0.7793,     0.1657],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1314, 0.0083, 0.0492, 0.1813, 0.6297], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9268442
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.366]
 [1.31 ]
 [1.366]
 [1.366]
 [1.366]] [[25.301]
 [45.548]
 [25.301]
 [25.301]
 [25.301]] [[1.851]
 [2.47 ]
 [1.851]
 [1.851]
 [1.851]]
printing an ep nov before normalisation:  2.5750860110969143
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.252]
 [1.252]
 [1.316]
 [1.252]
 [1.252]] [[46.511]
 [46.511]
 [63.182]
 [46.511]
 [46.511]] [[1.591]
 [1.591]
 [1.834]
 [1.591]
 [1.591]]
printing an ep nov before normalisation:  62.09319814808474
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7408,     0.0013,     0.0005,     0.0865,     0.1708],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9986,     0.0004,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9870,     0.0002,     0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0536,     0.0001,     0.0028,     0.8660,     0.0774],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2193, 0.0023, 0.0903, 0.1931, 0.4949], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.09531254818704
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.5982, 0.0102, 0.0030, 0.0661, 0.3224], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9566,     0.0315,     0.0000,     0.0113],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9711,     0.0052,     0.0236],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1869, 0.0007, 0.0199, 0.5426, 0.2499], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2648, 0.0065, 0.0829, 0.1405, 0.5054], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3001 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.500765800476074
maxi score, test score, baseline:  0.3001 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3001 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7009,     0.0487,     0.0006,     0.0822,     0.1676],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9982,     0.0000,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9789,     0.0062,     0.0148],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0918,     0.0003,     0.0063,     0.7817,     0.1198],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2012, 0.0223, 0.1162, 0.0802, 0.5801], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.539497514964154
line 256 mcts: sample exp_bonus 36.96710290786417
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([    0.5187,     0.1840,     0.0003,     0.0691,     0.2279],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9994,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9862,     0.0002,     0.0135],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0758,     0.0007,     0.0007,     0.8244,     0.0984],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2185, 0.1273, 0.0505, 0.1239, 0.4798], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.52588951394185
actions average: 
K:  1  action  0 :  tensor([    0.5742,     0.0002,     0.0003,     0.1257,     0.2996],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0042,     0.9933,     0.0002,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9793,     0.0034,     0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0709, 0.0008, 0.0009, 0.7433, 0.1841], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2289, 0.0020, 0.0010, 0.1907, 0.5773], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.38784408569336
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.462655955397594
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.92952424
printing an ep nov before normalisation:  37.09404468536377
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.423]
 [1.423]
 [1.428]
 [1.423]
 [1.423]] [[91.524]
 [91.524]
 [96.581]
 [91.524]
 [91.524]] [[1.735]
 [1.735]
 [1.761]
 [1.735]
 [1.735]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3021 1.0 1.0
printing an ep nov before normalisation:  95.60024008556334
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.917 0.    0.042]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.7227,     0.0062,     0.0003,     0.0703,     0.2004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9961,     0.0026,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9841,     0.0029,     0.0129],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0629,     0.0001,     0.0010,     0.8537,     0.0823],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3150, 0.0015, 0.0009, 0.1853, 0.4973], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.409]
 [1.353]
 [1.353]
 [1.353]] [[26.467]
 [31.08 ]
 [26.467]
 [26.467]
 [26.467]] [[2.51 ]
 [2.884]
 [2.51 ]
 [2.51 ]
 [2.51 ]]
printing an ep nov before normalisation:  33.27209800380679
actions average: 
K:  3  action  0 :  tensor([    0.7079,     0.0036,     0.0003,     0.0837,     0.2044],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0027,     0.9929,     0.0000,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0002,     0.9751,     0.0090,     0.0155],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0994,     0.0003,     0.0007,     0.7798,     0.1198],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1908, 0.0267, 0.1993, 0.1402, 0.4430], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.2923104763031
Printing some Q and Qe and total Qs values:  [[1.37 ]
 [1.276]
 [1.29 ]
 [1.37 ]
 [1.37 ]] [[19.294]
 [19.841]
 [17.698]
 [26.58 ]
 [19.294]] [[2.11 ]
 [2.055]
 [1.913]
 [2.642]
 [2.11 ]]
printing an ep nov before normalisation:  38.738508224487305
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.949796464708115
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.74198289276462
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7815,     0.0002,     0.0002,     0.0540,     0.1641],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0024,     0.9946,     0.0001,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0000,     0.9691,     0.0022,     0.0284],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0298,     0.0002,     0.0031,     0.8623,     0.1046],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2165, 0.0025, 0.0289, 0.1141, 0.6381], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]] [[18.719]
 [18.719]
 [18.719]
 [18.719]
 [18.719]] [[1.659]
 [1.659]
 [1.659]
 [1.659]
 [1.659]]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.759946823120117
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.45319080352783
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.92355937
printing an ep nov before normalisation:  61.28728662756778
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.8258,     0.0027,     0.0001,     0.0607,     0.1107],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9353,     0.0405,     0.0000,     0.0234],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9629,     0.0099,     0.0271],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0300,     0.0000,     0.0009,     0.9528,     0.0164],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2055, 0.0348, 0.1987, 0.1629, 0.3982], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.062]
 [1.062]
 [1.238]
 [1.062]
 [1.062]] [[37.14]
 [37.14]
 [61.7 ]
 [37.14]
 [37.14]] [[1.827]
 [1.827]
 [2.773]
 [1.827]
 [1.827]]
printing an ep nov before normalisation:  25.433096885681152
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.112525707497696
printing an ep nov before normalisation:  30.985609620164354
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.35048808873944
Printing some Q and Qe and total Qs values:  [[1.135]
 [1.135]
 [1.149]
 [1.135]
 [1.135]] [[ 89.984]
 [ 89.984]
 [101.512]
 [ 89.984]
 [ 89.984]] [[1.426]
 [1.426]
 [1.482]
 [1.426]
 [1.426]]
line 256 mcts: sample exp_bonus 72.66350240564432
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.67407512664795
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3081 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([0.6216, 0.0013, 0.0010, 0.1477, 0.2284], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9994,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9929,     0.0004,     0.0066],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1181, 0.0007, 0.0027, 0.6200, 0.2584], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3190, 0.0035, 0.2039, 0.1203, 0.3533], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9265425
Printing some Q and Qe and total Qs values:  [[1.425]
 [1.431]
 [1.455]
 [1.438]
 [1.426]] [[8.811]
 [7.463]
 [9.602]
 [5.806]
 [8.741]] [[1.883]
 [1.814]
 [1.957]
 [1.728]
 [1.88 ]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.94391119573702
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  86.43571468631657
Printing some Q and Qe and total Qs values:  [[1.189]
 [1.399]
 [1.404]
 [1.358]
 [1.418]] [[27.964]
 [31.243]
 [25.394]
 [30.667]
 [25.452]] [[2.412]
 [2.765]
 [2.515]
 [2.7  ]
 [2.531]]
actions average: 
K:  0  action  0 :  tensor([    0.7288,     0.0012,     0.0003,     0.1131,     0.1566],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9993,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0001,     0.9594,     0.0004,     0.0396],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0977,     0.0007,     0.0012,     0.7608,     0.1396],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1475, 0.0081, 0.0870, 0.2195, 0.5380], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.7418, 0.0017, 0.0139, 0.0791, 0.1636], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9739,     0.0209,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9558,     0.0084,     0.0356],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0790,     0.0002,     0.0007,     0.7690,     0.1511],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2173, 0.0015, 0.0462, 0.1359, 0.5991], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.7895,     0.0006,     0.0038,     0.0236,     0.1826],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9994,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0003,     0.9454,     0.0060,     0.0480],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0926,     0.0006,     0.0010,     0.7534,     0.1525],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3618, 0.0339, 0.0954, 0.0909, 0.4180], grad_fn=<DivBackward0>)
siam score:  -0.9162861
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.7750,     0.0004,     0.0003,     0.0339,     0.1904],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9992,     0.0002,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9838,     0.0008,     0.0153],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0545,     0.0004,     0.0021,     0.7868,     0.1562],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2661, 0.0037, 0.0822, 0.1609, 0.4871], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.53385409600499
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  77.30197986310814
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.444]
 [1.444]
 [1.444]
 [1.444]
 [1.444]] [[30.15]
 [30.15]
 [30.15]
 [30.15]
 [30.15]] [[1.744]
 [1.744]
 [1.744]
 [1.744]
 [1.744]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.148]
 [1.148]
 [1.148]
 [1.148]
 [1.148]]
maxi score, test score, baseline:  0.3081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.738]
 [0.923]
 [0.649]
 [0.738]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.836]
 [0.738]
 [0.923]
 [0.649]
 [0.738]]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 19.645828008651733
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.10365989797928
printing an ep nov before normalisation:  33.88473475315813
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.284]
 [1.313]
 [1.284]
 [1.284]
 [1.284]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.284]
 [1.313]
 [1.284]
 [1.284]
 [1.284]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.39611946737904
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3121 1.0 1.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.736]
 [0.724]
 [0.724]
 [0.724]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.724]
 [0.736]
 [0.724]
 [0.724]
 [0.724]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.23525126552592
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.89587122808953
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 1.0 1.0
siam score:  -0.9156546
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9172756
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.998]
 [0.998]
 [0.998]
 [0.998]] [[37.039]
 [37.039]
 [37.039]
 [37.039]
 [37.039]] [[1.9]
 [1.9]
 [1.9]
 [1.9]
 [1.9]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.352]
 [1.115]
 [1.059]
 [1.196]] [[28.92 ]
 [28.216]
 [28.888]
 [32.872]
 [27.193]] [[2.09 ]
 [2.233]
 [2.037]
 [2.223]
 [2.014]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7615,     0.0063,     0.0006,     0.0709,     0.1607],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9966,     0.0010,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9768,     0.0005,     0.0226],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0739,     0.0003,     0.0005,     0.7788,     0.1465],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1942, 0.0125, 0.1328, 0.1189, 0.5416], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.194]
 [1.039]
 [1.039]
 [1.039]] [[75.866]
 [70.194]
 [75.866]
 [75.866]
 [75.866]] [[2.027]
 [2.087]
 [2.027]
 [2.027]
 [2.027]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.71987009048462
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6616,     0.0008,     0.0005,     0.1295,     0.2077],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9979,     0.0001,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9549,     0.0028,     0.0421],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0512,     0.0003,     0.0002,     0.8409,     0.1073],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2472, 0.0007, 0.0688, 0.1964, 0.4870], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([    0.8795,     0.0145,     0.0004,     0.0014,     0.1041],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9623,     0.0012,     0.0363],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0493,     0.0001,     0.0076,     0.8648,     0.0782],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2851, 0.0074, 0.0700, 0.1324, 0.5051], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3181 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  60.2818625479352
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.903362
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.012918731952993312
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  81.56900735669295
printing an ep nov before normalisation:  73.04490131744426
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90909225
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90906364
Printing some Q and Qe and total Qs values:  [[1.324]
 [1.475]
 [1.324]
 [1.324]
 [1.324]] [[28.334]
 [36.49 ]
 [28.334]
 [28.334]
 [28.334]] [[1.695]
 [1.952]
 [1.695]
 [1.695]
 [1.695]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.159879684448242
printing an ep nov before normalisation:  0.654528688114624
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90813744
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.24113352292016
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.965744136195866
main train batch thing paused
add a thread
Adding thread: now have 2 threads
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.72191753697729
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.287]
 [1.287]
 [1.339]
 [1.287]
 [1.287]] [[51.876]
 [51.876]
 [72.322]
 [51.876]
 [51.876]] [[1.712]
 [1.712]
 [2.092]
 [1.712]
 [1.712]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
printing an ep nov before normalisation:  0.0008003501562825477
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.665]
 [0.944]
 [0.684]
 [0.665]
 [0.661]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.665]
 [0.944]
 [0.684]
 [0.665]
 [0.661]]
printing an ep nov before normalisation:  67.69293279784326
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7772,     0.0003,     0.0003,     0.0475,     0.1747],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0154,     0.9640,     0.0001,     0.0004,     0.0202],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0149,     0.9432,     0.0138,     0.0273],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.2343,     0.0003,     0.0026,     0.6791,     0.0837],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2898, 0.0075, 0.0388, 0.2609, 0.4031], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.11136103855448
printing an ep nov before normalisation:  22.324944812300487
Printing some Q and Qe and total Qs values:  [[1.24]
 [1.24]
 [1.24]
 [1.24]
 [1.24]] [[20.413]
 [20.413]
 [20.413]
 [20.413]
 [20.413]] [[42.066]
 [42.066]
 [42.066]
 [42.066]
 [42.066]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90885663
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.31402740704433
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.25853015939139
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.909369
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.423733131178302
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.764916173702122
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.611]
 [1.125]
 [0.906]
 [0.705]
 [0.906]] [[16.934]
 [49.633]
 [ 0.   ]
 [17.877]
 [ 0.   ]] [[0.676]
 [1.466]
 [0.828]
 [0.778]
 [0.828]]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.43890380859375
printing an ep nov before normalisation:  38.76874196811001
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6574, 0.0010, 0.0007, 0.0551, 0.2858], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9970,     0.0001,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9874,     0.0003,     0.0123],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0280,     0.0007,     0.0016,     0.8560,     0.1137],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3150, 0.0011, 0.0633, 0.1712, 0.4494], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 65.3440115801137
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.430431774684365
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.80650232962953
printing an ep nov before normalisation:  33.44103312576851
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.9089473
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.169]
 [1.179]
 [1.169]
 [1.169]
 [1.169]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.169]
 [1.179]
 [1.169]
 [1.169]
 [1.169]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  69.30261960860047
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3421 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.8619788572048
printing an ep nov before normalisation:  46.310629364184294
Printing some Q and Qe and total Qs values:  [[1.145]
 [1.275]
 [1.033]
 [1.199]
 [1.033]] [[34.472]
 [23.249]
 [26.527]
 [35.949]
 [26.527]] [[1.747]
 [1.572]
 [1.419]
 [1.841]
 [1.419]]
maxi score, test score, baseline:  0.3421 1.0 1.0
printing an ep nov before normalisation:  37.07426971238999
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3421 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.065726085222344
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3421 1.0 1.0
printing an ep nov before normalisation:  34.408789415725494
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.94004133290612
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.749693870544434
printing an ep nov before normalisation:  26.938960552215576
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.940861225128174
printing an ep nov before normalisation:  67.90417549899601
printing an ep nov before normalisation:  32.57419309359668
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.86659812927246
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.369]
 [1.342]
 [1.292]
 [1.316]] [[32.957]
 [28.587]
 [38.161]
 [31.2  ]
 [28.098]] [[1.692]
 [1.694]
 [1.882]
 [1.676]
 [1.63 ]]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8990],
        [0.5466],
        [0.5466],
        [0.5000],
        [0.9123],
        [0.0000],
        [0.4964],
        [0.1595],
        [0.5894],
        [0.0000]], dtype=torch.float64)
0.0 0.8989726688718673
0.0 0.546554533617727
0.0 0.546554533617727
0.0 0.49997865048065093
0.0 0.9123187713486514
0.0 0.0
0.0 0.4964158868702802
0.0 0.15946705080151302
0.0 0.5893814188400586
0.0 0.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  69.94707858072775
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.92694250213013
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
siam score:  -0.9069383
printing an ep nov before normalisation:  4.457517462697069e-05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.84656427095435
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.216]
 [1.216]
 [1.286]
 [1.216]
 [1.216]] [[83.747]
 [83.747]
 [83.146]
 [83.747]
 [83.747]] [[2.085]
 [2.085]
 [2.147]
 [2.085]
 [2.085]]
Printing some Q and Qe and total Qs values:  [[1.111]
 [1.217]
 [1.238]
 [1.137]
 [1.217]] [[34.355]
 [45.223]
 [48.725]
 [52.484]
 [45.223]] [[1.238]
 [1.467]
 [1.529]
 [1.47 ]
 [1.467]]
printing an ep nov before normalisation:  45.262473776831406
Printing some Q and Qe and total Qs values:  [[1.289]
 [1.461]
 [1.289]
 [1.289]
 [1.289]] [[47.086]
 [48.337]
 [47.086]
 [47.086]
 [47.086]] [[1.494]
 [1.675]
 [1.494]
 [1.494]
 [1.494]]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]] [[39.762]
 [39.762]
 [39.762]
 [39.762]
 [39.762]] [[0.48]
 [0.48]
 [0.48]
 [0.48]
 [0.48]]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  60.64557123269394
printing an ep nov before normalisation:  11.913422346115112
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.09 ]
 [1.09 ]
 [1.249]
 [1.09 ]
 [1.09 ]] [[37.171]
 [37.171]
 [74.464]
 [37.171]
 [37.171]] [[1.483]
 [1.483]
 [2.445]
 [1.483]
 [1.483]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
siam score:  -0.9053116
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
printing an ep nov before normalisation:  71.55736523346737
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.20696913652509
actions average: 
K:  1  action  0 :  tensor([0.8163, 0.0039, 0.0018, 0.0575, 0.1205], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9966,     0.0003,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9584,     0.0005,     0.0409],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0700, 0.0017, 0.0011, 0.8087, 0.1185], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1899, 0.0213, 0.1104, 0.2380, 0.4403], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.042 0.375 0.292 0.25  0.042]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.46170606725251
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([0.7883, 0.0012, 0.0010, 0.0425, 0.1670], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9793,     0.0025,     0.0028,     0.0146],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9714,     0.0009,     0.0275],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0245,     0.0005,     0.0050,     0.9045,     0.0655],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2028, 0.0009, 0.1172, 0.0771, 0.6021], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 0.0
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.703]
 [0.703]
 [0.891]
 [0.703]
 [0.703]] [[54.303]
 [54.303]
 [68.602]
 [54.303]
 [54.303]] [[0.703]
 [0.703]
 [0.891]
 [0.703]
 [0.703]]
printing an ep nov before normalisation:  81.66576567921683
printing an ep nov before normalisation:  79.49728572986129
printing an ep nov before normalisation:  40.87489174617275
printing an ep nov before normalisation:  60.92982822175417
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9013098
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  94.43385562171414
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.72247869667065
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.378454208374023
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.757]
 [0.761]
 [0.678]
 [0.678]] [[54.974]
 [64.135]
 [56.641]
 [54.974]
 [54.974]] [[0.678]
 [0.757]
 [0.761]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  46.82473659515381
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
printing an ep nov before normalisation:  45.65022873588165
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.84707339070313
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7269,     0.0119,     0.0002,     0.0812,     0.1797],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9813,     0.0001,     0.0000,     0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0043,     0.9446,     0.0026,     0.0483],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0000,     0.0433,     0.7989,     0.1573],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2875, 0.0016, 0.0025, 0.0824, 0.6259], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.189]
 [0.685]
 [1.157]
 [1.157]] [[58.422]
 [61.374]
 [79.268]
 [58.422]
 [58.422]] [[2.335]
 [2.453]
 [2.476]
 [2.335]
 [2.335]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
printing an ep nov before normalisation:  0.04932845574757039
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.168]
 [1.316]
 [1.168]
 [1.168]
 [1.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.168]
 [1.316]
 [1.168]
 [1.168]
 [1.168]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  92.88139892129507
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.385]
 [0.006]
 [0.876]
 [0.385]
 [0.385]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.385]
 [0.006]
 [0.876]
 [0.385]
 [0.385]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.27593261307837
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
printing an ep nov before normalisation:  63.382256322671424
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.092]
 [1.103]
 [1.103]
 [1.103]
 [1.103]] [[33.962]
 [44.125]
 [44.125]
 [44.125]
 [44.125]] [[1.425]
 [3.196]
 [3.196]
 [3.196]
 [3.196]]
printing an ep nov before normalisation:  81.12242560819685
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9035442
Printing some Q and Qe and total Qs values:  [[0.78 ]
 [0.78 ]
 [0.894]
 [0.78 ]
 [0.78 ]] [[48.477]
 [48.477]
 [52.872]
 [48.477]
 [48.477]] [[0.78 ]
 [0.78 ]
 [0.894]
 [0.78 ]
 [0.78 ]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.364]
 [1.413]
 [1.353]
 [1.364]
 [1.364]] [[58.13 ]
 [60.084]
 [64.855]
 [58.13 ]
 [58.13 ]] [[1.93 ]
 [2.009]
 [2.02 ]
 [1.93 ]
 [1.93 ]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
printing an ep nov before normalisation:  75.03216672580443
actions average: 
K:  3  action  0 :  tensor([    0.7832,     0.0017,     0.0003,     0.0391,     0.1757],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0059,     0.9923,     0.0001,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0003,     0.9814,     0.0011,     0.0171],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0313,     0.0005,     0.0736,     0.7513,     0.1433],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1856, 0.0639, 0.0741, 0.0251, 0.6514], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7417, 0.0028, 0.0014, 0.0754, 0.1788], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9729,     0.0006,     0.0000,     0.0260],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0013,     0.9520,     0.0180,     0.0284],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0268,     0.0003,     0.0040,     0.8861,     0.0828],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3678, 0.0016, 0.0007, 0.1453, 0.4846], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  54.482570872642185
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  8.949107498665398
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4221 1.0 1.0
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  13.012919425964355
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.244]
 [1.033]
 [1.033]
 [1.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.033]
 [1.244]
 [1.033]
 [1.033]
 [1.033]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7128, 0.0346, 0.0024, 0.0648, 0.1854], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9740,     0.0224,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9800,     0.0001,     0.0197],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0563, 0.0112, 0.0160, 0.8029, 0.1135], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2777, 0.0570, 0.0083, 0.1143, 0.5427], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.6410524349502
printing an ep nov before normalisation:  28.896074295043945
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.81149847690636
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4241 1.0 1.0
printing an ep nov before normalisation:  61.049151994457574
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90646386
maxi score, test score, baseline:  0.4261 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.99569034576416
printing an ep nov before normalisation:  78.00832792670137
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4261 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 64.78086056735971
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.59886999279452
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.49093196081566
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.37 ]
 [1.309]
 [1.309]
 [1.309]] [[43.526]
 [49.131]
 [43.526]
 [43.526]
 [43.526]] [[2.305]
 [2.594]
 [2.305]
 [2.305]
 [2.305]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4321 1.0 1.0
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.843]
 [0.945]
 [0.966]
 [0.843]
 [0.843]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.843]
 [0.945]
 [0.966]
 [0.843]
 [0.843]]
printing an ep nov before normalisation:  48.289241790771484
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.858]
 [0.858]
 [0.858]
 [0.858]
 [0.858]] [[79.489]
 [79.489]
 [79.489]
 [79.489]
 [79.489]] [[2.858]
 [2.858]
 [2.858]
 [2.858]
 [2.858]]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.021]
 [1.165]
 [1.021]
 [1.021]
 [1.021]] [[64.928]
 [72.581]
 [64.928]
 [64.928]
 [64.928]] [[1.686]
 [1.958]
 [1.686]
 [1.686]
 [1.686]]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  27.61126599819564
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.758847913910365
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.66043723431594
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
siam score:  -0.88804156
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.342023319791473
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.398]
 [1.164]
 [1.193]
 [1.216]] [[18.179]
 [20.318]
 [15.079]
 [20.74 ]
 [23.076]] [[1.951]
 [2.36 ]
 [1.654]
 [2.193]
 [2.427]]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.561]
 [0.623]
 [0.197]
 [0.491]] [[44.778]
 [35.226]
 [42.604]
 [40.435]
 [37.865]] [[2.541]
 [1.678]
 [2.29 ]
 [1.702]
 [1.805]]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4341 1.0 1.0
printing an ep nov before normalisation:  32.474913597106934
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4341 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.220128322315553
printing an ep nov before normalisation:  25.34888744354248
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.065]
 [0.879]
 [1.205]
 [0.776]
 [1.065]] [[48.915]
 [43.705]
 [63.186]
 [32.9  ]
 [48.915]] [[1.93 ]
 [1.585]
 [2.51 ]
 [1.148]
 [1.93 ]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8985226
actions average: 
K:  1  action  0 :  tensor([    0.7456,     0.0002,     0.0002,     0.0710,     0.1830],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9972,     0.0001,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9703,     0.0007,     0.0288],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0390,     0.0001,     0.0015,     0.8857,     0.0738],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2488, 0.0007, 0.0393, 0.1157, 0.5956], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  92.16388861951506
Printing some Q and Qe and total Qs values:  [[0.978]
 [1.161]
 [0.978]
 [0.817]
 [0.887]] [[ 0.   ]
 [58.625]
 [ 0.   ]
 [25.981]
 [29.502]] [[0.724]
 [1.645]
 [0.724]
 [0.89 ]
 [1.005]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.8624,     0.0003,     0.0005,     0.0008,     0.1359],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9975,     0.0000,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9771,     0.0021,     0.0208],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0159,     0.0001,     0.0023,     0.9111,     0.0706],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2131, 0.0009, 0.1181, 0.1166, 0.5513], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.7748,     0.0010,     0.0005,     0.0871,     0.1366],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0021,     0.9797,     0.0001,     0.0000,     0.0182],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0001,     0.9635,     0.0012,     0.0349],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0015,     0.0001,     0.0325,     0.8778,     0.0881],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2097, 0.0087, 0.0463, 0.1060, 0.6292], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.788]
 [1.04 ]
 [0.788]
 [0.788]
 [0.788]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.788]
 [1.04 ]
 [0.788]
 [0.788]
 [0.788]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.225]
 [1.225]
 [1.225]
 [1.225]
 [1.354]] [[54.444]
 [54.444]
 [54.444]
 [54.444]
 [43.207]] [[2.222]
 [2.222]
 [2.222]
 [2.222]
 [2.021]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4361 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.89893156
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4361 1.0 1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.967]
 [0.979]
 [0.967]
 [0.967]
 [0.967]] [[34.875]
 [47.783]
 [34.875]
 [34.875]
 [34.875]] [[2.14 ]
 [2.979]
 [2.14 ]
 [2.14 ]
 [2.14 ]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.66063665357153
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.7169,     0.0012,     0.0003,     0.1075,     0.1742],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9977,     0.0001,     0.0001,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0000,     0.9534,     0.0019,     0.0445],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0025,     0.0001,     0.0037,     0.9362,     0.0575],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1600, 0.0019, 0.2952, 0.0739, 0.4691], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9054029
printing an ep nov before normalisation:  40.66689809163411
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.931]
 [0.931]
 [1.041]
 [0.931]
 [0.931]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.931]
 [0.931]
 [1.041]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  60.916814097626606
printing an ep nov before normalisation:  71.80754955971554
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
printing an ep nov before normalisation:  0.14608904650458499
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.346]
 [1.346]
 [1.346]
 [1.346]
 [1.346]] [[67.617]
 [67.617]
 [67.617]
 [67.617]
 [67.617]] [[3.346]
 [3.346]
 [3.346]
 [3.346]
 [3.346]]
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4361 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.3548535549585
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4361 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  66.79800172659928
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.909]
 [0.724]
 [0.724]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.724]
 [0.724]
 [0.909]
 [0.724]
 [0.724]]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.651]
 [0.586]
 [0.586]
 [0.586]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.586]
 [0.651]
 [0.586]
 [0.586]
 [0.586]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.386509313462064
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 51.339813519138
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4481 1.0 1.0
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.326]
 [1.326]
 [1.326]
 [1.326]
 [1.326]] [[87.514]
 [87.514]
 [87.514]
 [87.514]
 [87.514]] [[3.297]
 [3.297]
 [3.297]
 [3.297]
 [3.297]]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.23971623702657
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  72.12025534402241
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.121883392333984
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.76942934430035
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.30477785686833
printing an ep nov before normalisation:  48.91472421540415
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  84.01391853976094
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Sims:  25 1 epoch:  75423 pick best:  False frame count:  75423
printing an ep nov before normalisation:  74.44510990812674
siam score:  -0.8876941
maxi score, test score, baseline:  0.4481 1.0 1.0
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  90.83492082767006
Printing some Q and Qe and total Qs values:  [[1.172]
 [1.172]
 [1.172]
 [1.172]
 [1.172]] [[75.716]
 [75.716]
 [75.716]
 [75.716]
 [75.716]] [[2.839]
 [2.839]
 [2.839]
 [2.839]
 [2.839]]
printing an ep nov before normalisation:  54.045993010815785
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.71428579142324
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4521 1.0 1.0
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.978859901428223
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.6745788029262
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  93.98232407424443
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  62.89592042861685
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.57399012974713
printing an ep nov before normalisation:  42.346541101728214
printing an ep nov before normalisation:  0.003829599199036693
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  85.9322885557504
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.88894784
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[1.3 ]
 [1.3 ]
 [1.35]
 [1.3 ]
 [1.3 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.3 ]
 [1.3 ]
 [1.35]
 [1.3 ]
 [1.3 ]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  66.52549173960476
printing an ep nov before normalisation:  77.54203035816315
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  58.30371705527219
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8850392
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  61.91807293454737
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.5157419499175
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.051]
 [1.408]
 [1.307]
 [1.271]
 [1.381]] [[29.309]
 [41.036]
 [46.067]
 [43.47 ]
 [40.087]] [[1.405]
 [2.196]
 [2.281]
 [2.149]
 [2.134]]
printing an ep nov before normalisation:  54.9431093047164
printing an ep nov before normalisation:  49.48567325983593
Printing some Q and Qe and total Qs values:  [[1.24 ]
 [0.314]
 [1.347]
 [1.313]
 [1.326]] [[19.857]
 [21.303]
 [47.331]
 [18.06 ]
 [17.596]] [[1.509]
 [0.623]
 [2.377]
 [1.532]
 [1.533]]
printing an ep nov before normalisation:  80.10492183304095
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.863]
 [1.106]
 [0.863]
 [0.863]
 [0.863]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.863]
 [1.106]
 [0.863]
 [0.863]
 [0.863]]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.15 ]
 [1.15 ]
 [1.173]
 [1.15 ]
 [1.15 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.15 ]
 [1.15 ]
 [1.173]
 [1.15 ]
 [1.15 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.7465, 0.0014, 0.0017, 0.1032, 0.1472], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0021,     0.9974,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0001,     0.9800,     0.0004,     0.0192],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0728,     0.0002,     0.0062,     0.8668,     0.0540],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3568, 0.0064, 0.0007, 0.1008, 0.5353], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.27110137373897
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.654276946094406
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.80007907322475
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.35236095351919
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.628044707265005
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([0.7162, 0.0010, 0.0018, 0.0782, 0.2029], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0207,     0.9473,     0.0226,     0.0000,     0.0094],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9764,     0.0003,     0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0938, 0.0014, 0.0293, 0.7140, 0.1616], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0563, 0.0014, 0.4937, 0.0529, 0.3956], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  84.99943857768328
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.88721704
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.136]
 [1.156]
 [0.887]
 [1.136]
 [1.136]] [[50.715]
 [47.695]
 [32.907]
 [50.715]
 [50.715]] [[3.377]
 [3.156]
 [1.706]
 [3.377]
 [3.377]]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.02733938637489
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8877261
Printing some Q and Qe and total Qs values:  [[1.084]
 [1.397]
 [1.337]
 [1.062]
 [1.311]] [[21.771]
 [27.475]
 [26.333]
 [30.58 ]
 [24.35 ]] [[2.004]
 [2.559]
 [2.451]
 [2.356]
 [2.341]]
printing an ep nov before normalisation:  32.83474922180176
maxi score, test score, baseline:  0.4861 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]] [[26.28]
 [26.28]
 [26.28]
 [26.28]
 [26.28]] [[0.836]
 [0.836]
 [0.836]
 [0.836]
 [0.836]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.31220626266263
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.264]
 [1.417]
 [1.264]
 [1.264]] [[52.186]
 [52.186]
 [69.595]
 [52.186]
 [52.186]] [[1.918]
 [1.918]
 [2.326]
 [1.918]
 [1.918]]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88567317
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.5437, 0.0007, 0.0019, 0.1730, 0.2808], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9984,     0.0001,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0050,     0.9600,     0.0002,     0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0896,     0.0005,     0.0327,     0.6992,     0.1780],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2392, 0.0182, 0.2136, 0.1337, 0.3953], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  94.80405036750949
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.8765, 0.0155, 0.0013, 0.0035, 0.1032], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0050,     0.9684,     0.0003,     0.0004,     0.0258],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0007,     0.0000,     0.9556,     0.0032,     0.0405],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0009,     0.0000,     0.0011,     0.9487,     0.0493],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0681,     0.0004,     0.2752,     0.0598,     0.5966],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.422]
 [1.446]
 [1.404]
 [1.349]
 [1.422]] [[94.453]
 [91.133]
 [93.378]
 [95.324]
 [94.453]] [[2.725]
 [2.69 ]
 [2.687]
 [2.667]
 [2.725]]
Printing some Q and Qe and total Qs values:  [[1.26 ]
 [1.26 ]
 [1.404]
 [1.26 ]
 [1.26 ]] [[56.187]
 [56.187]
 [67.676]
 [56.187]
 [56.187]] [[2.263]
 [2.263]
 [2.737]
 [2.263]
 [2.263]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.14100472765933
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.175]
 [1.041]
 [1.187]
 [0.273]
 [0.999]] [[52.039]
 [48.049]
 [44.651]
 [46.618]
 [43.613]] [[2.323]
 [2.041]
 [2.061]
 [1.219]
 [1.834]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.894107597261154
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.025467049737586
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([    0.8471,     0.0027,     0.0002,     0.0466,     0.1034],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0054,     0.9808,     0.0069,     0.0000,     0.0069],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9767,     0.0031,     0.0200],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0039,     0.0002,     0.0275,     0.8729,     0.0956],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1226, 0.0076, 0.0785, 0.1574, 0.6339], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.354177474975586
printing an ep nov before normalisation:  91.365508054972
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.084]
 [0.98 ]
 [0.98 ]
 [0.98 ]] [[52.207]
 [55.894]
 [52.207]
 [52.207]
 [52.207]] [[1.751]
 [1.943]
 [1.751]
 [1.751]
 [1.751]]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.365760929157375
printing an ep nov before normalisation:  80.34742116480032
printing an ep nov before normalisation:  46.73856862720072
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  84.72395346705852
actions average: 
K:  4  action  0 :  tensor([    0.8638,     0.0028,     0.0007,     0.0434,     0.0893],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0010,     0.9941,     0.0001,     0.0000,     0.0048],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0025,     0.9357,     0.0002,     0.0610],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1262, 0.0008, 0.0047, 0.7213, 0.1470], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2194, 0.0244, 0.1698, 0.0030, 0.5835], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.6794473761105
printing an ep nov before normalisation:  82.46164732748356
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  91.08985286517438
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.993]
 [1.4  ]
 [1.265]
 [1.262]
 [1.374]] [[12.964]
 [12.846]
 [ 7.933]
 [12.334]
 [16.382]] [[1.421]
 [1.825]
 [1.527]
 [1.67 ]
 [1.916]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.001606627469641353
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.728415489196777
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.817]
 [0.815]
 [0.817]] [[83.992]
 [84.582]
 [80.137]
 [86.461]
 [80.137]] [[0.815]
 [0.815]
 [0.817]
 [0.815]
 [0.817]]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.558111667633057
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.81376493879245
printing an ep nov before normalisation:  95.74070913370154
deleting a thread, now have 1 threads
Frames:  79844 train batches done:  9354 episodes:  4867
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.8703,     0.0012,     0.0002,     0.0369,     0.0914],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9992,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0000,     0.9451,     0.0020,     0.0525],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0810,     0.0006,     0.0283,     0.8473,     0.0428],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1895, 0.0486, 0.1984, 0.0934, 0.4701], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  24.467965534755162
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.996]
 [0.974]
 [0.974]
 [0.974]] [[83.112]
 [80.02 ]
 [83.112]
 [83.112]
 [83.112]] [[0.974]
 [0.996]
 [0.974]
 [0.974]
 [0.974]]
actions average: 
K:  0  action  0 :  tensor([    0.6935,     0.0005,     0.0003,     0.1064,     0.1994],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9991,     0.0001,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9784,     0.0003,     0.0210],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0879,     0.0003,     0.0005,     0.7908,     0.1204],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2722, 0.0005, 0.1998, 0.1150, 0.4125], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  74.34777714911708
printing an ep nov before normalisation:  64.69356506106618
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.701]
 [0.717]
 [0.865]
 [0.067]
 [0.801]] [[30.434]
 [43.121]
 [47.748]
 [34.151]
 [42.544]] [[0.701]
 [0.717]
 [0.865]
 [0.067]
 [0.801]]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.04142314864452601
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0006953184475833041
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
printing an ep nov before normalisation:  50.25938629969841
printing an ep nov before normalisation:  49.411625718210885
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.072]
 [1.295]
 [0.959]
 [1.129]] [[34.788]
 [48.37 ]
 [60.354]
 [39.1  ]
 [38.388]] [[1.193]
 [1.325]
 [1.656]
 [1.127]
 [1.29 ]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  103.52917806601444
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5521 1.0 1.0
maxi score, test score, baseline:  0.5521 1.0 1.0
maxi score, test score, baseline:  0.5521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.636255806772613
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.422]
 [1.377]
 [1.28 ]
 [1.315]] [[15.85 ]
 [29.879]
 [28.575]
 [19.491]
 [15.694]] [[1.278]
 [1.585]
 [1.527]
 [1.341]
 [1.338]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8571,     0.0003,     0.0017,     0.0010,     0.1400],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9954,     0.0002,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9593,     0.0006,     0.0401],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0365,     0.0001,     0.0012,     0.9066,     0.0556],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2437, 0.0009, 0.0054, 0.0631, 0.6869], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5601 1.0 1.0
printing an ep nov before normalisation:  48.639224354631644
printing an ep nov before normalisation:  30.019523910877087
maxi score, test score, baseline:  0.5601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5621 1.0 1.0
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5621 1.0 1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.355]
 [1.355]
 [1.355]
 [1.355]
 [1.355]] [[30.233]
 [30.233]
 [30.233]
 [30.233]
 [30.233]] [[1.757]
 [1.757]
 [1.757]
 [1.757]
 [1.757]]
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5640999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.23 ]
 [1.092]
 [1.092]
 [1.092]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.092]
 [1.23 ]
 [1.092]
 [1.092]
 [1.092]]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.47245658809447
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5660999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5680999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.266]
 [1.266]
 [1.339]
 [1.266]
 [1.266]] [[49.564]
 [49.564]
 [74.713]
 [49.564]
 [49.564]] [[1.403]
 [1.403]
 [1.587]
 [1.403]
 [1.403]]
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.55179590450499
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.8853203097399
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.255]
 [1.255]
 [1.356]
 [1.255]
 [1.255]] [[49.77 ]
 [49.77 ]
 [72.162]
 [49.77 ]
 [49.77 ]] [[1.795]
 [1.795]
 [2.349]
 [1.795]
 [1.795]]
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.902328479817243
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.087]
 [1.093]
 [1.087]
 [1.087]] [[38.512]
 [38.512]
 [42.9  ]
 [38.512]
 [38.512]] [[1.375]
 [1.375]
 [1.441]
 [1.375]
 [1.375]]
siam score:  -0.8743975
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.75870729963977
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5720999999999999 1.0 1.0
printing an ep nov before normalisation:  30.52996835081513
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.868]
 [0.937]
 [0.783]
 [0.868]] [[20.565]
 [20.565]
 [28.958]
 [30.134]
 [20.565]] [[0.868]
 [0.868]
 [0.937]
 [0.783]
 [0.868]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.450349172487456
using explorer policy with actor:  1
siam score:  -0.8760589
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.87657636
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.451]
 [1.336]
 [1.227]
 [1.339]] [[75.369]
 [70.383]
 [74.524]
 [82.901]
 [75.369]] [[1.63 ]
 [1.714]
 [1.623]
 [1.56 ]
 [1.63 ]]
printing an ep nov before normalisation:  63.41683174901054
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.92612569477915
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.5683828209048158
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.426]
 [1.426]
 [1.4  ]
 [1.109]
 [1.426]] [[34.503]
 [34.503]
 [45.238]
 [34.133]
 [34.503]] [[2.619]
 [2.619]
 [2.965]
 [2.29 ]
 [2.619]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.916]
 [1.121]
 [0.916]
 [0.916]
 [0.916]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.916]
 [1.121]
 [0.916]
 [0.916]
 [0.916]]
siam score:  -0.8745488
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.767]
 [0.674]
 [0.674]
 [0.674]] [[41.476]
 [46.846]
 [41.476]
 [41.476]
 [41.476]] [[0.674]
 [0.767]
 [0.674]
 [0.674]
 [0.674]]
maxi score, test score, baseline:  0.5740999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  92.0587976605058
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5761 1.0 1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.83098616185865
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]] [[59.035]
 [59.035]
 [59.035]
 [59.035]
 [59.035]] [[0.724]
 [0.724]
 [0.724]
 [0.724]
 [0.724]]
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5761 1.0 1.0
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.17 ]
 [1.19 ]
 [1.327]
 [1.182]
 [1.186]] [[32.849]
 [53.335]
 [57.88 ]
 [40.985]
 [36.29 ]] [[1.305]
 [1.543]
 [1.729]
 [1.403]
 [1.358]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.007144775691145
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.822916384001736
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.273]
 [0.261]
 [0.252]
 [0.279]
 [0.252]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.273]
 [0.261]
 [0.252]
 [0.279]
 [0.252]]
maxi score, test score, baseline:  0.5821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8575595
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85458356
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8055,     0.0031,     0.0003,     0.0967,     0.0945],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9990,     0.0001,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0002,     0.9249,     0.0003,     0.0743],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0180,     0.0008,     0.0028,     0.9461,     0.0323],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2297, 0.0014, 0.1151, 0.1628, 0.4910], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8547861
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.5983],
        [0.5334],
        [0.7945],
        [0.0000],
        [0.8144],
        [0.6501],
        [0.0000],
        [0.4517],
        [0.8077],
        [0.6880]], dtype=torch.float64)
0.0 0.5982859566107015
0.0 0.5334394827143714
0.0 0.7944587830669945
0.0 0.0
0.0 0.8144335462056175
0.0 0.6500834863237709
0.0 0.0
0.0 0.4516556568591564
0.0 0.8076627499718829
0.0 0.6879732842671865
maxi score, test score, baseline:  0.5841 1.0 1.0
printing an ep nov before normalisation:  29.236203610902695
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.5879005180716
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.51011290757993
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.37447748561584
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86187
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.317055225372314
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.74633787081394
maxi score, test score, baseline:  0.5861 1.0 1.0
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.22268500397222
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  19.087595127414566
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.321]
 [1.396]
 [1.297]
 [1.321]
 [1.321]] [[39.696]
 [40.599]
 [63.497]
 [39.696]
 [39.696]] [[2.001]
 [2.103]
 [2.687]
 [2.001]
 [2.001]]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.685845851898193
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5881 1.0 1.0
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8620878
maxi score, test score, baseline:  0.5881 1.0 1.0
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.044345496758275
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8546634
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.34222137927327
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.5172],
        [0.5658],
        [0.9374],
        [0.6201],
        [0.0000],
        [0.6961],
        [0.5094],
        [0.6857],
        [0.1795]], dtype=torch.float64)
0.0 0.0
0.0 0.5171955329708794
0.0 0.5657572036437555
0.0 0.9373641868492169
0.0 0.620097603199804
0.9801 0.9801
0.0 0.6961428331188579
0.0 0.5093724273862417
0.0 0.685724565721588
0.0 0.17952981241804317
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.063432367868394
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5961 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5981 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 54.59531678028803
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  93.3518035973233
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.234436786434784
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.289]
 [1.289]
 [1.361]
 [1.29 ]
 [1.289]] [[37.715]
 [37.715]
 [52.942]
 [37.584]
 [37.715]] [[1.795]
 [1.795]
 [2.179]
 [1.793]
 [1.795]]
Printing some Q and Qe and total Qs values:  [[1.329]
 [1.404]
 [1.372]
 [1.231]
 [1.329]] [[27.997]
 [34.694]
 [51.479]
 [39.52 ]
 [27.997]] [[1.649]
 [1.857]
 [2.156]
 [1.779]
 [1.649]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8418808
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.769]
 [0.887]
 [0.773]
 [0.793]] [[44.081]
 [44.081]
 [42.685]
 [25.583]
 [23.915]] [[0.769]
 [0.769]
 [0.887]
 [0.773]
 [0.793]]
maxi score, test score, baseline:  0.6041 1.0 1.0
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8397685
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.63645076751709
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8460105
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  92.005219157555
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  78.32247492232068
siam score:  -0.8448115
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.59286069813643
actions average: 
K:  4  action  0 :  tensor([    0.9306,     0.0003,     0.0004,     0.0029,     0.0659],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0013,     0.9928,     0.0005,     0.0002,     0.0052],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0000,     0.9607,     0.0041,     0.0349],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0018,     0.9483,     0.0498],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1650, 0.0039, 0.3171, 0.0746, 0.4394], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.780901616096095
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
siam score:  -0.8481995
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.18 ]
 [1.18 ]
 [1.265]
 [1.18 ]
 [1.18 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.18 ]
 [1.18 ]
 [1.265]
 [1.18 ]
 [1.18 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6141 1.0 1.0
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.47520128727781
siam score:  -0.8492659
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8507526
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.60161225499366
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.21239755180741
Printing some Q and Qe and total Qs values:  [[1.357]
 [1.357]
 [1.225]
 [1.231]
 [1.357]] [[68.27 ]
 [68.27 ]
 [80.394]
 [81.754]
 [68.27 ]] [[2.508]
 [2.508]
 [2.639]
 [2.674]
 [2.508]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
siam score:  -0.8587258
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6161 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85382515
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6221 1.0 1.0
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 83.01021227026514
Printing some Q and Qe and total Qs values:  [[1.294]
 [1.341]
 [1.294]
 [1.294]
 [1.294]] [[22.646]
 [45.481]
 [22.646]
 [22.646]
 [22.646]] [[1.575]
 [2.296]
 [1.575]
 [1.575]
 [1.575]]
maxi score, test score, baseline:  0.6241 1.0 1.0
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.841]
 [0.841]
 [1.151]
 [0.841]
 [0.841]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.841]
 [0.841]
 [1.151]
 [0.841]
 [0.841]]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.447885462561736
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.66770312899093
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.305]
 [1.478]
 [1.305]
 [1.305]
 [1.305]] [[28.145]
 [32.405]
 [28.145]
 [28.145]
 [28.145]] [[1.542]
 [1.777]
 [1.542]
 [1.542]
 [1.542]]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.85756599426626
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.801109550953242
Printing some Q and Qe and total Qs values:  [[1.356]
 [1.356]
 [1.149]
 [1.356]
 [1.356]] [[27.734]
 [27.734]
 [64.364]
 [27.734]
 [27.734]] [[1.845]
 [1.845]
 [2.571]
 [1.845]
 [1.845]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([0.7908, 0.0762, 0.0012, 0.0381, 0.0936], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9980,     0.0006,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0000,     0.9627,     0.0026,     0.0345],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0734,     0.0000,     0.0710,     0.8349,     0.0207],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1713, 0.0887, 0.1993, 0.1159, 0.4249], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.991226750515853
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.32872286391131
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6261 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 71.57566277386397
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  106.42235053640285
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  14.202849273612504
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.116]
 [1.242]
 [1.223]
 [1.116]
 [1.116]] [[39.659]
 [27.462]
 [50.922]
 [39.659]
 [39.659]] [[2.124]
 [1.888]
 [2.564]
 [2.124]
 [2.124]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [1.053]
 [0.786]
 [0.786]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.786]
 [0.786]
 [1.053]
 [0.786]
 [0.786]]
printing an ep nov before normalisation:  29.5269520724422
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.22 ]
 [1.22 ]
 [1.222]
 [1.181]
 [1.22 ]] [[39.424]
 [39.424]
 [78.729]
 [45.557]
 [39.424]] [[1.675]
 [1.675]
 [2.541]
 [1.771]
 [1.675]]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  87.97268216673993
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.14620770240732
Printing some Q and Qe and total Qs values:  [[1.323]
 [1.323]
 [1.345]
 [1.323]
 [1.323]] [[16.564]
 [16.564]
 [44.936]
 [16.564]
 [16.564]] [[1.733]
 [1.733]
 [2.457]
 [1.733]
 [1.733]]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.23376551128444
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6281 1.0 1.0
printing an ep nov before normalisation:  84.51345082586832
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.253]
 [1.224]
 [1.226]
 [1.215]] [[37.103]
 [36.517]
 [35.317]
 [36.813]
 [44.238]] [[1.719]
 [1.863]
 [1.794]
 [1.845]
 [2.076]]
printing an ep nov before normalisation:  55.83360635426041
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6281 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 62.562752828444864
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.34]
 [1.34]
 [1.34]
 [1.34]
 [1.34]] [[54.716]
 [54.716]
 [54.716]
 [54.716]
 [54.716]] [[3.007]
 [3.007]
 [3.007]
 [3.007]
 [3.007]]
maxi score, test score, baseline:  0.6281 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.2  ]
 [1.2  ]
 [1.244]
 [1.2  ]
 [1.2  ]] [[77.106]
 [77.106]
 [79.259]
 [77.106]
 [77.106]] [[2.033]
 [2.033]
 [2.113]
 [2.033]
 [2.033]]
maxi score, test score, baseline:  0.6281 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9376,     0.0410,     0.0001,     0.0067,     0.0147],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0013,     0.9829,     0.0012,     0.0000,     0.0147],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9317,     0.0010,     0.0670],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0000,     0.0003,     0.9957,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1119, 0.0100, 0.1505, 0.0173, 0.7103], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.857135
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6301 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.94 ]
 [1.026]
 [0.973]
 [0.952]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.936]
 [0.94 ]
 [1.026]
 [0.973]
 [0.952]]
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6341 1.0 1.0
maxi score, test score, baseline:  0.6341 1.0 1.0
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.4381],
        [0.8494],
        [0.7878],
        [0.9257],
        [0.8477],
        [0.0000],
        [0.0000],
        [0.2508],
        [0.8494]], dtype=torch.float64)
0.0 0.0
0.0 0.43812860398501297
0.0 0.8493973927192087
0.0 0.7878251258022829
0.0 0.9256764271073201
0.0 0.8477001736398435
0.970299 0.970299
0.0 0.0
0.0 0.25083336792287514
0.0 0.8493973927192087
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8461799
maxi score, test score, baseline:  0.6341 1.0 1.0
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6341 1.0 1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  92.22123853931808
maxi score, test score, baseline:  0.6361 1.0 1.0
printing an ep nov before normalisation:  38.56276512145996
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.84324783
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  91.82157040558253
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.069]
 [1.204]
 [1.069]
 [1.069]
 [1.069]] [[42.101]
 [66.753]
 [42.101]
 [42.101]
 [42.101]] [[1.62 ]
 [2.368]
 [1.62 ]
 [1.62 ]
 [1.62 ]]
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.658]
 [0.658]
 [0.943]
 [0.658]] [[56.221]
 [56.221]
 [56.221]
 [84.451]
 [56.221]] [[1.357]
 [1.357]
 [1.357]
 [2.312]
 [1.357]]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.8576091948988
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6381 1.0 1.0
maxi score, test score, baseline:  0.6381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6381 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6381 1.0 1.0
printing an ep nov before normalisation:  20.11326789855957
printing an ep nov before normalisation:  24.218497276306152
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.0248490526663
maxi score, test score, baseline:  0.6401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8163,     0.0046,     0.0008,     0.0295,     0.1487],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9984,     0.0003,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0010,     0.0006,     0.8718,     0.0344,     0.0923],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0314,     0.0001,     0.0015,     0.9440,     0.0230],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2242, 0.0026, 0.1387, 0.0863, 0.5482], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.663]
 [0.693]
 [0.182]
 [0.657]
 [0.654]] [[ 0.   ]
 [51.743]
 [34.286]
 [39.581]
 [36.34 ]] [[0.663]
 [0.693]
 [0.182]
 [0.657]
 [0.654]]
maxi score, test score, baseline:  0.6421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.426]
 [1.426]
 [1.426]
 [1.426]
 [1.426]] [[79.215]
 [79.215]
 [79.215]
 [79.215]
 [79.215]] [[2.759]
 [2.759]
 [2.759]
 [2.759]
 [2.759]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.474]
 [1.474]
 [1.474]
 [1.474]] [[36.784]
 [36.784]
 [36.784]
 [36.784]
 [36.784]] [[2.089]
 [2.089]
 [2.089]
 [2.089]
 [2.089]]
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.82362651824951
printing an ep nov before normalisation:  23.054648521544657
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  64.79479167071611
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([    0.9777,     0.0004,     0.0000,     0.0086,     0.0132],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9934,     0.0009,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9715,     0.0006,     0.0276],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0321,     0.0001,     0.0004,     0.9311,     0.0363],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0647, 0.0023, 0.1871, 0.0528, 0.6930], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6481 1.0 1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  102.46784110508275
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.8207,     0.0032,     0.0005,     0.0240,     0.1516],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9977,     0.0000,     0.0001,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9519,     0.0012,     0.0465],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0222,     0.0001,     0.0008,     0.9476,     0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.1474,     0.0319,     0.1852,     0.0005,     0.6351],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6481 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9884,     0.0025,     0.0000,     0.0002,     0.0088],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9988,     0.0003,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0001,     0.9595,     0.0057,     0.0343],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0235,     0.0002,     0.0003,     0.9041,     0.0719],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1346, 0.0231, 0.1711, 0.0843, 0.5868], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6481 1.0 1.0
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.45121758622514
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6541 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6541 1.0 1.0
printing an ep nov before normalisation:  81.39091595674189
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.259]
 [1.265]
 [1.259]
 [1.259]
 [1.259]] [[39.835]
 [52.009]
 [39.835]
 [39.835]
 [39.835]] [[1.907]
 [2.248]
 [1.907]
 [1.907]
 [1.907]]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.04883783236744
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 18.71272087097168
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.337226181511085
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.200151358524682
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.3942042659403455
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.24 ]
 [1.24 ]
 [1.342]
 [1.24 ]
 [1.24 ]] [[39.599]
 [39.599]
 [54.048]
 [39.599]
 [39.599]] [[1.938]
 [1.938]
 [2.525]
 [1.938]
 [1.938]]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  100.46277861434854
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.   ]
 [0.818]
 [0.764]
 [0.764]] [[46.753]
 [40.572]
 [62.181]
 [46.753]
 [46.753]] [[0.764]
 [0.   ]
 [0.818]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.60895637454415
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.242]
 [1.314]
 [1.242]
 [1.214]
 [1.242]] [[66.349]
 [62.336]
 [66.349]
 [79.569]
 [66.349]] [[2.39 ]
 [2.367]
 [2.39 ]
 [2.677]
 [2.39 ]]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6521 1.0 1.0
printing an ep nov before normalisation:  27.102863788604736
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6521 1.0 1.0
printing an ep nov before normalisation:  45.718143452459174
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.79691371583075
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.190348625183105
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6501 1.0 1.0
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.096336364746094
maxi score, test score, baseline:  0.6501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [1.258]
 [1.02 ]
 [1.02 ]
 [1.02 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.02 ]
 [1.258]
 [1.02 ]
 [1.02 ]
 [1.02 ]]
maxi score, test score, baseline:  0.6481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6481 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.249]
 [1.249]
 [1.342]
 [1.249]
 [1.249]] [[68.424]
 [68.424]
 [88.496]
 [68.424]
 [68.424]] [[1.882]
 [1.882]
 [2.217]
 [1.882]
 [1.882]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.82066286
maxi score, test score, baseline:  0.6481 1.0 1.0
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9930,     0.0030,     0.0001,     0.0001,     0.0039],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9976,     0.0000,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0025, 0.0012, 0.9416, 0.0041, 0.0506], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0051,     0.9817,     0.0132],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1440, 0.0169, 0.2515, 0.0097, 0.5779], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.86260965135362
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.49420926784529
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  77.657060077115
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  81.06986336283695
printing an ep nov before normalisation:  25.817105269841207
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.974]
 [0.515]
 [0.547]
 [0.831]
 [0.83 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.974]
 [0.515]
 [0.547]
 [0.831]
 [0.83 ]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([0.7819, 0.0010, 0.0031, 0.0692, 0.1448], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0051,     0.9940,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0004,     0.9591,     0.0003,     0.0401],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0187,     0.0003,     0.0013,     0.9669,     0.0128],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1993, 0.0012, 0.3483, 0.0627, 0.3885], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.41233178379044
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6621 1.0 1.0
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.357]
 [1.357]
 [1.393]
 [1.357]
 [1.357]] [[55.043]
 [55.043]
 [70.037]
 [55.043]
 [55.043]] [[2.141]
 [2.141]
 [2.472]
 [2.141]
 [2.141]]
Printing some Q and Qe and total Qs values:  [[1.411]
 [1.411]
 [1.399]
 [1.411]
 [1.411]] [[34.341]
 [34.341]
 [50.993]
 [34.341]
 [34.341]] [[1.944]
 [1.944]
 [2.378]
 [1.944]
 [1.944]]
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.073]
 [1.146]
 [1.049]
 [1.073]
 [1.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.073]
 [1.146]
 [1.049]
 [1.073]
 [1.073]]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.649999442431884
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.248]
 [1.248]
 [1.357]
 [1.175]
 [1.248]] [[47.821]
 [47.821]
 [50.419]
 [47.398]
 [47.821]] [[2.117]
 [2.117]
 [2.315]
 [2.03 ]
 [2.117]]
actions average: 
K:  0  action  0 :  tensor([    0.8477,     0.0016,     0.0002,     0.0374,     0.1132],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0011,     0.9976,     0.0000,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0002,     0.9597,     0.0002,     0.0396],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0402,     0.0003,     0.0019,     0.8859,     0.0717],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1974, 0.0411, 0.2178, 0.1345, 0.4092], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.84 ]
 [0.894]
 [0.84 ]
 [0.84 ]] [[50.481]
 [50.481]
 [62.891]
 [50.481]
 [50.481]] [[0.84 ]
 [0.84 ]
 [0.894]
 [0.84 ]
 [0.84 ]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6681 1.0 1.0
maxi score, test score, baseline:  0.6681 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.78370809421614
printing an ep nov before normalisation:  23.095180988311768
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  95.32869077484604
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.43 ]
 [1.486]
 [1.397]
 [1.345]
 [1.43 ]] [[38.403]
 [40.238]
 [48.706]
 [37.66 ]
 [38.403]] [[2.044]
 [2.148]
 [2.276]
 [1.94 ]
 [2.044]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.081008911132812
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.113]
 [1.385]
 [1.318]
 [1.213]
 [1.254]] [[14.81 ]
 [25.516]
 [28.783]
 [18.86 ]
 [16.71 ]] [[1.374]
 [1.836]
 [1.827]
 [1.547]
 [1.549]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  16.24860286358344
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6701 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.281]
 [0.302]
 [0.375]
 [0.296]
 [0.284]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.281]
 [0.302]
 [0.375]
 [0.296]
 [0.284]]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.21334206358263
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6741 1.0 1.0
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.07396541513155
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.78594678290873
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.34954067488766
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6741 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8225616
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  24.695920944213867
printing an ep nov before normalisation:  24.50404167175293
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.128]
 [1.098]
 [1.163]
 [1.098]
 [1.146]] [[23.364]
 [54.28 ]
 [27.349]
 [54.28 ]
 [26.103]] [[2.573]
 [6.849]
 [3.163]
 [6.849]
 [2.972]]
maxi score, test score, baseline:  0.6741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.185]
 [1.185]
 [1.185]
 [1.185]
 [1.185]] [[62.558]
 [62.558]
 [62.558]
 [62.558]
 [62.558]] [[2.535]
 [2.535]
 [2.535]
 [2.535]
 [2.535]]
maxi score, test score, baseline:  0.6741 1.0 1.0
maxi score, test score, baseline:  0.6741 1.0 1.0
siam score:  -0.82218915
actions average: 
K:  2  action  0 :  tensor([    0.8918,     0.0008,     0.0001,     0.0492,     0.0580],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9958,     0.0005,     0.0000,     0.0026],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0001,     0.9372,     0.0009,     0.0615],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0001,     0.0022,     0.9754,     0.0219],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.2752,     0.0004,     0.1080,     0.0858,     0.5307],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.128177971757708
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.7986,     0.0014,     0.0006,     0.0105,     0.1888],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9990,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0005,     0.9377,     0.0003,     0.0612],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0236,     0.0001,     0.0660,     0.8794,     0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0228, 0.0290, 0.2633, 0.2825, 0.4025], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.36322832107544
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.447287559509277
maxi score, test score, baseline:  0.6761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.10303497314453
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.86458530736682
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6781 1.0 1.0
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.811463
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6781 1.0 1.0
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8057756
maxi score, test score, baseline:  0.6781 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.429]
 [1.456]
 [1.431]
 [1.329]
 [1.391]] [[107.969]
 [101.596]
 [103.133]
 [107.485]
 [109.702]] [[2.083]
 [2.066]
 [2.052]
 [1.98 ]
 [2.057]]
maxi score, test score, baseline:  0.6781 1.0 1.0
printing an ep nov before normalisation:  35.263946102314264
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([0.8081, 0.0059, 0.0019, 0.0431, 0.1410], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9972,     0.0002,     0.0001,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0011,     0.9118,     0.0072,     0.0793],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0404, 0.0037, 0.0030, 0.9237, 0.0292], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0009, 0.5646, 0.0012, 0.4326], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.31044657118615
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.625]
 [0.772]
 [0.625]
 [0.625]
 [0.625]] [[47.019]
 [79.127]
 [47.019]
 [47.019]
 [47.019]] [[0.625]
 [0.772]
 [0.625]
 [0.625]
 [0.625]]
printing an ep nov before normalisation:  35.75684309005737
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.6801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.042]
 [1.283]
 [1.042]
 [1.042]] [[45.323]
 [45.323]
 [79.724]
 [45.323]
 [45.323]] [[1.566]
 [1.566]
 [2.415]
 [1.566]
 [1.566]]
printing an ep nov before normalisation:  48.4586435996312
maxi score, test score, baseline:  0.6821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.414]
 [1.414]
 [1.414]
 [1.414]
 [1.414]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.804021
maxi score, test score, baseline:  0.6841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.54508590698242
printing an ep nov before normalisation:  49.70216505557562
printing an ep nov before normalisation:  39.312306432572015
maxi score, test score, baseline:  0.6841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.863]
 [0.912]
 [0.714]
 [0.809]] [[50.469]
 [50.469]
 [65.641]
 [61.278]
 [57.556]] [[0.863]
 [0.863]
 [0.912]
 [0.714]
 [0.809]]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.19789125338758
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.408]
 [1.127]
 [1.127]
 [1.127]] [[11.013]
 [16.11 ]
 [13.821]
 [13.821]
 [13.821]] [[1.256]
 [1.586]
 [1.269]
 [1.269]
 [1.269]]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.27825730713099
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.19]
 [1.19]
 [1.19]
 [1.19]
 [1.19]] [[55.865]
 [55.865]
 [55.865]
 [55.865]
 [55.865]] [[112.921]
 [112.921]
 [112.921]
 [112.921]
 [112.921]]
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  76.7879129001321
printing an ep nov before normalisation:  12.824773788452148
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.2163357253667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.622438430786133
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[26.497]
 [26.497]
 [26.497]
 [26.497]
 [26.497]] [[2.4]
 [2.4]
 [2.4]
 [2.4]
 [2.4]]
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6880999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6900999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6900999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6920999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6920999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.80140775
printing an ep nov before normalisation:  34.86192226409912
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.943]
 [0.97 ]
 [0.943]
 [0.943]
 [0.943]] [[105.635]
 [108.267]
 [105.635]
 [105.635]
 [105.635]] [[0.943]
 [0.97 ]
 [0.943]
 [0.943]
 [0.943]]
maxi score, test score, baseline:  0.6920999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.189]
 [1.189]
 [1.209]
 [1.189]
 [1.189]] [[41.221]
 [41.221]
 [63.074]
 [41.221]
 [41.221]] [[1.824]
 [1.824]
 [2.485]
 [1.824]
 [1.824]]
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.571769783064774
printing an ep nov before normalisation:  9.228274907516578
maxi score, test score, baseline:  0.6940999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  102.50871936848547
printing an ep nov before normalisation:  33.119827942828714
printing an ep nov before normalisation:  48.90898483106397
maxi score, test score, baseline:  0.6960999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6960999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  44.682620151214536
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.989979655937795
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.20388126373291
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.50150138157025
actions average: 
K:  4  action  0 :  tensor([    0.8819,     0.0339,     0.0004,     0.0003,     0.0836],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9971,     0.0001,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0219,     0.8947,     0.0001,     0.0829],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0022, 0.0011, 0.0832, 0.8394, 0.0741], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1056, 0.0068, 0.3260, 0.0690, 0.4926], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.24670784745507
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.156]
 [1.374]
 [0.097]
 [1.234]] [[21.708]
 [27.285]
 [43.7  ]
 [29.445]
 [25.89 ]] [[1.258]
 [1.334]
 [1.806]
 [0.309]
 [1.391]]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  67.80583093533596
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  95.28413476267163
printing an ep nov before normalisation:  86.78005792055194
printing an ep nov before normalisation:  96.96820932861036
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8027341
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.167]
 [1.266]
 [1.167]
 [1.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.167]
 [1.167]
 [1.266]
 [1.167]
 [1.167]]
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  103.14733367569127
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  88.36971934742061
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.73023976671311
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7001 1.0 1.0
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7001 1.0 1.0
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.6980999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8665,     0.0431,     0.0001,     0.0002,     0.0902],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9995,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0022, 0.0098, 0.9096, 0.0027, 0.0757], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0729, 0.0012, 0.0018, 0.8243, 0.0999], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1669, 0.0014, 0.2361, 0.0875, 0.5081], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.362]
 [1.362]
 [1.362]
 [1.362]
 [1.362]] [[19.028]
 [19.028]
 [19.028]
 [19.028]
 [19.028]] [[1.746]
 [1.746]
 [1.746]
 [1.746]
 [1.746]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.291]
 [1.291]
 [1.271]
 [1.291]
 [1.291]] [[77.696]
 [77.696]
 [88.766]
 [77.696]
 [77.696]] [[2.874]
 [2.874]
 [3.112]
 [2.874]
 [2.874]]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.1285293896993
siam score:  -0.7998014
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7992618
printing an ep nov before normalisation:  61.9083316361321
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  66.90896161833238
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.54748667125865
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.59469340552498
printing an ep nov before normalisation:  48.76308141777537
siam score:  -0.79478747
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.62969613299228
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.93005625151957
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.798971
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.86038929618179
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  18.91235353073669
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.388]
 [0.388]
 [0.388]
 [0.388]
 [0.388]]
printing an ep nov before normalisation:  28.24540138244629
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79590243
printing an ep nov before normalisation:  47.31369566947104
siam score:  -0.7968193
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  61.38455584943127
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  22.951360532078155
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7041 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7041 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7957835
maxi score, test score, baseline:  0.7061 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([    0.8124,     0.0044,     0.0003,     0.0382,     0.1447],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0000,     0.9365,     0.0005,     0.0626],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0763,     0.0003,     0.0002,     0.8364,     0.0868],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1211, 0.0020, 0.1760, 0.0520, 0.6489], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.186]
 [1.233]
 [1.186]
 [1.186]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.186]
 [1.186]
 [1.233]
 [1.186]
 [1.186]]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.793539
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.45989370346069
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.362]
 [1.362]
 [1.27 ]
 [1.362]
 [1.362]] [[25.967]
 [25.967]
 [52.889]
 [25.967]
 [25.967]] [[1.691]
 [1.691]
 [2.18 ]
 [1.691]
 [1.691]]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7061 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.913100094406097
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.2085033040949
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.21 ]
 [1.21 ]
 [1.347]
 [1.224]
 [1.268]] [[34.406]
 [34.406]
 [31.682]
 [38.072]
 [27.413]] [[1.641]
 [1.641]
 [1.716]
 [1.736]
 [1.54 ]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0011,     0.9846,     0.0002,     0.0000,     0.0140],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0002,     0.8948,     0.0045,     0.1001],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0060, 0.0040, 0.0012, 0.9638, 0.0250], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0678, 0.0015, 0.1979, 0.0410, 0.6919], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.46545696258545
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7081 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.54995073486405
maxi score, test score, baseline:  0.7081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.704]
 [0.731]
 [0.65 ]
 [0.674]
 [0.704]] [[41.855]
 [45.788]
 [26.234]
 [37.844]
 [41.855]] [[0.704]
 [0.731]
 [0.65 ]
 [0.674]
 [0.704]]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.7804505
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[100.439]
 [100.439]
 [100.439]
 [100.439]
 [100.439]] [[2.878]
 [2.878]
 [2.878]
 [2.878]
 [2.878]]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.605203652351076
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.072]
 [1.193]
 [1.072]
 [1.072]] [[53.056]
 [53.056]
 [73.243]
 [53.056]
 [53.056]] [[1.224]
 [1.224]
 [1.417]
 [1.224]
 [1.224]]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7898802
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
printing an ep nov before normalisation:  39.062284983910786
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8560,     0.0056,     0.0005,     0.0001,     0.1379],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0010,     0.9977,     0.0003,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0085,     0.8863,     0.0002,     0.1044],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0020, 0.0011, 0.0253, 0.8791, 0.0924], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.1591,     0.0589,     0.1988,     0.0000,     0.5831],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  80.67599408844909
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7061 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  89.03406856995313
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.9018, 0.0180, 0.0016, 0.0269, 0.0516], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0025,     0.9723,     0.0003,     0.0216,     0.0032],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9040,     0.0057,     0.0900],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0219,     0.0001,     0.0016,     0.9498,     0.0267],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0044,     0.0002,     0.6041,     0.0024,     0.3889],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.94744147589485
printing an ep nov before normalisation:  30.286245346069336
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.8341,     0.0004,     0.0001,     0.0416,     0.1237],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9945,     0.0000,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0046,     0.9367,     0.0012,     0.0573],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0095,     0.0007,     0.0040,     0.9673,     0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1799, 0.0010, 0.2065, 0.1429, 0.4697], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  74.22745024594178
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.125]
 [1.202]
 [1.125]
 [1.125]
 [1.125]] [[33.663]
 [45.455]
 [33.663]
 [33.663]
 [33.663]] [[1.782]
 [2.521]
 [1.782]
 [1.782]
 [1.782]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78859407
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.78350526
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7859111
printing an ep nov before normalisation:  38.55550536939481
siam score:  -0.7838271
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.78380704
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7101 1.0 1.0
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7121 1.0 1.0
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.83184108558832
actions average: 
K:  0  action  0 :  tensor([    0.8487,     0.0007,     0.0003,     0.0256,     0.1247],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9989,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0012,     0.8859,     0.0016,     0.1108],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0191,     0.0001,     0.0004,     0.9554,     0.0251],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1468, 0.0072, 0.2155, 0.1544, 0.4761], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7121 1.0 1.0
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.78248686
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.185]
 [1.185]
 [1.295]
 [1.185]
 [1.185]] [[40.189]
 [40.189]
 [53.988]
 [40.189]
 [40.189]] [[1.456]
 [1.456]
 [1.738]
 [1.456]
 [1.456]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Sims:  25 1 epoch:  97353 pick best:  False frame count:  97353
maxi score, test score, baseline:  0.7121 1.0 1.0
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7101 1.0 1.0
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.70966693631246
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.276467698263325
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.30262997382997
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.872]
 [0.788]
 [0.788]] [[36.298]
 [36.298]
 [65.339]
 [36.298]
 [36.298]] [[0.788]
 [0.788]
 [0.872]
 [0.788]
 [0.788]]
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.536]
 [0.536]
 [0.617]
 [0.536]
 [0.536]] [[50.72 ]
 [50.72 ]
 [31.884]
 [50.72 ]
 [50.72 ]] [[1.156]
 [1.156]
 [0.936]
 [1.156]
 [1.156]]
maxi score, test score, baseline:  0.7081 1.0 1.0
printing an ep nov before normalisation:  30.802261451987977
maxi score, test score, baseline:  0.7081 1.0 1.0
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.136]
 [1.313]
 [1.204]
 [0.068]
 [1.249]] [[26.855]
 [31.114]
 [28.439]
 [37.254]
 [30.175]] [[1.382]
 [1.674]
 [1.493]
 [0.594]
 [1.584]]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  76.14556756461452
printing an ep nov before normalisation:  42.67533089467932
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.18179416656494
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7081 1.0 1.0
maxi score, test score, baseline:  0.7081 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7081 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.385]
 [1.385]
 [1.385]
 [1.385]] [[33.699]
 [33.699]
 [33.699]
 [33.699]
 [33.699]] [[2.718]
 [2.718]
 [2.718]
 [2.718]
 [2.718]]
siam score:  -0.7853291
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.222]
 [1.127]
 [1.127]] [[74.737]
 [74.737]
 [88.043]
 [74.737]
 [74.737]] [[2.055]
 [2.055]
 [2.405]
 [2.055]
 [2.055]]
printing an ep nov before normalisation:  47.29734351647744
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  99.18258451859226
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  13.728408813476562
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.85090351104736
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.480274002703077
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  114.01872603649105
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.059714895051385
Printing some Q and Qe and total Qs values:  [[1.255]
 [1.255]
 [1.278]
 [1.255]
 [1.255]] [[24.023]
 [24.023]
 [29.559]
 [24.023]
 [24.023]] [[2.243]
 [2.243]
 [2.575]
 [2.243]
 [2.243]]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7891071
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.79806469638952
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78723425
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.7659,     0.0223,     0.0006,     0.0000,     0.2112],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9992,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0007,     0.0062,     0.9555,     0.0002,     0.0374],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0000,     0.0325,     0.9027,     0.0646],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0663, 0.0803, 0.2607, 0.0069, 0.5859], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.78588974
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.47193026149527
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.476287841796875
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.79006284
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  78.12429028622466
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.029]
 [1.155]
 [1.029]
 [1.029]
 [1.029]] [[54.692]
 [68.788]
 [54.692]
 [54.692]
 [54.692]] [[1.757]
 [2.112]
 [1.757]
 [1.757]
 [1.757]]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  4  action  0 :  tensor([    0.9215,     0.0234,     0.0004,     0.0176,     0.0370],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9996,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0002,     0.9282,     0.0104,     0.0607],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0008,     0.9989,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0478, 0.1854, 0.3194, 0.0135, 0.4339], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.015]
 [1.173]
 [0.337]
 [0.951]
 [1.034]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [1.173]
 [0.337]
 [0.951]
 [1.034]]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.70178076686055
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  111.62184121879439
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7021 1.0 1.0
printing an ep nov before normalisation:  3.5988843992706165
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.775679769381693
maxi score, test score, baseline:  0.7021 1.0 1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.78122497
printing an ep nov before normalisation:  39.4769811630249
maxi score, test score, baseline:  0.7021 1.0 1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.542 0.417 0.   ]
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 97.44239254562098
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.26 ]
 [0.946]
 [0.946]
 [1.158]] [[43.841]
 [56.686]
 [42.697]
 [42.697]
 [34.355]] [[1.898]
 [2.34 ]
 [1.635]
 [1.635]
 [1.613]]
printing an ep nov before normalisation:  75.70827180592326
Printing some Q and Qe and total Qs values:  [[1.14]
 [1.14]
 [1.14]
 [1.14]
 [1.14]] [[58.081]
 [58.081]
 [58.081]
 [58.081]
 [58.081]] [[2.56]
 [2.56]
 [2.56]
 [2.56]
 [2.56]]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7061 1.0 1.0
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.792]
 [0.792]
 [0.845]
 [0.792]
 [0.792]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.792]
 [0.792]
 [0.845]
 [0.792]
 [0.792]]
maxi score, test score, baseline:  0.7061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.15258473947929
printing an ep nov before normalisation:  39.1857385635376
printing an ep nov before normalisation:  38.87187480926514
printing an ep nov before normalisation:  52.66820549152872
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.213]
 [1.314]
 [1.161]
 [1.213]] [[58.259]
 [58.259]
 [56.628]
 [60.95 ]
 [58.259]] [[2.959]
 [2.959]
 [2.983]
 [3.036]
 [2.959]]
printing an ep nov before normalisation:  35.5960750579834
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.287749380851764
actions average: 
K:  3  action  0 :  tensor([    0.8995,     0.0005,     0.0006,     0.0011,     0.0983],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9810,     0.0101,     0.0002,     0.0075],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0002,     0.9410,     0.0002,     0.0581],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0004,     0.0129,     0.9063,     0.0803],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2530, 0.0018, 0.2399, 0.0030, 0.5023], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7281 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7281 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([    0.8707,     0.0005,     0.0019,     0.0005,     0.1264],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9956,     0.0000,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0024,     0.9538,     0.0000,     0.0435],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0000,     0.1202,     0.8225,     0.0572],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1239, 0.1031, 0.2577, 0.0753, 0.4400], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7863231
siam score:  -0.7865715
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.417 0.25  0.083 0.25 ]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.201]
 [1.201]
 [1.233]
 [1.201]
 [1.201]] [[22.689]
 [22.689]
 [28.926]
 [22.689]
 [22.689]] [[1.27 ]
 [1.27 ]
 [1.347]
 [1.27 ]
 [1.27 ]]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.623]
 [0.679]
 [0.53 ]
 [0.623]
 [0.623]] [[45.188]
 [44.336]
 [40.122]
 [45.188]
 [45.188]] [[0.623]
 [0.679]
 [0.53 ]
 [0.623]
 [0.623]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]] [[36.983]
 [36.983]
 [36.983]
 [36.983]
 [36.983]] [[2.376]
 [2.376]
 [2.376]
 [2.376]
 [2.376]]
siam score:  -0.7747659
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.439]
 [1.439]
 [1.334]
 [1.439]
 [1.439]] [[74.695]
 [74.695]
 [76.236]
 [74.695]
 [74.695]] [[2.025]
 [2.025]
 [1.937]
 [2.025]
 [2.025]]
maxi score, test score, baseline:  0.7301 1.0 1.0
printing an ep nov before normalisation:  52.93075350576461
Printing some Q and Qe and total Qs values:  [[1.213]
 [1.288]
 [1.168]
 [1.071]
 [1.294]] [[35.572]
 [43.949]
 [34.182]
 [42.561]
 [41.727]] [[1.532]
 [1.757]
 [1.462]
 [1.515]
 [1.723]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77741575
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.5   0.333 0.083 0.042]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.23756324695398234
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  48.94813449354002
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.162790689153404
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.68423584591524
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.16005589003526666
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7281 1.0 1.0
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.00050165207677
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7603812
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.1]
 [1.1]
 [1.1]
 [1.1]
 [1.1]] [[71.364]
 [71.364]
 [71.364]
 [71.364]
 [71.364]] [[1.811]
 [1.811]
 [1.811]
 [1.811]
 [1.811]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.75906754
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.788]
 [0.883]
 [0.788]
 [0.788]] [[30.727]
 [30.727]
 [57.961]
 [30.727]
 [30.727]] [[0.788]
 [0.788]
 [0.883]
 [0.788]
 [0.788]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  15.634584426879883
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  75.38623809242823
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.978]
 [1.175]
 [0.978]
 [0.978]
 [0.978]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.978]
 [1.175]
 [0.978]
 [0.978]
 [0.978]]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.256]
 [1.256]
 [1.291]
 [1.256]
 [1.256]] [[21.7  ]
 [21.7  ]
 [16.944]
 [21.7  ]
 [21.7  ]] [[3.302]
 [3.302]
 [2.624]
 [3.302]
 [3.302]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.49760596630509
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.363]
 [1.428]
 [1.365]
 [1.363]
 [1.363]] [[33.082]
 [40.181]
 [39.885]
 [33.082]
 [33.082]] [[1.593]
 [1.727]
 [1.662]
 [1.593]
 [1.593]]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.292]
 [1.355]
 [1.292]
 [1.292]
 [1.292]] [[41.995]
 [44.954]
 [41.995]
 [41.995]
 [41.995]] [[1.549]
 [1.64 ]
 [1.549]
 [1.549]
 [1.549]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.913227990193036
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.21799823441989
printing an ep nov before normalisation:  27.178142070770264
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.361]
 [1.406]
 [1.297]
 [1.361]
 [1.361]] [[35.949]
 [50.113]
 [51.171]
 [35.949]
 [35.949]] [[1.716]
 [1.901]
 [1.802]
 [1.716]
 [1.716]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7301 1.0 1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.97039031982422
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.7675748
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.038]
 [1.247]
 [1.226]
 [0.53 ]
 [1.211]] [[23.337]
 [23.856]
 [36.221]
 [31.223]
 [35.041]] [[0.388]
 [1.62 ]
 [2.151]
 [1.232]
 [2.083]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 52.83543676642725
printing an ep nov before normalisation:  54.4683297475179
siam score:  -0.76922417
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.393]
 [1.393]
 [1.414]
 [1.262]
 [1.393]] [[75.519]
 [75.519]
 [73.489]
 [80.827]
 [75.519]] [[2.607]
 [2.607]
 [2.583]
 [2.595]
 [2.607]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8699,     0.0005,     0.0006,     0.0332,     0.0958],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9966,     0.0001,     0.0001,     0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9252,     0.0065,     0.0680],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0486,     0.0006,     0.0004,     0.8847,     0.0657],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0630,     0.0005,     0.2758,     0.0620,     0.5988],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.147455350333644
siam score:  -0.77393556
maxi score, test score, baseline:  0.7301 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 19.918818846806392
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.188721247056186
printing an ep nov before normalisation:  50.74817026806901
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.10353888435275849
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7424, 0.0021, 0.0163, 0.0982, 0.1410], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0049,     0.9846,     0.0030,     0.0001,     0.0074],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0002,     0.9108,     0.0016,     0.0871],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0453,     0.0004,     0.0010,     0.9161,     0.0371],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0320, 0.0009, 0.4463, 0.0081, 0.5128], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  70.58810838166126
printing an ep nov before normalisation:  62.58596254814055
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7301 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.397]
 [1.397]
 [1.319]
 [1.397]
 [1.397]] [[15.036]
 [15.036]
 [35.542]
 [15.036]
 [15.036]] [[1.892]
 [1.892]
 [2.49 ]
 [1.892]
 [1.892]]
printing an ep nov before normalisation:  25.113200539493533
maxi score, test score, baseline:  0.7321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.83681928281391
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.163]
 [1.267]
 [1.355]
 [1.163]
 [1.163]] [[37.015]
 [42.593]
 [44.5  ]
 [37.015]
 [37.015]] [[1.522]
 [1.738]
 [1.864]
 [1.522]
 [1.522]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([0.8728, 0.0723, 0.0065, 0.0043, 0.0441], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9989,     0.0002,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0000,     0.9601,     0.0003,     0.0393],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0399, 0.0009, 0.0655, 0.8386, 0.0552], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0263, 0.0029, 0.2099, 0.0117, 0.7492], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.29317902666406
maxi score, test score, baseline:  0.7361 1.0 1.0
maxi score, test score, baseline:  0.7361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7758461
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.308]
 [1.308]
 [1.39 ]
 [1.308]
 [1.308]] [[40.812]
 [40.812]
 [53.094]
 [40.812]
 [40.812]] [[1.502]
 [1.502]
 [1.671]
 [1.502]
 [1.502]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.603271961212158
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.688395977020264
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.8824, 0.0634, 0.0041, 0.0029, 0.0472], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9795,     0.0000,     0.0000,     0.0191],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0218,     0.8675,     0.0090,     0.1012],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0066,     0.0001,     0.2912,     0.5074,     0.1947],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0294,     0.3991,     0.2559,     0.0000,     0.3156],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.512232806172044
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.053]
 [1.122]
 [1.053]
 [1.053]
 [1.047]] [[39.457]
 [35.886]
 [39.457]
 [39.457]
 [36.926]] [[2.301]
 [2.154]
 [2.301]
 [2.301]
 [2.142]]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.91262559019784
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7401 1.0 1.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.69538212265891
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.305238008304904
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7401 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.227]
 [1.25 ]
 [1.227]
 [1.227]
 [1.227]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.227]
 [1.25 ]
 [1.227]
 [1.227]
 [1.227]]
maxi score, test score, baseline:  0.7421 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([    0.9962,     0.0003,     0.0007,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9970,     0.0004,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0002,     0.9197,     0.0026,     0.0773],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0338,     0.0005,     0.0081,     0.9009,     0.0567],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0084, 0.0047, 0.4446, 0.0026, 0.5396], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.43534995761861
printing an ep nov before normalisation:  0.0009784080023678143
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.08 ]
 [1.112]
 [1.058]
 [1.058]
 [1.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.08 ]
 [1.112]
 [1.058]
 [1.058]
 [1.004]]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 52.31346695503045
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([    0.9446,     0.0122,     0.0003,     0.0223,     0.0207],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9927,     0.0043,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0002,     0.9304,     0.0156,     0.0537],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0000,     0.0015,     0.9940,     0.0045],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0086, 0.0016, 0.2568, 0.0653, 0.6676], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.7401 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  2.0981992747692857e-05
printing an ep nov before normalisation:  27.345423686586344
siam score:  -0.761101
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.034502029418945
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.76028216
using explorer policy with actor:  1
maxi score, test score, baseline:  0.7401 1.0 1.0
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.088]
 [1.124]
 [1.28 ]
 [1.033]
 [1.084]] [[39.42 ]
 [50.6  ]
 [46.355]
 [37.417]
 [39.827]] [[1.866]
 [2.378]
 [2.353]
 [1.725]
 [1.879]]
printing an ep nov before normalisation:  47.26571796987838
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.005248383255320732
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7381 1.0 1.0
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7381 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7381 1.0 1.0
maxi score, test score, baseline:  0.7381 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.981]
 [1.145]
 [0.049]
 [0.913]] [[35.282]
 [33.324]
 [37.825]
 [31.361]
 [28.502]] [[2.037]
 [1.93 ]
 [2.31 ]
 [0.905]
 [1.631]]
printing an ep nov before normalisation:  39.944056698026635
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20001
maxi score, test score, baseline:  0.7381 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.055773042495204
maxi score, test score, baseline:  0.7381 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.01343906531283
maxi score, test score, baseline:  0.7401 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.7635753
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7421 1.0 1.0
maxi score, test score, baseline:  0.7421 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.59857177734375
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.211542704857717
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.217485427856445
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.7421 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
UNIT TEST: sample policy line 217 mcts : [0.    0.708 0.    0.208 0.083]
printing an ep nov before normalisation:  22.597544193267822
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7441 1.0 1.0
printing an ep nov before normalisation:  26.55232725014406
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.05 ]
 [1.192]
 [1.05 ]
 [1.05 ]
 [1.05 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.05 ]
 [1.192]
 [1.05 ]
 [1.05 ]
 [1.05 ]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.7441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.080588561723175
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.664576488534458
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.7421 1.0 1.0
maxi score, test score, baseline:  0.7421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
