append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
printing an ep nov before normalisation:  7.801042795181274
siam score:  0.002426923088602383
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3925, 0.0749, 0.1074, 0.2819, 0.1433], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1481, 0.4988, 0.1419, 0.0626, 0.1487], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0524, 0.1204, 0.5191, 0.1454, 0.1627], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3032, 0.1292, 0.1775, 0.2246, 0.1654], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2739, 0.1437, 0.2133, 0.1953, 0.1739], grad_fn=<DivBackward0>)
deleting a thread, now have 2 threads
Frames:  942 train batches done:  29 episodes:  80
siam score:  -0.07700768611679125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 1 threads
Frames:  942 train batches done:  72 episodes:  80
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.3607, 0.1965, 0.0577, 0.2251, 0.1600], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1546, 0.6373, 0.0801, 0.0256, 0.1025], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.1041, 0.0826, 0.4319, 0.1988, 0.1826], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3503, 0.0507, 0.1765, 0.2615, 0.1610], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2823, 0.0513, 0.2104, 0.1649, 0.2911], grad_fn=<DivBackward0>)
siam score:  -0.45357653
siam score:  -0.4975902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4876, 0.0832, 0.0268, 0.2324, 0.1701], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1300, 0.6427, 0.0426, 0.0296, 0.1551], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0446, 0.0714, 0.5494, 0.1617, 0.1728], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3516, 0.0099, 0.1739, 0.2947, 0.1700], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2104, 0.0612, 0.3271, 0.1464, 0.2549], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.4258, 0.0991, 0.0357, 0.2981, 0.1413], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1043, 0.6156, 0.1526, 0.0197, 0.1078], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0740, 0.0416, 0.5252, 0.2065, 0.1527], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.3678, 0.0692, 0.0743, 0.3127, 0.1759], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3328, 0.0727, 0.1362, 0.2539, 0.2045], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5755974
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.45465298517371
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.268341045364764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.27397293498652
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.49411905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.5100, 0.0450, 0.0272, 0.2424, 0.1754], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0868, 0.7518, 0.0502, 0.0270, 0.0842], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0153, 0.0784, 0.6934, 0.0872, 0.1257], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3062, 0.0047, 0.0664, 0.3516, 0.2712], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3330, 0.1350, 0.0647, 0.2369, 0.2304], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.361140251159668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5842013
actions average: 
K:  0  action  0 :  tensor([0.5952, 0.0106, 0.0010, 0.1770, 0.2162], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0515, 0.7735, 0.0169, 0.0275, 0.1305], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0049, 0.0030, 0.8911, 0.0456, 0.0555], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2118, 0.0013, 0.0212, 0.4976, 0.2681], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3086, 0.0117, 0.0224, 0.3060, 0.3514], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.65988642650314
actions average: 
K:  1  action  0 :  tensor([0.4564, 0.0052, 0.0010, 0.3207, 0.2167], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0508, 0.8954, 0.0088, 0.0059, 0.0392], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0103, 0.0105, 0.8854, 0.0321, 0.0617], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1735, 0.0027, 0.1100, 0.3889, 0.3249], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1566, 0.0290, 0.1061, 0.3594, 0.3489], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4552, 0.0389, 0.0039, 0.1889, 0.3131], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0715, 0.7920, 0.0159, 0.0113, 0.1093], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0022, 0.0316, 0.8892, 0.0343, 0.0428], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2391, 0.0076, 0.1078, 0.3412, 0.3044], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2276, 0.0079, 0.1553, 0.2716, 0.3376], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.4453, 0.0135, 0.0038, 0.3454, 0.1919], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0428, 0.8322, 0.0613, 0.0089, 0.0548], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0240, 0.0200, 0.6827, 0.1406, 0.1327], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2400, 0.0186, 0.0520, 0.4081, 0.2813], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2580, 0.0694, 0.0623, 0.3405, 0.2698], grad_fn=<DivBackward0>)
siam score:  -0.61403334
actions average: 
K:  3  action  0 :  tensor([0.5447, 0.0570, 0.0066, 0.1886, 0.2032], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0152, 0.9428, 0.0183, 0.0020, 0.0217], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0093, 0.0371, 0.7486, 0.0741, 0.1309], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1837, 0.0011, 0.1682, 0.3019, 0.3451], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2728, 0.1802, 0.0394, 0.1882, 0.3194], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.62689334
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6304201
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.48718762397766
printing an ep nov before normalisation:  73.00869396754673
printing an ep nov before normalisation:  40.81700801849365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5460, 0.0131, 0.0015, 0.2368, 0.2026], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0070, 0.9768, 0.0051, 0.0012, 0.0099], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0014,     0.9734,     0.0163,     0.0083],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1681,     0.0004,     0.0400,     0.5691,     0.2223],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1672, 0.0273, 0.0454, 0.4084, 0.3517], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.66720736
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.67422134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6662387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6667, 0.0122, 0.0008, 0.1494, 0.1709], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0096, 0.9620, 0.0032, 0.0017, 0.0234], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0009, 0.0040, 0.9165, 0.0496, 0.0289], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.3123, 0.0025, 0.0096, 0.4439, 0.2317], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3223, 0.0046, 0.0917, 0.2436, 0.3379], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6816771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.5445427982311101
siam score:  -0.6696262
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.61157584
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.28666067123413
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.673]
 [19.673]
 [19.673]
 [19.673]
 [19.673]] [[1.641]
 [1.641]
 [1.641]
 [1.641]
 [1.641]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  127.47322243375582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6525604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 87.36178857689248
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5262, 0.0013, 0.0007, 0.3166, 0.1552], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0051,     0.9910,     0.0003,     0.0011,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0341,     0.8807,     0.0304,     0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2348, 0.0017, 0.0295, 0.5643, 0.1696], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1439, 0.0148, 0.1320, 0.4940, 0.2153], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.027]
 [74.451]
 [77.244]
 [76.593]
 [72.318]] [[0.722]
 [0.812]
 [0.869]
 [0.856]
 [0.769]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.23198699951172
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6837,     0.0020,     0.0002,     0.1336,     0.1805],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0099,     0.9796,     0.0001,     0.0005,     0.0099],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0004,     0.9624,     0.0225,     0.0145],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1614, 0.0008, 0.0932, 0.4622, 0.2824], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1950, 0.0020, 0.0385, 0.3504, 0.4140], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.677]
 [83.677]
 [83.677]
 [83.677]
 [83.677]] [[0.873]
 [0.873]
 [0.873]
 [0.873]
 [0.873]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  103.8363499970171
printing an ep nov before normalisation:  63.66946836466555
actions average: 
K:  2  action  0 :  tensor([0.6389, 0.0014, 0.0007, 0.1216, 0.2374], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0537,     0.8823,     0.0004,     0.0016,     0.0620],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0048,     0.0008,     0.8025,     0.1045,     0.0874],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2171, 0.0004, 0.0162, 0.4114, 0.3549], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2720, 0.0056, 0.0120, 0.3410, 0.3694], grad_fn=<DivBackward0>)
siam score:  -0.7008272
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  57.37588722662053
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.424]
 [38.424]
 [59.486]
 [57.558]
 [54.333]] [[0.166]
 [0.166]
 [0.529]
 [0.496]
 [0.44 ]]
printing an ep nov before normalisation:  66.37725830078125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.98036258423981
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.4657685020220015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.18312012067557
printing an ep nov before normalisation:  50.23432731628418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.803]
 [49.737]
 [44.992]
 [40.161]
 [39.984]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.813]
 [55.211]
 [59.592]
 [51.906]
 [53.954]] [[0.289]
 [0.433]
 [0.5  ]
 [0.382]
 [0.414]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.62607648607376
printing an ep nov before normalisation:  11.491709154402738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.257914991739018
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6850, 0.0436, 0.0021, 0.1124, 0.1568], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0088, 0.9247, 0.0252, 0.0142, 0.0272], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0004,     0.9787,     0.0117,     0.0092],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1879, 0.0060, 0.0898, 0.4605, 0.2558], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1771, 0.0233, 0.0454, 0.3798, 0.3744], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  69.04717767774245
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.333 0.25  0.125]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.551506275057974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.239567389701577
printing an ep nov before normalisation:  100.34804344177246
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.46 ]
 [40.597]
 [61.634]
 [57.46 ]
 [57.46 ]] [[1.334]
 [0.745]
 [1.48 ]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.6701424403985358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  9341 train batches done:  1095 episodes:  777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.41275256806323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.53759305149663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  99.923104145585
printing an ep nov before normalisation:  77.39324121220426
actions average: 
K:  1  action  0 :  tensor([    0.7745,     0.0589,     0.0001,     0.0742,     0.0923],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0038,     0.9863,     0.0053,     0.0001,     0.0045],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0036,     0.9855,     0.0044,     0.0065],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1721, 0.0033, 0.0253, 0.4686, 0.3306], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1985, 0.0210, 0.0562, 0.3761, 0.3482], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7270532
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
actions average: 
K:  0  action  0 :  tensor([    0.6062,     0.0028,     0.0004,     0.1598,     0.2309],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0014,     0.9896,     0.0000,     0.0000,     0.0090],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0309,     0.9263,     0.0128,     0.0295],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1294, 0.0021, 0.0417, 0.5177, 0.3091], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1643, 0.0027, 0.0491, 0.3322, 0.4516], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.72518593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  94.71559155615563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[10.432]
 [10.432]
 [46.773]
 [20.52 ]
 [10.432]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 66.602]
 [ 63.308]
 [117.001]
 [ 64.681]
 [ 63.308]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.128584678253596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.138]
 [35.138]
 [64.071]
 [37.993]
 [35.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.17125129699707
printing an ep nov before normalisation:  7.5376877191875735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.20982757394949
printing an ep nov before normalisation:  38.497964599051784
UNIT TEST: sample policy line 217 mcts : [0.208 0.042 0.25  0.292 0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.13986836013895
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.6970435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.22290823194715
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.91971243585711
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.8260196404118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.05431731583887
printing an ep nov before normalisation:  62.698080285617834
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
siam score:  -0.7212627
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.58970232630281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.96458193840856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.85060501098633
printing an ep nov before normalisation:  82.05832944215183
line 256 mcts: sample exp_bonus 59.4373904274624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6613,     0.0034,     0.0003,     0.1298,     0.2051],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0571,     0.8905,     0.0021,     0.0008,     0.0495],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0015,     0.9888,     0.0055,     0.0042],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1727, 0.0009, 0.0779, 0.4509, 0.2977], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1973, 0.0043, 0.0971, 0.3192, 0.3821], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.274151661162854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7334598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.502044677734375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.41139535304572
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.339]
 [31.339]
 [52.663]
 [31.339]
 [31.339]] [[0.463]
 [0.463]
 [1.064]
 [0.463]
 [0.463]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.133]
 [43.133]
 [29.958]
 [43.133]
 [43.133]] [[1.514]
 [1.514]
 [0.861]
 [1.514]
 [1.514]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.17494589767034
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([0.5529, 0.0236, 0.0016, 0.1929, 0.2290], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0152,     0.9650,     0.0127,     0.0002,     0.0069],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0002,     0.9757,     0.0106,     0.0133],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1557, 0.0014, 0.0898, 0.4447, 0.3085], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2800, 0.0067, 0.0390, 0.2714, 0.4029], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.705500398100572
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  100.22241070643419
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.86259330543107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.795]
 [53.282]
 [36.795]
 [36.795]
 [57.014]] [[0.174]
 [0.409]
 [0.174]
 [0.174]
 [0.463]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.08814175205451
siam score:  -0.74126023
printing an ep nov before normalisation:  17.699713706970215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  104.71806763181166
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  51.49249124365307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.31307291984558
printing an ep nov before normalisation:  60.29565674918039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7352283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7312346
printing an ep nov before normalisation:  104.55469744102139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.73256385
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.668]
 [57.668]
 [75.963]
 [57.668]
 [57.668]] [[1.098]
 [1.098]
 [1.599]
 [1.098]
 [1.098]]
siam score:  -0.73497605
siam score:  -0.73540246
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.27322212532792
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.794]
 [87.207]
 [99.82 ]
 [75.29 ]
 [75.809]] [[0.532]
 [0.655]
 [0.805]
 [0.514]
 [0.52 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.251]
 [54.262]
 [57.053]
 [59.421]
 [50.394]] [[0.203]
 [0.211]
 [0.233]
 [0.252]
 [0.18 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.18218502083678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[94.608]
 [94.608]
 [94.608]
 [94.608]
 [94.608]] [[0.506]
 [0.506]
 [0.506]
 [0.506]
 [0.506]]
printing an ep nov before normalisation:  100.41173234340425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.73568242711149
printing an ep nov before normalisation:  67.51879392641466
printing an ep nov before normalisation:  13.151330581232799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.36346484453297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7159,     0.0007,     0.0001,     0.1310,     0.1523],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0046,     0.9931,     0.0001,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0017, 0.0145, 0.9300, 0.0362, 0.0176], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1566, 0.0007, 0.0025, 0.5273, 0.3130], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1872, 0.0153, 0.0011, 0.3887, 0.4078], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.7227223055992
printing an ep nov before normalisation:  95.67057400648392
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.080582300164814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.655]
 [59.655]
 [73.469]
 [65.628]
 [66.332]] [[0.202]
 [0.202]
 [0.297]
 [0.243]
 [0.248]]
printing an ep nov before normalisation:  65.40812908770954
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  8.688836631231709
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  13952 train batches done:  1635 episodes:  1156
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.70749426
printing an ep nov before normalisation:  62.19430368558672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.74631528802229
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.83008948910692
siam score:  -0.7231993
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.258]
 [55.839]
 [39.789]
 [32.563]
 [32.162]] [[0.869]
 [2.206]
 [1.333]
 [0.94 ]
 [0.918]]
printing an ep nov before normalisation:  17.685583606930294
siam score:  -0.7259864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.66639474672916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7348297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.23329565229033
actions average: 
K:  2  action  0 :  tensor([    0.7545,     0.0015,     0.0003,     0.0833,     0.1605],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0357,     0.9340,     0.0000,     0.0001,     0.0302],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0017,     0.9456,     0.0153,     0.0374],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.2195, 0.0011, 0.0101, 0.4520, 0.3174], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2778, 0.0010, 0.0034, 0.3194, 0.3983], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7399284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.07944747379848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  60.81967353820801
printing an ep nov before normalisation:  56.54730453886907
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.845]
 [77.845]
 [96.141]
 [77.845]
 [77.845]] [[1.244]
 [1.244]
 [1.801]
 [1.244]
 [1.244]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.6454928588383
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.14600588773743
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.92624883896414
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  88.18014811217476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.9752450339373
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.820973980441714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.98380470275879
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.08369878065429
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.09493330344621
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.232]
 [40.739]
 [58.387]
 [42.404]
 [34.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.6511,     0.0632,     0.0003,     0.1257,     0.1596],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9995,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0109,     0.9783,     0.0077,     0.0031],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1515, 0.0008, 0.0023, 0.4460, 0.3993], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1704, 0.0097, 0.0065, 0.3125, 0.5008], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.8368131107142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.27836670995923
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.202934503555298
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.71553993225098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7324164
siam score:  -0.7368811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.962]
 [49.962]
 [64.528]
 [50.49 ]
 [51.372]] [[0.398]
 [0.398]
 [0.599]
 [0.405]
 [0.417]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[90.661]
 [90.661]
 [97.318]
 [90.661]
 [90.661]] [[1.07 ]
 [1.07 ]
 [1.169]
 [1.07 ]
 [1.07 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.4536857827467
siam score:  -0.7404173
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.369]
 [60.384]
 [58.546]
 [63.085]
 [50.987]] [[0.956]
 [0.77 ]
 [0.721]
 [0.842]
 [0.521]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  98.06546488174342
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.11 ]
 [72.11 ]
 [95.448]
 [74.161]
 [75.282]] [[0.854]
 [0.854]
 [1.38 ]
 [0.9  ]
 [0.925]]
printing an ep nov before normalisation:  2.49755044288122
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([0.4563, 0.0028, 0.0015, 0.2732, 0.2662], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0234,     0.9279,     0.0096,     0.0007,     0.0384],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9988,     0.0006,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0948, 0.0009, 0.0635, 0.5671, 0.2737], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1650, 0.0016, 0.0464, 0.3538, 0.4332], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.278]
 [22.327]
 [37.817]
 [24.568]
 [25.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.25829137991869
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74183303
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.14003454107835
printing an ep nov before normalisation:  33.27747881412506
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  67.58790016174316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.68026534175675
siam score:  -0.73347884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.075386486060548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[93.169]
 [93.169]
 [93.169]
 [93.169]
 [93.169]] [[1.999]
 [1.999]
 [1.999]
 [1.999]
 [1.999]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7384719
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6806,     0.0009,     0.0007,     0.1606,     0.1573],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9981,     0.0011,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9976,     0.0012,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1486,     0.0005,     0.0006,     0.5977,     0.2526],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2033, 0.0100, 0.0005, 0.4453, 0.3409], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.556]
 [38.556]
 [52.179]
 [41.939]
 [38.556]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.553]
 [33.553]
 [28.53 ]
 [38.194]
 [33.553]] [[0.541]
 [0.541]
 [0.405]
 [0.667]
 [0.541]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8130,     0.0026,     0.0003,     0.0916,     0.0925],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0130,     0.9708,     0.0090,     0.0000,     0.0071],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9972,     0.0015,     0.0013],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1420, 0.0020, 0.0501, 0.5375, 0.2684], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3345, 0.0552, 0.0043, 0.1997, 0.4063], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.33374722743604
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6604,     0.0056,     0.0003,     0.0820,     0.2518],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0376,     0.9533,     0.0005,     0.0003,     0.0083],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1249,     0.0004,     0.0012,     0.5829,     0.2905],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1363, 0.0019, 0.0064, 0.3516, 0.5037], grad_fn=<DivBackward0>)
UNIT TEST: sample policy line 217 mcts : [0.083 0.125 0.167 0.542 0.083]
printing an ep nov before normalisation:  104.69564411214195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7219511
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72013336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.79816409281987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.362]
 [34.362]
 [34.362]
 [34.362]
 [34.362]] [[0.593]
 [0.593]
 [0.593]
 [0.593]
 [0.593]]
printing an ep nov before normalisation:  19.227441285712665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [33.096]
 [ 0.   ]
 [12.48 ]
 [ 0.   ]] [[-0.103]
 [ 0.171]
 [-0.103]
 [ 0.   ]
 [-0.103]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7310788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.06741428141997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.803]
 [68.803]
 [76.121]
 [68.803]
 [68.803]] [[1.724]
 [1.724]
 [2.   ]
 [1.724]
 [1.724]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  62.36487637931628
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.36895930231296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.22999334335327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.654720306396484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.36420144089026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.797]
 [61.21 ]
 [55.614]
 [54.691]
 [55.389]] [[0.293]
 [0.305]
 [0.257]
 [0.25 ]
 [0.256]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74850726
printing an ep nov before normalisation:  65.8506447722739
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.308]
 [21.839]
 [14.674]
 [12.248]
 [13.222]] [[0.098]
 [0.21 ]
 [0.103]
 [0.067]
 [0.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.48669937678746
printing an ep nov before normalisation:  56.67693332224837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.00734514892315
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 46.341]
 [ 46.341]
 [100.559]
 [ 46.341]
 [ 46.341]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.666]
 [22.666]
 [41.013]
 [24.17 ]
 [24.067]] [[0.179]
 [0.179]
 [0.455]
 [0.202]
 [0.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7352437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
STARTED EXPV TRAINING ON FRAME NO.  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.32308547147774
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.11400950212217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.05723071569953
printing an ep nov before normalisation:  18.867940059168337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72973204
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.269]
 [39.269]
 [39.269]
 [39.269]
 [54.775]] [[0.266]
 [0.266]
 [0.266]
 [0.266]
 [0.61 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.19801441338077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.99506762817124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.9503435439139
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.474]
 [95.346]
 [94.094]
 [94.8  ]
 [95.178]] [[1.225]
 [1.514]
 [1.493]
 [1.505]
 [1.511]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.2323657544575
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.105141308795346
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7018, 0.0020, 0.0009, 0.0637, 0.2315], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0024,     0.9949,     0.0000,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9923,     0.0046,     0.0031],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0781, 0.0005, 0.0627, 0.4986, 0.3601], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2194, 0.0005, 0.0055, 0.3762, 0.3984], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20023
siam score:  -0.74746794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7466727
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.09724426269531
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  41.74703061580658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.40183124524942
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  94.54028032215285
printing an ep nov before normalisation:  61.46255731582642
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  86.95837931139056
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.853]
 [52.853]
 [62.35 ]
 [52.853]
 [52.853]] [[0.83 ]
 [0.83 ]
 [1.097]
 [0.83 ]
 [0.83 ]]
printing an ep nov before normalisation:  92.76107474205537
printing an ep nov before normalisation:  42.86752191639737
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6691,     0.0137,     0.0003,     0.1188,     0.1981],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9978,     0.0000,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0015,     0.9715,     0.0112,     0.0157],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1061, 0.0006, 0.0353, 0.5924, 0.2656], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2609, 0.0323, 0.0202, 0.2750, 0.4116], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  96.80726539403653
printing an ep nov before normalisation:  53.85187882656732
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.3055568143526
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75221777
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.30734485914479
printing an ep nov before normalisation:  42.61989422888474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.88695579892422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.29587410086911
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8030,     0.0031,     0.0003,     0.0608,     0.1327],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0217, 0.9329, 0.0012, 0.0037, 0.0405], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9953,     0.0025,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1162,     0.0005,     0.0033,     0.5240,     0.3561],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1282, 0.0913, 0.0191, 0.3204, 0.4410], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.007]
 [71.228]
 [61.085]
 [61.395]
 [67.122]] [[0.285]
 [0.322]
 [0.233]
 [0.236]
 [0.286]]
printing an ep nov before normalisation:  93.77135061959679
actions average: 
K:  1  action  0 :  tensor([    0.6019,     0.0004,     0.0007,     0.2229,     0.1741],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9991,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9984,     0.0007,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1019,     0.0005,     0.0101,     0.5884,     0.2991],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0944, 0.0249, 0.0285, 0.3590, 0.4932], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7561305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.611168259819635
printing an ep nov before normalisation:  67.67578202056805
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.217]
 [35.217]
 [35.217]
 [35.217]
 [35.217]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.294632461077775
printing an ep nov before normalisation:  30.973375548764565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7723,     0.0007,     0.0004,     0.0659,     0.1606],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0181,     0.9584,     0.0003,     0.0001,     0.0231],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9513,     0.0254,     0.0233],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1006, 0.0013, 0.0031, 0.6263, 0.2687], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1890, 0.0013, 0.0013, 0.4672, 0.3411], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.70700693130493
siam score:  -0.74280393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.501]
 [60.211]
 [67.561]
 [65.135]
 [65.807]] [[0.589]
 [0.67 ]
 [0.831]
 [0.778]
 [0.792]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.50675717299632
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.82615225092205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.899]
 [29.517]
 [29.517]
 [29.517]
 [29.517]] [[1.366]
 [0.307]
 [0.307]
 [0.307]
 [0.307]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  99.35786737523676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.941919764122346
actions average: 
K:  3  action  0 :  tensor([    0.8246,     0.0003,     0.0001,     0.0755,     0.0995],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0147,     0.9784,     0.0000,     0.0002,     0.0066],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0035,     0.9947,     0.0009,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0843, 0.0011, 0.0462, 0.5493, 0.3191], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1677, 0.0008, 0.0591, 0.3879, 0.3844], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.03747724885079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72226244
actions average: 
K:  4  action  0 :  tensor([0.7484, 0.0029, 0.0012, 0.1356, 0.1119], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0003,     0.9226,     0.0392,     0.0376],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0852, 0.0008, 0.0087, 0.4588, 0.4464], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0941, 0.0010, 0.0060, 0.4109, 0.4880], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
deleting a thread, now have 1 threads
Frames:  24517 train batches done:  2869 episodes:  2007
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  93.33387253558253
siam score:  -0.7088388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.37822136536151
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 62.255]
 [ 62.255]
 [103.29 ]
 [ 62.255]
 [ 62.255]] [[0.123]
 [0.123]
 [0.602]
 [0.123]
 [0.123]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.84260133432318
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actions average: 
K:  1  action  0 :  tensor([    0.7044,     0.0006,     0.0010,     0.1349,     0.1591],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9959,     0.0011,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9992,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0768, 0.0018, 0.0104, 0.6288, 0.2823], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1584, 0.0019, 0.0023, 0.3979, 0.4395], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.386]
 [42.085]
 [40.762]
 [35.799]
 [36.401]] [[1.342]
 [1.185]
 [1.121]
 [0.884]
 [0.913]]
siam score:  -0.7280944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7286103
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7293228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.432]
 [29.178]
 [28.424]
 [27.172]
 [29.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7277111
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.294]
 [42.795]
 [43.544]
 [33.65 ]
 [41.872]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.2157128541152
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.12453970451774
printing an ep nov before normalisation:  82.1714226787609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.605732652700134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [   -0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 1.9028249065740734e-12
0.0 0.0
0.0 3.6248395714934e-21
0.0 0.0
0.0 0.0
0.0 2.0931074013799084e-12
0.0 0.0
0.0 2.0137997619407776e-21
0.0 1.0119568830973687e-11
printing an ep nov before normalisation:  53.862156456662944
printing an ep nov before normalisation:  22.138149225566746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.93883466720581
printing an ep nov before normalisation:  40.09438514709473
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7735,     0.0003,     0.0001,     0.0563,     0.1697],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0029,     0.9944,     0.0000,     0.0000,     0.0026],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0026,     0.9817,     0.0071,     0.0083],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1074,     0.0005,     0.0002,     0.5541,     0.3378],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1929, 0.0004, 0.0668, 0.3084, 0.4315], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7939,     0.0121,     0.0002,     0.0824,     0.1114],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9967,     0.0000,     0.0001,     0.0023],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9906,     0.0058,     0.0034],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1367,     0.0005,     0.0310,     0.5805,     0.2513],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2231, 0.0059, 0.0018, 0.3377, 0.4315], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  80.95433964039817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6812,     0.0005,     0.0004,     0.1290,     0.1889],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0150,     0.9718,     0.0002,     0.0013,     0.0117],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0018, 0.0275, 0.9647, 0.0021, 0.0039], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1229, 0.0006, 0.0251, 0.5791, 0.2721], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2315, 0.0154, 0.0238, 0.3964, 0.3330], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.751105599748517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7085411
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  35.47744280172072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72400403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.963]
 [60.565]
 [81.159]
 [60.565]
 [60.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.60586307769604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.119804791041783
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74829376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.73624134063721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.087]
 [54.056]
 [73.534]
 [54.918]
 [54.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.2629360829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.38568764503835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.6275,     0.0013,     0.0004,     0.1765,     0.1944],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9833,     0.0006,     0.0009,     0.0134],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0004,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0629,     0.0002,     0.0009,     0.6551,     0.2809],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2210, 0.0025, 0.0031, 0.3910, 0.3824], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7064,     0.0012,     0.0002,     0.1567,     0.1355],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9963,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9712,     0.0027,     0.0259],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0756, 0.0009, 0.0008, 0.5939, 0.3289], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1352, 0.0007, 0.0005, 0.4558, 0.4078], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[93.307]
 [88.8  ]
 [69.328]
 [86.149]
 [81.309]] [[0.889]
 [0.811]
 [0.473]
 [0.765]
 [0.681]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7500027
actions average: 
K:  3  action  0 :  tensor([0.5036, 0.0164, 0.0011, 0.1836, 0.2953], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9462,     0.0112,     0.0005,     0.0408],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9678,     0.0159,     0.0162],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1365,     0.0002,     0.0012,     0.4918,     0.3701],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1499, 0.0411, 0.0032, 0.3688, 0.4370], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
line 256 mcts: sample exp_bonus 0.024038633698907453
printing an ep nov before normalisation:  43.74530685927876
siam score:  -0.7555017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.87186060510703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75313485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7552597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.17394388448068
line 256 mcts: sample exp_bonus 53.32970090620179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.014933537470995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74634117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75123185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  89.75349225318233
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.1659879471174
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75603163
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.113]
 [75.113]
 [81.57 ]
 [75.113]
 [75.113]] [[0.764]
 [0.764]
 [0.875]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.605446912333335
printing an ep nov before normalisation:  16.431621014064444
printing an ep nov before normalisation:  21.2402826430922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.93537028947256
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7003,     0.0004,     0.0003,     0.1551,     0.1439],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9993,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9952,     0.0018,     0.0029],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1382,     0.0003,     0.0030,     0.5996,     0.2589],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1461, 0.0042, 0.0008, 0.3635, 0.4853], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.645567973454796
printing an ep nov before normalisation:  73.58681148377275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.59080366001177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7366826
siam score:  -0.7378889
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.202]
 [42.953]
 [34.923]
 [39.277]
 [47.641]] [[0.942]
 [0.894]
 [0.59 ]
 [0.755]
 [1.072]]
actions average: 
K:  2  action  0 :  tensor([0.6642, 0.0043, 0.0013, 0.1358, 0.1944], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0804,     0.8574,     0.0002,     0.0003,     0.0618],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0037,     0.9381,     0.0281,     0.0301],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1524, 0.0039, 0.0080, 0.4827, 0.3530], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2981, 0.0025, 0.0007, 0.3297, 0.3691], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.71477231208173
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.167]
 [35.052]
 [38.436]
 [28.167]
 [28.167]] [[0.698]
 [1.207]
 [1.457]
 [0.698]
 [0.698]]
printing an ep nov before normalisation:  84.36211868475033
printing an ep nov before normalisation:  80.52585264168555
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  2  action  0 :  tensor([    0.8482,     0.0002,     0.0004,     0.0730,     0.0783],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0169,     0.9648,     0.0000,     0.0000,     0.0183],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0003,     0.9981,     0.0005,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0626, 0.0018, 0.0306, 0.5996, 0.3054], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1401, 0.0014, 0.0547, 0.4131, 0.3908], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.8045,     0.0001,     0.0005,     0.0781,     0.1168],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0106,     0.9783,     0.0044,     0.0000,     0.0066],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9965,     0.0012,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0493, 0.0569, 0.0207, 0.4138, 0.4592], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0704, 0.0024, 0.0120, 0.4121, 0.5032], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [1. 0. 0. 0. 0.]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7393839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.56506541396311
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.332]
 [66.332]
 [78.727]
 [66.332]
 [66.332]] [[1.4]
 [1.4]
 [2. ]
 [1.4]
 [1.4]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.299]
 [76.299]
 [76.299]
 [76.299]
 [76.299]] [[1.899]
 [1.899]
 [1.899]
 [1.899]
 [1.899]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.08637261324613
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.25382163937267
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actions average: 
K:  0  action  0 :  tensor([    0.5777,     0.0005,     0.0029,     0.2469,     0.1721],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0055,     0.9914,     0.0002,     0.0001,     0.0028],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0006,     0.9983,     0.0003,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1735, 0.0006, 0.0025, 0.4908, 0.3325], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1885, 0.0076, 0.0022, 0.3261, 0.4756], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7036,     0.0003,     0.0013,     0.1689,     0.1259],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0046,     0.9855,     0.0004,     0.0002,     0.0093],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9668,     0.0203,     0.0128],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0412,     0.0001,     0.0008,     0.6730,     0.2849],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1955, 0.0015, 0.0062, 0.3305, 0.4662], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  55.127217051214345
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.422]
 [58.442]
 [50.718]
 [47.132]
 [47.132]] [[0.922]
 [0.767]
 [0.566]
 [0.472]
 [0.472]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.947]
 [63.947]
 [64.773]
 [61.048]
 [63.053]] [[0.731]
 [0.731]
 [0.749]
 [0.668]
 [0.711]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.57435961608245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.951]
 [62.867]
 [57.425]
 [72.136]
 [74.271]] [[0.804]
 [0.954]
 [0.871]
 [1.095]
 [1.127]]
printing an ep nov before normalisation:  46.23998851359964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7334039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.73205096
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.69574925301878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.04493745318012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7489,     0.0006,     0.0003,     0.0645,     0.1857],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0015,     0.9943,     0.0001,     0.0001,     0.0040],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9966,     0.0016,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0826,     0.0005,     0.0006,     0.6032,     0.3130],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1299, 0.0055, 0.0014, 0.3732, 0.4901], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.459500312805176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.041]
 [50.136]
 [58.954]
 [51.856]
 [50.434]] [[0.988]
 [1.029]
 [1.365]
 [1.095]
 [1.041]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.796648198682476
actions average: 
K:  2  action  0 :  tensor([    0.6424,     0.0022,     0.0003,     0.1115,     0.2435],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0014,     0.9979,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0112,     0.9574,     0.0145,     0.0168],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1315, 0.0018, 0.0025, 0.5521, 0.3120], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3022, 0.0014, 0.0017, 0.3632, 0.3315], grad_fn=<DivBackward0>)
siam score:  -0.7443103
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.178]
 [34.221]
 [23.724]
 [48.46 ]
 [33.897]] [[1.036]
 [0.631]
 [0.437]
 [0.894]
 [0.625]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  114.84752186777993
printing an ep nov before normalisation:  86.36110989093771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.239449752663894
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.14779103272841
printing an ep nov before normalisation:  38.916442185149876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.642]
 [39.999]
 [31.711]
 [37.298]
 [39.999]] [[1.227]
 [0.675]
 [0.382]
 [0.579]
 [0.675]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 69.93744680816913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.19197680594228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.064364722579114
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.66859595895327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.31771029549842
printing an ep nov before normalisation:  77.1354318647294
siam score:  -0.75219345
printing an ep nov before normalisation:  48.5248649007718
siam score:  -0.75123143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.8740119934082
actions average: 
K:  1  action  0 :  tensor([    0.7101,     0.0003,     0.0005,     0.1251,     0.1640],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9964,     0.0000,     0.0000,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9998,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1380, 0.0006, 0.0012, 0.4826, 0.3776], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1770, 0.0007, 0.0015, 0.4110, 0.4099], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.877]
 [43.672]
 [44.118]
 [43.717]
 [43.69 ]] [[0.974]
 [0.965]
 [0.985]
 [0.967]
 [0.966]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.8839342399141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.496076974080204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7559662
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.426]
 [76.426]
 [77.919]
 [76.426]
 [76.426]] [[1.719]
 [1.719]
 [1.762]
 [1.719]
 [1.719]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 5.777841335119395e-10
0.0 1.2097122847988323e-09
0.0 6.254066513970214e-10
0.0 0.0
0.0 6.238324962480516e-10
0.0 1.961155138581479e-09
0.0 0.0
0.0 5.966566968997822e-10
0.0 1.0599426662826303e-09
0.0 1.4362522390907893e-09
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actions average: 
K:  4  action  0 :  tensor([    0.7065,     0.0061,     0.0006,     0.0890,     0.1978],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0277,     0.9466,     0.0002,     0.0002,     0.0252],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0082,     0.9798,     0.0061,     0.0060],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0326,     0.0003,     0.0797,     0.6186,     0.2687],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0812, 0.0009, 0.0207, 0.4842, 0.4131], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.266]
 [61.266]
 [65.542]
 [59.25 ]
 [61.266]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  35.26166044887637
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  22.768149375915527
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.65817058093529
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.618]
 [28.294]
 [82.728]
 [30.589]
 [29.721]] [[0.053]
 [0.052]
 [0.244]
 [0.06 ]
 [0.057]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.8220, 0.0453, 0.0011, 0.0703, 0.0613], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0447,     0.8870,     0.0009,     0.0006,     0.0669],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9707,     0.0210,     0.0080],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0613, 0.0012, 0.0291, 0.5629, 0.3456], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1178, 0.0474, 0.0469, 0.3577, 0.4303], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.27113255055924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74113035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.17 ]
 [42.17 ]
 [51.926]
 [42.17 ]
 [42.17 ]] [[0.755]
 [0.755]
 [1.   ]
 [0.755]
 [0.755]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.377804682079876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.555]
 [51.977]
 [63.929]
 [50.849]
 [51.977]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7551122
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.01450351030779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.01451284137565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.01275392905428
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.07255373750493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.22708760082233
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.11434268951416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.4223883152008
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.765482526720476
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 77.11533335183191
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  53.82413081845709
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.208 0.167 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.42356819752014
actions average: 
K:  0  action  0 :  tensor([    0.7793,     0.0013,     0.0003,     0.0887,     0.1304],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9982,     0.0001,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0004,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1357,     0.0004,     0.0007,     0.5884,     0.2747],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2295, 0.0118, 0.0011, 0.3261, 0.4315], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.39 ]
 [30.881]
 [41.858]
 [27.173]
 [27.855]] [[0.73 ]
 [0.674]
 [1.085]
 [0.535]
 [0.56 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.851]
 [ 0.   ]
 [37.533]
 [ 0.   ]
 [ 0.   ]] [[0.284]
 [0.   ]
 [0.315]
 [0.   ]
 [0.   ]]
siam score:  -0.745826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.5780079639543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74495184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  91.76904971440709
siam score:  -0.74472094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.2002926382968
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7574582
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.21070265743076
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 -7.09234736663649e-12
0.0 -1.764437638539739e-12
0.0 4.878151123196252e-12
0.0 0.0
0.0 1.0379045003898903e-12
0.0 -8.649203763799468e-14
0.0 0.0
0.0 1.8163328612836398e-12
0.0 0.0
0.0 1.9374217198571276e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.91975406621664
printing an ep nov before normalisation:  41.602845191955566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.025]
 [46.017]
 [33.605]
 [35.658]
 [31.994]] [[0.333]
 [0.479]
 [0.35 ]
 [0.371]
 [0.333]]
siam score:  -0.7443505
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.38210192061971
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.247]
 [44.247]
 [44.247]
 [57.917]
 [44.247]] [[0.624]
 [0.624]
 [0.624]
 [1.074]
 [0.624]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.28330842859569
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.990734656731206
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.583 0.083 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7498701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.76325226466197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.576837196409514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.1419906566113
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.238174864890695
printing an ep nov before normalisation:  70.00792719044199
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6166,     0.0014,     0.0003,     0.1185,     0.2631],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     1.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9989,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1043,     0.0008,     0.0004,     0.4648,     0.4297],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1899, 0.0006, 0.0005, 0.3727, 0.4363], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6955,     0.0005,     0.0004,     0.1429,     0.1608],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9714,     0.0001,     0.0007,     0.0268],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9958,     0.0022,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1122,     0.0005,     0.0006,     0.5645,     0.3222],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1873, 0.0008, 0.0007, 0.4005, 0.4107], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  50.90372253131068
printing an ep nov before normalisation:  68.71478635436058
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.376]
 [44.376]
 [75.552]
 [44.376]
 [44.376]] [[0.324]
 [0.324]
 [0.551]
 [0.324]
 [0.324]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.74139184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.31010875329326
printing an ep nov before normalisation:  18.638328114656133
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74427587
printing an ep nov before normalisation:  38.66861711425181
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6121,     0.0077,     0.0003,     0.1737,     0.2062],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0020,     0.9979,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9994,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1225,     0.0003,     0.0005,     0.5586,     0.3181],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1258, 0.0113, 0.0008, 0.3860, 0.4762], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  77.45178486279647
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 4.2747826432661685e-10
0.0 3.582154377655022e-10
0.0 4.103528401863799e-10
0.0 3.466255042225886e-10
0.0 3.5223018851736375e-10
0.0 3.582154377655022e-10
0.0 2.554628928403065e-10
0.0 0.0
0.0 0.0
0.0 2.4193553759485437e-10
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  81.4634375030531
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.559]
 [59.652]
 [69.821]
 [55.663]
 [56.143]] [[0.813]
 [0.99 ]
 [1.286]
 [0.875]
 [0.888]]
line 256 mcts: sample exp_bonus 46.58015279701702
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.411]
 [39.98 ]
 [28.771]
 [34.411]
 [34.411]] [[0.798]
 [1.061]
 [0.531]
 [0.798]
 [0.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.15067745962544
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.36260795593262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.60443687438965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.03648509937647759
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.006]
 [42.006]
 [42.006]
 [42.006]
 [42.006]] [[1.792]
 [1.792]
 [1.792]
 [1.792]
 [1.792]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.413414001464844
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.089436527106336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.812]
 [71.812]
 [77.966]
 [73.68 ]
 [74.865]] [[0.797]
 [0.797]
 [0.905]
 [0.83 ]
 [0.851]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.7280678171641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.83600616455078
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.6590796886083
siam score:  -0.7553334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.127326074041676
printing an ep nov before normalisation:  50.97428442412306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.16113847252178
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.463]
 [50.463]
 [50.463]
 [54.004]
 [50.463]] [[1.042]
 [1.042]
 [1.042]
 [1.13 ]
 [1.042]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.23 ]
 [33.415]
 [33.415]
 [33.415]
 [33.415]] [[1.323]
 [0.815]
 [0.815]
 [0.815]
 [0.815]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.98804629328825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.10718474047805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.043]
 [63.043]
 [95.879]
 [63.043]
 [86.859]] [[0.611]
 [0.611]
 [1.844]
 [0.611]
 [1.505]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.70288924570639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.38714146614075
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.374]
 [31.156]
 [50.92 ]
 [26.48 ]
 [29.834]] [[0.327]
 [0.44 ]
 [1.246]
 [0.249]
 [0.386]]
printing an ep nov before normalisation:  86.73238720769048
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.941231486818513
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.923]
 [66.923]
 [74.964]
 [66.923]
 [66.923]] [[1.069]
 [1.069]
 [1.296]
 [1.069]
 [1.069]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.18135772301486
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.47841826970383
printing an ep nov before normalisation:  85.19257969326443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7384079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.40937048910752
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.7425265298075
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.386]
 [23.334]
 [38.858]
 [26.697]
 [23.334]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.467]
 [24.467]
 [27.249]
 [15.641]
 [15.994]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.333 0.292 0.292]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.31612815660019
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.47343021141508
printing an ep nov before normalisation:  26.58700427863826
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.86043184897273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7343292
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.344]
 [60.658]
 [62.498]
 [50.204]
 [57.191]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.93632111256959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.968379974365234
printing an ep nov before normalisation:  82.58967959351972
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.67345365702956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.265716077587058
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7339638
printing an ep nov before normalisation:  31.8085832206498
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.383360602502194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.341835379600525
siam score:  -0.7365483
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.9381160736084
actions average: 
K:  4  action  0 :  tensor([    0.7135,     0.0098,     0.0005,     0.1033,     0.1729],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9992,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0032,     0.9963,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1235,     0.0004,     0.0011,     0.5854,     0.2896],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1167, 0.0135, 0.0029, 0.4706, 0.3964], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.917]
 [35.528]
 [41.725]
 [68.679]
 [55.963]] [[0.608]
 [0.456]
 [0.631]
 [1.395]
 [1.035]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.374918212907524
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.80142879486084
printing an ep nov before normalisation:  73.84245198692109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.1099756926214468
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.13 ]
 [32.941]
 [39.955]
 [53.323]
 [35.707]] [[0.54 ]
 [0.497]
 [0.752]
 [1.238]
 [0.598]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.600014629057341
printing an ep nov before normalisation:  44.58351335293855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.118]
 [51.118]
 [54.434]
 [55.885]
 [47.052]] [[0.984]
 [0.984]
 [1.106]
 [1.159]
 [0.835]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.078293535310536
printing an ep nov before normalisation:  71.04123034828417
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.877140534699166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.472226620855864
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  29.916887395875012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.9021816916706
printing an ep nov before normalisation:  7.628532881243798
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.73608124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.76435477454547
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 26.03722375326276
printing an ep nov before normalisation:  21.98166951002804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.6546662921987
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.601]
 [46.924]
 [42.561]
 [37.669]
 [37.498]] [[0.506]
 [0.667]
 [0.605]
 [0.535]
 [0.533]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.30752904042845
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7392046
deleting a thread, now have 1 threads
Frames:  42292 train batches done:  4954 episodes:  3401
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.13405276506188
siam score:  -0.7377902
printing an ep nov before normalisation:  35.90339938401526
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.132]
 [46.132]
 [63.577]
 [46.132]
 [46.132]] [[0.966]
 [0.966]
 [1.543]
 [0.966]
 [0.966]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.66279032152423
printing an ep nov before normalisation:  34.16525048197344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7858,     0.0001,     0.0002,     0.0815,     0.1324],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0820,     0.8787,     0.0001,     0.0007,     0.0385],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9917,     0.0066,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1023,     0.0004,     0.0010,     0.5642,     0.3320],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0529, 0.0007, 0.0047, 0.5279, 0.4138], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7061, 0.0034, 0.0008, 0.1163, 0.1734], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9998,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9914,     0.0020,     0.0065],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0441, 0.0013, 0.0083, 0.6141, 0.3322], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2569, 0.0232, 0.0090, 0.3105, 0.4004], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7455231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.14068778306899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.151]
 [29.92 ]
 [29.92 ]
 [29.92 ]
 [29.92 ]] [[0.789]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
siam score:  -0.74846697
siam score:  -0.74430895
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.166486740112305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6611, 0.0102, 0.0008, 0.1781, 0.1498], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9889,     0.0053,     0.0000,     0.0054],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0015,     0.9956,     0.0012,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0700,     0.0003,     0.0056,     0.6092,     0.3149],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0655, 0.0008, 0.0566, 0.4423, 0.4348], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.763629446620385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.65081132139106
printing an ep nov before normalisation:  37.66652119939248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.111]
 [26.458]
 [49.378]
 [25.783]
 [24.385]] [[0.235]
 [0.203]
 [0.651]
 [0.19 ]
 [0.163]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7473602
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.282]
 [56.282]
 [56.282]
 [56.282]
 [56.282]] [[1.607]
 [1.607]
 [1.607]
 [1.607]
 [1.607]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6750,     0.0002,     0.0002,     0.1287,     0.1959],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0027,     0.9926,     0.0000,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0006,     0.9681,     0.0139,     0.0173],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0569, 0.0006, 0.0011, 0.5347, 0.4068], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1138, 0.0006, 0.0548, 0.4266, 0.4042], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.76187900143795
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.56813993331495
printing an ep nov before normalisation:  55.442865736381755
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.71848201751709
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.7408687
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.550317399804044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.407669716319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.111]
 [29.324]
 [35.621]
 [31.535]
 [40.315]] [[0.127]
 [0.076]
 [0.123]
 [0.093]
 [0.158]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.74259216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.08887016773224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74447036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.81204627192096
siam score:  -0.755406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7564164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.45436096191406
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.07614326477051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.86191208605738
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([0.6969, 0.0425, 0.0147, 0.1062, 0.1397], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9978,     0.0001,     0.0003,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0000,     0.9986,     0.0004,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1016,     0.0004,     0.0010,     0.5586,     0.3385],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2662, 0.0190, 0.0089, 0.3328, 0.3731], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.306170396186715
printing an ep nov before normalisation:  25.492158665659204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.7671,     0.0034,     0.0001,     0.0714,     0.1580],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0022,     0.9928,     0.0019,     0.0003,     0.0028],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0008,     0.0001,     0.9880,     0.0056,     0.0055],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0598, 0.0007, 0.0031, 0.5054, 0.4310], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0845, 0.0014, 0.0029, 0.3820, 0.5292], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.094078495045096
printing an ep nov before normalisation:  37.34882540171705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.433254815992946
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6651,     0.0051,     0.0005,     0.1522,     0.1771],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0144,     0.9659,     0.0003,     0.0002,     0.0192],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9980,     0.0011,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1247,     0.0005,     0.0008,     0.5534,     0.3206],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2076, 0.0036, 0.0031, 0.3310, 0.4547], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.167 0.083 0.167 0.167 0.417]
siam score:  -0.7408411
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.629758414474445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.730703353881836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  71.82627936570472
using explorer policy with actor:  1
printing an ep nov before normalisation:  84.22000408172607
printing an ep nov before normalisation:  42.163171893115226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.732]
 [27.738]
 [40.549]
 [25.556]
 [26.123]] [[0.45 ]
 [0.517]
 [0.944]
 [0.444]
 [0.463]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[95.45 ]
 [92.31 ]
 [96.283]
 [94.843]
 [96.571]] [[1.117]
 [1.047]
 [1.136]
 [1.104]
 [1.142]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6602,     0.0124,     0.0004,     0.1153,     0.2117],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0019,     0.9975,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.9988,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0910,     0.0005,     0.0009,     0.5655,     0.3422],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1180, 0.0018, 0.0025, 0.3356, 0.5421], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.7174,     0.0005,     0.0005,     0.1226,     0.1590],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9948,     0.0001,     0.0001,     0.0049],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9981,     0.0011,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0400, 0.0011, 0.0026, 0.5657, 0.3906], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2459, 0.0147, 0.0049, 0.3127, 0.4217], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.8800,     0.0037,     0.0002,     0.0626,     0.0535],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9995,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0052,     0.9858,     0.0037,     0.0048],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1185, 0.0008, 0.0168, 0.5135, 0.3504], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1053, 0.1190, 0.0111, 0.3602, 0.4043], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74336034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.28437423706055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.41760737756566
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.89]
 [50.89]
 [50.89]
 [50.89]
 [50.89]] [[101.779]
 [101.779]
 [101.779]
 [101.779]
 [101.779]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  80.61619902819821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.86845739162294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6974,     0.0004,     0.0007,     0.1506,     0.1509],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9963,     0.0000,     0.0013,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0009,     0.9964,     0.0015,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0746,     0.0004,     0.0013,     0.6362,     0.2875],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0287, 0.0206, 0.0016, 0.3762, 0.5729], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.53495951135355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6785,     0.0018,     0.0004,     0.0933,     0.2260],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9960,     0.0003,     0.0001,     0.0028],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9996,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0666, 0.0009, 0.0009, 0.6227, 0.3089], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1386, 0.0012, 0.0014, 0.3949, 0.4639], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.49738441624979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.776]
 [44.048]
 [48.703]
 [38.802]
 [49.48 ]] [[1.579]
 [1.426]
 [1.577]
 [1.256]
 [1.602]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.31840927933191
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.467893819223676
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.64788192590026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.114]
 [77.114]
 [77.114]
 [77.114]
 [77.114]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.35186243057252
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.17653836350747
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.82793175310279
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.12963640956595
printing an ep nov before normalisation:  44.71825122833252
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7181,     0.0015,     0.0003,     0.1543,     0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0039,     0.9932,     0.0001,     0.0002,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9868,     0.0121,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0350, 0.0008, 0.0121, 0.6446, 0.3074], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2561, 0.0014, 0.0036, 0.3486, 0.3903], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7422415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.84044362903158
printing an ep nov before normalisation:  42.12850185640433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.02927729898334
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.936]
 [46.936]
 [56.895]
 [49.202]
 [46.936]] [[0.352]
 [0.352]
 [0.508]
 [0.387]
 [0.352]]
printing an ep nov before normalisation:  46.34468078613281
printing an ep nov before normalisation:  96.37317322417142
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.227]
 [64.11 ]
 [75.431]
 [77.595]
 [71.939]] [[0.541]
 [0.401]
 [0.557]
 [0.587]
 [0.509]]
printing an ep nov before normalisation:  76.64535759090096
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.105486145702315
printing an ep nov before normalisation:  95.99559652365545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.18371403553331
printing an ep nov before normalisation:  39.86453229332846
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75356597
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.056]
 [35.205]
 [66.573]
 [59.885]
 [45.05 ]] [[0.106]
 [0.123]
 [0.297]
 [0.26 ]
 [0.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.72196104689444
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.4357904447043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7232,     0.0003,     0.0008,     0.1001,     0.1756],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0012,     0.9974,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0229,     0.9735,     0.0004,     0.0027],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0872,     0.0006,     0.0011,     0.6253,     0.2858],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0749, 0.0007, 0.0046, 0.5112, 0.4086], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.37366261810676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74447596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.7448273
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.67493455389204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.03971502707841
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.082025170877806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.69569587610576
printing an ep nov before normalisation:  61.56373793280593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [55.572]
 [ 0.   ]
 [ 0.   ]] [[-0.669]
 [-0.669]
 [ 1.104]
 [-0.669]
 [-0.669]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.04863313529099
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.016]
 [63.175]
 [62.138]
 [59.878]
 [57.741]] [[0.514]
 [1.109]
 [1.084]
 [1.031]
 [0.98 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.7477378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.6982,     0.0058,     0.0002,     0.1159,     0.1799],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0073,     0.9810,     0.0049,     0.0000,     0.0067],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9991,     0.0004,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0538, 0.0137, 0.0092, 0.5568, 0.3664], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0583, 0.0227, 0.0736, 0.3601, 0.4853], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.74308914
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [42.308]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.656]
 [ 0.775]
 [-0.656]
 [-0.656]
 [-0.656]]
actions average: 
K:  4  action  0 :  tensor([    0.7628,     0.0096,     0.0004,     0.0981,     0.1292],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0010,     0.9982,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9988,     0.0006,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0341,     0.0004,     0.0025,     0.6212,     0.3418],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1477, 0.0028, 0.0041, 0.4163, 0.4291], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.238361155942812
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.72765981999509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7514788
siam score:  -0.75137734
actions average: 
K:  2  action  0 :  tensor([    0.7860,     0.0231,     0.0002,     0.0888,     0.1019],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0006,     0.9963,     0.0010,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0984, 0.0009, 0.0048, 0.5880, 0.3078], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2171, 0.0016, 0.0012, 0.3670, 0.4130], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.76725695034861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.405275120773666
Starting evaluation
printing an ep nov before normalisation:  40.303629252533014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.385637312628816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.74382014318767
printing an ep nov before normalisation:  46.33150259227247
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7854,     0.0003,     0.0001,     0.1159,     0.0983],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0011,     0.9719,     0.0007,     0.0011,     0.0252],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9962,     0.0017,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0979,     0.0005,     0.0009,     0.5182,     0.3825],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1088, 0.0014, 0.0013, 0.4159, 0.4727], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.957944935558004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.235820706666296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.511]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 0.495]
 [-0.191]
 [-0.191]
 [-0.191]
 [-0.191]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.6532098113121
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.43291551727402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.956]
 [43.956]
 [43.956]
 [43.956]
 [43.956]] [[0.182]
 [0.182]
 [0.182]
 [0.182]
 [0.182]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.64041966421814
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.28537336677295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.35235655199978
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.90723872152657
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.43013730125541
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 61.83899545345517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.45599862349547
siam score:  -0.7258672
printing an ep nov before normalisation:  59.76308492232032
actions average: 
K:  3  action  0 :  tensor([    0.8460,     0.0003,     0.0006,     0.0474,     0.1057],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0075,     0.9848,     0.0012,     0.0000,     0.0064],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9939,     0.0033,     0.0028],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0622,     0.0004,     0.0465,     0.5898,     0.3011],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1577, 0.0095, 0.0281, 0.3927, 0.4121], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.716]
 [40.716]
 [40.716]
 [40.716]
 [40.716]] [[0.527]
 [0.527]
 [0.527]
 [0.527]
 [0.527]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  82.34745934024143
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.694967639491885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.003]
 [73.003]
 [90.804]
 [77.212]
 [76.644]] [[0.227]
 [0.227]
 [0.333]
 [0.252]
 [0.249]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.5637, 0.0006, 0.0009, 0.2371, 0.1976], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0033,     0.9903,     0.0029,     0.0001,     0.0034],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0003,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0648,     0.0002,     0.0018,     0.6938,     0.2394],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1065,     0.0002,     0.0019,     0.5104,     0.3809],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.14358293291107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.90064226420508
siam score:  -0.7525502
printing an ep nov before normalisation:  28.76966340201242
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.879617790078814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.27137944423501
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7542586
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.47029952471263
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.402293311225044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.7096,     0.0002,     0.0004,     0.1329,     0.1568],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0366,     0.9603,     0.0002,     0.0007,     0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1191,     0.0003,     0.0003,     0.5767,     0.3036],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.2438,     0.0003,     0.0010,     0.3722,     0.3827],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7501349
siam score:  -0.750404
siam score:  -0.7505393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.33855938911438
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8915,     0.0018,     0.0001,     0.0366,     0.0700],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0324,     0.9148,     0.0000,     0.0000,     0.0528],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9981,     0.0005,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0967, 0.0011, 0.0015, 0.5807, 0.3201], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1284, 0.0013, 0.0012, 0.4830, 0.3860], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7631,     0.0148,     0.0003,     0.0762,     0.1456],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9807,     0.0129,     0.0001,     0.0061],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9998,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0906,     0.0006,     0.0010,     0.5900,     0.3179],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1360, 0.0012, 0.0041, 0.4420, 0.4167], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.217]
 [51.217]
 [51.217]
 [51.217]
 [51.217]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75627595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.65886926651001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.26666559740507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.48832251528268
actions average: 
K:  1  action  0 :  tensor([    0.8521,     0.0053,     0.0002,     0.0483,     0.0942],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0117,     0.9782,     0.0022,     0.0009,     0.0070],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9998,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0434,     0.0004,     0.0026,     0.6153,     0.3383],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1115, 0.0188, 0.0279, 0.3588, 0.4830], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7384622
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7405339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.43185997009277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.328392007102774
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7557051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7591052
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.219]
 [28.219]
 [48.745]
 [28.219]
 [28.219]] [[0.705]
 [0.705]
 [1.683]
 [0.705]
 [0.705]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.417]
 [49.417]
 [53.829]
 [49.417]
 [49.417]] [[0.502]
 [0.502]
 [0.587]
 [0.502]
 [0.502]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75387955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  28.655868607546243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.157626264452915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.948]
 [28.948]
 [62.757]
 [28.948]
 [28.948]] [[0.338]
 [0.338]
 [0.994]
 [0.338]
 [0.338]]
printing an ep nov before normalisation:  33.28170116790645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.44979985555013
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.77931620917364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7398643
printing an ep nov before normalisation:  87.38531729075663
siam score:  -0.73862654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.372816554309374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7777,     0.0071,     0.0002,     0.0912,     0.1237],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9978,     0.0012,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0000,     0.9980,     0.0008,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1324,     0.0003,     0.0007,     0.5629,     0.3036],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2079, 0.0190, 0.0018, 0.3223, 0.4489], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.32350351642901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76126367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.511878424647882
printing an ep nov before normalisation:  63.96519660944539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.32861630364628
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.81085318872435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7403,     0.0013,     0.0002,     0.1271,     0.1311],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0106,     0.9756,     0.0001,     0.0000,     0.0136],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0002,     0.9957,     0.0020,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0428, 0.0010, 0.0443, 0.6263, 0.2856], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1311, 0.0010, 0.0202, 0.4575, 0.3902], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.316365843551395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7587726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.592708863396126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.23999048378532
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.708754159655534
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.167 0.5   0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7539415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.167 0.042 0.125 0.583 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.49247835489538
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.31898858562664
siam score:  -0.7503496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.657074247087753
actions average: 
K:  0  action  0 :  tensor([    0.7304,     0.0027,     0.0005,     0.0916,     0.1748],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0012,     0.9972,     0.0004,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9990,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1098, 0.0054, 0.0020, 0.5949, 0.2878], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2869, 0.0082, 0.0011, 0.2709, 0.4329], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.167]
 [44.927]
 [39.564]
 [68.334]
 [40.575]] [[0.298]
 [0.325]
 [0.286]
 [0.495]
 [0.294]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.997]
 [85.771]
 [90.913]
 [91.018]
 [88.57 ]] [[0.503]
 [0.525]
 [0.59 ]
 [0.591]
 [0.561]]
actions average: 
K:  4  action  0 :  tensor([0.7206, 0.0137, 0.0011, 0.1183, 0.1463], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0014, 0.8819, 0.0030, 0.0391, 0.0746], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9987,     0.0010,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0488,     0.0005,     0.0107,     0.6627,     0.2774],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1013, 0.1691, 0.0051, 0.2528, 0.4717], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  72.84323365715257
actions average: 
K:  2  action  0 :  tensor([    0.7723,     0.0002,     0.0001,     0.0993,     0.1281],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0126,     0.9607,     0.0004,     0.0006,     0.0257],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9966,     0.0024,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0928,     0.0004,     0.0009,     0.5937,     0.3122],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.1462,     0.0004,     0.0013,     0.3713,     0.4808],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7467317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7559,     0.0034,     0.0004,     0.0640,     0.1763],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0029,     0.9936,     0.0001,     0.0003,     0.0032],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9990,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0461,     0.0006,     0.0012,     0.6750,     0.2772],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0493, 0.0760, 0.0012, 0.4542, 0.4194], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.96083892513086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.167 0.208 0.125 0.167 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  46.35090756550875
siam score:  -0.7505852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74892366
printing an ep nov before normalisation:  55.35626447683648
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  79.08406749971562
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75317025
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.21139558356029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.407449268297135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.7367,     0.0006,     0.0015,     0.0969,     0.1643],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.1063,     0.8599,     0.0003,     0.0003,     0.0332],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9977,     0.0007,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0340,     0.0001,     0.0011,     0.7366,     0.2281],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2228, 0.0092, 0.0024, 0.2982, 0.4674], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  26.125508811181856
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.649222373962402
printing an ep nov before normalisation:  43.716755519866524
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.764194011688232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.229]
 [38.515]
 [37.441]
 [31.265]
 [31.246]] [[0.654]
 [0.554]
 [0.526]
 [0.36 ]
 [0.359]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.73794174194336
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.422539433544998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.68289403614256
printing an ep nov before normalisation:  63.174532413421346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0. 0. 1. 0. 0.]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.147]
 [60.147]
 [60.147]
 [60.147]
 [60.147]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
actions average: 
K:  3  action  0 :  tensor([    0.7618,     0.0002,     0.0011,     0.1057,     0.1312],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9969,     0.0000,     0.0008,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0002,     0.9919,     0.0042,     0.0036],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0548,     0.0003,     0.0013,     0.6085,     0.3352],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0909,     0.0005,     0.0035,     0.3941,     0.5109],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.50303585363285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.1855763082009
printing an ep nov before normalisation:  48.88706962204161
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.683142636946716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8012,     0.0003,     0.0003,     0.0681,     0.1301],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0022,     0.9961,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9996,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0996,     0.0004,     0.0012,     0.5496,     0.3492],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1444, 0.0071, 0.0012, 0.3330, 0.5143], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.92995711735317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.658473733392746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.99608306791812
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75530726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7572822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.294]
 [26.941]
 [49.837]
 [43.648]
 [38.175]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  18.196773529052734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.038817456202
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.006265389679626
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.162]
 [34.162]
 [57.893]
 [38.138]
 [38.284]] [[0.126]
 [0.126]
 [0.293]
 [0.154]
 [0.155]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.778549671173096
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.419]
 [32.419]
 [38.645]
 [32.419]
 [32.419]] [[0.711]
 [0.711]
 [0.972]
 [0.711]
 [0.711]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.05848322150382
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [   -0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 2.5601644192855418e-11
0.0 1.269703164530127e-11
0.0 0.0
0.0 0.0
0.0 1.956449970586267e-11
0.0 6.850169664391633e-12
0.0 1.1589933573523383e-12
0.0 2.696821845407168e-11
0.0 6.694483988507491e-12
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.233]
 [50.948]
 [54.38 ]
 [43.7  ]
 [43.073]] [[0.446]
 [0.791]
 [0.871]
 [0.621]
 [0.606]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  8.371439576148987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.066868525867411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.417 0.125 0.167 0.167 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74991256
printing an ep nov before normalisation:  25.735665251985015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7469046
siam score:  -0.7457221
printing an ep nov before normalisation:  70.89948590437797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7473152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.21891403198242
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.195422649383545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.458 0.458 0.042]
printing an ep nov before normalisation:  60.69301264827816
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.509]
 [70.509]
 [70.509]
 [70.509]
 [70.509]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.890567779541016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.6765,     0.0582,     0.0003,     0.1325,     0.1325],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9992,     0.0001,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0011,     0.9767,     0.0003,     0.0214],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0116, 0.0452, 0.0065, 0.6392, 0.2975], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1474, 0.0268, 0.0041, 0.3471, 0.4745], grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([    0.8616,     0.0011,     0.0002,     0.0590,     0.0780],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0017,     0.9957,     0.0002,     0.0005,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9979,     0.0008,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1093,     0.0006,     0.0020,     0.6415,     0.2466],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2056, 0.0006, 0.0011, 0.3856, 0.4072], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7449982
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.17872558422761
siam score:  -0.7420431
printing an ep nov before normalisation:  52.08432674407959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 62.895818707569944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.303160667419434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  63.76113210405622
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8858,     0.0003,     0.0001,     0.0499,     0.0639],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0070,     0.9838,     0.0002,     0.0022,     0.0067],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0010,     0.9923,     0.0004,     0.0063],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0550,     0.0004,     0.0005,     0.6184,     0.3258],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1324, 0.0306, 0.0040, 0.3177, 0.5153], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.059995619564813
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.81718302657386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.04811688327775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.7766,     0.0003,     0.0003,     0.0847,     0.1380],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9985,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9986,     0.0007,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0608,     0.0006,     0.0003,     0.6715,     0.2668],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1668, 0.0005, 0.0009, 0.4503, 0.3815], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7512657
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.4317202550615
printing an ep nov before normalisation:  33.12472706804211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8380,     0.0004,     0.0007,     0.0786,     0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0038,     0.9937,     0.0000,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0004,     0.9983,     0.0005,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1184, 0.0016, 0.0009, 0.5561, 0.3230], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1744, 0.0080, 0.0072, 0.3156, 0.4948], grad_fn=<DivBackward0>)
siam score:  -0.74834013
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.6376,     0.0014,     0.0006,     0.1693,     0.1911],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0040,     0.9942,     0.0000,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0003,     0.9939,     0.0023,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0644, 0.0008, 0.0008, 0.5608, 0.3731], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1328, 0.0009, 0.0011, 0.3922, 0.4730], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7467575
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  94.81693405528507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 36.0717031955719
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7483142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.25  0.25  0.167 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.85980178652977
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.8]
 [39.8]
 [39.8]
 [39.8]
 [39.8]] [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7045, 0.0058, 0.0012, 0.1026, 0.1859], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0095,     0.9795,     0.0009,     0.0001,     0.0099],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0029,     0.9954,     0.0005,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0963,     0.0006,     0.0004,     0.5598,     0.3429],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1474, 0.0036, 0.0373, 0.3167, 0.4949], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.67269944225437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.33847913496039
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.913131952285767
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.5758743406611
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.93249720256119
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.60631752136469
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.487615771135713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0015759256437301399
actions average: 
K:  3  action  0 :  tensor([    0.7217,     0.0001,     0.0002,     0.1602,     0.1178],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9973,     0.0001,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9976,     0.0006,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0486, 0.0014, 0.0008, 0.6280, 0.3212], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0758, 0.0008, 0.0027, 0.3960, 0.5247], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.8537,     0.0005,     0.0007,     0.0593,     0.0858],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9919,     0.0053,     0.0007,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0163, 0.0116, 0.9532, 0.0124, 0.0065], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0382,     0.0003,     0.0038,     0.6865,     0.2711],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1061, 0.0006, 0.0016, 0.4750, 0.4168], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  46.63569493259999
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.141]
 [36.141]
 [47.306]
 [36.141]
 [36.141]] [[0.087]
 [0.087]
 [0.158]
 [0.087]
 [0.087]]
using explorer policy with actor:  1
siam score:  -0.7658653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76429045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.334648135304874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7645211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  9.915163999105232
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.05337542938079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75575376
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.131]
 [24.166]
 [51.605]
 [29.788]
 [29.672]] [[0.067]
 [0.159]
 [0.996]
 [0.331]
 [0.327]]
printing an ep nov before normalisation:  61.43433661072638
printing an ep nov before normalisation:  69.36627114138996
actions average: 
K:  0  action  0 :  tensor([0.7823, 0.0019, 0.0019, 0.0977, 0.1161], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0044,     0.9923,     0.0031,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9991,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0755,     0.0006,     0.0038,     0.6417,     0.2784],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2662, 0.0421, 0.0026, 0.2603, 0.4288], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.42259613056001
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.47283720349623
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  62.41343041513332
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.53439207929547
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.376]
 [44.376]
 [44.376]
 [44.376]
 [44.376]] [[1.113]
 [1.113]
 [1.113]
 [1.113]
 [1.113]]
printing an ep nov before normalisation:  34.14186239242554
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.6782,     0.0003,     0.0005,     0.1346,     0.1864],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9983,     0.0002,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0000,     0.9980,     0.0012,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0243,     0.0005,     0.0576,     0.5750,     0.3425],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0913, 0.0012, 0.0789, 0.3857, 0.4429], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.67 ]
 [28.67 ]
 [36.603]
 [28.67 ]
 [28.67 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.35776635973479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.46670975436024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.703862404613574
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0013394540155786672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.478504180908203
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  77.01752505351462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.308553787950636
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.8124,     0.0005,     0.0006,     0.0875,     0.0990],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0376, 0.9473, 0.0037, 0.0012, 0.0101], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0001,     0.9989,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0991, 0.0021, 0.0006, 0.5986, 0.2997], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0808, 0.0243, 0.0054, 0.4017, 0.4878], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.665969374702968
printing an ep nov before normalisation:  40.40401909566902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74333143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  73.77613463277497
printing an ep nov before normalisation:  62.455830120366194
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.74876297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.26444445491304
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.73657065226565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.71 ]
 [22.036]
 [25.896]
 [24.571]
 [27.022]] [[0.851]
 [0.789]
 [1.142]
 [1.021]
 [1.245]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.15261289808485
printing an ep nov before normalisation:  33.25517654418945
printing an ep nov before normalisation:  30.68237379558082
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.344]
 [36.344]
 [47.437]
 [39.518]
 [40.691]] [[0.442]
 [0.442]
 [0.605]
 [0.489]
 [0.506]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.7608585357666
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.118]
 [36.118]
 [55.   ]
 [36.118]
 [36.118]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.063684102779476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7654269
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.297496540611156
siam score:  -0.7697337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.378]
 [50.378]
 [50.378]
 [50.378]
 [50.378]] [[1.659]
 [1.659]
 [1.659]
 [1.659]
 [1.659]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7561,     0.0003,     0.0004,     0.1225,     0.1207],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0054,     0.9863,     0.0002,     0.0000,     0.0081],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9959,     0.0017,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0769,     0.0003,     0.0026,     0.6536,     0.2666],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1344, 0.0007, 0.0323, 0.4537, 0.3789], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.912]
 [48.78 ]
 [62.708]
 [63.801]
 [60.512]] [[0.522]
 [0.55 ]
 [0.997]
 [1.032]
 [0.927]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  67.40125394103411
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.28617064158122
printing an ep nov before normalisation:  9.098886343350898
printing an ep nov before normalisation:  20.578620831171673
siam score:  -0.75593317
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.521]
 [25.521]
 [25.521]
 [25.521]
 [25.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.65248680114746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.962460955296276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.779]
 [36.604]
 [36.028]
 [34.779]
 [34.779]] [[1.437]
 [1.514]
 [1.49 ]
 [1.437]
 [1.437]]
printing an ep nov before normalisation:  11.522021970063179
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.71613836288452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.181301067383785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.06282844490832
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
UNIT TEST: sample policy line 217 mcts : [0.042 0.167 0.042 0.708 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.87130492709003
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.21443414039946
printing an ep nov before normalisation:  59.45297783819997
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.39103659841797
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.326]
 [47.326]
 [47.326]
 [47.326]
 [47.326]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  31.66090431681843
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.07127510542673
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.8692103431318
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.905967712402344
printing an ep nov before normalisation:  10.123880012161381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.496]
 [25.164]
 [61.387]
 [37.916]
 [34.557]] [[0.664]
 [0.211]
 [0.79 ]
 [0.415]
 [0.361]]
printing an ep nov before normalisation:  45.51480293672912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7598391
printing an ep nov before normalisation:  62.88585328996699
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.630795490823054
printing an ep nov before normalisation:  43.02287288077613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.680107908531294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.982]
 [34.982]
 [22.377]
 [34.982]
 [34.982]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  32.68770558802502
printing an ep nov before normalisation:  39.85319269824557
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.587]
 [26.587]
 [26.587]
 [26.587]
 [26.587]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.04847387557376
printing an ep nov before normalisation:  34.81477822349874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.727847067919775
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.833]
 [34.833]
 [52.176]
 [42.36 ]
 [35.32 ]] [[0.115]
 [0.115]
 [0.224]
 [0.162]
 [0.118]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [28.95 ]
 [13.293]
 [ 0.   ]] [[-0.218]
 [-0.218]
 [ 0.624]
 [ 0.169]
 [-0.218]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.605519546945025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.6379984497849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7431, 0.0076, 0.0014, 0.0935, 0.1545], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9987,     0.0001,     0.0004,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9986,     0.0006,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0697,     0.0004,     0.0017,     0.6705,     0.2577],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0695, 0.0113, 0.0032, 0.3732, 0.5427], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.75656897
line 256 mcts: sample exp_bonus 73.40340536585755
printing an ep nov before normalisation:  79.90248715451943
printing an ep nov before normalisation:  45.07348784348332
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.03135661749404
printing an ep nov before normalisation:  47.45385800790348
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.279666900634766
printing an ep nov before normalisation:  67.72346466552749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7563461
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.737668618360004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.954510566751175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.337427328835176
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.257]
 [65.21 ]
 [45.84 ]
 [45.84 ]
 [45.84 ]] [[0.752]
 [1.16 ]
 [0.694]
 [0.694]
 [0.694]]
printing an ep nov before normalisation:  64.48299619547423
printing an ep nov before normalisation:  44.99925564027548
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8992,     0.0020,     0.0002,     0.0310,     0.0676],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0016,     0.9959,     0.0000,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0012, 0.0022, 0.9744, 0.0105, 0.0118], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0851, 0.0012, 0.0012, 0.5574, 0.3550], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0839, 0.0019, 0.0008, 0.5033, 0.4102], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.532599386955592
printing an ep nov before normalisation:  30.44268890808702
printing an ep nov before normalisation:  30.229811668395996
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.241]
 [34.904]
 [32.86 ]
 [25.711]
 [29.33 ]] [[0.732]
 [1.109]
 [0.993]
 [0.588]
 [0.793]]
siam score:  -0.757674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  42.07537651062012
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.554]
 [42.554]
 [11.439]
 [42.554]
 [42.554]] [[1.27 ]
 [1.27 ]
 [0.079]
 [1.27 ]
 [1.27 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.894]
 [69.894]
 [69.894]
 [69.123]
 [68.277]] [[1.258]
 [1.258]
 [1.258]
 [1.238]
 [1.215]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.488207613186184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.036]
 [52.133]
 [52.133]
 [52.133]
 [52.133]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7616002
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.699]
 [46.719]
 [50.688]
 [41.95 ]
 [41.864]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.597244717483605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.06942272186279
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.21633554436087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.11058260612142
printing an ep nov before normalisation:  71.99204603612353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7684,     0.0008,     0.0004,     0.0980,     0.1324],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9984,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0110,     0.9844,     0.0007,     0.0031],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0835, 0.0010, 0.0046, 0.5517, 0.3592], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0234, 0.0051, 0.0123, 0.4688, 0.4903], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75153196
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.14162904116312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.540231704711914
printing an ep nov before normalisation:  53.38678286067847
printing an ep nov before normalisation:  66.38997399375243
printing an ep nov before normalisation:  42.788591384887695
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.544113668678406
printing an ep nov before normalisation:  33.90890598297119
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.7146,     0.0007,     0.0002,     0.1443,     0.1402],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0073,     0.9845,     0.0002,     0.0002,     0.0078],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0015,     0.9962,     0.0008,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1406,     0.0005,     0.0004,     0.5616,     0.2969],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1369, 0.0145, 0.0020, 0.4125, 0.4341], grad_fn=<DivBackward0>)
siam score:  -0.7544629
siam score:  -0.75861174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  50.95878601074219
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  26.809446811676025
deleting a thread, now have 1 threads
Frames:  73952 train batches done:  8664 episodes:  5835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  25.671121760858284
actions average: 
K:  1  action  0 :  tensor([    0.8161,     0.0010,     0.0004,     0.0682,     0.1143],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9955,     0.0009,     0.0035],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0832, 0.0009, 0.0013, 0.5205, 0.3941], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1063, 0.0011, 0.0011, 0.3165, 0.5750], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.938687281708
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 41.214010140356606
UNIT TEST: sample policy line 217 mcts : [0.083 0.292 0.542 0.042 0.042]
actions average: 
K:  1  action  0 :  tensor([    0.8449,     0.0003,     0.0001,     0.0859,     0.0687],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9999,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0004,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0453, 0.0009, 0.0019, 0.6727, 0.2793], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1150, 0.0008, 0.0014, 0.5047, 0.3780], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.432]
 [43.286]
 [45.857]
 [46.389]
 [49.427]] [[0.816]
 [1.161]
 [1.23 ]
 [1.244]
 [1.325]]
printing an ep nov before normalisation:  71.09990707124996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7575038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75703186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7804,     0.0132,     0.0004,     0.0670,     0.1390],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0255,     0.9642,     0.0025,     0.0075],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0721, 0.0006, 0.0146, 0.6056, 0.3071], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0107, 0.0016, 0.0011, 0.5820, 0.4047], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.730390092799293
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.70056426735672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.3500844954985
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75177413
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7838,     0.0337,     0.0002,     0.0723,     0.1100],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0108,     0.9697,     0.0005,     0.0001,     0.0188],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0001,     0.9955,     0.0015,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0591,     0.0004,     0.0033,     0.5713,     0.3660],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2873, 0.0066, 0.0043, 0.2269, 0.4750], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.72218670962948
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.34156882018943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.641]
 [60.641]
 [60.641]
 [60.641]
 [60.641]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.175]
 [52.175]
 [60.654]
 [56.682]
 [52.175]] [[0.937]
 [0.937]
 [1.227]
 [1.091]
 [0.937]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.52597927967805
printing an ep nov before normalisation:  54.87584767784663
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7679567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.41908214326628
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.45625270209214
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7571809
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75951815
printing an ep nov before normalisation:  29.741441233696076
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.014]
 [27.014]
 [27.014]
 [27.014]
 [27.014]] [[0.16]
 [0.16]
 [0.16]
 [0.16]
 [0.16]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.05440324233435
siam score:  -0.7580545
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.759285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.53217682772112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.362808877842575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76112795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76138353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.538]
 [34.633]
 [34.633]
 [34.633]
 [34.633]] [[0.985]
 [0.942]
 [0.942]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.49375343322754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.18494384966954
siam score:  -0.766476
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.533123573694105
actions average: 
K:  0  action  0 :  tensor([    0.8113,     0.0003,     0.0002,     0.0656,     0.1227],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0214,     0.9713,     0.0001,     0.0000,     0.0071],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9982,     0.0008,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0874,     0.0009,     0.0004,     0.5641,     0.3472],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2773, 0.0007, 0.0006, 0.3074, 0.4140], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.93051262982316
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.87055699659294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.149]
 [37.149]
 [37.149]
 [37.149]
 [37.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.211640675862625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.5006566359767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.15920323237113
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.6775037030914
printing an ep nov before normalisation:  42.13022291660309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.039]
 [42.039]
 [42.039]
 [42.039]
 [42.039]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8954,     0.0000,     0.0008,     0.0610,     0.0428],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9881,     0.0009,     0.0039,     0.0055],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0000,     0.9973,     0.0013,     0.0013],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0236,     0.0003,     0.0101,     0.6950,     0.2710],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1251, 0.0020, 0.0131, 0.4025, 0.4574], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.851347119092594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.712748527526855
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.050161716809576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.202]
 [62.869]
 [45.964]
 [45.964]
 [45.964]] [[1.141]
 [1.349]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.76224872990352
siam score:  -0.7698839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7669455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7192,     0.0006,     0.0003,     0.1123,     0.1677],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9983,     0.0001,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0009,     0.0001,     0.9820,     0.0027,     0.0143],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0732,     0.0004,     0.0005,     0.5977,     0.3282],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1806, 0.0008, 0.0007, 0.4541, 0.3638], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.985293933323454
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.19913680767225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7596096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8321,     0.0001,     0.0010,     0.0816,     0.0852],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0017,     0.9946,     0.0001,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0000,     0.9987,     0.0004,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1265,     0.0003,     0.0010,     0.5420,     0.3302],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1501, 0.0148, 0.0411, 0.3063, 0.4876], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.7382,     0.0932,     0.0006,     0.0286,     0.1394],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0024,     0.9940,     0.0005,     0.0017,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0002,     0.9914,     0.0012,     0.0067],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1065,     0.0003,     0.0010,     0.6091,     0.2831],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1355, 0.0041, 0.0122, 0.3545, 0.4937], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.71196497589699
printing an ep nov before normalisation:  40.04326760297179
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7692231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.256]
 [45.256]
 [45.256]
 [32.82 ]
 [45.256]] [[1.722]
 [1.722]
 [1.722]
 [1.   ]
 [1.722]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([0.6972, 0.0018, 0.0008, 0.0887, 0.2115], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0019,     0.9838,     0.0009,     0.0002,     0.0132],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9998,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0030,     0.0002,     0.0005,     0.6686,     0.3278],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1992, 0.0034, 0.0019, 0.3128, 0.4827], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.99246881067157
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.644]
 [32.062]
 [21.312]
 [15.743]
 [19.276]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.203475952148438
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.292 0.042 0.042 0.583 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.64379429249994
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75683045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.7033,     0.0025,     0.0004,     0.0943,     0.1995],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9908,     0.0000,     0.0002,     0.0086],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0042,     0.9853,     0.0008,     0.0098],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0982, 0.0008, 0.0018, 0.5685, 0.3307], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1518, 0.0009, 0.0009, 0.3560, 0.4904], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.10687782302368
actions average: 
K:  4  action  0 :  tensor([    0.7694,     0.0970,     0.0002,     0.0737,     0.0598],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9961,     0.0001,     0.0004,     0.0029],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0011,     0.0003,     0.9586,     0.0210,     0.0189],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0734,     0.0005,     0.0046,     0.6620,     0.2595],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0632, 0.0386, 0.0013, 0.4350, 0.4620], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  80.24573864256809
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7536403
printing an ep nov before normalisation:  46.06341932981877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.217]
 [65.217]
 [67.433]
 [68.861]
 [67.917]] [[1.388]
 [1.388]
 [1.447]
 [1.485]
 [1.46 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.6732,     0.0003,     0.0002,     0.1656,     0.1606],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9986,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0882,     0.0004,     0.0010,     0.6552,     0.2551],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1278, 0.0055, 0.0012, 0.4259, 0.4396], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.3832194736313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.76794699412956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[11.888]
 [11.888]
 [11.888]
 [11.888]
 [11.888]] [[0.148]
 [0.148]
 [0.148]
 [0.148]
 [0.148]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  26.80648666269932
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.842]
 [31.581]
 [45.228]
 [32.358]
 [42.842]] [[0.491]
 [0.24 ]
 [0.544]
 [0.257]
 [0.491]]
printing an ep nov before normalisation:  51.59601142444618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.657681702199554
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.8511,     0.0007,     0.0052,     0.0537,     0.0893],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9980,     0.0001,     0.0001,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0018,     0.0003,     0.9894,     0.0055,     0.0031],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0143, 0.0011, 0.0052, 0.6630, 0.3164], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1415, 0.0858, 0.0035, 0.3619, 0.4073], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.5939196666188877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.074]
 [60.377]
 [50.074]
 [50.074]
 [50.074]] [[0.939]
 [1.349]
 [0.939]
 [0.939]
 [0.939]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.84860496042876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.085234249919797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7608262
printing an ep nov before normalisation:  28.988140162328474
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.333 0.083 0.083]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.  ]
 [ 0.  ]
 [54.04]
 [ 0.  ]
 [ 0.  ]] [[-0.245]
 [-0.245]
 [ 0.562]
 [-0.245]
 [-0.245]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.33135819035717
printing an ep nov before normalisation:  61.386511676582444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75315773
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7561981
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.50940313554692
siam score:  -0.7560829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.87508375317366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.6237671846316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7613,     0.0006,     0.0006,     0.0962,     0.1413],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9909,     0.0035,     0.0001,     0.0032],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9994,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0738, 0.0007, 0.0037, 0.6263, 0.2955], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1380, 0.0044, 0.0082, 0.3275, 0.5220], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.390128135681152
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  28.089383022923645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.02556044301682
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.7695472
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  28.363389660156813
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76962173
printing an ep nov before normalisation:  30.045053958892822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [46.127]
 [ 0.   ]
 [ 0.   ]] [[-0.19 ]
 [-0.19 ]
 [ 0.366]
 [-0.19 ]
 [-0.19 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7692148
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.714510406205825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.39848119422359
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.65927791595459
siam score:  -0.76832914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.21 ]
 [40.21 ]
 [71.364]
 [40.21 ]
 [40.21 ]] [[0.319]
 [0.319]
 [0.79 ]
 [0.319]
 [0.319]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.10317760413413
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.178033864707498
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.191]
 [35.191]
 [68.531]
 [49.054]
 [35.191]] [[0.181]
 [0.181]
 [0.509]
 [0.318]
 [0.181]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.7737, 0.0010, 0.0019, 0.0889, 0.1345], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9966,     0.0006,     0.0001,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0038,     0.9886,     0.0004,     0.0070],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0401,     0.0003,     0.0007,     0.7378,     0.2211],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1609, 0.0029, 0.0032, 0.4328, 0.4001], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  28.855092959087514
printing an ep nov before normalisation:  53.90273687855278
printing an ep nov before normalisation:  36.727434682964756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.8120156183275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.99479879651751
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.188181207722607
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.9934104915093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.355066287047805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.55268347397796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.83756088125855
siam score:  -0.77268976
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.429]
 [33.651]
 [28.701]
 [44.958]
 [26.608]] [[0.127]
 [0.221]
 [0.164]
 [0.349]
 [0.141]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.37318357815572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.21357345184461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.930438557893865
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.04]
 [75.04]
 [75.04]
 [75.04]
 [75.04]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.762026
siam score:  -0.7610014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.01680040359497
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.7830,     0.0113,     0.0007,     0.0663,     0.1387],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9921,     0.0008,     0.0023,     0.0044],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9999,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0656,     0.0004,     0.0049,     0.6417,     0.2874],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1324, 0.0009, 0.0105, 0.3970, 0.4592], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.391]
 [32.391]
 [32.391]
 [69.373]
 [32.391]] [[0.389]
 [0.389]
 [0.389]
 [1.17 ]
 [0.389]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.787]
 [34.787]
 [34.787]
 [34.787]
 [34.787]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7621254
printing an ep nov before normalisation:  42.06469760661075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.41536378773429
printing an ep nov before normalisation:  27.705295085906982
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.118]
 [70.118]
 [70.118]
 [70.118]
 [70.118]] [[1.374]
 [1.374]
 [1.374]
 [1.374]
 [1.374]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.436119079589844
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.144]
 [40.144]
 [40.144]
 [40.144]
 [40.144]] [[80.287]
 [80.287]
 [80.287]
 [80.287]
 [80.287]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74805224
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75438935
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.542 0.208 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9149,     0.0007,     0.0005,     0.0446,     0.0394],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0073,     0.9910,     0.0005,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0015,     0.0001,     0.9918,     0.0027,     0.0038],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0279,     0.0006,     0.0006,     0.6560,     0.3149],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0307, 0.0527, 0.0038, 0.3012, 0.6116], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76315796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.92365837097168
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.495428372392965
siam score:  -0.76986676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77529794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.04645649592082
siam score:  -0.7654553
printing an ep nov before normalisation:  58.26051849503449
siam score:  -0.76550734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7644796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  8.818099465170093
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76211566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.162971961409198
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.28230211469862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.247]
 [45.207]
 [32.247]
 [32.247]
 [32.247]] [[0.7  ]
 [1.108]
 [0.7  ]
 [0.7  ]
 [0.7  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.784]
 [33.784]
 [33.784]
 [33.784]
 [33.784]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
printing an ep nov before normalisation:  51.5942275980418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.10940933227539
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.6122,     0.0006,     0.0002,     0.1991,     0.1879],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0036,     0.9928,     0.0001,     0.0002,     0.0033],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9995,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0629,     0.0004,     0.0026,     0.6562,     0.2778],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1017, 0.0163, 0.0020, 0.3986, 0.4814], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.29187326505675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.98300201496997
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76020133
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.79290533065796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.95399284362793
actions average: 
K:  2  action  0 :  tensor([    0.7232,     0.0399,     0.0005,     0.0807,     0.1557],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9995,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0001,     0.9974,     0.0010,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0432, 0.0013, 0.0010, 0.6858, 0.2686], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1669, 0.0401, 0.0029, 0.3284, 0.4617], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.007550629731284
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.57898497732473
line 256 mcts: sample exp_bonus 47.03969760899078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.75657310176042
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.74222199806857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.964]
 [28.964]
 [28.964]
 [28.964]
 [28.964]] [[0.14]
 [0.14]
 [0.14]
 [0.14]
 [0.14]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  33.149031566828945
printing an ep nov before normalisation:  35.53049890343317
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.10144686079273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.81437110900879
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.30889203316376
printing an ep nov before normalisation:  90.78198268745493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7659436
printing an ep nov before normalisation:  26.895957522922092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7234,     0.0009,     0.0004,     0.1305,     0.1448],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9991,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0034,     0.9960,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1535,     0.0008,     0.0005,     0.6060,     0.2392],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1614, 0.0015, 0.0007, 0.4405, 0.3958], grad_fn=<DivBackward0>)
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.41659707906808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.983]
 [41.983]
 [41.983]
 [41.983]
 [41.983]] [[83.966]
 [83.966]
 [83.966]
 [83.966]
 [83.966]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7603493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.7386474609375
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7397,     0.0008,     0.0003,     0.1184,     0.1408],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9984,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0000,     0.9925,     0.0050,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0959, 0.0015, 0.0010, 0.5637, 0.3380], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1541, 0.0021, 0.0031, 0.4227, 0.4181], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.74714368594473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7663,     0.0008,     0.0003,     0.1028,     0.1299],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0010,     0.9321,     0.0004,     0.0004,     0.0661],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0013,     0.0002,     0.9909,     0.0009,     0.0066],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0247,     0.0004,     0.0029,     0.7223,     0.2498],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0959, 0.0010, 0.0006, 0.4813, 0.4212], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76949847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.645412602625086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.303]
 [62.445]
 [62.445]
 [69.314]
 [62.445]] [[1.158]
 [1.396]
 [1.396]
 [1.597]
 [1.396]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.784727774680874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76201814
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.48703561124135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.1404010762325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.2594456097734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76462144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7633808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7671827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 67.41043974271872
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.768028
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.53430662236975
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  68.10424753485066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.160979747772217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.84050311107069
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.574737548828125
printing an ep nov before normalisation:  102.3861268678231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76761556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.911]
 [35.917]
 [47.751]
 [32.825]
 [30.105]] [[0.204]
 [0.273]
 [0.436]
 [0.231]
 [0.193]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7648,     0.0013,     0.0005,     0.0946,     0.1389],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9993,     0.0004,     0.0002,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9994,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1012, 0.0006, 0.0059, 0.5746, 0.3177], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2509, 0.0022, 0.0009, 0.3359, 0.4101], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.259480476379395
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  96.81407483877432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7629924
printing an ep nov before normalisation:  44.66238021850586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.054930030148704
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  82.58798352152073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.083 0.583 0.083 0.167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.959028244018555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.040027796965575
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.428]
 [35.428]
 [57.595]
 [35.428]
 [35.428]] [[0.467]
 [0.467]
 [0.959]
 [0.467]
 [0.467]]
printing an ep nov before normalisation:  53.85052701625314
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.832]
 [44.832]
 [69.448]
 [44.832]
 [44.832]] [[0.56 ]
 [0.56 ]
 [0.867]
 [0.56 ]
 [0.56 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7580887
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 64.65907171023412
printing an ep nov before normalisation:  33.569071826700835
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.232]
 [27.232]
 [42.164]
 [27.232]
 [27.232]] [[0.294]
 [0.294]
 [0.602]
 [0.294]
 [0.294]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.184]
 [57.184]
 [57.184]
 [57.184]
 [57.184]] [[0.921]
 [0.921]
 [0.921]
 [0.921]
 [0.921]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7553453
printing an ep nov before normalisation:  0.0006647629390954535
printing an ep nov before normalisation:  23.777512446309544
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.902]
 [34.561]
 [47.681]
 [36.462]
 [36.709]] [[0.197]
 [0.262]
 [0.444]
 [0.288]
 [0.292]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.804690207289006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.54259147488001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.081]
 [36.081]
 [36.081]
 [72.117]
 [36.081]] [[0.483]
 [0.483]
 [0.483]
 [1.465]
 [0.483]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.37666032839485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.78250978131316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.963]
 [35.963]
 [55.096]
 [35.963]
 [35.963]] [[0.759]
 [0.759]
 [1.342]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.44451976472718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.96524319797456
printing an ep nov before normalisation:  38.63561153411865
printing an ep nov before normalisation:  28.359079360961914
printing an ep nov before normalisation:  42.814688309206
siam score:  -0.76347536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.97980805612084
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.74833316467753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.76808786392212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8603,     0.0002,     0.0008,     0.0570,     0.0816],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0063,     0.9834,     0.0043,     0.0000,     0.0060],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9989,     0.0007,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0266, 0.0059, 0.1024, 0.5396, 0.3256], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0911, 0.0008, 0.0029, 0.4554, 0.4498], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75824714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.680615425109863
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  95.02120056868115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.15113032286635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.516]
 [17.516]
 [17.516]
 [17.516]
 [17.516]] [[0.555]
 [0.555]
 [0.555]
 [0.555]
 [0.555]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.08978014350157
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.583 0.042 0.292]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.29653549194336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.761]
 [54.761]
 [54.761]
 [54.761]
 [54.761]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  63.229567860221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5536, 0.0015, 0.0008, 0.1695, 0.2746], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9982,     0.0007,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0003,     0.9791,     0.0154,     0.0050],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0955,     0.0003,     0.0016,     0.6200,     0.2826],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0709, 0.0008, 0.0055, 0.5161, 0.4066], grad_fn=<DivBackward0>)
siam score:  -0.7668553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.55286542342427
printing an ep nov before normalisation:  0.30062553452012253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.1687082052659
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.54102659225464
printing an ep nov before normalisation:  40.741052119281406
printing an ep nov before normalisation:  40.048742445918464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6441,     0.0004,     0.0005,     0.1743,     0.1807],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9991,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9998,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1175,     0.0008,     0.0006,     0.5792,     0.3019],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2539, 0.0008, 0.0007, 0.3048, 0.4398], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7566,     0.0005,     0.0009,     0.0885,     0.1535],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9995,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1107, 0.0098, 0.0006, 0.4767, 0.4022], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1115, 0.0034, 0.0022, 0.3245, 0.5583], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.774]
 [72.774]
 [72.774]
 [72.774]
 [72.774]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7680159
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76244324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.4295476616181
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.2046239324514
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.140756607055664
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7220,     0.0046,     0.0005,     0.1133,     0.1596],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0117,     0.9680,     0.0017,     0.0003,     0.0183],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0002,     0.9911,     0.0065,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0015,     0.0005,     0.0013,     0.7774,     0.2193],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1823, 0.0379, 0.0007, 0.4154, 0.3637], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7515259
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.85450773593328
siam score:  -0.7553921
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.8884,     0.0004,     0.0002,     0.0411,     0.0700],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9996,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9909,     0.0064,     0.0026],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0962, 0.0018, 0.0022, 0.5811, 0.3188], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0225, 0.0022, 0.0060, 0.4640, 0.5054], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.472]
 [51.472]
 [51.472]
 [51.472]
 [51.472]] [[1.593]
 [1.593]
 [1.593]
 [1.593]
 [1.593]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.755415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.459]
 [42.405]
 [40.   ]
 [37.454]
 [47.521]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  50.08345269122792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.159064292907715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.68289599837252
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.48 ]
 [30.289]
 [62.374]
 [35.831]
 [34.893]] [[0.325]
 [0.126]
 [0.426]
 [0.178]
 [0.169]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.649]
 [36.649]
 [50.969]
 [51.462]
 [36.649]] [[0.465]
 [0.465]
 [0.81 ]
 [0.822]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.397]
 [37.397]
 [26.191]
 [37.397]
 [37.397]] [[1.815]
 [1.815]
 [1.   ]
 [1.815]
 [1.815]]
siam score:  -0.7646153
actions average: 
K:  2  action  0 :  tensor([    0.9325,     0.0000,     0.0009,     0.0013,     0.0653],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0057,     0.9876,     0.0010,     0.0000,     0.0057],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9673,     0.0297,     0.0029],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0812,     0.0003,     0.0009,     0.6252,     0.2924],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1371, 0.0048, 0.0565, 0.3490, 0.4526], grad_fn=<DivBackward0>)
siam score:  -0.76824355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77143264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.8213866189389
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.315]
 [55.315]
 [61.84 ]
 [66.151]
 [61.662]] [[0.977]
 [0.977]
 [1.092]
 [1.169]
 [1.089]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9482,     0.0003,     0.0002,     0.0015,     0.0497],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0011,     0.9818,     0.0002,     0.0006,     0.0163],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9950,     0.0025,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0193, 0.0019, 0.0033, 0.6338, 0.3418], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0512, 0.0022, 0.0038, 0.4874, 0.4554], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.1505978542541
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75719935
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.109]
 [26.109]
 [36.559]
 [26.109]
 [26.109]] [[0.476]
 [0.476]
 [0.839]
 [0.476]
 [0.476]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.556]
 [46.556]
 [59.345]
 [46.556]
 [46.556]] [[1.24 ]
 [1.24 ]
 [1.581]
 [1.24 ]
 [1.24 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[89.051]
 [89.051]
 [89.774]
 [89.051]
 [89.051]] [[1.65 ]
 [1.65 ]
 [1.667]
 [1.65 ]
 [1.65 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.197184562683105
actions average: 
K:  1  action  0 :  tensor([    0.7356,     0.0008,     0.0006,     0.1176,     0.1454],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9989,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0015,     0.9833,     0.0006,     0.0145],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0172,     0.0003,     0.0018,     0.8060,     0.1747],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0991, 0.0038, 0.0015, 0.4795, 0.4161], grad_fn=<DivBackward0>)
siam score:  -0.76579237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.4402730920447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.578]
 [50.578]
 [50.578]
 [59.169]
 [58.   ]] [[0.97 ]
 [0.97 ]
 [0.97 ]
 [1.353]
 [1.301]]
printing an ep nov before normalisation:  58.88560530712454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.286]
 [57.56 ]
 [55.727]
 [58.862]
 [57.471]] [[1.498]
 [1.667]
 [1.595]
 [1.718]
 [1.664]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.216]
 [31.216]
 [47.017]
 [36.773]
 [31.216]] [[1.004]
 [1.004]
 [1.512]
 [1.183]
 [1.004]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.863763666336595
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.495]
 [14.229]
 [31.172]
 [21.832]
 [14.229]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.628]
 [24.628]
 [48.621]
 [28.543]
 [24.628]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 58.24908077910025
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.75865966
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.097]
 [67.097]
 [77.855]
 [67.097]
 [67.097]] [[1.052]
 [1.052]
 [1.307]
 [1.052]
 [1.052]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.887776374816895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7557918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75333667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.952]
 [23.952]
 [25.744]
 [26.605]
 [24.672]] [[0.083]
 [0.083]
 [0.096]
 [0.102]
 [0.088]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.464428031774463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.52604469406238
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.58352686375795
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.726]
 [65.726]
 [82.044]
 [65.726]
 [65.726]] [[0.946]
 [0.946]
 [1.326]
 [0.946]
 [0.946]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.00831190613329
actions average: 
K:  4  action  0 :  tensor([    0.8111,     0.0007,     0.0144,     0.0387,     0.1350],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0040,     0.9863,     0.0021,     0.0000,     0.0077],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9996,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0655,     0.0006,     0.0024,     0.6359,     0.2956],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0542, 0.0006, 0.0018, 0.5295, 0.4139], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.26342747508532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.61401319689401
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6374,     0.0002,     0.0001,     0.1830,     0.1793],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9992,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9992,     0.0004,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0676,     0.0007,     0.0006,     0.6972,     0.2340],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1654, 0.0010, 0.0005, 0.3885, 0.4447], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8485,     0.0001,     0.0002,     0.0814,     0.0698],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9990,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0471, 0.0009, 0.0008, 0.6939, 0.2573], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2297, 0.0010, 0.0012, 0.3255, 0.4426], grad_fn=<DivBackward0>)
siam score:  -0.7608624
printing an ep nov before normalisation:  29.62568998336792
siam score:  -0.76621413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.19414281253445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7614357
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7607,     0.0002,     0.0002,     0.1253,     0.1136],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0026,     0.9961,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9948,     0.0010,     0.0040],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1259, 0.0007, 0.0010, 0.5764, 0.2961], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1840, 0.0007, 0.0039, 0.3210, 0.4904], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.366162332627
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7987,     0.0003,     0.0010,     0.0669,     0.1332],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0473,     0.9192,     0.0004,     0.0006,     0.0325],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0004,     0.9888,     0.0043,     0.0063],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0968,     0.0006,     0.0092,     0.6256,     0.2678],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1076, 0.0701, 0.0096, 0.3604, 0.4523], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.14440155029297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7576038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.238]
 [24.09 ]
 [27.793]
 [26.529]
 [27.026]] [[0.242]
 [0.376]
 [0.506]
 [0.462]
 [0.479]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  57.332623295206965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.10353984076811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.44]
 [74.44]
 [74.44]
 [74.44]
 [74.44]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.8486518946266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7221, 0.0009, 0.0009, 0.1046, 0.1714], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9991,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0007,     0.0001,     0.9969,     0.0022,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0725, 0.0014, 0.0010, 0.5921, 0.3329], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3431, 0.0008, 0.0029, 0.2570, 0.3961], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.726556188411905
printing an ep nov before normalisation:  74.99689942662658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.496]
 [57.496]
 [57.496]
 [57.496]
 [57.496]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
actions average: 
K:  4  action  0 :  tensor([    0.8017,     0.0006,     0.0072,     0.0934,     0.0970],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0093,     0.9806,     0.0007,     0.0001,     0.0094],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9932,     0.0019,     0.0045],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0351, 0.0007, 0.0009, 0.5521, 0.4113], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1133, 0.0014, 0.0016, 0.3678, 0.5160], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.724]
 [49.724]
 [72.188]
 [54.514]
 [49.724]] [[0.723]
 [0.723]
 [1.05 ]
 [0.792]
 [0.723]]
printing an ep nov before normalisation:  39.52758550643921
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.015]
 [53.015]
 [53.015]
 [64.112]
 [53.015]] [[1.192]
 [1.192]
 [1.192]
 [1.603]
 [1.192]]
printing an ep nov before normalisation:  21.968953609466553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75886256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.73034527801926
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.79943799972534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.35342339019304
printing an ep nov before normalisation:  82.2858257031545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7732158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.84357510473442
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
siam score:  -0.7720346
printing an ep nov before normalisation:  28.225707446768457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76750046
printing an ep nov before normalisation:  54.19294590132508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.78922299446383
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.82624544884472
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.788860321044922
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76300627
printing an ep nov before normalisation:  32.946149413494304
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75801516
actions average: 
K:  1  action  0 :  tensor([    0.9108,     0.0002,     0.0010,     0.0458,     0.0423],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9972,     0.0000,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0001,     0.9977,     0.0009,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0650, 0.0011, 0.0123, 0.6174, 0.3042], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1283, 0.0013, 0.0146, 0.4193, 0.4365], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.7067,     0.0035,     0.0004,     0.0498,     0.2396],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9977,     0.0001,     0.0001,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0002,     0.9731,     0.0251,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0488, 0.0010, 0.0053, 0.6475, 0.2974], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2190, 0.0027, 0.0029, 0.3160, 0.4594], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.236303733395744
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.94767554676193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7562155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7240,     0.0013,     0.0005,     0.1074,     0.1668],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9982,     0.0000,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9910,     0.0032,     0.0056],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0948, 0.0025, 0.0201, 0.6012, 0.2815], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2294, 0.0104, 0.0021, 0.3274, 0.4306], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7611797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.847]
 [56.247]
 [46.05 ]
 [56.046]
 [56.046]] [[0.822]
 [0.691]
 [0.453]
 [0.686]
 [0.686]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.19 ]
 [23.223]
 [21.854]
 [20.217]
 [17.609]] [[0.967]
 [0.969]
 [0.88 ]
 [0.773]
 [0.603]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.834]
 [45.415]
 [44.878]
 [41.034]
 [36.803]] [[0.43 ]
 [0.709]
 [0.696]
 [0.603]
 [0.501]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.720779597431601
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([0.7818, 0.0262, 0.0175, 0.0735, 0.1009], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0581,     0.8959,     0.0002,     0.0000,     0.0458],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0003,     0.9792,     0.0032,     0.0168],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1004,     0.0003,     0.0009,     0.5686,     0.3298],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1069, 0.0008, 0.0024, 0.4663, 0.4237], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.85398504766893
printing an ep nov before normalisation:  70.227358887027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77569205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7353,     0.0003,     0.0003,     0.0772,     0.1870],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0122,     0.9431,     0.0002,     0.0069,     0.0375],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9986,     0.0006,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0390,     0.0005,     0.0013,     0.6994,     0.2598],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1791, 0.0014, 0.0010, 0.3862, 0.4324], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.022]
 [42.022]
 [42.022]
 [51.969]
 [42.022]] [[1.171]
 [1.171]
 [1.171]
 [1.667]
 [1.171]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.19691984095539
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7657991
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76858735
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
siam score:  -0.77225524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.360844373703003
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.6853,     0.0004,     0.0003,     0.1218,     0.1922],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0053,     0.9827,     0.0013,     0.0007,     0.0099],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9995,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0994,     0.0001,     0.0006,     0.6091,     0.2907],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2749, 0.0204, 0.0369, 0.2419, 0.4259], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.27260111174237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.972136488642406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.11837089175296
siam score:  -0.76952714
printing an ep nov before normalisation:  15.750058889389038
siam score:  -0.7691844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.582087201002736
printing an ep nov before normalisation:  49.06337261199951
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7655603
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.034]
 [50.034]
 [50.034]
 [64.201]
 [50.034]] [[0.959]
 [0.959]
 [0.959]
 [1.412]
 [0.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.950212102041895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7670994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76130354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.004007125802255966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7573755
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.380899918937644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.251792267055144
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7623823
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.125]
 [28.311]
 [28.311]
 [34.034]
 [28.311]] [[0.776]
 [0.786]
 [0.786]
 [1.11 ]
 [0.786]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.911]
 [32.911]
 [32.911]
 [32.911]
 [32.911]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.399]
 [66.399]
 [66.399]
 [66.399]
 [66.399]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.44419542617382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.513]
 [42.513]
 [56.458]
 [42.513]
 [42.513]] [[0.343]
 [0.343]
 [0.483]
 [0.343]
 [0.343]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.236]
 [20.382]
 [24.995]
 [18.392]
 [18.128]] [[0.186]
 [0.233]
 [0.333]
 [0.189]
 [0.184]]
UNIT TEST: sample policy line 217 mcts : [0.458 0.125 0.167 0.125 0.125]
printing an ep nov before normalisation:  34.155240058898926
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.865]
 [58.865]
 [58.865]
 [58.865]
 [63.274]] [[0.895]
 [0.895]
 [0.895]
 [0.895]
 [1.   ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.48475213842568
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.341]
 [35.813]
 [31.718]
 [36.482]
 [31.091]] [[0.544]
 [0.678]
 [0.52 ]
 [0.704]
 [0.496]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.466341495513916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.70517788936545
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  18.718891143798828
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.166]
 [45.166]
 [67.787]
 [45.166]
 [45.166]] [[0.968]
 [0.968]
 [1.609]
 [0.968]
 [0.968]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.79830380340059
actions average: 
K:  2  action  0 :  tensor([    0.8293,     0.0003,     0.0005,     0.0566,     0.1134],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0009,     0.9648,     0.0210,     0.0029,     0.0105],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0001,     0.9978,     0.0008,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0757,     0.0003,     0.0003,     0.6885,     0.2352],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1826, 0.0033, 0.0007, 0.3338, 0.4796], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.398]
 [21.987]
 [35.932]
 [26.01 ]
 [22.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  49.417542101217315
actions average: 
K:  4  action  0 :  tensor([    0.7912,     0.0012,     0.0002,     0.0649,     0.1424],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0023,     0.9897,     0.0006,     0.0007,     0.0067],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0031,     0.9951,     0.0008,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0254,     0.0002,     0.0007,     0.7509,     0.2228],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1729, 0.0015, 0.0005, 0.3733, 0.4518], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.08134652168009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.956]
 [29.363]
 [37.036]
 [42.179]
 [42.179]] [[0.889]
 [1.009]
 [1.667]
 [2.108]
 [2.108]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 0.0
0.0 -4.97329236314811e-12
0.0 0.0
0.0 -5.673877897175688e-12
0.0 -1.3146790251812808e-12
0.0 -1.608751965877677e-11
0.0 -5.0511352065274404e-12
0.0 -4.246759220165142e-12
0.0 -5.405752576510581e-12
0.0 -9.730354631798473e-12
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.234]
 [42.534]
 [35.142]
 [27.507]
 [35.055]] [[1.207]
 [0.831]
 [0.594]
 [0.349]
 [0.591]]
printing an ep nov before normalisation:  70.95553067810582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.82480329422245
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.19154336559035
siam score:  -0.7621364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8278,     0.0090,     0.0002,     0.0785,     0.0845],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9994,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9989,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0437,     0.0005,     0.0015,     0.6680,     0.2863],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1616, 0.0012, 0.0013, 0.4457, 0.3902], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.35789321833694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7289,     0.0003,     0.0005,     0.1217,     0.1486],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9966,     0.0002,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9876,     0.0119,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0685, 0.0009, 0.0008, 0.6674, 0.2624], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1342, 0.0029, 0.0038, 0.4533, 0.4058], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.107]
 [26.747]
 [33.188]
 [35.854]
 [29.192]] [[0.11 ]
 [0.116]
 [0.175]
 [0.199]
 [0.138]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.82675097237638
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.81497573852539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.23229087865989
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.788811819893976
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7654634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.778656373038274
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.406]
 [58.406]
 [66.464]
 [81.903]
 [58.406]] [[0.724]
 [0.724]
 [0.869]
 [1.147]
 [0.724]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7477,     0.0130,     0.0003,     0.0779,     0.1611],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9965,     0.0000,     0.0001,     0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0000,     0.9976,     0.0006,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0344,     0.0004,     0.0006,     0.6735,     0.2911],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1369, 0.0020, 0.0442, 0.4049, 0.4120], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
printing an ep nov before normalisation:  33.77325011713476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.29920705159505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.76735853764376
printing an ep nov before normalisation:  57.86773605981388
siam score:  -0.769071
printing an ep nov before normalisation:  9.1895771572689
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.867479654784413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.63352937143737
printing an ep nov before normalisation:  74.3642437548763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7784,     0.0002,     0.0005,     0.0921,     0.1288],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9931,     0.0001,     0.0000,     0.0052],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1054,     0.0004,     0.0078,     0.5971,     0.2892],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2097, 0.0010, 0.0123, 0.3305, 0.4466], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.24243218896009
printing an ep nov before normalisation:  60.59719707895543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.71130159777698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.337]
 [48.337]
 [48.337]
 [48.337]
 [48.337]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.935044460306965
printing an ep nov before normalisation:  47.425619955384114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77152205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[10.288]
 [10.288]
 [10.288]
 [10.288]
 [10.288]] [[0.157]
 [0.157]
 [0.157]
 [0.157]
 [0.157]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7670874
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[86.799]
 [86.799]
 [86.12 ]
 [86.799]
 [86.799]] [[1.841]
 [1.841]
 [1.825]
 [1.841]
 [1.841]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.46932097425329
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.803]
 [69.169]
 [31.803]
 [31.803]
 [31.803]] [[0.333]
 [0.899]
 [0.333]
 [0.333]
 [0.333]]
siam score:  -0.7715603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.2728399875646
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.50012018854787
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  21.228718799122184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.132]
 [78.072]
 [78.072]
 [78.072]
 [77.739]] [[2.   ]
 [1.92 ]
 [1.92 ]
 [1.92 ]
 [1.911]]
printing an ep nov before normalisation:  21.195502307480822
printing an ep nov before normalisation:  49.371788368194196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.536]
 [26.536]
 [40.634]
 [26.536]
 [26.536]] [[0.458]
 [0.458]
 [0.921]
 [0.458]
 [0.458]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.208 0.25  0.25  0.167 0.125]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.789]
 [60.936]
 [60.936]
 [60.936]
 [60.936]] [[1.333]
 [1.039]
 [1.039]
 [1.039]
 [1.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.630112502440433
printing an ep nov before normalisation:  34.00901556015015
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.13045883618459
printing an ep nov before normalisation:  41.410346031188965
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.44717238489971
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.733695570952364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76735365
actions average: 
K:  0  action  0 :  tensor([    0.7986,     0.0001,     0.0001,     0.0920,     0.1093],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9948,     0.0002,     0.0002,     0.0041],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9998,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0862,     0.0003,     0.0007,     0.6546,     0.2582],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1283, 0.0052, 0.0012, 0.3721, 0.4932], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.17078354014585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.39481037750025
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.23476791381836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.6849,     0.0024,     0.0004,     0.0708,     0.2415],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0024,     0.9884,     0.0000,     0.0000,     0.0092],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9946,     0.0042,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0392,     0.0006,     0.0005,     0.5908,     0.3689],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1292, 0.0376, 0.0041, 0.2287, 0.6004], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.99076747894287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.455673694610596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.54164409637451
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.031]
 [41.031]
 [47.954]
 [41.031]
 [46.197]] [[1.354]
 [1.354]
 [1.711]
 [1.354]
 [1.62 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.498158486229947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 55.15366751310895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.722102642059326
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.367]
 [54.543]
 [52.327]
 [48.548]
 [47.954]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.57842732559241
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.912]
 [26.78 ]
 [26.78 ]
 [26.78 ]
 [26.78 ]] [[0.847]
 [0.329]
 [0.329]
 [0.329]
 [0.329]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  22.59361743927002
printing an ep nov before normalisation:  81.93410873865272
printing an ep nov before normalisation:  30.135469454557164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  3.154081427039403
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8203,     0.0377,     0.0001,     0.0532,     0.0888],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9974,     0.0013,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0439,     0.0004,     0.0018,     0.7071,     0.2467],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1491, 0.0675, 0.0121, 0.3626, 0.4086], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.76915014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.473]
 [24.871]
 [39.759]
 [22.095]
 [22.018]] [[0.165]
 [0.297]
 [0.605]
 [0.24 ]
 [0.238]]
printing an ep nov before normalisation:  59.05677838952791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.91188777689026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.8345978769447
printing an ep nov before normalisation:  63.707104387342575
printing an ep nov before normalisation:  51.24262337756824
printing an ep nov before normalisation:  25.45660972595215
printing an ep nov before normalisation:  10.149100955061385
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.47216553467516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.79894317556884
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.435]
 [46.435]
 [69.101]
 [46.435]
 [46.435]] [[0.323]
 [0.323]
 [0.585]
 [0.323]
 [0.323]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.13661415768141
printing an ep nov before normalisation:  40.42058957718548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.55159853799299
printing an ep nov before normalisation:  52.4657998442096
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8958,     0.0001,     0.0002,     0.0337,     0.0703],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9890,     0.0004,     0.0001,     0.0098],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0001,     0.9980,     0.0010,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0374,     0.0003,     0.0004,     0.6755,     0.2865],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0378, 0.0012, 0.0008, 0.4908, 0.4694], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.2206373638686
siam score:  -0.757975
line 256 mcts: sample exp_bonus 53.11797253560904
actions average: 
K:  0  action  0 :  tensor([    0.7699,     0.0013,     0.0003,     0.0910,     0.1375],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9992,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0003,     0.9975,     0.0011,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0473,     0.0003,     0.0004,     0.7159,     0.2361],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1986, 0.0502, 0.0008, 0.4222, 0.3282], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  84.03654664113006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.68128260455408
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.04243988206967
printing an ep nov before normalisation:  45.40993868180592
printing an ep nov before normalisation:  55.11431137380832
printing an ep nov before normalisation:  47.85202960935007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7556885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.571]
 [40.571]
 [61.089]
 [47.391]
 [40.571]] [[0.632]
 [0.632]
 [0.952]
 [0.739]
 [0.632]]
UNIT TEST: sample policy line 217 mcts : [0.458 0.083 0.125 0.125 0.208]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  13.517729043960571
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.009]
 [75.009]
 [75.009]
 [75.009]
 [75.009]] [[1.952]
 [1.952]
 [1.952]
 [1.952]
 [1.952]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.167]
 [33.893]
 [29.368]
 [24.049]
 [27.748]] [[1.022]
 [1.299]
 [1.034]
 [0.723]
 [0.939]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actions average: 
K:  1  action  0 :  tensor([0.7876, 0.0028, 0.0011, 0.0505, 0.1580], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0012,     0.9943,     0.0004,     0.0000,     0.0040],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9993,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0616, 0.0008, 0.0048, 0.6100, 0.3228], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2141, 0.0086, 0.0035, 0.3548, 0.4191], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 55.77207482184076
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.003]
 [40.003]
 [47.838]
 [40.003]
 [40.003]] [[0.486]
 [0.486]
 [0.667]
 [0.486]
 [0.486]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.316]
 [52.537]
 [37.316]
 [37.316]
 [37.316]] [[0.852]
 [1.337]
 [0.852]
 [0.852]
 [0.852]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  74.03642880653561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.83987339598934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  20.17244577407837
siam score:  -0.73416305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  40.439743995666504
printing an ep nov before normalisation:  31.60238289865369
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.769]
 [72.769]
 [72.769]
 [72.769]
 [72.769]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
printing an ep nov before normalisation:  19.990311861038208
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7474449
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  24.649163550422447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
using another actor
printing an ep nov before normalisation:  36.44831445482042
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  51.03044639815746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actions average: 
K:  3  action  0 :  tensor([    0.8952,     0.0020,     0.0003,     0.0207,     0.0820],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9985,     0.0001,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0003,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0020, 0.0010, 0.0028, 0.6712, 0.3230], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1355, 0.0030, 0.0039, 0.3144, 0.5432], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [10.485]
 [10.157]
 [ 0.   ]
 [15.086]] [[-0.189]
 [ 0.08 ]
 [ 0.072]
 [-0.189]
 [ 0.198]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  14.369722859302243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([    0.8036,     0.0057,     0.0003,     0.0771,     0.1132],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9993,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9959,     0.0023,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0635, 0.0008, 0.0114, 0.6134, 0.3108], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0986, 0.0090, 0.0791, 0.2996, 0.5137], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  82.02188316069922
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  18.0413114144568
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.632]
 [51.07 ]
 [36.454]
 [49.294]
 [47.16 ]] [[1.316]
 [1.473]
 [1.051]
 [1.422]
 [1.36 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  18.61344337463379
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
siam score:  -0.75046617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  93.36979944493584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  37.061130832914365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  64.8336395160857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actions average: 
K:  2  action  0 :  tensor([    0.8400,     0.0003,     0.0004,     0.0781,     0.0812],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0061, 0.9724, 0.0109, 0.0019, 0.0087], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9952,     0.0028,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1107, 0.0012, 0.0023, 0.5488, 0.3369], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1197, 0.0017, 0.0194, 0.3754, 0.4838], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
siam score:  -0.7561
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[17.445]
 [17.445]
 [17.445]
 [37.833]
 [17.445]] [[0.278]
 [0.278]
 [0.278]
 [0.896]
 [0.278]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actions average: 
K:  4  action  0 :  tensor([0.7962, 0.0633, 0.0014, 0.0500, 0.0890], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9924,     0.0001,     0.0000,     0.0071],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0003,     0.9636,     0.0202,     0.0158],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0031,     0.0005,     0.0009,     0.7903,     0.2052],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1945, 0.0330, 0.0026, 0.3117, 0.4581], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
printing an ep nov before normalisation:  35.79841341291155
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.213]
 [78.213]
 [78.213]
 [78.213]
 [78.213]] [[1.368]
 [1.368]
 [1.368]
 [1.368]
 [1.368]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.00741326692706
printing an ep nov before normalisation:  68.57221477656525
printing an ep nov before normalisation:  66.92214962644313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.907]
 [30.154]
 [29.438]
 [49.022]
 [25.835]] [[0.284]
 [0.369]
 [0.355]
 [0.748]
 [0.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.6049,     0.0010,     0.0003,     0.1270,     0.2667],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9851,     0.0009,     0.0001,     0.0118],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0000,     0.9994,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0479,     0.0002,     0.0013,     0.6733,     0.2773],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2121, 0.0010, 0.0006, 0.3473, 0.4390], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  76.81451591384494
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  30.2057483784032
printing an ep nov before normalisation:  77.071895322014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
actions average: 
K:  3  action  0 :  tensor([    0.8034,     0.0004,     0.0005,     0.0669,     0.1288],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9816,     0.0022,     0.0009,     0.0138],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0009,     0.8939,     0.0904,     0.0147],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0829, 0.0013, 0.0042, 0.6556, 0.2560], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1400, 0.0446, 0.0019, 0.3652, 0.4483], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7375862
printing an ep nov before normalisation:  26.296872278696306
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.53374528884888
printing an ep nov before normalisation:  78.49582456840061
printing an ep nov before normalisation:  62.79802364075713
actions average: 
K:  3  action  0 :  tensor([    0.9525,     0.0005,     0.0008,     0.0244,     0.0218],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9745,     0.0042,     0.0027,     0.0177],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0030,     0.0007,     0.9808,     0.0056,     0.0098],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0392,     0.0006,     0.0037,     0.6999,     0.2565],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0905, 0.0032, 0.0303, 0.4486, 0.4274], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[38.814]
 [48.172]
 [38.814]
 [38.814]
 [38.814]] [[1.344]
 [1.668]
 [1.344]
 [1.344]
 [1.344]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  57.09091179061707
printing an ep nov before normalisation:  41.492065638682156
printing an ep nov before normalisation:  41.885081896481786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  49.551021041794655
actions average: 
K:  2  action  0 :  tensor([    0.7449,     0.0010,     0.0005,     0.0845,     0.1692],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9980,     0.0003,     0.0006,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0007,     0.9639,     0.0010,     0.0343],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0023,     0.0004,     0.0091,     0.8245,     0.1638],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2001, 0.0552, 0.0243, 0.2846, 0.4357], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  78.43118391914261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  41.30882597415493
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[22.303]
 [57.882]
 [22.303]
 [22.303]
 [22.303]] [[0.247]
 [0.994]
 [0.247]
 [0.247]
 [0.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[38.393]
 [32.35 ]
 [42.444]
 [42.444]
 [40.063]] [[1.399]
 [1.179]
 [1.547]
 [1.547]
 [1.46 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  76.68111845026297
printing an ep nov before normalisation:  23.568387031555176
using another actor
printing an ep nov before normalisation:  48.45970208415436
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  53.98044940255159
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  87.10062207742814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  51.849609195005044
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
actions average: 
K:  0  action  0 :  tensor([    0.8953,     0.0001,     0.0003,     0.0596,     0.0447],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0237,     0.9528,     0.0206,     0.0002,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9982,     0.0008,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0736,     0.0002,     0.0003,     0.7135,     0.2123],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1891, 0.0005, 0.0011, 0.3669, 0.4424], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.65761827899232
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  51.40561580657959
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.046637058258057
printing an ep nov before normalisation:  69.53710098829563
actions average: 
K:  3  action  0 :  tensor([    0.8107,     0.0004,     0.0013,     0.0835,     0.1040],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0077,     0.9828,     0.0016,     0.0004,     0.0074],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0008,     0.9849,     0.0011,     0.0131],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1220, 0.0007, 0.0011, 0.6349, 0.2412], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2101, 0.0041, 0.0840, 0.3048, 0.3969], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  33.95036566597359
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[36.938]
 [39.897]
 [36.938]
 [36.938]
 [36.938]] [[1.526]
 [1.648]
 [1.526]
 [1.526]
 [1.526]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.001]
 [0.001]] [[63.498]
 [61.738]
 [58.591]
 [61.505]
 [63.48 ]] [[1.522]
 [1.44 ]
 [1.293]
 [1.429]
 [1.521]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  33.599853515625
printing an ep nov before normalisation:  60.89003216265863
using explorer policy with actor:  1
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7459311
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.7478393
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.   ]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  71.32115548217003
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  14.067490895589193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  50.33548355102539
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[52.061]
 [49.745]
 [42.171]
 [42.693]
 [45.179]] [[1.078]
 [0.992]
 [0.712]
 [0.73 ]
 [0.823]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9878,     0.0064,     0.0022,     0.0007,     0.0029],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9991,     0.0001,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0017,     0.9704,     0.0136,     0.0141],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0955, 0.0045, 0.0694, 0.5835, 0.2470], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0662, 0.1079, 0.0089, 0.2928, 0.5241], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  80.2559531075418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.7014,     0.0002,     0.0006,     0.1871,     0.1108],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9909,     0.0060,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9994,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0524, 0.0083, 0.0044, 0.6421, 0.2928], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1563, 0.0017, 0.0023, 0.3133, 0.5264], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
actions average: 
K:  1  action  0 :  tensor([    0.7337,     0.0003,     0.0003,     0.1165,     0.1493],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9872,     0.0063,     0.0003,     0.0058],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0000,     0.9973,     0.0007,     0.0018],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0924, 0.0035, 0.0013, 0.6555, 0.2473], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.1198,     0.0003,     0.0007,     0.3165,     0.5626],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7509463
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  37.86546552580839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7529458
actions average: 
K:  0  action  0 :  tensor([    0.7998,     0.0007,     0.0002,     0.0712,     0.1282],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9977,     0.0006,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0001,     0.9948,     0.0012,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0853,     0.0004,     0.0008,     0.6430,     0.2705],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2001, 0.0013, 0.0804, 0.3298, 0.3884], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.59938513001785
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
actions average: 
K:  0  action  0 :  tensor([    0.7904,     0.0002,     0.0003,     0.1020,     0.1071],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9991,     0.0004,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0007,     0.9872,     0.0032,     0.0089],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0502,     0.0002,     0.0081,     0.8086,     0.1329],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1655, 0.0026, 0.0565, 0.3231, 0.4524], grad_fn=<DivBackward0>)
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  43.55293401803864
actions average: 
K:  0  action  0 :  tensor([    0.6074,     0.0003,     0.0003,     0.1370,     0.2550],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9978,     0.0005,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0002,     0.9903,     0.0030,     0.0061],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0748,     0.0005,     0.0002,     0.6919,     0.2327],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1414,     0.0022,     0.0003,     0.4003,     0.4559],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.94613436612434
actions average: 
K:  4  action  0 :  tensor([    0.7830,     0.0005,     0.0002,     0.0614,     0.1550],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9950,     0.0002,     0.0006,     0.0026],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0016, 0.0288, 0.9529, 0.0016, 0.0151], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0506, 0.0010, 0.0083, 0.6895, 0.2506], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0016,     0.0006,     0.0052,     0.3672,     0.6253],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  39.0429889393727
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.57435219005195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 4.24 ]
 [30.023]
 [30.023]
 [30.023]
 [30.023]] [[0.  ]
 [1.46]
 [1.46]
 [1.46]
 [1.46]]
printing an ep nov before normalisation:  4.723291741042743
printing an ep nov before normalisation:  27.4348481181077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7612291
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  57.39689929311719
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  60.13613713519408
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  50.727515482679024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  44.56152726633045
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.684403268038523
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  61.2574471700519
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using another actor
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  28.65464407013816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[55.059]
 [55.059]
 [65.667]
 [55.059]
 [55.059]] [[0.919]
 [0.919]
 [1.147]
 [0.919]
 [0.919]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.46554997596156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7551911
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.8782,     0.0029,     0.0004,     0.0407,     0.0778],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0023,     0.9938,     0.0003,     0.0002,     0.0034],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9759,     0.0134,     0.0104],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0449,     0.0003,     0.0006,     0.8117,     0.1426],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1124, 0.0773, 0.0199, 0.3670, 0.4233], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  47.45146797980752
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[28.039]
 [32.569]
 [46.789]
 [46.789]
 [46.789]] [[1.226]
 [1.668]
 [3.054]
 [3.054]
 [3.054]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.8501,     0.0003,     0.0005,     0.0661,     0.0831],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0052,     0.9888,     0.0012,     0.0002,     0.0046],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0055, 0.0010, 0.9751, 0.0069, 0.0115], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0395,     0.0003,     0.0040,     0.7229,     0.2333],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1027, 0.0064, 0.0495, 0.2723, 0.5691], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  53.292899213093165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7530645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.983]
 [33.983]
 [52.395]
 [41.345]
 [33.983]] [[0.297]
 [0.297]
 [0.568]
 [0.405]
 [0.297]]
printing an ep nov before normalisation:  69.28597124646052
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  64.74168986297425
siam score:  -0.7560561
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  35.799840765417756
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.559]
 [33.244]
 [40.543]
 [56.811]
 [45.102]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  27.931044101715088
printing an ep nov before normalisation:  43.64895290371341
printing an ep nov before normalisation:  21.71601977096287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  35.91766739070572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.748502
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.7507522
using another actor
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  46.02840911393357
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.18921103938151
printing an ep nov before normalisation:  48.301551057050304
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  18.13762664794922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.669]
 [28.669]
 [28.669]
 [28.669]
 [28.669]] [[0.124]
 [0.124]
 [0.124]
 [0.124]
 [0.124]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  68.00260152536512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  31.59768585398546
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.55075027530566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.013]
 [0.006]
 [0.006]
 [0.006]] [[35.03 ]
 [41.033]
 [35.03 ]
 [35.03 ]
 [35.03 ]] [[1.104]
 [1.41 ]
 [1.104]
 [1.104]
 [1.104]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  34.10380653031645
printing an ep nov before normalisation:  77.33835039197372
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[42.379]
 [38.659]
 [38.659]
 [38.659]
 [38.659]] [[1.367]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
printing an ep nov before normalisation:  28.927512168884277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[20.43]
 [20.43]
 [52.24]
 [20.43]
 [20.43]] [[0.311]
 [0.311]
 [1.228]
 [0.311]
 [0.311]]
printing an ep nov before normalisation:  72.69386041126755
printing an ep nov before normalisation:  43.42974182329605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  19.661898116580307
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  35.1386099782649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  34.00940447565782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  21.714702429844692
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  79.04672545846593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  59.7146075547335
printing an ep nov before normalisation:  82.9931074681515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  42.661323072080606
siam score:  -0.7741784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
siam score:  -0.77250123
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  43.44912665231317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  61.33802640618336
using another actor
printing an ep nov before normalisation:  61.59454127774315
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427, 0.08337165004067143]
printing an ep nov before normalisation:  57.64532357215334
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  63.85797084871028
printing an ep nov before normalisation:  18.496351675186773
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  50.352541132897244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
line 256 mcts: sample exp_bonus 29.918026856574432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[34.981]
 [45.006]
 [47.314]
 [33.642]
 [34.981]] [[0.307]
 [0.462]
 [0.497]
 [0.287]
 [0.307]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.   ]
 [0.001]] [[40.471]
 [40.471]
 [40.471]
 [50.636]
 [40.471]] [[0.995]
 [0.995]
 [0.995]
 [1.471]
 [0.995]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.8495,     0.0346,     0.0002,     0.0189,     0.0968],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9935,     0.0020,     0.0001,     0.0030],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0090, 0.0047, 0.9188, 0.0252, 0.0423], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0619, 0.0016, 0.0737, 0.7207, 0.1421], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1212, 0.0721, 0.0064, 0.3859, 0.4144], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  59.98093301021155
using another actor
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[64.479]
 [64.479]
 [64.479]
 [64.479]
 [64.479]] [[1.519]
 [1.519]
 [1.519]
 [1.519]
 [1.519]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  54.84938194188652
siam score:  -0.7535831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
actions average: 
K:  0  action  0 :  tensor([    0.7718,     0.0005,     0.0001,     0.0485,     0.1791],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9974,     0.0008,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9940,     0.0002,     0.0057],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0721,     0.0002,     0.0015,     0.7220,     0.2041],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1253, 0.0005, 0.0010, 0.3619, 0.5112], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using another actor
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  26.300935758924076
printing an ep nov before normalisation:  47.178798366401885
siam score:  -0.7517671
printing an ep nov before normalisation:  63.78740178119736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
siam score:  -0.75226015
printing an ep nov before normalisation:  32.13392460020232
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  60.92808549381602
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.011]
 [0.01 ]
 [0.011]
 [0.006]] [[53.583]
 [54.463]
 [69.885]
 [54.463]
 [63.626]] [[1.096]
 [1.113]
 [1.423]
 [1.113]
 [1.293]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.98501491546631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  69.26957449279723
actions average: 
K:  0  action  0 :  tensor([    0.8461,     0.0002,     0.0002,     0.0750,     0.0785],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9909,     0.0015,     0.0015,     0.0058],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9838,     0.0016,     0.0144],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0531,     0.0003,     0.0009,     0.7137,     0.2319],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1570, 0.0010, 0.0009, 0.2919, 0.5491], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  31.101374353424724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.002]
 [0.002]] [[30.088]
 [65.649]
 [33.045]
 [30.37 ]
 [29.689]] [[0.349]
 [1.137]
 [0.414]
 [0.355]
 [0.34 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.001]
 [0.004]
 [0.003]] [[33.263]
 [28.056]
 [22.814]
 [43.596]
 [30.977]] [[0.599]
 [0.426]
 [0.247]
 [0.948]
 [0.523]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.001]
 [0.002]
 [0.002]] [[31.709]
 [38.298]
 [54.304]
 [31.709]
 [31.709]] [[0.449]
 [0.627]
 [1.055]
 [0.449]
 [0.449]]
siam score:  -0.7798523
using another actor
siam score:  -0.7833036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
printing an ep nov before normalisation:  65.21371437059695
printing an ep nov before normalisation:  40.86720892584616
printing an ep nov before normalisation:  46.96731648385234
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using another actor
from probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
siam score:  -0.7935593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.84274101257324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.08336998453994252, 0.5831500773002875, 0.08336998453994252]
Printing some Q and Qe and total Qs values:  [[0.199]
 [0.415]
 [0.199]
 [0.174]
 [0.199]] [[26.085]
 [26.604]
 [26.085]
 [25.174]
 [26.085]] [[1.653]
 [1.924]
 [1.653]
 [1.531]
 [1.653]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.80236189705985
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
printing an ep nov before normalisation:  39.26971748393005
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
printing an ep nov before normalisation:  48.05195665781944
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
printing an ep nov before normalisation:  75.01360399990642
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  23.143868446350098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  67.0169420140161
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  61.369074927289766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  68.33465025639606
printing an ep nov before normalisation:  35.260748863220215
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[50.872]
 [40.882]
 [40.882]
 [40.882]
 [40.882]] [[1.34 ]
 [0.956]
 [0.956]
 [0.956]
 [0.956]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  43.78059762729233
printing an ep nov before normalisation:  29.237402512854793
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.003]
 [0.004]
 [0.004]] [[21.515]
 [42.515]
 [20.23 ]
 [18.5  ]
 [18.648]] [[0.339]
 [0.907]
 [0.303]
 [0.258]
 [0.262]]
printing an ep nov before normalisation:  63.151018907936425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  27.959880828857422
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  23.949012756347656
printing an ep nov before normalisation:  38.29079473133137
line 256 mcts: sample exp_bonus 59.58288347594758
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[31.586]
 [31.586]
 [45.856]
 [31.586]
 [31.586]] [[0.633]
 [0.633]
 [1.177]
 [0.633]
 [0.633]]
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.014]
 [0.005]
 [0.005]
 [0.005]] [[42.527]
 [51.675]
 [42.527]
 [42.527]
 [42.527]] [[0.845]
 [1.197]
 [0.845]
 [0.845]
 [0.845]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  61.69388209643232
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
using explorer policy with actor:  1
siam score:  -0.77542204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
UNIT TEST: sample policy line 217 mcts : [0.083 0.667 0.125 0.083 0.042]
printing an ep nov before normalisation:  70.66197311781755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
siam score:  -0.76495624
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.76527196
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  54.71976226933805
actions average: 
K:  1  action  0 :  tensor([    0.8662,     0.0001,     0.0000,     0.0751,     0.0586],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0095,     0.9663,     0.0023,     0.0000,     0.0218],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0039,     0.0001,     0.9727,     0.0182,     0.0051],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0176,     0.0002,     0.0012,     0.8195,     0.1615],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.2199,     0.0004,     0.0004,     0.4170,     0.3624],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  19.818061446824746
printing an ep nov before normalisation:  42.16262217237815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.08336915176460127, 0.5831542411769935, 0.08336915176460127]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
siam score:  -0.76273865
printing an ep nov before normalisation:  58.969066748965375
printing an ep nov before normalisation:  40.348232422340985
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.003]
 [0.001]
 [0.001]] [[58.846]
 [58.846]
 [62.282]
 [58.846]
 [58.846]] [[1.096]
 [1.096]
 [1.189]
 [1.096]
 [1.096]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
printing an ep nov before normalisation:  77.16769768833677
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.937]
 [31.937]
 [39.26 ]
 [33.021]
 [31.937]] [[0.166]
 [0.166]
 [0.25 ]
 [0.179]
 [0.166]]
printing an ep nov before normalisation:  31.301603317260742
line 256 mcts: sample exp_bonus 1.919711918986877e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
from probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
using another actor
printing an ep nov before normalisation:  36.907124519348145
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[34.317]
 [34.317]
 [44.572]
 [35.239]
 [34.317]] [[0.161]
 [0.161]
 [0.249]
 [0.169]
 [0.161]]
siam score:  -0.74965346
printing an ep nov before normalisation:  29.37560499821492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.0833686520914037, 0.5831567395429815, 0.0833686520914037]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[20.009]
 [20.009]
 [46.598]
 [20.009]
 [20.009]] [[0.257]
 [0.257]
 [0.905]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.56065977848202
printing an ep nov before normalisation:  24.943125483184758
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[36.093]
 [36.093]
 [36.093]
 [36.093]
 [36.093]] [[0.575]
 [0.575]
 [0.575]
 [0.575]
 [0.575]]
using another actor
from probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.028]
 [0.121]
 [0.011]
 [0.027]] [[29.779]
 [30.399]
 [30.159]
 [36.234]
 [29.133]] [[0.83 ]
 [0.886]
 [0.966]
 [1.188]
 [0.816]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.   ]
 [0.002]
 [0.002]] [[10.696]
 [10.696]
 [18.523]
 [10.696]
 [10.696]] [[0.152]
 [0.152]
 [1.212]
 [0.152]
 [0.152]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using another actor
from probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[46.627]
 [46.627]
 [46.627]
 [46.627]
 [46.627]] [[1.673]
 [1.673]
 [1.673]
 [1.673]
 [1.673]]
using another actor
siam score:  -0.7495407
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  18.03116630283501
from probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.   ]
 [0.   ]
 [0.   ]] [[31.564]
 [28.937]
 [30.766]
 [25.001]
 [25.95 ]] [[0.265]
 [0.225]
 [0.253]
 [0.166]
 [0.18 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
siam score:  -0.7494247
printing an ep nov before normalisation:  52.380474127368046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[23.59 ]
 [23.715]
 [29.348]
 [21.535]
 [26.672]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
printing an ep nov before normalisation:  30.536272525787354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.62675514492507
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.08336831897260823, 0.5831584051369589, 0.08336831897260823]
Starting evaluation
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.181207754145312
printing an ep nov before normalisation:  39.49035383400254
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.002]
 [0.002]] [[73.23]
 [73.23]
 [78.83]
 [73.23]
 [73.23]] [[0.002]
 [0.002]
 [0.003]
 [0.002]
 [0.002]]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]] [[30.024]
 [30.024]
 [30.024]
 [30.024]
 [30.024]] [[0.072]
 [0.072]
 [0.072]
 [0.072]
 [0.072]]
actor:  0 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.5831519308627057, 0.08336961382745885]
printing an ep nov before normalisation:  63.747767754024544
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[45.701]
 [45.701]
 [45.701]
 [45.701]
 [45.701]] [[1.668]
 [1.668]
 [1.668]
 [1.668]
 [1.668]]
printing an ep nov before normalisation:  52.96541276951801
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.   ]]
maxi score, test score, baseline:  0.0101 0.25 0.25
from probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
using explorer policy with actor:  1
from probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
using another actor
actions average: 
K:  1  action  0 :  tensor([    0.7746,     0.0010,     0.0001,     0.0515,     0.1728],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0039,     0.9902,     0.0028,     0.0000,     0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9828,     0.0061,     0.0109],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1239,     0.0008,     0.0004,     0.7442,     0.1308],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0047, 0.0462, 0.2133, 0.3209, 0.4150], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[33.144]
 [33.144]
 [39.668]
 [33.144]
 [33.144]] [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  19.24014552460057
from probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.69815138768038
printing an ep nov before normalisation:  0.00018708360727259787
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
from probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[41.428]
 [41.428]
 [41.428]
 [41.428]
 [41.428]] [[69.073]
 [69.073]
 [69.073]
 [69.073]
 [69.073]]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
siam score:  -0.7609738
printing an ep nov before normalisation:  43.82428553706088
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
printing an ep nov before normalisation:  23.759784698486328
printing an ep nov before normalisation:  61.75334560112068
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
using explorer policy with actor:  1
using another actor
siam score:  -0.809666
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.08337020589996606, 0.5831489705001698, 0.08337020589996606]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.5831519308627057, 0.08336961382745885]
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.5831519308627057, 0.08336961382745885]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.5831519308627057, 0.08336961382745885]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.08336961382745885, 0.5831519308627057, 0.08336961382745885]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
from probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
maxi score, test score, baseline:  0.0101 0.25 0.25
printing an ep nov before normalisation:  30.371952056884766
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 27.33113169670105
maxi score, test score, baseline:  0.0101 0.25 0.25
printing an ep nov before normalisation:  18.524815800591295
printing an ep nov before normalisation:  54.491065112399475
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07696466185248821, 0.15385209455035545, 0.07696466185248821, 0.07696466185248821, 0.5382892580396917, 0.07696466185248821]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.07147498870392087, 0.2142625056480395, 0.07147498870392087, 0.07147498870392087, 0.4998375395362769, 0.07147498870392087]
printing an ep nov before normalisation:  20.350532802067786
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07147498870392087, 0.2142625056480395, 0.07147498870392087, 0.07147498870392087, 0.4998375395362769, 0.07147498870392087]
printing an ep nov before normalisation:  61.13250112974389
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07147498870392087, 0.2142625056480395, 0.07147498870392087, 0.07147498870392087, 0.4998375395362769, 0.07147498870392087]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07147498870392087, 0.2142625056480395, 0.07147498870392087, 0.07147498870392087, 0.4998375395362769, 0.07147498870392087]
printing an ep nov before normalisation:  35.619203334397085
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07147498870392087, 0.2142625056480395, 0.07147498870392087, 0.07147498870392087, 0.4998375395362769, 0.07147498870392087]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
maxi score, test score, baseline:  0.0101 0.25 0.25
from probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
printing an ep nov before normalisation:  32.52484206211303
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07296133287577367, 0.19790177793029767, 0.07296133287577367, 0.07296133287577367, 0.5102528905666077, 0.07296133287577367]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0741174008414826, 0.1851765198317035, 0.0741174008414826, 0.0741174008414826, 0.5183538768023662, 0.0741174008414826]
printing an ep nov before normalisation:  55.63844872215521
siam score:  -0.8713035
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0741174008414826, 0.1851765198317035, 0.0741174008414826, 0.0741174008414826, 0.5183538768023662, 0.0741174008414826]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[31.623]
 [19.199]
 [18.046]
 [18.273]
 [18.928]] [[1.635]
 [0.845]
 [0.771]
 [0.786]
 [0.827]]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0741174008414826, 0.1851765198317035, 0.0741174008414826, 0.0741174008414826, 0.5183538768023662, 0.0741174008414826]
printing an ep nov before normalisation:  54.82658746455888
siam score:  -0.8774799
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.2959420293589
printing an ep nov before normalisation:  44.67509456614116
printing an ep nov before normalisation:  56.60404348843218
printing an ep nov before normalisation:  56.555611046577795
printing an ep nov before normalisation:  66.28944333961674
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0741174008414826, 0.1851765198317035, 0.0741174008414826, 0.0741174008414826, 0.5183538768023662, 0.0741174008414826]
using another actor
actions average: 
K:  2  action  0 :  tensor([    0.9242,     0.0004,     0.0006,     0.0003,     0.0745],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9982,     0.0008,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0001,     0.9365,     0.0285,     0.0345],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0539,     0.0004,     0.0011,     0.8616,     0.0829],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1222, 0.0072, 0.1394, 0.1058, 0.6254], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
using another actor
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[19.75]
 [19.75]
 [19.75]
 [19.75]
 [19.75]] [[0.334]
 [0.334]
 [0.334]
 [0.334]
 [0.334]]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
printing an ep nov before normalisation:  54.164141432144156
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
printing an ep nov before normalisation:  30.600218551771857
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
printing an ep nov before normalisation:  35.05278911121869
printing an ep nov before normalisation:  18.343878163215678
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.003]
 [0.009]
 [0.017]
 [0.017]] [[45.903]
 [24.544]
 [36.908]
 [38.104]
 [38.104]] [[1.076]
 [0.563]
 [0.851]
 [0.887]
 [0.887]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07504226938861093, 0.17499615732830806, 0.07504226938861093, 0.07504226938861093, 0.5248347651172481, 0.07504226938861093]
printing an ep nov before normalisation:  37.95456751926581
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.5288405418396
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  57.605704656817565
printing an ep nov before normalisation:  30.14447245091567
Printing some Q and Qe and total Qs values:  [[0.193]
 [0.193]
 [0.193]
 [0.193]
 [0.193]] [[55.063]
 [55.063]
 [55.063]
 [55.063]
 [55.063]] [[2.193]
 [2.193]
 [2.193]
 [2.193]
 [2.193]]
printing an ep nov before normalisation:  23.993103504180908
printing an ep nov before normalisation:  59.550220114122624
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07337671937815231, 0.15333953133973602, 0.07337671937815231, 0.11335812535894418, 0.5131721851668629, 0.07337671937815231]
maxi score, test score, baseline:  0.0101 0.25 0.25
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07411666992442655, 0.14815666731821864, 0.07411666992442655, 0.1111366686213226, 0.5183566542871791, 0.07411666992442655]
printing an ep nov before normalisation:  74.98619387313074
printing an ep nov before normalisation:  48.31433843225758
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
printing an ep nov before normalisation:  23.821611404418945
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.464472770690918
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07531015357065678, 0.13979710399136963, 0.07531015357065678, 0.1075536287810132, 0.5267188065156468, 0.07531015357065678]
printing an ep nov before normalisation:  62.46447269244493
printing an ep nov before normalisation:  75.27402672250189
siam score:  -0.9005278
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07531015357065678, 0.13979710399136963, 0.07531015357065678, 0.1075536287810132, 0.5267188065156468, 0.07531015357065678]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07531015357065678, 0.13979710399136963, 0.07531015357065678, 0.1075536287810132, 0.5267188065156468, 0.07531015357065678]
printing an ep nov before normalisation:  30.66753844855795
printing an ep nov before normalisation:  36.421670955113726
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.042]
 [0.137]
 [0.074]] [[77.2  ]
 [77.2  ]
 [87.293]
 [72.708]
 [77.2  ]] [[1.476]
 [1.476]
 [1.709]
 [1.42 ]
 [1.476]]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07531015357065678, 0.13979710399136963, 0.07531015357065678, 0.1075536287810132, 0.5267188065156468, 0.07531015357065678]
from probs:  [0.07531015357065678, 0.13979710399136963, 0.07531015357065678, 0.1075536287810132, 0.5267188065156468, 0.07531015357065678]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.015]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.015]
 [0.013]
 [0.013]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07579840222628595, 0.13637724518653974, 0.07579840222628595, 0.10608782370641286, 0.5301397244281896, 0.07579840222628595]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07579840222628595, 0.13637724518653974, 0.07579840222628595, 0.10608782370641286, 0.5301397244281896, 0.07579840222628595]
printing an ep nov before normalisation:  55.06098563153376
printing an ep nov before normalisation:  55.926749980158434
printing an ep nov before normalisation:  26.75290584564209
printing an ep nov before normalisation:  21.65920227816663
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07579840222628595, 0.13637724518653974, 0.07579840222628595, 0.10608782370641286, 0.5301397244281896, 0.07579840222628595]
printing an ep nov before normalisation:  29.684370756149292
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07579840222628595, 0.13637724518653974, 0.07579840222628595, 0.10608782370641286, 0.5301397244281896, 0.07579840222628595]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07623085362776441, 0.13334820923128163, 0.07623085362776441, 0.10478953142952303, 0.5331696984559021, 0.07623085362776441]
line 256 mcts: sample exp_bonus 31.704802253980265
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07623085362776441, 0.13334820923128163, 0.07623085362776441, 0.10478953142952303, 0.5331696984559021, 0.07623085362776441]
printing an ep nov before normalisation:  7.146400241946367
siam score:  -0.89841795
printing an ep nov before normalisation:  1.3392589104500985
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
from probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
printing an ep nov before normalisation:  65.44939955142168
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
printing an ep nov before normalisation:  55.79822850383136
siam score:  -0.8974239
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.07661655558084964, 0.13064662223233983, 0.07661655558084964, 0.10363158890659474, 0.5358721221185165, 0.07661655558084964]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  16.509066820144653
line 256 mcts: sample exp_bonus 41.44825506358098
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0750412925019569, 0.1250187693190713, 0.0750412925019569, 0.10003003091051411, 0.5248385838559868, 0.10003003091051411]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0750412925019569, 0.1250187693190713, 0.0750412925019569, 0.10003003091051411, 0.5248385838559868, 0.10003003091051411]
printing an ep nov before normalisation:  65.11033824946323
from probs:  [0.0750412925019569, 0.1250187693190713, 0.0750412925019569, 0.10003003091051411, 0.5248385838559868, 0.10003003091051411]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0750412925019569, 0.1250187693190713, 0.0750412925019569, 0.10003003091051411, 0.5248385838559868, 0.10003003091051411]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0750412925019569, 0.1250187693190713, 0.0750412925019569, 0.10003003091051411, 0.5248385838559868, 0.10003003091051411]
printing an ep nov before normalisation:  9.658590718985904
maxi score, test score, baseline:  0.0101 0.25 0.25
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.0754377191345763, 0.12303543089044952, 0.0754377191345763, 0.09923657501251291, 0.5276159808153721, 0.09923657501251291]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0101 0.25 0.25
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([    0.9954,     0.0031,     0.0002,     0.0004,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0010,     0.9733,     0.0013,     0.0242],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0281,     0.0000,     0.0072,     0.8996,     0.0650],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0038, 0.0041, 0.2351, 0.1418, 0.6152], grad_fn=<DivBackward0>)
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
maxi score, test score, baseline:  0.0101 0.25 0.25
using another actor
printing an ep nov before normalisation:  40.060651148766205
line 256 mcts: sample exp_bonus 30.593238325152797
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[35.11]
 [35.11]
 [35.11]
 [35.11]
 [35.11]] [[23.439]
 [23.439]
 [23.439]
 [23.439]
 [23.439]]
printing an ep nov before normalisation:  18.612645380184713
using another actor
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
from probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
printing an ep nov before normalisation:  72.57139334566337
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
printing an ep nov before normalisation:  64.2109816460538
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
printing an ep nov before normalisation:  32.56020685189948
siam score:  -0.90372664
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07368585046866465, 0.12017625856766564, 0.09693105451816515, 0.09693105451816515, 0.5153447274091743, 0.09693105451816515]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.026]
 [0.028]
 [0.029]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.022]
 [0.026]
 [0.028]
 [0.029]]
siam score:  -0.90617967
printing an ep nov before normalisation:  54.12194709202311
printing an ep nov before normalisation:  2.23027752898588
maxi score, test score, baseline:  0.0101 0.25 0.25
printing an ep nov before normalisation:  62.138690377768135
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  85.45620282443494
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07411608518246927, 0.11854036429488404, 0.09632822473867667, 0.09632822473867667, 0.5183588763066169, 0.09632822473867667]
printing an ep nov before normalisation:  0.0012236841939738952
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07450970606561047, 0.11704368788148257, 0.09577669697354652, 0.09577669697354652, 0.5211165151322675, 0.09577669697354652]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.0003342274681017443
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.088]
 [0.088]
 [0.088]
 [0.088]
 [0.088]]
siam score:  -0.9090552
printing an ep nov before normalisation:  42.97601465056029
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
printing an ep nov before normalisation:  20.426894709288774
maxi score, test score, baseline:  0.0101 0.25 0.25
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
using explorer policy with actor:  0
printing an ep nov before normalisation:  42.60405466838094
printing an ep nov before normalisation:  53.44999766777446
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
maxi score, test score, baseline:  0.0101 0.25 0.25
printing an ep nov before normalisation:  26.854946613311768
from probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
printing an ep nov before normalisation:  37.31700717969052
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
maxi score, test score, baseline:  0.0101 0.25 0.25
probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
from probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
printing an ep nov before normalisation:  17.68329502868639
maxi score, test score, baseline:  0.0101 0.25 0.25
actor:  0 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.9117,     0.0104,     0.0003,     0.0150,     0.0627],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0175,     0.9640,     0.0001,     0.0002,     0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0020,     0.0014,     0.9675,     0.0008,     0.0282],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0010, 0.0166, 0.0063, 0.9447, 0.0314], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0422, 0.1090, 0.2150, 0.0742, 0.5596], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.86849903706239
from probs:  [0.07487119622236026, 0.11566918308649643, 0.09527018965442836, 0.09527018965442836, 0.5236490517278583, 0.09527018965442836]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07337592366720401, 0.1133576706669737, 0.1133576706669737, 0.09336679716708886, 0.5131751406646708, 0.09336679716708886]
printing an ep nov before normalisation:  15.492278337478638
using another actor
from probs:  [0.07337592366720401, 0.1133576706669737, 0.1133576706669737, 0.09336679716708886, 0.5131751406646708, 0.09336679716708886]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07337592366720401, 0.1133576706669737, 0.1133576706669737, 0.09336679716708886, 0.5131751406646708, 0.09336679716708886]
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.61694160131165
maxi score, test score, baseline:  0.0121 0.25 0.25
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 46.24716472202535
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07337592366720401, 0.1133576706669737, 0.1133576706669737, 0.09336679716708886, 0.5131751406646708, 0.09336679716708886]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07376016162229194, 0.11220423267513664, 0.11220423267513664, 0.09298219714871431, 0.5158669787300061, 0.09298219714871431]
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.203]
 [0.196]
 [0.2  ]
 [0.206]] [[20.039]
 [25.756]
 [33.904]
 [43.842]
 [26.218]] [[0.309]
 [0.582]
 [0.891]
 [1.281]
 [0.604]]
printing an ep nov before normalisation:  60.85619054956843
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07376016162229194, 0.11220423267513664, 0.11220423267513664, 0.09298219714871431, 0.5158669787300061, 0.09298219714871431]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07411593899582541, 0.11113623006416193, 0.11113623006416193, 0.09262608452999367, 0.5183594318158634, 0.09262608452999367]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07411593899582541, 0.11113623006416193, 0.11113623006416193, 0.09262608452999367, 0.5183594318158634, 0.09262608452999367]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07444630498197796, 0.11014450950508327, 0.11014450950508327, 0.09229540724353062, 0.5206738615207943, 0.09229540724353062]
siam score:  -0.92183053
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07444630498197796, 0.11014450950508327, 0.11014450950508327, 0.09229540724353062, 0.5206738615207943, 0.09229540724353062]
maxi score, test score, baseline:  0.0121 0.25 0.25
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07314211927871375, 0.10821382454919608, 0.12574967718443725, 0.09067797191395491, 0.5115384351597431, 0.09067797191395491]
printing an ep nov before normalisation:  34.98673704400144
printing an ep nov before normalisation:  18.351366624202466
printing an ep nov before normalisation:  48.84293062124811
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07314211927871375, 0.10821382454919608, 0.12574967718443725, 0.09067797191395491, 0.5115384351597431, 0.09067797191395491]
from probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
printing an ep nov before normalisation:  32.57209300994873
from probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07348869470725024, 0.10737159360158348, 0.1243130430487501, 0.09043014415441686, 0.5139663803335824, 0.09043014415441686]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.13556432723999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.207]
 [0.207]
 [0.234]
 [0.207]
 [0.207]] [[48.156]
 [48.156]
 [60.085]
 [48.156]
 [48.156]] [[0.975]
 [0.975]
 [1.305]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07411583457651136, 0.10584754843599317, 0.12171340536573408, 0.08998169150625226, 0.5183598286092568, 0.08998169150625226]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07295945962454924, 0.10419519530525503, 0.11981306314560793, 0.10419519530525503, 0.5102597591544307, 0.08857732746490214]
printing an ep nov before normalisation:  38.40608623466788
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.79122041124389
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07327481180952287, 0.10356406203346138, 0.11870868714543066, 0.10356406203346138, 0.5124689400566315, 0.08841943692149214]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.74212138616008
printing an ep nov before normalisation:  46.330942148948964
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07327481180952287, 0.10356406203346138, 0.11870868714543066, 0.10356406203346138, 0.5124689400566315, 0.08841943692149214]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07327481180952287, 0.10356406203346138, 0.11870868714543066, 0.10356406203346138, 0.5124689400566315, 0.08841943692149214]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07357161485486055, 0.10297005226911511, 0.1176692709762424, 0.10297005226911511, 0.514548176068679, 0.08827083356198784]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07357161485486055, 0.10297005226911511, 0.1176692709762424, 0.10297005226911511, 0.514548176068679, 0.08827083356198784]
using another actor
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07385145860448068, 0.10240998416207636, 0.11668924694087421, 0.10240998416207636, 0.5165086047472139, 0.08813072138327853]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.113]
 [0.123]
 [0.071]
 [0.113]] [[49.689]
 [49.689]
 [46.768]
 [59.397]
 [49.689]] [[1.048]
 [1.048]
 [0.981]
 [1.26 ]
 [1.048]]
actions average: 
K:  3  action  0 :  tensor([    0.9004,     0.0001,     0.0003,     0.0566,     0.0427],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9925,     0.0002,     0.0001,     0.0067],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0041,     0.9610,     0.0016,     0.0332],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0020,     0.0001,     0.0233,     0.9451,     0.0296],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0025, 0.0017, 0.2744, 0.1371, 0.5843], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07385145860448068, 0.10240998416207636, 0.11668924694087421, 0.10240998416207636, 0.5165086047472139, 0.08813072138327853]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07385145860448068, 0.10240998416207636, 0.11668924694087421, 0.10240998416207636, 0.5165086047472139, 0.08813072138327853]
from probs:  [0.07385145860448068, 0.10240998416207636, 0.11668924694087421, 0.10240998416207636, 0.5165086047472139, 0.08813072138327853]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07385145860448068, 0.10240998416207636, 0.11668924694087421, 0.10240998416207636, 0.5165086047472139, 0.08813072138327853]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.0741157562618712, 0.10188102938330985, 0.11576366594402916, 0.10188102938330985, 0.5183601262048895, 0.08799839282259052]
actions average: 
K:  2  action  0 :  tensor([0.9020, 0.0034, 0.0032, 0.0262, 0.0653], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0072,     0.9746,     0.0012,     0.0000,     0.0170],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0003,     0.9738,     0.0024,     0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0038,     0.0001,     0.0053,     0.9584,     0.0324],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0234, 0.0012, 0.1628, 0.1127, 0.6999], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.0741157562618712, 0.10188102938330985, 0.11576366594402916, 0.10188102938330985, 0.5183601262048895, 0.08799839282259052]
printing an ep nov before normalisation:  45.13136973774542
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.121]
 [0.121]
 [0.121]] [[55.331]
 [55.331]
 [55.331]
 [55.331]
 [55.331]] [[1.08]
 [1.08]
 [1.08]
 [1.08]
 [1.08]]
printing an ep nov before normalisation:  42.94966220855713
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.0741157562618712, 0.10188102938330985, 0.11576366594402916, 0.10188102938330985, 0.5183601262048895, 0.08799839282259052]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.0741157562618712, 0.10188102938330985, 0.11576366594402916, 0.10188102938330985, 0.5183601262048895, 0.08799839282259052]
siam score:  -0.91611814
siam score:  -0.9175966
maxi score, test score, baseline:  0.0121 0.25 0.25
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07310194763102042, 0.1004867434463315, 0.11417914135398705, 0.11417914135398705, 0.5112586806759979, 0.08679434553867597]
printing an ep nov before normalisation:  36.9112787712758
printing an ep nov before normalisation:  29.10630702972412
Printing some Q and Qe and total Qs values:  [[0.742]
 [0.742]
 [0.742]
 [0.742]
 [0.326]] [[18.16 ]
 [18.16 ]
 [18.16 ]
 [18.16 ]
 [42.169]] [[0.741]
 [0.741]
 [0.741]
 [0.741]
 [1.092]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07310194763102042, 0.1004867434463315, 0.11417914135398705, 0.11417914135398705, 0.5112586806759979, 0.08679434553867597]
printing an ep nov before normalisation:  41.84908807044586
printing an ep nov before normalisation:  33.699541091918945
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07363515122395434, 0.0995974346033159, 0.11257857629299668, 0.11257857629299668, 0.5149939686731011, 0.08661629291363512]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0121 0.25 0.25
printing an ep nov before normalisation:  42.62114500613839
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07363515122395434, 0.0995974346033159, 0.11257857629299668, 0.11257857629299668, 0.5149939686731011, 0.08661629291363512]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07363515122395434, 0.0995974346033159, 0.11257857629299668, 0.11257857629299668, 0.5149939686731011, 0.08661629291363512]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07363515122395434, 0.0995974346033159, 0.11257857629299668, 0.11257857629299668, 0.5149939686731011, 0.08661629291363512]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07269249555302831, 0.09832181494765695, 0.11113647464497127, 0.1239511343422856, 0.5083909252617153, 0.08550715525034262]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07269249555302831, 0.09832181494765695, 0.11113647464497127, 0.1239511343422856, 0.5083909252617153, 0.08550715525034262]
using another actor
from probs:  [0.07269249555302831, 0.09832181494765695, 0.11113647464497127, 0.1239511343422856, 0.5083909252617153, 0.08550715525034262]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.071773712666722, 0.09707850040004058, 0.12238328813335916, 0.12238328813335916, 0.5019551041331379, 0.08442610653338128]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07087790703614806, 0.09586627911367465, 0.12085465119120124, 0.12085465119120124, 0.49568023235410014, 0.09586627911367465]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07087790703614806, 0.09586627911367465, 0.12085465119120124, 0.12085465119120124, 0.49568023235410014, 0.09586627911367465]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.33656431322651
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07087790703614806, 0.09586627911367465, 0.12085465119120124, 0.12085465119120124, 0.49568023235410014, 0.09586627911367465]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.07147257225443307, 0.09527109585749147, 0.11906961946054986, 0.11906961946054986, 0.4998459971094842, 0.09527109585749147]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07147257225443307, 0.09527109585749147, 0.11906961946054986, 0.11906961946054986, 0.4998459971094842, 0.09527109585749147]
printing an ep nov before normalisation:  23.47698211669922
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93294275
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07201318020771871, 0.09473001695786622, 0.11744685370801372, 0.11744685370801372, 0.5036330784605214, 0.09473001695786622]
printing an ep nov before normalisation:  82.9933091303469
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.07201318020771871, 0.09473001695786622, 0.11744685370801372, 0.11744685370801372, 0.5036330784605214, 0.09473001695786622]
printing an ep nov before normalisation:  17.161584430602097
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.0121 0.25 0.25
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.0714725350773289, 0.0934404115979453, 0.1154082881185617, 0.12639222637886993, 0.499846127229349, 0.0934404115979453]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07069684855880615, 0.0924258639794538, 0.1250193871104253, 0.1250193871104253, 0.4944126492614356, 0.0924258639794538]
printing an ep nov before normalisation:  31.422529220581055
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07069684855880615, 0.0924258639794538, 0.1250193871104253, 0.1250193871104253, 0.4944126492614356, 0.0924258639794538]
maxi score, test score, baseline:  0.0121 0.25 0.25
probs:  [0.07069684855880615, 0.0924258639794538, 0.1250193871104253, 0.1250193871104253, 0.4944126492614356, 0.0924258639794538]
printing an ep nov before normalisation:  30.949139579442424
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.21614677198689
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.08 ]
 [0.082]
 [0.075]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.103]
 [0.08 ]
 [0.082]
 [0.075]
 [0.08 ]]
printing an ep nov before normalisation:  17.725836123266685
Printing some Q and Qe and total Qs values:  [[0.13 ]
 [0.148]
 [0.015]
 [0.104]
 [0.134]] [[27.654]
 [48.693]
 [24.979]
 [17.548]
 [26.564]] [[0.13 ]
 [0.148]
 [0.015]
 [0.104]
 [0.134]]
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.341]
 [0.341]
 [0.341]
 [0.341]] [[70.18]
 [70.18]
 [70.18]
 [70.18]
 [70.18]] [[1.904]
 [1.904]
 [1.904]
 [1.904]
 [1.904]]
printing an ep nov before normalisation:  36.17259863104447
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.520347553026525
printing an ep nov before normalisation:  59.26240578941431
printing an ep nov before normalisation:  34.66988568291449
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0161 0.25 0.25
probs:  [0.07049157744366247, 0.0911005251343062, 0.13231842051559373, 0.12201394667027185, 0.49297500510185954, 0.0911005251343062]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0301 0.25 0.25
probs:  [0.07075166311007101, 0.09094429543777535, 0.13132956009318406, 0.12123324392933188, 0.49479694199186225, 0.09094429543777535]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07075217958099705, 0.09094470317798015, 0.1313297503719463, 0.12123348857345473, 0.4947951751176417, 0.09094470317798015]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07004575966155761, 0.09003629214537327, 0.1300173571130046, 0.1300173571130046, 0.4898469418216866, 0.09003629214537327]
printing an ep nov before normalisation:  0.008844080566632329
printing an ep nov before normalisation:  0.005722364776659106
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07055806548506907, 0.0897797857213886, 0.12822322619402762, 0.12822322619402762, 0.4934359106840985, 0.0897797857213886]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06988713412259738, 0.08892573068864379, 0.12700292382073664, 0.13652222210375983, 0.48873625857561853, 0.08892573068864379]
Printing some Q and Qe and total Qs values:  [[0.591]
 [0.4  ]
 [0.62 ]
 [0.591]
 [0.591]] [[28.415]
 [45.272]
 [26.782]
 [28.415]
 [28.415]] [[1.112]
 [1.509]
 [1.083]
 [1.112]
 [1.112]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  63.30784970715673
printing an ep nov before normalisation:  21.91075563430786
using another actor
using another actor
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06922886592960942, 0.08808779510452372, 0.1352351180418095, 0.1352351180418095, 0.4841253077777241, 0.08808779510452372]
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.093]
 [0.001]
 [0.051]
 [0.071]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.08 ]
 [0.093]
 [0.001]
 [0.051]
 [0.071]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.188]
 [0.188]
 [0.196]
 [0.188]
 [0.188]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.188]
 [0.188]
 [0.196]
 [0.188]
 [0.188]]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06922886592960942, 0.08808779510452372, 0.1352351180418095, 0.1352351180418095, 0.4841253077777241, 0.08808779510452372]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06922886592960942, 0.08808779510452372, 0.1352351180418095, 0.1352351180418095, 0.4841253077777241, 0.08808779510452372]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06949065703569116, 0.08800037315587698, 0.13427466345634148, 0.13427466345634148, 0.4859592697398718, 0.08800037315587698]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06949065703569116, 0.08800037315587698, 0.13427466345634148, 0.13427466345634148, 0.4859592697398718, 0.08800037315587698]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06949065703569116, 0.08800037315587698, 0.13427466345634148, 0.13427466345634148, 0.4859592697398718, 0.08800037315587698]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06998619224338207, 0.0878348952138346, 0.13245665263996595, 0.13245665263996595, 0.4894307120490169, 0.0878348952138346]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06998619224338207, 0.0878348952138346, 0.13245665263996595, 0.13245665263996595, 0.4894307120490169, 0.0878348952138346]
printing an ep nov before normalisation:  19.561827937245063
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07022092038000324, 0.08775651061394205, 0.13159548619878902, 0.13159548619878902, 0.4910750859945345, 0.08775651061394205]
printing an ep nov before normalisation:  40.764963394681395
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07022092038000324, 0.08775651061394205, 0.13159548619878902, 0.13159548619878902, 0.4910750859945345, 0.08775651061394205]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.0704475550120564, 0.08768082874124035, 0.13076401306420016, 0.13076401306420016, 0.4926627613770627, 0.08768082874124035]
printing an ep nov before normalisation:  64.94098110116715
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 55.66517919611378
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  38.63802074558565
Printing some Q and Qe and total Qs values:  [[0.393]
 [1.028]
 [0.668]
 [0.781]
 [0.585]] [[30.723]
 [26.402]
 [31.681]
 [38.802]
 [31.029]] [[1.651]
 [1.95 ]
 [2.001]
 [2.669]
 [1.867]]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07108287802678356, 0.08746867036504925, 0.12843315121071344, 0.12843315121071344, 0.4971134788216912, 0.08746867036504925]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.07050586109590597, 0.08675839161490777, 0.1273897179124123, 0.1355159831719132, 0.49307165458995295, 0.08675839161490777]
UNIT TEST: sample policy line 217 mcts : [0.    0.125 0.833 0.    0.042]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.0701512895723623, 0.08601683101252192, 0.12568068461292098, 0.13361345533300076, 0.49058813773659227, 0.09394960173260174]
Printing some Q and Qe and total Qs values:  [[0.15 ]
 [0.15 ]
 [0.153]
 [0.15 ]
 [0.15 ]] [[74.656]
 [74.656]
 [74.931]
 [74.656]
 [74.656]] [[0.814]
 [0.814]
 [0.82 ]
 [0.814]
 [0.814]]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.0701512895723623, 0.08601683101252192, 0.12568068461292098, 0.13361345533300076, 0.49058813773659227, 0.09394960173260174]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.92822015
actions average: 
K:  0  action  0 :  tensor([    0.8970,     0.0005,     0.0019,     0.0331,     0.0675],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9886,     0.0070,     0.0000,     0.0039],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0001,     0.9493,     0.0219,     0.0281],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0370,     0.0001,     0.0010,     0.8595,     0.1024],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1967, 0.0015, 0.0973, 0.0901, 0.6143], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.070357765183697, 0.08597542488363803, 0.1250195741334906, 0.1328284039834611, 0.4920345770821048, 0.09378425473360855]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
Printing some Q and Qe and total Qs values:  [[0.144]
 [0.378]
 [0.317]
 [0.096]
 [0.317]] [[28.6  ]
 [44.013]
 [40.591]
 [25.574]
 [40.591]] [[0.535]
 [1.361]
 [1.169]
 [0.371]
 [1.169]]
from probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
from probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
from probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
from probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
maxi score, test score, baseline:  0.032100000000000004 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
probs:  [0.06981321074940089, 0.0853097636961634, 0.1240511460630697, 0.13954769900983222, 0.4882201403119889, 0.09305804016954466]
printing an ep nov before normalisation:  45.938739773563874
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9207225
maxi score, test score, baseline:  0.034100000000000005 0.5 0.5
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.87768162921112
printing an ep nov before normalisation:  28.41295254647034
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.06874905134362298, 0.0840089394459155, 0.1297886037527931, 0.14504849185508564, 0.4807660301055212, 0.09163888349706177]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.135]
 [0.079]
 [0.135]
 [0.135]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.135]
 [0.135]
 [0.079]
 [0.135]
 [0.135]]
Printing some Q and Qe and total Qs values:  [[0.159]
 [0.273]
 [0.02 ]
 [0.078]
 [0.147]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.159]
 [0.273]
 [0.02 ]
 [0.078]
 [0.147]]
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.06874905134362298, 0.0840089394459155, 0.1297886037527931, 0.14504849185508564, 0.4807660301055212, 0.09163888349706177]
maxi score, test score, baseline:  0.0361 0.5 0.5
probs:  [0.06874905134362298, 0.0840089394459155, 0.1297886037527931, 0.14504849185508564, 0.4807660301055212, 0.09163888349706177]
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [0.526]
 [0.945]
 [0.945]] [[33.549]
 [33.549]
 [33.379]
 [33.549]
 [33.549]] [[1.521]
 [1.521]
 [1.095]
 [1.521]
 [1.521]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0361 0.5 0.5
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.285]
 [0.295]
 [0.285]
 [0.224]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.285]
 [0.285]
 [0.295]
 [0.285]
 [0.224]]
maxi score, test score, baseline:  0.0361 0.5 0.5
UNIT TEST: sample policy line 217 mcts : [0.167 0.458 0.125 0.042 0.208]
using another actor
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06771690689163282, 0.09026242177708356, 0.13535345154798506, 0.14286862317646865, 0.47353617482974625, 0.09026242177708356]
Printing some Q and Qe and total Qs values:  [[0.135]
 [0.186]
 [0.135]
 [0.135]
 [0.135]] [[ 0.   ]
 [64.328]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.588]
 [ 1.216]
 [-0.588]
 [-0.588]
 [-0.588]]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06771690689163282, 0.09026242177708356, 0.13535345154798506, 0.14286862317646865, 0.47353617482974625, 0.09026242177708356]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06794873617356663, 0.09016027053451414, 0.1345833392564091, 0.1419871840433916, 0.4751601994576041, 0.09016027053451414]
printing an ep nov before normalisation:  54.80940320694424
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.44533094367105
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06817379725858108, 0.090061101571489, 0.1338357101973048, 0.14113147830160747, 0.47673681109952876, 0.090061101571489]
from probs:  [0.06817379725858108, 0.090061101571489, 0.1338357101973048, 0.14113147830160747, 0.47673681109952876, 0.090061101571489]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06817379725858108, 0.090061101571489, 0.1338357101973048, 0.14113147830160747, 0.47673681109952876, 0.090061101571489]
printing an ep nov before normalisation:  57.86187462247727
printing an ep nov before normalisation:  45.89940294724161
printing an ep nov before normalisation:  46.32118435954826
siam score:  -0.92428046
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06790466149049047, 0.09646234973420408, 0.13215946003884604, 0.13929888209977445, 0.4748517189634092, 0.08932292767327567]
printing an ep nov before normalisation:  41.471918933034594
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.386]
 [0.386]
 [0.386]
 [0.386]] [[41.987]
 [41.987]
 [41.987]
 [41.987]
 [41.987]] [[2.143]
 [2.143]
 [2.143]
 [2.143]
 [2.143]]
siam score:  -0.9247725
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06790466149049047, 0.09646234973420408, 0.13215946003884604, 0.13929888209977445, 0.4748517189634092, 0.08932292767327567]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0381 0.5 0.5
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
siam score:  -0.9256564
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06742386091943071, 0.09577894827578383, 0.13122280747122525, 0.13831157931031351, 0.4714838557474628, 0.09577894827578383]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06742386091943071, 0.09577894827578383, 0.13122280747122525, 0.13831157931031351, 0.4714838557474628, 0.09577894827578383]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.0686777135492853, 0.0907252280006961, 0.12747108541971414, 0.14216942838732133, 0.4802313166422871, 0.0907252280006961]
from probs:  [0.0686777135492853, 0.0907252280006961, 0.12747108541971414, 0.14216942838732133, 0.4802313166422871, 0.0907252280006961]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9266388
Printing some Q and Qe and total Qs values:  [[0.571]
 [0.407]
 [0.802]
 [0.41 ]
 [0.571]] [[52.844]
 [61.479]
 [46.208]
 [56.267]
 [52.844]] [[1.634]
 [1.813]
 [1.601]
 [1.609]
 [1.634]]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.0688905730519236, 0.09061859385519984, 0.12683196186066023, 0.1413173090628444, 0.4817229683141721, 0.09061859385519984]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.651006384129566
printing an ep nov before normalisation:  45.76789734900728
maxi score, test score, baseline:  0.0381 0.5 0.5
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9282596
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[30.669]
 [30.669]
 [30.669]
 [30.669]
 [30.669]] [[3.167]
 [3.167]
 [3.167]
 [3.167]
 [3.167]]
printing an ep nov before normalisation:  57.63775242283263
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06860810335174092, 0.09696238671991222, 0.1253166700880835, 0.13949381177216916, 0.4797452121902247, 0.0898738158778694]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.254]
 [0.271]
 [0.308]
 [0.258]
 [0.294]] [[43.328]
 [45.453]
 [50.896]
 [40.843]
 [45.093]] [[0.254]
 [0.271]
 [0.308]
 [0.258]
 [0.294]]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06833740347074072, 0.09610096013782571, 0.13080540597168192, 0.13774629513845316, 0.47784986431024407, 0.08916007097105445]
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06833740347074072, 0.09610096013782571, 0.13080540597168192, 0.13774629513845316, 0.47784986431024407, 0.08916007097105445]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06786689547774606, 0.09543892464674715, 0.1299039611079985, 0.14368997569249906, 0.47455432572051226, 0.08854591735449688]
printing an ep nov before normalisation:  52.91722085313821
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06828295013770143, 0.09511487282741922, 0.12865477618956647, 0.14207073753442537, 0.47746977115589784, 0.08840689215498979]
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06828295013770143, 0.09511487282741922, 0.12865477618956647, 0.14207073753442537, 0.47746977115589784, 0.08840689215498979]
printing an ep nov before normalisation:  29.48314666748047
maxi score, test score, baseline:  0.0381 0.5 0.5
maxi score, test score, baseline:  0.0381 0.5 0.5
probs:  [0.06828295013770143, 0.09511487282741922, 0.12865477618956647, 0.14207073753442537, 0.47746977115589784, 0.08840689215498979]
maxi score, test score, baseline:  0.0381 0.5 0.5
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06828295013770143, 0.09511487282741922, 0.12865477618956647, 0.14207073753442537, 0.47746977115589784, 0.08840689215498979]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06848271238711713, 0.09495928432766981, 0.12805499925336067, 0.141293285223637, 0.47886957746568376, 0.08834014134253165]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06848271238711713, 0.09495928432766981, 0.12805499925336067, 0.141293285223637, 0.47886957746568376, 0.08834014134253165]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
deleting a thread, now have 1 threads
Frames:  143302 train batches done:  16786 episodes:  10662
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06848271238711713, 0.09495928432766981, 0.12805499925336067, 0.141293285223637, 0.47886957746568376, 0.08834014134253165]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.06867725265904073, 0.0948077630610743, 0.12747090106361625, 0.14053615626463306, 0.4802327914910696, 0.08827513546056591]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.88166427612305
maxi score, test score, baseline:  0.040100000000000004 0.5 0.5
probs:  [0.07034626030243066, 0.09074352282662182, 0.12473896036694042, 0.13833713538306783, 0.4918896858023812, 0.0839444353185581]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.07034626030243066, 0.09074352282662182, 0.12473896036694042, 0.13833713538306783, 0.4918896858023812, 0.0839444353185581]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.0698717228383483, 0.09013112968613586, 0.12389680776578181, 0.13740307899764018, 0.48856613102595803, 0.09013112968613586]
from probs:  [0.0698717228383483, 0.09013112968613586, 0.12389680776578181, 0.13740307899764018, 0.48856613102595803, 0.09013112968613586]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.0698717228383483, 0.09013112968613586, 0.12389680776578181, 0.13740307899764018, 0.48856613102595803, 0.09013112968613586]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.0698717228383483, 0.09013112968613586, 0.12389680776578181, 0.13740307899764018, 0.48856613102595803, 0.09013112968613586]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.0698717228383483, 0.09013112968613586, 0.12389680776578181, 0.13740307899764018, 0.48856613102595803, 0.09013112968613586]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940355667268758, 0.09623475942964733, 0.1230659621866071, 0.13648156356508695, 0.48528719940556364, 0.08952695874040739]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940355667268758, 0.09623475942964733, 0.1230659621866071, 0.13648156356508695, 0.48528719940556364, 0.08952695874040739]
siam score:  -0.92877394
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  23.53529869834111
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06894163434633323, 0.09559391588824234, 0.12224619743015146, 0.14223540858658326, 0.4820519982459245, 0.08893084550276506]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06894163434633323, 0.09559391588824234, 0.12224619743015146, 0.14223540858658326, 0.4820519982459245, 0.08893084550276506]
printing an ep nov before normalisation:  33.94725561141968
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06913141106906356, 0.09543305302796777, 0.12173469498687196, 0.1414609264560501, 0.48338227192180494, 0.08885764253824172]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06931625919680115, 0.09527636785543195, 0.12123647651406275, 0.14070655800803583, 0.484677997734894, 0.08878634069077425]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06931625919680115, 0.09527636785543195, 0.12123647651406275, 0.14070655800803583, 0.484677997734894, 0.08878634069077425]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06931625919680115, 0.09527636785543195, 0.12123647651406275, 0.14070655800803583, 0.484677997734894, 0.08878634069077425]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06886980008686941, 0.09466238028373901, 0.126903105529826, 0.13979939562826083, 0.48155108323678314, 0.0882142352345216]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.984]
 [1.217]
 [1.008]
 [0.653]
 [0.984]] [[43.796]
 [28.552]
 [25.783]
 [23.985]
 [43.796]] [[4.346]
 [2.884]
 [2.367]
 [1.812]
 [4.346]]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06886980008686941, 0.09466238028373901, 0.126903105529826, 0.13979939562826083, 0.48155108323678314, 0.0882142352345216]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.379011123069056
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
siam score:  -0.9369519
printing an ep nov before normalisation:  22.68533706665039
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06905444826518473, 0.09451850523948437, 0.12634857645735892, 0.13908060494450872, 0.48284537409755385, 0.08815249099590945]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06923445183001585, 0.09437824920721606, 0.12580799592871633, 0.13837989461731642, 0.4841071085538193, 0.088092299862916]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06923445183001585, 0.09437824920721606, 0.12580799592871633, 0.13837989461731642, 0.4841071085538193, 0.088092299862916]
printing an ep nov before normalisation:  32.940017076856854
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.98717021942139
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
siam score:  -0.9357608
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
printing an ep nov before normalisation:  43.12463538744823
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
Printing some Q and Qe and total Qs values:  [[0.262]
 [1.416]
 [1.401]
 [1.381]
 [1.313]] [[17.721]
 [13.389]
 [12.36 ]
 [14.159]
 [15.453]] [[1.758]
 [2.546]
 [2.445]
 [2.577]
 [2.618]]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
siam score:  -0.93619674
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
printing an ep nov before normalisation:  53.382327509972995
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
printing an ep nov before normalisation:  41.414090073210836
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.503]
 [0.509]
 [0.631]
 [0.509]
 [0.509]] [[60.507]
 [55.338]
 [52.683]
 [55.338]
 [55.338]] [[1.503]
 [1.384]
 [1.443]
 [1.384]
 [1.384]]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
printing an ep nov before normalisation:  25.44299193790981
printing an ep nov before normalisation:  54.33608033732583
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06940998384959664, 0.094241477334806, 0.12528084419131771, 0.1376965909339224, 0.48533749972685364, 0.08803360396350367]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  63.04942996057396
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06898223599373898, 0.09982995094308456, 0.12450812290256101, 0.13684720888229923, 0.4823416163149698, 0.08749086496334632]
siam score:  -0.9437296
printing an ep nov before normalisation:  21.56932830810547
printing an ep nov before normalisation:  51.010763479729576
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06898223599373898, 0.09982995094308456, 0.12450812290256101, 0.13684720888229923, 0.4823416163149698, 0.08749086496334632]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.54233070740894
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
printing an ep nov before normalisation:  60.69803701468769
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.042100000000000005 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
printing an ep nov before normalisation:  19.995499849319458
from probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06880245521733686, 0.10003571419052722, 0.1250223213690795, 0.13751562495835565, 0.48108147366344983, 0.08754241060125108]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06837582642938471, 0.09941503913589479, 0.12424640930110287, 0.13666209438370688, 0.47809343415531796, 0.09320719659459277]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06837582642938471, 0.09941503913589479, 0.12424640930110287, 0.13666209438370688, 0.47809343415531796, 0.09320719659459277]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06837582642938471, 0.09941503913589479, 0.12424640930110287, 0.13666209438370688, 0.47809343415531796, 0.09320719659459277]
maxi score, test score, baseline:  0.0461 0.5 0.5
probs:  [0.06837582642938471, 0.09941503913589479, 0.12424640930110287, 0.13666209438370688, 0.47809343415531796, 0.09320719659459277]
actor:  0 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06837582642938471, 0.09941503913589479, 0.12424640930110287, 0.13666209438370688, 0.47809343415531796, 0.09320719659459277]
printing an ep nov before normalisation:  46.50069257152474
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06855973796293388, 0.09921815318285038, 0.12374488535878356, 0.13600825144675013, 0.47938250190981485, 0.09308647013886706]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.8078670501709
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06873919163352882, 0.09902603958192197, 0.12325551794063647, 0.1353702571199937, 0.48064032373167564, 0.09296866999224333]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
printing an ep nov before normalisation:  0.05939944475358061
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06832579359880392, 0.09843014249712924, 0.12853449139545456, 0.13455536117511963, 0.47774493861602846, 0.09240927271746417]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06850482742077804, 0.09825083931347156, 0.12799685120616508, 0.1339460535847038, 0.47899979153994865, 0.09230163693493286]
maxi score, test score, baseline:  0.048100000000000004 0.5 0.5
probs:  [0.06850482742077804, 0.09825083931347156, 0.12799685120616508, 0.1339460535847038, 0.47899979153994865, 0.09230163693493286]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  52.773110094117726
printing an ep nov before normalisation:  42.24821387685498
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06850482742077804, 0.09825083931347156, 0.12799685120616508, 0.1339460535847038, 0.47899979153994865, 0.09230163693493286]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
printing an ep nov before normalisation:  50.84927019363214
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
using another actor
from probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
printing an ep nov before normalisation:  41.908098072867986
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0700128783975246, 0.09469044135985874, 0.12553739506277645, 0.13170678580335995, 0.48953144875720506, 0.0885210506192752]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.72627613580484
Printing some Q and Qe and total Qs values:  [[0.423]
 [0.423]
 [0.466]
 [0.423]
 [0.423]] [[44.168]
 [44.168]
 [51.57 ]
 [44.168]
 [44.168]] [[1.154]
 [1.154]
 [1.411]
 [1.154]
 [1.154]]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.07017570374617327, 0.09455236806292949, 0.12502319845887475, 0.13111736453806386, 0.49067316321021825, 0.08845820198374042]
printing an ep nov before normalisation:  38.729400634765625
printing an ep nov before normalisation:  40.08633611172176
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.398]
 [1.215]
 [1.188]
 [0.659]
 [0.931]] [[48.193]
 [50.088]
 [50.591]
 [51.657]
 [54.764]] [[1.087]
 [1.963]
 [1.952]
 [1.456]
 [1.825]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.07017570374617327, 0.09455236806292949, 0.12502319845887475, 0.13111736453806386, 0.49067316321021825, 0.08845820198374042]
printing an ep nov before normalisation:  46.19111772894105
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
printing an ep nov before normalisation:  53.25203895568848
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06975110144539874, 0.09397999275071572, 0.1303233297086912, 0.1303233297086912, 0.4876994764621167, 0.08792276992438647]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
from probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
printing an ep nov before normalisation:  53.76709490590857
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
printing an ep nov before normalisation:  41.352258386898065
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
printing an ep nov before normalisation:  65.80608780478268
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
Printing some Q and Qe and total Qs values:  [[0.301]
 [0.507]
 [0.677]
 [0.507]
 [0.418]] [[45.399]
 [52.312]
 [55.178]
 [52.312]
 [55.405]] [[0.766]
 [1.1  ]
 [1.324]
 [1.1  ]
 [1.069]]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.0693316160620735, 0.09341451518073571, 0.12953886385872906, 0.13555958863839462, 0.48476162585899685, 0.08739379040107016]
Printing some Q and Qe and total Qs values:  [[0.073]
 [1.226]
 [0.863]
 [0.73 ]
 [0.956]] [[36.136]
 [32.054]
 [32.95 ]
 [27.511]
 [35.609]] [[1.011]
 [1.967]
 [1.647]
 [1.252]
 [1.869]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.92498225
printing an ep nov before normalisation:  20.755475448154925
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06850763046704188, 0.0923037604548297, 0.1339469879334584, 0.13989602043040533, 0.47899087275638186, 0.08635472795788275]
actions average: 
K:  4  action  0 :  tensor([    0.9185,     0.0004,     0.0014,     0.0121,     0.0675],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0059, 0.9777, 0.0048, 0.0014, 0.0102], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0038, 0.0017, 0.9093, 0.0177, 0.0676], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0423, 0.0135, 0.0218, 0.8379, 0.0844], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0866, 0.0131, 0.0515, 0.1184, 0.7304], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.989519119262695
using explorer policy with actor:  1
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06850763046704188, 0.0923037604548297, 0.1339469879334584, 0.13989602043040533, 0.47899087275638186, 0.08635472795788275]
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06850763046704188, 0.0923037604548297, 0.1339469879334584, 0.13989602043040533, 0.47899087275638186, 0.08635472795788275]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.050100000000000006 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.09138437066822
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.48 ]
 [0.48 ]
 [0.085]
 [0.555]
 [0.48 ]] [[37.468]
 [37.468]
 [43.895]
 [44.155]
 [37.468]] [[1.245]
 [1.245]
 [1.109]
 [1.59 ]
 [1.245]]
maxi score, test score, baseline:  0.0521 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
printing an ep nov before normalisation:  48.02763248799977
maxi score, test score, baseline:  0.0521 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
maxi score, test score, baseline:  0.0521 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
maxi score, test score, baseline:  0.0521 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.91374336449028
maxi score, test score, baseline:  0.0541 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.417 0.083 0.333]
maxi score, test score, baseline:  0.0541 0.5 0.5
probs:  [0.06810295287424406, 0.09175824418442549, 0.133155003977243, 0.1449826496323337, 0.47615672797487374, 0.08584442135688013]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0541 0.5 0.5
probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
actions average: 
K:  4  action  0 :  tensor([    0.9612,     0.0024,     0.0000,     0.0008,     0.0354],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0045,     0.9910,     0.0010,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0016, 0.0011, 0.9563, 0.0022, 0.0388], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0008,     0.0013,     0.9586,     0.0390],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0906, 0.0139, 0.1220, 0.0673, 0.7063], grad_fn=<DivBackward0>)
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.129]
 [0.001]
 [0.075]
 [0.047]] [[31.244]
 [46.268]
 [42.495]
 [24.374]
 [32.968]] [[0.081]
 [0.129]
 [0.001]
 [0.075]
 [0.047]]
maxi score, test score, baseline:  0.056100000000000004 0.5 0.5
probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.056100000000000004 0.5 0.5
probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
maxi score, test score, baseline:  0.056100000000000004 0.5 0.5
probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.06828144672173432, 0.09166011284726279, 0.1325727785669376, 0.14426211162970184, 0.4774081039184826, 0.08581544631588066]
maxi score, test score, baseline:  0.056100000000000004 0.5 0.5
printing an ep nov before normalisation:  38.234977166155446
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06788514257699071, 0.09112785412750271, 0.1318025993408987, 0.14923463300378267, 0.47463259471095054, 0.0853171762398747]
printing an ep nov before normalisation:  48.90784848142302
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9254671
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06749342105871271, 0.09060175013241072, 0.13104132601138224, 0.15414965508508027, 0.4718891798484279, 0.08482466786398621]
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06749342105871271, 0.09060175013241072, 0.13104132601138224, 0.15414965508508027, 0.4718891798484279, 0.08482466786398621]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06710620313765565, 0.09582556761717806, 0.13028880499260495, 0.15326429657622284, 0.4691773058509693, 0.0843378218253691]
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06710620313765565, 0.09582556761717806, 0.13028880499260495, 0.15326429657622284, 0.4691773058509693, 0.0843378218253691]
maxi score, test score, baseline:  0.058100000000000006 0.5 0.5
probs:  [0.06710620313765565, 0.09582556761717806, 0.13028880499260495, 0.15326429657622284, 0.4691773058509693, 0.0843378218253691]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0601 0.5 0.5
probs:  [0.06710620313765565, 0.09582556761717806, 0.13028880499260495, 0.15326429657622284, 0.4691773058509693, 0.0843378218253691]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0601 0.5 0.5
probs:  [0.0672909505936497, 0.09568401232879742, 0.12975568641097465, 0.1524701357990928, 0.4704724272327471, 0.08432678763473833]
maxi score, test score, baseline:  0.0601 0.5 0.5
probs:  [0.0672909505936497, 0.09568401232879742, 0.12975568641097465, 0.1524701357990928, 0.4704724272327471, 0.08432678763473833]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.34 ]
 [0.256]
 [0.435]
 [0.835]
 [0.56 ]] [[28.159]
 [28.974]
 [34.963]
 [33.177]
 [32.513]] [[0.913]
 [0.861]
 [1.272]
 [1.603]
 [1.302]]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.231]
 [0.06 ]
 [0.065]
 [0.062]] [[16.614]
 [31.687]
 [22.063]
 [14.969]
 [18.439]] [[0.084]
 [0.231]
 [0.06 ]
 [0.065]
 [0.062]]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06747154709649522, 0.09554563754088338, 0.12923454607414914, 0.15169381842965965, 0.47173844949568466, 0.08431600136312811]
maxi score, test score, baseline:  0.0621 0.5 0.5
probs:  [0.06747154709649522, 0.09554563754088338, 0.12923454607414914, 0.15169381842965965, 0.47173844949568466, 0.08431600136312811]
maxi score, test score, baseline:  0.0621 0.5 0.5
maxi score, test score, baseline:  0.0621 0.5 0.5
probs:  [0.06747154709649522, 0.09554563754088338, 0.12923454607414914, 0.15169381842965965, 0.47173844949568466, 0.08431600136312811]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0621 0.5 0.5
probs:  [0.06764813098904822, 0.09541033725380106, 0.12872498477150446, 0.1509347497833067, 0.47297634245443965, 0.08430545474789992]
using another actor
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.902]
 [0.902]
 [1.   ]
 [0.902]] [[29.623]
 [29.623]
 [29.623]
 [25.383]
 [29.623]] [[0.902]
 [0.902]
 [0.902]
 [1.   ]
 [0.902]]
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.60892182505954
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06764813098904822, 0.09541033725380106, 0.12872498477150446, 0.1509347497833067, 0.47297634245443965, 0.08430545474789992]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9313734
maxi score, test score, baseline:  0.0641 0.5 0.5
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06727503182004785, 0.10040557676892078, 0.1280143642263149, 0.1501013941922302, 0.47036332869800207, 0.08384030429448432]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06727503182004785, 0.10040557676892078, 0.1280143642263149, 0.1501013941922302, 0.47036332869800207, 0.08384030429448432]
Printing some Q and Qe and total Qs values:  [[0.479]
 [0.58 ]
 [0.479]
 [0.479]
 [0.479]] [[40.48 ]
 [59.514]
 [40.48 ]
 [40.48 ]
 [40.48 ]] [[1.027]
 [1.899]
 [1.027]
 [1.027]
 [1.027]]
actions average: 
K:  3  action  0 :  tensor([    0.8684,     0.0001,     0.0004,     0.0447,     0.0865],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9923,     0.0013,     0.0003,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9854,     0.0017,     0.0127],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0001,     0.0022,     0.8830,     0.1144],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0700, 0.0089, 0.0661, 0.1701, 0.6850], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.0711905197924
printing an ep nov before normalisation:  0.00029164510806367616
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06745086833666697, 0.10021938888602465, 0.12752648934382274, 0.1493721697100612, 0.47159595511207847, 0.08383512861134582]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06745086833666697, 0.10021938888602465, 0.12752648934382274, 0.1493721697100612, 0.47159595511207847, 0.08383512861134582]
printing an ep nov before normalisation:  34.550988387376584
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06745086833666697, 0.10021938888602465, 0.12752648934382274, 0.1493721697100612, 0.47159595511207847, 0.08383512861134582]
printing an ep nov before normalisation:  32.3805434251037
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06745086833666697, 0.10021938888602465, 0.12752648934382274, 0.1493721697100612, 0.47159595511207847, 0.08383512861134582]
maxi score, test score, baseline:  0.0641 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.418]
 [0.775]
 [0.463]
 [0.467]] [[25.647]
 [18.176]
 [19.34 ]
 [25.706]
 [20.816]] [[2.351]
 [1.466]
 [1.971]
 [2.463]
 [1.849]]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06745086833666697, 0.10021938888602465, 0.12752648934382274, 0.1493721697100612, 0.47159595511207847, 0.08383512861134582]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
maxi score, test score, baseline:  0.0641 0.5 0.5
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
maxi score, test score, baseline:  0.0641 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
maxi score, test score, baseline:  0.0661 0.5 0.5
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06708492446851595, 0.09967531282427436, 0.12683396978740638, 0.14856089535791198, 0.4690330475228699, 0.08881185003902156]
printing an ep nov before normalisation:  43.290030463184785
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.33 ]
 [1.068]
 [0.671]
 [0.671]] [[30.626]
 [51.927]
 [39.988]
 [30.626]
 [30.626]] [[1.088]
 [1.538]
 [1.833]
 [1.088]
 [1.088]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06725996882605956, 0.09949997893652673, 0.12636665402858271, 0.14785999410222747, 0.4702600952068992, 0.08875330889970434]
printing an ep nov before normalisation:  46.21063232421875
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0661 0.5 0.5
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0669009166477112, 0.09896847915380402, 0.13103604165989688, 0.1470698229129433, 0.46774544797387174, 0.08827929165177308]
from probs:  [0.0669009166477112, 0.09896847915380402, 0.13103604165989688, 0.1470698229129433, 0.46774544797387174, 0.08827929165177308]
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0669009166477112, 0.09896847915380402, 0.13103604165989688, 0.1470698229129433, 0.46774544797387174, 0.08827929165177308]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0669009166477112, 0.09896847915380402, 0.13103604165989688, 0.1470698229129433, 0.46774544797387174, 0.08827929165177308]
siam score:  -0.908271
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0669009166477112, 0.09896847915380402, 0.13103604165989688, 0.1470698229129433, 0.46774544797387174, 0.08827929165177308]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06654568507985437, 0.10375879328026248, 0.13033958485198252, 0.14628805979501458, 0.4652575586556556, 0.08781031833723042]
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06654568507985437, 0.10375879328026248, 0.13033958485198252, 0.14628805979501458, 0.4652575586556556, 0.08781031833723042]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06619421346356257, 0.10321038043312725, 0.12965049969710202, 0.1508025951082818, 0.46279600242318414, 0.08734630887474239]
siam score:  -0.9063832
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  13.330763036441589
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06584644241723904, 0.10266774170833436, 0.1289686697734024, 0.15526959783847047, 0.4603603633932602, 0.0868871848692935]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0660289148224278, 0.10246672152465222, 0.12849372631195535, 0.1545207310992585, 0.4616393875894357, 0.08685051865227032]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.0660289148224278, 0.10246672152465222, 0.12849372631195535, 0.1545207310992585, 0.4616393875894357, 0.08685051865227032]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.58866357803345
maxi score, test score, baseline:  0.0661 0.5 0.5
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06620762551781913, 0.10226984541740541, 0.1280285739171099, 0.1537873024168144, 0.4628920444132684, 0.08681460831758273]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06638268963938018, 0.10207698654739743, 0.12757291291026687, 0.1530688392731363, 0.4641191409001436, 0.08677943072967575]
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06638268963938018, 0.10207698654739743, 0.12757291291026687, 0.1530688392731363, 0.4641191409001436, 0.08677943072967575]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06604631769432681, 0.10155938203750559, 0.12692585656834757, 0.15736562600535794, 0.46176332037546175, 0.0863394973190004]
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06604631769432681, 0.10155938203750559, 0.12692585656834757, 0.15736562600535794, 0.46176332037546175, 0.0863394973190004]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[5.164]
 [8.236]
 [5.895]
 [6.726]
 [7.563]] [[1.7  ]
 [1.819]
 [1.728]
 [1.761]
 [1.793]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06571334425244468, 0.10104700709742237, 0.13133300382168897, 0.15657133442524446, 0.45943130166791046, 0.08590400873528907]
printing an ep nov before normalisation:  26.46860361099243
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06571334425244468, 0.10104700709742237, 0.13133300382168897, 0.15657133442524446, 0.45943130166791046, 0.08590400873528907]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9051633
printing an ep nov before normalisation:  32.839019278548015
maxi score, test score, baseline:  0.0661 0.5 0.5
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06623247741847486, 0.1005270786251745, 0.12992245108805991, 0.15441859480713105, 0.46307000566742784, 0.0858293923937318]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.016]
 [0.017]
 [0.017]] [[28.353]
 [28.353]
 [15.479]
 [28.353]
 [28.353]] [[1.014]
 [1.014]
 [0.149]
 [1.014]
 [1.014]]
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
maxi score, test score, baseline:  0.0661 0.5 0.5
maxi score, test score, baseline:  0.0661 0.5 0.5
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.375]
 [0.375]
 [0.435]
 [0.375]
 [0.375]] [[40.845]
 [40.845]
 [46.363]
 [40.845]
 [40.845]] [[1.209]
 [1.209]
 [1.484]
 [1.209]
 [1.209]]
maxi score, test score, baseline:  0.0661 0.5 0.5
probs:  [0.06590996433082727, 0.10003723447683739, 0.13416450462284749, 0.153665801849139, 0.46081123316323, 0.08541126155711877]
printing an ep nov before normalisation:  32.02728748321533
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06590996433082727, 0.10003723447683739, 0.13416450462284749, 0.153665801849139, 0.46081123316323, 0.08541126155711877]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06590996433082727, 0.10003723447683739, 0.13416450462284749, 0.153665801849139, 0.46081123316323, 0.08541126155711877]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.06590996433082727, 0.10003723447683739, 0.13416450462284749, 0.153665801849139, 0.46081123316323, 0.08541126155711877]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.0655905831892716, 0.09955214723767633, 0.13351371128608108, 0.15292031931374092, 0.4585743957493835, 0.08984884322384641]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.0655905831892716, 0.09955214723767633, 0.13351371128608108, 0.15292031931374092, 0.4585743957493835, 0.08984884322384641]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06527428859244977, 0.09907174795052208, 0.13769741578831898, 0.15218204122749285, 0.45635917545014365, 0.08941533099107285]
Printing some Q and Qe and total Qs values:  [[1.347]
 [1.415]
 [1.347]
 [1.347]
 [1.347]] [[31.307]
 [35.926]
 [31.307]
 [31.307]
 [31.307]] [[2.619]
 [2.99 ]
 [2.619]
 [2.619]
 [2.619]]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06527428859244977, 0.09907174795052208, 0.13769741578831898, 0.15218204122749285, 0.45635917545014365, 0.08941533099107285]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06544739364994594, 0.0989214839389402, 0.13717758712636216, 0.1515236258216454, 0.4575724513210214, 0.08935745814208468]
actions average: 
K:  2  action  0 :  tensor([    0.7798,     0.0004,     0.0003,     0.0671,     0.1524],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9991,     0.0006,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0005,     0.9638,     0.0001,     0.0355],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0468,     0.0063,     0.0002,     0.8535,     0.0931],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0693, 0.0118, 0.0900, 0.0973, 0.7316], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06544739364994594, 0.0989214839389402, 0.13717758712636216, 0.1515236258216454, 0.4575724513210214, 0.08935745814208468]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06544739364994594, 0.0989214839389402, 0.13717758712636216, 0.1515236258216454, 0.4575724513210214, 0.08935745814208468]
printing an ep nov before normalisation:  34.48020935058594
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06544739364994594, 0.0989214839389402, 0.13717758712636216, 0.1515236258216454, 0.4575724513210214, 0.08935745814208468]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.0650021972037954, 0.09784764118410767, 0.14007749773022343, 0.14946191029602693, 0.4544553186846407, 0.09315543490120591]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.06517300222660055, 0.0977129557112019, 0.13955003876283217, 0.14884716832986117, 0.4556524440418167, 0.09306439092768741]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06517300222660055, 0.0977129557112019, 0.13955003876283217, 0.14884716832986117, 0.4556524440418167, 0.09306439092768741]
printing an ep nov before normalisation:  28.332593087696644
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.385]
 [0.431]
 [0.431]
 [0.431]] [[48.259]
 [67.72 ]
 [48.259]
 [48.259]
 [48.259]] [[1.124]
 [1.528]
 [1.124]
 [1.124]
 [1.124]]
from probs:  [0.06624631664532017, 0.09493784522284773, 0.13797513808913908, 0.14753898094831494, 0.4631457953011182, 0.0901559237932598]
actions average: 
K:  0  action  0 :  tensor([    0.8513,     0.0025,     0.0002,     0.0518,     0.0942],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0000,     0.9861,     0.0013,     0.0122],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0198,     0.0001,     0.0025,     0.8474,     0.1302],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0840, 0.0029, 0.1272, 0.2127, 0.5731], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06624631664532017, 0.09493784522284773, 0.13797513808913908, 0.14753898094831494, 0.4631457953011182, 0.0901559237932598]
printing an ep nov before normalisation:  55.691749801970026
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06624631664532017, 0.09493784522284773, 0.13797513808913908, 0.14753898094831494, 0.4631457953011182, 0.0901559237932598]
maxi score, test score, baseline:  0.06810000000000001 0.5 0.5
probs:  [0.06624631664532017, 0.09493784522284773, 0.13797513808913908, 0.14753898094831494, 0.4631457953011182, 0.0901559237932598]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06593142312660367, 0.09448629531118846, 0.13731860358806564, 0.15159603968035804, 0.46094048834669327, 0.08972714994709101]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.167]
 [0.661]
 [1.072]
 [1.072]] [[56.997]
 [50.981]
 [54.993]
 [56.997]
 [56.997]] [[1.862]
 [1.83 ]
 [1.409]
 [1.862]
 [1.862]]
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.106166438792464
siam score:  -0.9201076
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.502]
 [1.016]
 [0.502]
 [0.502]] [[54.296]
 [54.296]
 [49.447]
 [54.296]
 [54.296]] [[1.783]
 [1.783]
 [2.123]
 [1.783]
 [1.783]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06531055011260115, 0.09359597798815432, 0.14073835778074292, 0.15488107171851953, 0.45659230239108667, 0.08888174000889544]
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06531055011260115, 0.09359597798815432, 0.14073835778074292, 0.15488107171851953, 0.45659230239108667, 0.08888174000889544]
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06531055011260115, 0.09359597798815432, 0.14073835778074292, 0.15488107171851953, 0.45659230239108667, 0.08888174000889544]
printing an ep nov before normalisation:  41.40344020254052
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06531055011260115, 0.09359597798815432, 0.14073835778074292, 0.15488107171851953, 0.45659230239108667, 0.08888174000889544]
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06531055011260115, 0.09359597798815432, 0.14073835778074292, 0.15488107171851953, 0.45659230239108667, 0.08888174000889544]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06470128470777065, 0.0973924758701648, 0.13942400736467153, 0.15810468802889677, 0.45232540849044417, 0.08805213553805219]
printing an ep nov before normalisation:  47.44852066040039
maxi score, test score, baseline:  0.07010000000000001 0.5 0.5
probs:  [0.06470128470777065, 0.0973924758701648, 0.13942400736467153, 0.15810468802889677, 0.45232540849044417, 0.08805213553805219]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.05802536010742
printing an ep nov before normalisation:  31.435627937316895
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9075206
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06440090360788653, 0.0969400100356802, 0.13877600401427206, 0.16201822289126755, 0.45022173696601153, 0.08764312248488201]
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06440090360788653, 0.0969400100356802, 0.13877600401427206, 0.16201822289126755, 0.45022173696601153, 0.08764312248488201]
maxi score, test score, baseline:  0.0721 0.5 0.5
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06440090360788653, 0.0969400100356802, 0.13877600401427206, 0.16201822289126755, 0.45022173696601153, 0.08764312248488201]
printing an ep nov before normalisation:  45.157781446539005
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06457567113545819, 0.0968149328821556, 0.13826541227076658, 0.16129345637555045, 0.45144681209582715, 0.08760371524024206]
line 256 mcts: sample exp_bonus 29.722813983246706
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06457567113545819, 0.0968149328821556, 0.13826541227076658, 0.16129345637555045, 0.45144681209582715, 0.08760371524024206]
printing an ep nov before normalisation:  44.23061246609218
maxi score, test score, baseline:  0.0721 0.5 0.5
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.392]
 [0.392]
 [0.392]
 [0.392]] [[56.697]
 [56.697]
 [56.697]
 [56.697]
 [56.697]] [[2.059]
 [2.059]
 [2.059]
 [2.059]
 [2.059]]
maxi score, test score, baseline:  0.0721 0.5 0.5
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.06457567113545819, 0.0968149328821556, 0.13826541227076658, 0.16129345637555045, 0.45144681209582715, 0.08760371524024206]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.0642799998435165, 0.09637134317614567, 0.13763164174666886, 0.165138507460351, 0.4493761198350664, 0.08720238793825162]
maxi score, test score, baseline:  0.0721 0.5 0.5
probs:  [0.0642799998435165, 0.09637134317614567, 0.13763164174666886, 0.165138507460351, 0.4493761198350664, 0.08720238793825162]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90880394
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
Printing some Q and Qe and total Qs values:  [[0.178]
 [0.178]
 [0.15 ]
 [0.178]
 [0.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.178]
 [0.178]
 [0.15 ]
 [0.178]
 [0.178]]
from probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.90584826
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.013]
 [0.178]
 [0.107]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.117]
 [0.013]
 [0.178]
 [0.107]
 [0.117]]
siam score:  -0.9061378
siam score:  -0.90537065
maxi score, test score, baseline:  0.0741 0.5 0.5
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
siam score:  -0.90532196
siam score:  -0.90595025
from probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
actions average: 
K:  4  action  0 :  tensor([    0.9111,     0.0002,     0.0002,     0.0439,     0.0446],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0052,     0.9891,     0.0001,     0.0009,     0.0046],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0008,     0.0020,     0.9579,     0.0152,     0.0241],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0226,     0.0003,     0.0029,     0.8014,     0.1728],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0519, 0.1199, 0.2319, 0.0018, 0.5945], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.06564710631859921, 0.0934082068722666, 0.13504985770276767, 0.16281095825643505, 0.4589293641622205, 0.08415450668771081]
siam score:  -0.90669113
printing an ep nov before normalisation:  55.8178918503712
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06534513919490037, 0.09297828305083662, 0.1390335228107304, 0.16206114269067728, 0.45681467715399743, 0.08376723509885788]
printing an ep nov before normalisation:  44.92649448545141
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.9327,     0.0027,     0.0003,     0.0050,     0.0593],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9953,     0.0024,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0032,     0.0001,     0.9722,     0.0030,     0.0215],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0009,     0.0005,     0.0006,     0.9145,     0.0834],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0371, 0.0017, 0.2245, 0.0925, 0.6442], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
printing an ep nov before normalisation:  44.45693390006651
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
from probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
maxi score, test score, baseline:  0.0741 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0761 0.5 0.5
maxi score, test score, baseline:  0.0761 0.5 0.5
maxi score, test score, baseline:  0.0761 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
maxi score, test score, baseline:  0.0761 0.5 0.5
probs:  [0.06550968636650942, 0.0928905231394843, 0.13852525109444247, 0.16134261507192155, 0.4579683467791496, 0.08376357754849267]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.92786554435202
maxi score, test score, baseline:  0.0781 0.5 0.5
probs:  [0.065671255898132, 0.0928043513284846, 0.13802617704573894, 0.1606370899043661, 0.45910113963824467, 0.08375998618503372]
Printing some Q and Qe and total Qs values:  [[1.   ]
 [0.946]
 [0.946]
 [1.   ]
 [0.946]] [[37.027]
 [41.079]
 [41.079]
 [35.682]
 [41.079]] [[1.   ]
 [0.946]
 [0.946]
 [1.   ]
 [0.946]]
printing an ep nov before normalisation:  53.81508075215628
printing an ep nov before normalisation:  40.792579190388224
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.425]
 [0.425]
 [0.418]
 [0.425]
 [0.425]] [[46.135]
 [46.135]
 [50.312]
 [46.135]
 [46.135]] [[0.425]
 [0.425]
 [0.418]
 [0.425]
 [0.425]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1181 1.0 1.0
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  0.003642840673592218
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.354]
 [0.799]
 [0.185]
 [0.36 ]] [[41.403]
 [38.316]
 [45.883]
 [50.585]
 [45.505]] [[1.109]
 [0.714]
 [1.323]
 [0.81 ]
 [0.875]]
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.907]
 [0.774]
 [0.809]
 [0.203]
 [0.774]] [[28.632]
 [25.425]
 [32.335]
 [25.65 ]
 [25.425]] [[1.313]
 [1.102]
 [1.305]
 [0.537]
 [1.102]]
siam score:  -0.92320734
siam score:  -0.9235951
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1181 1.0 1.0
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9252324
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.147243075900604
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
printing an ep nov before normalisation:  36.69271312530385
siam score:  -0.92892045
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.162236606421466
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.719]
 [0.996]
 [0.701]
 [0.719]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.719]
 [0.719]
 [0.996]
 [0.701]
 [0.719]]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.742839085049066
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1201 1.0 1.0
maxi score, test score, baseline:  0.1201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.325]
 [1.307]
 [1.325]
 [1.325]
 [1.325]] [[29.266]
 [37.266]
 [29.266]
 [29.266]
 [29.266]] [[2.664]
 [3.307]
 [2.664]
 [2.664]
 [2.664]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.713]
 [0.946]
 [0.727]
 [0.713]] [[30.688]
 [30.688]
 [42.142]
 [46.401]
 [30.688]] [[0.869]
 [0.869]
 [1.372]
 [1.253]
 [0.869]]
line 256 mcts: sample exp_bonus 37.63791342606525
printing an ep nov before normalisation:  36.6675076680649
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9309858
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.881]
 [0.656]
 [0.856]
 [0.856]] [[28.776]
 [43.463]
 [26.583]
 [28.776]
 [28.776]] [[0.856]
 [0.881]
 [0.656]
 [0.856]
 [0.856]]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.50096615645877
printing an ep nov before normalisation:  38.11767339706421
maxi score, test score, baseline:  0.1241 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  12.485590008300562
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.24 ]
 [0.211]
 [0.209]
 [0.226]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.24 ]
 [0.211]
 [0.209]
 [0.226]
 [0.255]]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.59526443481445
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.926849
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.30064780845158
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8871,     0.0019,     0.0003,     0.0662,     0.0445],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0026,     0.9901,     0.0001,     0.0000,     0.0072],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0022,     0.9418,     0.0005,     0.0551],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0015, 0.0090, 0.0027, 0.8980, 0.0887], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1334, 0.0864, 0.0153, 0.1539, 0.6111], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.563625812530518
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.300159490587916
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.278]
 [0.151]
 [0.204]
 [0.21 ]] [[20.736]
 [35.984]
 [36.141]
 [22.183]
 [22.46 ]] [[0.217]
 [0.278]
 [0.151]
 [0.204]
 [0.21 ]]
maxi score, test score, baseline:  0.1261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.352]
 [1.433]
 [1.316]
 [1.316]
 [1.423]] [[39.302]
 [38.933]
 [33.157]
 [33.157]
 [29.318]] [[2.169]
 [3.232]
 [2.849]
 [2.849]
 [2.778]]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.57963475616368
maxi score, test score, baseline:  0.1281 1.0 1.0
printing an ep nov before normalisation:  40.49284367659828
printing an ep nov before normalisation:  47.43225513640304
maxi score, test score, baseline:  0.1281 1.0 1.0
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.402]
 [1.383]
 [1.253]
 [1.253]] [[41.497]
 [37.933]
 [43.185]
 [41.497]
 [41.497]] [[2.054]
 [2.134]
 [2.217]
 [2.054]
 [2.054]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.792 0.083 0.042 0.042]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1281 1.0 1.0
siam score:  -0.91540146
maxi score, test score, baseline:  0.1281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.055329460176193
siam score:  -0.9166437
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([0.9330, 0.0009, 0.0014, 0.0266, 0.0380], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9989,     0.0001,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0001,     0.9759,     0.0005,     0.0232],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0218,     0.0000,     0.0052,     0.8726,     0.1004],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0638, 0.0081, 0.0049, 0.0886, 0.8345], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.64]
 [0.73]
 [0.64]
 [0.64]
 [0.64]] [[ 0.  ]
 [50.35]
 [ 0.  ]
 [ 0.  ]
 [ 0.  ]] [[0.149]
 [1.568]
 [0.149]
 [0.149]
 [0.149]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[42.106]
 [42.106]
 [42.106]
 [42.106]
 [42.106]] [[1.451]
 [1.451]
 [1.451]
 [1.451]
 [1.451]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.49677203177086
printing an ep nov before normalisation:  46.27702907521875
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 47.99869030204316
actions average: 
K:  2  action  0 :  tensor([    0.9977,     0.0000,     0.0000,     0.0001,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9987,     0.0005,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0002,     0.9567,     0.0004,     0.0421],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0017,     0.0001,     0.0020,     0.8162,     0.1801],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0713,     0.0006,     0.0011,     0.0330,     0.8940],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.016]
 [0.029]
 [0.025]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.047]
 [0.016]
 [0.029]
 [0.025]
 [0.026]]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.42834817594855
printing an ep nov before normalisation:  53.68932432809503
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.79579960583576
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  40.840142606325315
siam score:  -0.92895555
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  71.27823185599382
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.749673550634625
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.06932793646638
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.456]
 [1.481]
 [1.456]
 [1.456]
 [1.456]] [[35.69 ]
 [37.762]
 [35.69 ]
 [35.69 ]
 [35.69 ]] [[2.831]
 [2.976]
 [2.831]
 [2.831]
 [2.831]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  49.39832678344351
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1321 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.5  ]
 [0.5  ]
 [0.501]
 [0.5  ]
 [0.5  ]] [[49.733]
 [49.733]
 [50.839]
 [49.733]
 [49.733]] [[2.099]
 [2.099]
 [2.168]
 [2.099]
 [2.099]]
printing an ep nov before normalisation:  29.464166164398193
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.863174162684057
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  32.769294480340996
maxi score, test score, baseline:  0.1321 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.92597044
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.52911892309488
maxi score, test score, baseline:  0.1321 1.0 1.0
printing an ep nov before normalisation:  40.23544248220815
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.8909,     0.0006,     0.0001,     0.0480,     0.0604],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9859,     0.0005,     0.0135],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0542,     0.0013,     0.0005,     0.8140,     0.1300],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2280, 0.0016, 0.0022, 0.1446, 0.6236], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.272406578063965
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.16453453029905
siam score:  -0.92659086
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1321 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.5446157003539964
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1341 1.0 1.0
maxi score, test score, baseline:  0.1341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.411]
 [0.411]
 [0.411]
 [0.434]
 [0.497]] [[19.297]
 [19.297]
 [19.297]
 [27.255]
 [24.09 ]] [[0.538]
 [0.538]
 [0.538]
 [0.688]
 [0.701]]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.464]
 [0.464]
 [0.791]
 [0.464]] [[23.208]
 [23.208]
 [23.208]
 [31.955]
 [23.208]] [[0.579]
 [0.579]
 [0.579]
 [0.99 ]
 [0.579]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.92180526
printing an ep nov before normalisation:  23.299477118310033
printing an ep nov before normalisation:  30.738391876220703
printing an ep nov before normalisation:  21.79142777743359
printing an ep nov before normalisation:  15.984594821929932
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.276]
 [0.246]
 [0.277]
 [0.28 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.292]
 [0.276]
 [0.246]
 [0.277]
 [0.28 ]]
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  49.5583740990024
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  43.57525823180659
actions average: 
K:  0  action  0 :  tensor([0.7366, 0.0028, 0.0013, 0.0896, 0.1697], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9994,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0005,     0.9634,     0.0003,     0.0354],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0726,     0.0006,     0.0032,     0.7775,     0.1460],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1141, 0.0029, 0.0591, 0.0781, 0.7458], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.166390313042534
siam score:  -0.9076393
maxi score, test score, baseline:  0.1361 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.394]
 [1.067]
 [1.067]
 [1.067]] [[33.15 ]
 [27.935]
 [33.15 ]
 [33.15 ]
 [33.15 ]] [[2.358]
 [2.298]
 [2.358]
 [2.358]
 [2.358]]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.482300550006485
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90511435
line 256 mcts: sample exp_bonus 57.14048411162013
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.57179185322503
maxi score, test score, baseline:  0.1361 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.90104765
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1381 1.0 1.0
printing an ep nov before normalisation:  54.67811799826469
printing an ep nov before normalisation:  16.01006277031388
printing an ep nov before normalisation:  34.109731237314705
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.62577533721924
siam score:  -0.9007938
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1381 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9052141
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1381 1.0 1.0
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([    0.8833,     0.0007,     0.0013,     0.0560,     0.0588],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9956,     0.0000,     0.0001,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0008,     0.9449,     0.0006,     0.0533],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0598,     0.0000,     0.0005,     0.8428,     0.0968],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1817, 0.0190, 0.2204, 0.0019, 0.5770], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  70.90763857392129
maxi score, test score, baseline:  0.1381 1.0 1.0
printing an ep nov before normalisation:  57.51771621662344
maxi score, test score, baseline:  0.1381 1.0 1.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.91647422692523
maxi score, test score, baseline:  0.1381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.695]
 [0.728]
 [0.695]
 [0.695]] [[44.094]
 [44.094]
 [45.703]
 [44.094]
 [44.094]] [[1.849]
 [1.849]
 [1.967]
 [1.849]
 [1.849]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.67400870534641
siam score:  -0.9157207
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  32.482337951660156
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.895]
 [0.938]
 [0.759]] [[51.55 ]
 [51.55 ]
 [52.211]
 [52.235]
 [51.55 ]] [[1.707]
 [1.707]
 [1.866]
 [1.91 ]
 [1.707]]
printing an ep nov before normalisation:  32.79955401952174
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.91992974
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9180,     0.0007,     0.0012,     0.0018,     0.0782],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9972,     0.0003,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0007,     0.9693,     0.0006,     0.0290],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0152,     0.0000,     0.0013,     0.9202,     0.0633],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1491, 0.0042, 0.0552, 0.1510, 0.6404], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  38.435868594926674
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.198]
 [1.139]
 [0.952]
 [1.285]] [[26.586]
 [38.476]
 [30.679]
 [27.769]
 [31.094]] [[1.744]
 [2.505]
 [2.011]
 [1.662]
 [2.18 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.737667248887526
printing an ep nov before normalisation:  33.63756926714876
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  3  action  0 :  tensor([0.8439, 0.0010, 0.0012, 0.0513, 0.1025], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0052,     0.9880,     0.0003,     0.0009,     0.0056],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0014,     0.9629,     0.0008,     0.0344],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0030,     0.0000,     0.0005,     0.9667,     0.0297],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0397, 0.0021, 0.0090, 0.1104, 0.8388], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.797351584522005
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.273]
 [1.273]
 [1.274]
 [1.273]
 [1.273]] [[23.352]
 [23.352]
 [35.52 ]
 [23.352]
 [23.352]] [[1.994]
 [1.994]
 [2.71 ]
 [1.994]
 [1.994]]
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]] [[42.723]
 [42.723]
 [42.723]
 [42.723]
 [42.723]] [[0.552]
 [0.552]
 [0.552]
 [0.552]
 [0.552]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.764]
 [0.764]
 [0.814]
 [0.677]
 [0.764]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.764]
 [0.764]
 [0.814]
 [0.677]
 [0.764]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.99675787534375
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.052194059900216
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.62 ]
 [0.455]
 [0.455]
 [0.451]] [[43.263]
 [52.213]
 [43.263]
 [43.263]
 [46.235]] [[1.113]
 [1.459]
 [1.113]
 [1.113]
 [1.169]]
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.631]
 [0.01 ]
 [0.299]
 [0.367]] [[21.321]
 [35.23 ]
 [26.419]
 [20.115]
 [22.718]] [[0.645]
 [1.343]
 [0.44 ]
 [0.527]
 [0.678]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9164961
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
printing an ep nov before normalisation:  26.895833015441895
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.775177100537014
printing an ep nov before normalisation:  33.75550453099064
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.24769625987734
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
printing an ep nov before normalisation:  37.30331604003534
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]] [[50.635]
 [50.635]
 [50.635]
 [50.635]
 [50.635]] [[0.541]
 [0.541]
 [0.541]
 [0.541]
 [0.541]]
printing an ep nov before normalisation:  31.600602074537015
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.11504955081759
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.946]
 [1.089]
 [1.009]
 [0.772]] [[40.018]
 [40.018]
 [47.935]
 [51.635]
 [38.161]] [[1.257]
 [1.257]
 [1.541]
 [1.526]
 [1.05 ]]
line 256 mcts: sample exp_bonus 59.444150164339064
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.927908
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.419745291881426
maxi score, test score, baseline:  0.1561 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9282979
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.826]
 [1.209]
 [0.826]
 [0.741]] [[28.92 ]
 [28.92 ]
 [32.313]
 [28.92 ]
 [30.233]] [[1.191]
 [1.191]
 [1.656]
 [1.191]
 [1.138]]
printing an ep nov before normalisation:  34.91241693496704
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9249829
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.39723270518336
line 256 mcts: sample exp_bonus 46.116472072730666
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7842,     0.0026,     0.0008,     0.0672,     0.1453],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9975,     0.0001,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0030,     0.9686,     0.0003,     0.0280],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0013,     0.0027,     0.9712,     0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0422, 0.0059, 0.1366, 0.0594, 0.7559], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.027244197641544
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.683737803665785
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.5461721598531
siam score:  -0.91943634
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  21.952569484710693
printing an ep nov before normalisation:  21.830120086669922
printing an ep nov before normalisation:  27.985105514526367
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.8604040145874
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]] [[45.817]
 [45.817]
 [45.817]
 [45.817]
 [45.817]] [[2.007]
 [2.007]
 [2.007]
 [2.007]
 [2.007]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.991]
 [0.991]
 [1.338]
 [0.991]
 [0.991]] [[24.59 ]
 [24.59 ]
 [34.605]
 [24.59 ]
 [24.59 ]] [[1.586]
 [1.586]
 [2.564]
 [1.586]
 [1.586]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1681 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.281]
 [1.232]
 [1.376]
 [1.281]
 [1.281]] [[37.201]
 [47.079]
 [40.251]
 [37.201]
 [37.201]] [[1.494]
 [1.554]
 [1.624]
 [1.494]
 [1.494]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.237]
 [0.237]
 [0.223]
 [0.237]
 [0.237]] [[17.224]
 [17.224]
 [25.766]
 [17.224]
 [17.224]] [[0.237]
 [0.237]
 [0.223]
 [0.237]
 [0.237]]
maxi score, test score, baseline:  0.1681 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.423698996292714
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.924335
printing an ep nov before normalisation:  13.026660031885415
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.61434165832376
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.98 ]
 [1.266]
 [0.847]
 [1.045]] [[37.462]
 [46.64 ]
 [41.52 ]
 [37.462]
 [45.265]] [[1.521]
 [2.125]
 [2.148]
 [1.521]
 [2.12 ]]
Printing some Q and Qe and total Qs values:  [[0.64 ]
 [0.851]
 [1.138]
 [0.831]
 [1.007]] [[34.856]
 [38.183]
 [33.159]
 [41.587]
 [36.191]] [[1.548]
 [1.93 ]
 [1.958]
 [2.084]
 [1.983]]
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.8200, 0.0031, 0.0012, 0.0373, 0.1383], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9990,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0006,     0.9600,     0.0004,     0.0388],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0027,     0.0002,     0.0032,     0.9045,     0.0894],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0433, 0.0022, 0.0618, 0.0650, 0.8276], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.821191445839844
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.123]
 [0.652]
 [0.592]] [[33.1  ]
 [33.1  ]
 [34.507]
 [37.688]
 [33.1  ]] [[0.592]
 [0.592]
 [0.123]
 [0.652]
 [0.592]]
maxi score, test score, baseline:  0.1681 1.0 1.0
printing an ep nov before normalisation:  39.031033515930176
Printing some Q and Qe and total Qs values:  [[1.466]
 [1.492]
 [1.466]
 [1.466]
 [1.466]] [[52.294]
 [55.905]
 [52.294]
 [52.294]
 [52.294]] [[2.055]
 [2.147]
 [2.055]
 [2.055]
 [2.055]]
printing an ep nov before normalisation:  42.837717266035924
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9269159
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.32012414932251
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.49 ]
 [0.561]
 [0.52 ]
 [0.368]
 [0.51 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.49 ]
 [0.561]
 [0.52 ]
 [0.368]
 [0.51 ]]
actions average: 
K:  2  action  0 :  tensor([    0.9617,     0.0002,     0.0007,     0.0012,     0.0362],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0028,     0.9925,     0.0002,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0004,     0.9678,     0.0010,     0.0305],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0028,     0.0001,     0.0064,     0.9339,     0.0568],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1097, 0.0062, 0.1034, 0.1407, 0.6400], grad_fn=<DivBackward0>)
siam score:  -0.92299026
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.25  0.292 0.042 0.125 0.292]
printing an ep nov before normalisation:  50.902843385445316
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([    0.9723,     0.0007,     0.0009,     0.0045,     0.0216],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9963,     0.0002,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9836,     0.0033,     0.0129],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0235,     0.0003,     0.0067,     0.8514,     0.1180],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1170, 0.0033, 0.0051, 0.1391, 0.7355], grad_fn=<DivBackward0>)
siam score:  -0.92160916
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.54553156425045
printing an ep nov before normalisation:  49.440898039374126
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.349]
 [0.992]
 [1.312]
 [1.312]] [[41.522]
 [47.755]
 [46.206]
 [41.522]
 [41.522]] [[2.291]
 [2.641]
 [2.206]
 [2.291]
 [2.291]]
printing an ep nov before normalisation:  43.32376003264793
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.75359764426418
printing an ep nov before normalisation:  22.686009674537104
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.67934691518014
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.97439048875785
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9217989
printing an ep nov before normalisation:  50.648265339724446
printing an ep nov before normalisation:  51.0441620089719
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.482265412564995
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.109298128773844
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9196797
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.327206988021324
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  43.99983195712219
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.024288419309073106
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.9223731713017
siam score:  -0.90396154
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 30.44394493103027
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9862,     0.0001,     0.0009,     0.0066,     0.0062],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9742,     0.0137,     0.0000,     0.0114],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0011,     0.0001,     0.9574,     0.0049,     0.0366],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0253,     0.0004,     0.0006,     0.8017,     0.1721],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2043, 0.0034, 0.0033, 0.1012, 0.6878], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.275]
 [0.262]
 [0.847]
 [0.157]
 [0.315]] [[36.506]
 [45.171]
 [38.898]
 [38.214]
 [42.839]] [[0.427]
 [0.492]
 [1.021]
 [0.325]
 [0.525]]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7980,     0.0005,     0.0005,     0.0627,     0.1383],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9881,     0.0001,     0.0001,     0.0110],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0006,     0.9572,     0.0014,     0.0403],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0377,     0.0001,     0.0017,     0.8596,     0.1010],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0286, 0.0011, 0.0393, 0.2258, 0.7052], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.164]
 [0.165]
 [0.159]
 [0.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.157]
 [0.164]
 [0.165]
 [0.159]
 [0.167]]
maxi score, test score, baseline:  0.2181 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.438]
 [0.918]
 [0.438]
 [0.438]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.438]
 [0.438]
 [0.918]
 [0.438]
 [0.438]]
siam score:  -0.88350004
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.028534122517726
printing an ep nov before normalisation:  0.00018743998680292862
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.45 ]
 [0.45 ]
 [0.661]
 [0.45 ]
 [0.45 ]] [[28.113]
 [28.113]
 [35.513]
 [28.113]
 [28.113]] [[0.738]
 [0.738]
 [1.094]
 [0.738]
 [0.738]]
actions average: 
K:  3  action  0 :  tensor([0.7857, 0.0201, 0.0010, 0.0882, 0.1050], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9875,     0.0075,     0.0002,     0.0042],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0001,     0.9811,     0.0013,     0.0168],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0021,     0.0001,     0.0387,     0.8280,     0.1311],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0618, 0.0531, 0.1628, 0.0917, 0.6306], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  57.75476205088761
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2181 1.0 1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7844,     0.0002,     0.0011,     0.0571,     0.1571],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9994,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9275,     0.0014,     0.0709],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0519,     0.0006,     0.0015,     0.8126,     0.1333],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1239, 0.0018, 0.0806, 0.1048, 0.6889], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.552]
 [0.921]
 [0.851]
 [0.552]] [[43.183]
 [43.183]
 [38.271]
 [38.278]
 [43.183]] [[1.251]
 [1.251]
 [1.481]
 [1.411]
 [1.251]]
printing an ep nov before normalisation:  38.01656257377397
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2181 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.394]
 [0.127]
 [0.565]
 [0.347]] [[31.026]
 [29.669]
 [30.927]
 [26.902]
 [25.489]] [[0.256]
 [0.394]
 [0.127]
 [0.565]
 [0.347]]
printing an ep nov before normalisation:  25.581343173980713
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.047]
 [1.378]
 [1.334]
 [1.147]
 [1.247]] [[38.769]
 [41.748]
 [35.653]
 [34.871]
 [36.053]] [[1.92 ]
 [2.393]
 [2.059]
 [1.834]
 [1.991]]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8758818
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
line 256 mcts: sample exp_bonus 36.29466189015047
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8749934
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.418501975469166
printing an ep nov before normalisation:  56.976694784914876
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.583341385116896
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.075288787500416
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.8802,     0.0006,     0.0018,     0.0253,     0.0921],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9981,     0.0004,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9879,     0.0002,     0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0323,     0.0008,     0.0073,     0.8364,     0.1233],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1105, 0.0064, 0.1662, 0.1386, 0.5783], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 42.12015462607593
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.214528145581966
line 256 mcts: sample exp_bonus 33.56517098354988
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.45968641959999
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2221 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2241 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
printing an ep nov before normalisation:  28.656448192299376
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.836352348327637
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.88626933
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.88731825
maxi score, test score, baseline:  0.2261 1.0 1.0
printing an ep nov before normalisation:  0.0063665997282669196
printing an ep nov before normalisation:  52.23757618031746
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.695200443267822
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.688158821853296
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.58764269894785
actions average: 
K:  0  action  0 :  tensor([    0.8546,     0.0006,     0.0003,     0.0321,     0.1124],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0074,     0.9861,     0.0003,     0.0007,     0.0056],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9915,     0.0009,     0.0075],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0743,     0.0011,     0.0005,     0.7689,     0.1553],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2591, 0.0015, 0.1029, 0.1627, 0.4737], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.009675248207940967
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.582]
 [0.664]
 [0.582]
 [0.582]] [[27.91 ]
 [27.91 ]
 [29.128]
 [27.91 ]
 [27.91 ]] [[1.489]
 [1.489]
 [1.647]
 [1.489]
 [1.489]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.165]
 [0.271]
 [0.158]
 [0.179]
 [0.181]] [[27.407]
 [42.041]
 [39.66 ]
 [33.12 ]
 [30.952]] [[0.165]
 [0.271]
 [0.158]
 [0.179]
 [0.181]]
printing an ep nov before normalisation:  56.4889889559366
siam score:  -0.8889215
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.25488175603312
maxi score, test score, baseline:  0.2281 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.165]
 [1.165]
 [1.165]
 [1.165]
 [1.165]] [[52.985]
 [52.985]
 [52.985]
 [52.985]
 [52.985]] [[2.498]
 [2.498]
 [2.498]
 [2.498]
 [2.498]]
UNIT TEST: sample policy line 217 mcts : [0.    0.042 0.833 0.083 0.042]
printing an ep nov before normalisation:  20.521454500777054
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([    0.9564,     0.0001,     0.0027,     0.0022,     0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9977,     0.0001,     0.0002,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0001,     0.9590,     0.0005,     0.0399],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0005,     0.0065,     0.8969,     0.0950],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0336, 0.0009, 0.0878, 0.0843, 0.7935], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2281 1.0 1.0
actions average: 
K:  2  action  0 :  tensor([    0.7566,     0.0005,     0.0008,     0.0396,     0.2025],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0016,     0.9973,     0.0000,     0.0001,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0000,     0.9935,     0.0001,     0.0061],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0628,     0.0002,     0.0073,     0.8250,     0.1047],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0973, 0.0013, 0.0124, 0.1479, 0.7411], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9009595
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.8382],
        [0.9064],
        [0.5606],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.5587],
        [0.5696],
        [0.7876]], dtype=torch.float64)
0.0 0.0
0.0 0.8381509391233076
0.0 0.9063691013070048
0.0 0.5605526052316897
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.5586758600970427
0.0 0.5696450548761688
0.0 0.7875697275220462
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9000272
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.75 ]
 [0.915]
 [0.75 ]
 [0.75 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.75 ]
 [0.75 ]
 [0.915]
 [0.75 ]
 [0.75 ]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.720237614862867
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.773025391598026
printing an ep nov before normalisation:  52.25748930849649
printing an ep nov before normalisation:  58.452601006846066
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.8855465977497
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.039]
 [1.039]
 [1.103]
 [1.039]
 [1.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.039]
 [1.039]
 [1.103]
 [1.039]
 [1.039]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2321 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([    0.8423,     0.0006,     0.0015,     0.0429,     0.1126],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9969,     0.0001,     0.0000,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0000,     0.9821,     0.0049,     0.0126],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0001,     0.0005,     0.8531,     0.1458],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1014, 0.0239, 0.0721, 0.0876, 0.7151], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.70791432567842
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.830391617714696
printing an ep nov before normalisation:  45.84411757646696
printing an ep nov before normalisation:  46.711446597734806
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.004740225949185
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.91083455
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8792,     0.0001,     0.0007,     0.0318,     0.0883],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0024,     0.9959,     0.0001,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0011,     0.0234,     0.9452,     0.0004,     0.0299],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0003,     0.0021,     0.8670,     0.1302],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0437, 0.0045, 0.1986, 0.1206, 0.6325], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.2475579650490545e-05
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 31.824244909751258
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8425,     0.0222,     0.0002,     0.0242,     0.1109],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0051, 0.9375, 0.0111, 0.0244, 0.0220], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0019,     0.0068,     0.9563,     0.0005,     0.0346],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0439,     0.0000,     0.0002,     0.8341,     0.1217],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0095, 0.0117, 0.1859, 0.0260, 0.7669], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  26.209223111785835
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([0.7853, 0.0036, 0.0009, 0.0603, 0.1499], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0190,     0.9688,     0.0003,     0.0000,     0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0005,     0.9630,     0.0014,     0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0462,     0.0005,     0.0003,     0.9046,     0.0483],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0799, 0.0013, 0.0050, 0.2325, 0.6814], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.015]
 [0.253]
 [0.257]
 [0.237]] [[26.256]
 [32.586]
 [33.298]
 [26.962]
 [22.987]] [[0.289]
 [0.015]
 [0.253]
 [0.257]
 [0.237]]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.838]
 [0.746]
 [0.746]
 [0.746]] [[48.734]
 [55.276]
 [48.734]
 [48.734]
 [48.734]] [[1.941]
 [2.37 ]
 [1.941]
 [1.941]
 [1.941]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.859]
 [0.859]
 [0.988]
 [0.859]] [[32.173]
 [32.173]
 [32.173]
 [39.057]
 [32.173]] [[1.551]
 [1.551]
 [1.551]
 [1.955]
 [1.551]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.458]
 [1.485]
 [1.44 ]
 [1.422]
 [1.448]] [[49.625]
 [49.397]
 [50.02 ]
 [47.266]
 [49.699]] [[3.316]
 [3.331]
 [3.318]
 [3.159]
 [3.31 ]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.247]
 [1.388]
 [1.104]
 [1.247]
 [1.247]] [[40.945]
 [43.511]
 [43.798]
 [40.945]
 [40.945]] [[2.176]
 [2.415]
 [2.142]
 [2.176]
 [2.176]]
printing an ep nov before normalisation:  54.40964248457965
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.341498374938965
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.746]
 [0.833]
 [0.746]
 [0.746]] [[68.771]
 [68.771]
 [59.31 ]
 [68.771]
 [68.771]] [[1.746]
 [1.746]
 [1.645]
 [1.746]
 [1.746]]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([    0.8132,     0.0055,     0.0001,     0.0314,     0.1497],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9990,     0.0000,     0.0008,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9544,     0.0002,     0.0451],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0486,     0.0002,     0.0001,     0.8016,     0.1495],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1628, 0.0084, 0.0009, 0.2358, 0.5921], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([    0.9281,     0.0004,     0.0002,     0.0055,     0.0659],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9922,     0.0053,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0021,     0.0000,     0.9926,     0.0009,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0311, 0.0011, 0.0009, 0.7874, 0.1796], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0977, 0.0083, 0.0619, 0.0820, 0.7502], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.9204,     0.0020,     0.0005,     0.0250,     0.0521],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0015,     0.9849,     0.0007,     0.0038,     0.0091],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0015,     0.0001,     0.9698,     0.0006,     0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0233, 0.0018, 0.0044, 0.8284, 0.1421], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0968, 0.0053, 0.1324, 0.0538, 0.7118], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2541 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.62739153062638
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.647]
 [0.53 ]
 [0.53 ]
 [0.53 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.53 ]
 [0.647]
 [0.53 ]
 [0.53 ]
 [0.53 ]]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.406]
 [1.474]
 [1.406]
 [1.406]
 [1.406]] [[14.392]
 [20.532]
 [14.392]
 [14.392]
 [14.392]] [[2.221]
 [2.637]
 [2.221]
 [2.221]
 [2.221]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.336]
 [0.336]
 [0.336]
 [0.336]
 [0.336]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.675]
 [0.675]
 [0.927]
 [0.675]] [[52.807]
 [52.807]
 [52.807]
 [55.314]
 [52.807]] [[1.879]
 [1.879]
 [1.879]
 [2.219]
 [1.879]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.644]
 [0.766]
 [1.024]
 [0.766]
 [1.01 ]] [[27.724]
 [30.452]
 [30.441]
 [30.452]
 [31.147]] [[1.536]
 [1.844]
 [2.101]
 [1.844]
 [2.135]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.93 ]
 [0.93 ]
 [1.175]
 [0.93 ]
 [0.93 ]] [[39.391]
 [39.391]
 [39.029]
 [39.391]
 [39.391]] [[1.189]
 [1.189]
 [1.43 ]
 [1.189]
 [1.189]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.177]
 [1.177]
 [1.294]
 [1.177]
 [1.177]] [[53.726]
 [53.726]
 [42.075]
 [53.726]
 [53.726]] [[1.451]
 [1.451]
 [1.457]
 [1.451]
 [1.451]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.82 ]
 [0.98 ]
 [0.82 ]
 [0.728]] [[42.147]
 [42.147]
 [41.55 ]
 [42.147]
 [42.711]] [[1.544]
 [1.544]
 [1.689]
 [1.544]
 [1.465]]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.356146812438965
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.79313440519215
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.452432255886613
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.89728
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.973]
 [1.094]
 [1.264]
 [0.943]
 [1.094]] [[36.777]
 [33.113]
 [27.759]
 [30.711]
 [34.306]] [[1.64 ]
 [1.645]
 [1.645]
 [1.418]
 [1.683]]
printing an ep nov before normalisation:  22.112741047655213
maxi score, test score, baseline:  0.2621 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.815]
 [0.815]
 [0.815]
 [0.815]] [[33.578]
 [33.578]
 [33.578]
 [33.578]
 [33.578]] [[67.971]
 [67.971]
 [67.971]
 [67.971]
 [67.971]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89956313
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  51.765877231225886
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.83897501233518
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 56.61410804886152
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2661 1.0 1.0
printing an ep nov before normalisation:  29.689175432715242
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0001153628488737013
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.89872634
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.871]
 [0.871]
 [0.916]
 [0.871]] [[36.487]
 [36.487]
 [36.487]
 [44.373]
 [36.487]] [[1.092]
 [1.092]
 [1.092]
 [1.228]
 [1.092]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.557924358613512
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.3303496357075
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.61635798869426
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8964606
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.337950229644775
maxi score, test score, baseline:  0.2721 1.0 1.0
printing an ep nov before normalisation:  29.71440315246582
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.329777437736055
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90359193
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  39.07398462295532
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.3204148026649
printing an ep nov before normalisation:  32.293081283569336
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.195459473056864
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.31]
 [1.31]
 [1.31]
 [1.31]
 [1.31]] [[20.543]
 [20.543]
 [20.543]
 [20.543]
 [20.543]] [[15.012]
 [15.012]
 [15.012]
 [15.012]
 [15.012]]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.3504056202409
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8012,     0.0003,     0.0003,     0.0220,     0.1763],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9986,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0007,     0.9383,     0.0014,     0.0590],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0600, 0.0094, 0.0011, 0.8790, 0.0505], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0045, 0.0045, 0.1656, 0.0860, 0.7394], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.640879699566838
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8935702
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.60748007457086
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.8694,     0.0000,     0.0004,     0.0528,     0.0774],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0077,     0.9830,     0.0001,     0.0003,     0.0089],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0002,     0.9650,     0.0008,     0.0337],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0339,     0.0002,     0.0006,     0.9333,     0.0320],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0442, 0.0016, 0.0628, 0.1164, 0.7751], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2781 1.0 1.0
printing an ep nov before normalisation:  54.483345729355754
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2781 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([    0.8126,     0.0143,     0.0001,     0.0330,     0.1400],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9992,     0.0002,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0002,     0.9774,     0.0011,     0.0211],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0255,     0.0002,     0.0003,     0.8735,     0.1006],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0386, 0.0021, 0.1530, 0.0360, 0.7703], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.046720482804986
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8346,     0.0018,     0.0002,     0.0456,     0.1179],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9975,     0.0003,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9682,     0.0050,     0.0266],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0768,     0.0005,     0.0009,     0.7876,     0.1342],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1040, 0.0031, 0.0397, 0.2463, 0.6070], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.8266,     0.0021,     0.0006,     0.0231,     0.1476],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9990,     0.0003,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0000,     0.9773,     0.0005,     0.0220],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0430,     0.0003,     0.0006,     0.8115,     0.1446],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0421, 0.0010, 0.0075, 0.1317, 0.8177], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.112033367156982
printing an ep nov before normalisation:  29.900378353391332
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2801 1.0 1.0
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9578,     0.0005,     0.0002,     0.0204,     0.0210],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9959,     0.0008,     0.0001,     0.0029],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9872,     0.0010,     0.0117],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0484, 0.0010, 0.0019, 0.8001, 0.1486], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1491, 0.0010, 0.0618, 0.1742, 0.6139], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8986185
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  34.856562372177905
Printing some Q and Qe and total Qs values:  [[1.385]
 [1.375]
 [1.385]
 [1.385]
 [1.385]] [[37.089]
 [41.696]
 [37.089]
 [37.089]
 [37.089]] [[2.738]
 [3.031]
 [2.738]
 [2.738]
 [2.738]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8971782
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.084]
 [0.869]
 [0.619]
 [0.732]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.797]
 [0.084]
 [0.869]
 [0.619]
 [0.732]]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.52742744723981
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.018189591997952448
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.33 ]
 [1.408]
 [1.33 ]
 [1.33 ]
 [1.33 ]] [[22.283]
 [33.566]
 [22.283]
 [22.283]
 [22.283]] [[1.86 ]
 [2.407]
 [1.86 ]
 [1.86 ]
 [1.86 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6827,     0.0005,     0.0003,     0.1203,     0.1961],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0034,     0.9923,     0.0004,     0.0003,     0.0036],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0014,     0.0000,     0.9974,     0.0006,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0209,     0.0000,     0.0000,     0.9349,     0.0442],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1614, 0.0012, 0.0800, 0.3322, 0.4252], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.00323486328125
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.646237649202828
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.68595444122504
printing an ep nov before normalisation:  44.16557029483325
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8264,     0.0004,     0.0002,     0.0344,     0.1386],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9989,     0.0002,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0000,     0.9826,     0.0007,     0.0164],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0185,     0.0001,     0.0003,     0.8721,     0.1089],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1456, 0.0060, 0.0019, 0.1177, 0.7288], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  49.60032450159376
printing an ep nov before normalisation:  32.19540800367083
siam score:  -0.8945936
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  38.02454326038842
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.765296459197998
Printing some Q and Qe and total Qs values:  [[0.953]
 [0.985]
 [0.953]
 [0.953]
 [0.953]] [[40.604]
 [38.161]
 [40.604]
 [40.604]
 [40.604]] [[0.953]
 [0.985]
 [0.953]
 [0.953]
 [0.953]]
printing an ep nov before normalisation:  37.89660930633545
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]] [[29.123]
 [29.123]
 [29.123]
 [29.123]
 [29.123]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.822]]
printing an ep nov before normalisation:  37.21563455909169
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.81997449031112
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.93468061993143
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.78936993941217
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.41697497604675
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.8679, 0.0023, 0.0010, 0.0141, 0.1147], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9865,     0.0052,     0.0002,     0.0077],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0006,     0.9512,     0.0115,     0.0366],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0047,     0.0005,     0.0066,     0.8424,     0.1459],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0312, 0.0011, 0.1226, 0.1348, 0.7104], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3301 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8850068
Printing some Q and Qe and total Qs values:  [[1.257]
 [1.257]
 [1.257]
 [1.257]
 [1.257]] [[34.508]
 [34.508]
 [34.508]
 [34.508]
 [34.508]] [[3.257]
 [3.257]
 [3.257]
 [3.257]
 [3.257]]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.285]
 [1.326]
 [1.264]
 [1.264]] [[27.834]
 [34.035]
 [37.704]
 [27.834]
 [27.834]] [[1.999]
 [2.26 ]
 [2.444]
 [1.999]
 [1.999]]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8833623
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.568092262349296
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.104363712410006
printing an ep nov before normalisation:  46.57028462784011
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  2  action  0 :  tensor([    0.8103,     0.0041,     0.0004,     0.0713,     0.1139],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9980,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0009,     0.0005,     0.9253,     0.0009,     0.0723],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0275,     0.0001,     0.0016,     0.9045,     0.0663],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1091, 0.0010, 0.1254, 0.1411, 0.6233], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.81370191962072
maxi score, test score, baseline:  0.3321 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3321 1.0 1.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3321 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89023966
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.648]
 [0.648]
 [0.708]
 [0.648]
 [0.648]] [[52.532]
 [52.532]
 [50.848]
 [52.532]
 [52.532]] [[0.648]
 [0.648]
 [0.708]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.187055587768555
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.03707067759169
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
printing an ep nov before normalisation:  25.196748549630083
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  41.82136488587003
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.61402613024423
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.3381 1.0 1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.08692068509993
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.37106687487797
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8568,     0.0004,     0.0007,     0.0571,     0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9993,     0.0002,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9691,     0.0020,     0.0287],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0199,     0.0007,     0.0004,     0.8400,     0.1390],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1439, 0.0038, 0.0020, 0.1776, 0.6727], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3381 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.926]
 [0.807]
 [0.807]
 [0.807]] [[39.076]
 [39.604]
 [39.076]
 [39.076]
 [39.076]] [[0.807]
 [0.926]
 [0.807]
 [0.807]
 [0.807]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([    0.7776,     0.0004,     0.0016,     0.0504,     0.1700],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9977,     0.0001,     0.0005,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0001,     0.9732,     0.0017,     0.0248],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0352,     0.0000,     0.0013,     0.9217,     0.0417],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1839, 0.0009, 0.1231, 0.1375, 0.5546], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.54645773925301
Printing some Q and Qe and total Qs values:  [[0.521]
 [0.543]
 [0.573]
 [0.521]
 [0.521]] [[49.618]
 [57.705]
 [50.477]
 [49.618]
 [49.618]] [[1.126]
 [1.336]
 [1.198]
 [1.126]
 [1.126]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.7200,     0.0006,     0.0014,     0.1111,     0.1669],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9883,     0.0017,     0.0002,     0.0095],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0002,     0.9431,     0.0011,     0.0553],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0218,     0.0003,     0.0125,     0.7247,     0.2407],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0636, 0.0077, 0.1710, 0.1326, 0.6252], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.719175591836276
maxi score, test score, baseline:  0.3361 1.0 1.0
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 42.52119541168213
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.47952802774715
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.931396484375
Printing some Q and Qe and total Qs values:  [[1.35 ]
 [1.478]
 [1.35 ]
 [1.35 ]
 [1.35 ]] [[21.269]
 [26.053]
 [21.269]
 [21.269]
 [21.269]] [[1.579]
 [1.794]
 [1.579]
 [1.579]
 [1.579]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.86398506
printing an ep nov before normalisation:  35.62713861465454
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.389]
 [1.457]
 [1.389]
 [1.389]
 [1.389]] [[35.978]
 [52.385]
 [35.978]
 [35.978]
 [35.978]] [[1.814]
 [2.288]
 [1.814]
 [1.814]
 [1.814]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  80.07808241724972
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3361 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.856]
 [0.856]
 [1.11 ]
 [0.856]
 [0.856]] [[41.275]
 [41.275]
 [41.251]
 [41.275]
 [41.275]] [[1.65 ]
 [1.65 ]
 [1.904]
 [1.65 ]
 [1.65 ]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9128,     0.0003,     0.0003,     0.0001,     0.0865],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9972,     0.0002,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0006,     0.9574,     0.0003,     0.0415],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0017,     0.0001,     0.0002,     0.8852,     0.1128],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0990, 0.0070, 0.2434, 0.0814, 0.5692], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.755]
 [0.755]
 [0.811]
 [0.626]
 [0.755]] [[44.319]
 [44.319]
 [42.035]
 [40.244]
 [44.319]] [[1.406]
 [1.406]
 [1.409]
 [1.183]
 [1.406]]
siam score:  -0.86056244
maxi score, test score, baseline:  0.3361 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.42162969818478
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.85340090274447
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  64.09669209401277
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.403]
 [0.403]
 [0.403]
 [0.403]]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.39565578997258
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.74]
 [0.74]
 [0.85]
 [0.74]
 [0.74]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.74]
 [0.74]
 [0.85]
 [0.74]
 [0.74]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3421 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
line 256 mcts: sample exp_bonus 41.017339440863516
printing an ep nov before normalisation:  37.087167228819595
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.857144355773926
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.72280002194126
siam score:  -0.8757534
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  42.516587731652194
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.332]
 [1.479]
 [1.412]
 [1.261]
 [1.355]] [[42.287]
 [40.067]
 [41.947]
 [36.384]
 [44.117]] [[1.425]
 [2.461]
 [2.488]
 [2.058]
 [2.541]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.14958049993661
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.8606,     0.0003,     0.0011,     0.0037,     0.1343],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0011,     0.9837,     0.0001,     0.0002,     0.0149],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0004,     0.9487,     0.0078,     0.0424],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0011,     0.0000,     0.0015,     0.9547,     0.0426],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0669, 0.0049, 0.1100, 0.1988, 0.6193], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.44833437601726
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.883]
 [1.259]
 [0.963]
 [0.963]
 [1.046]] [[49.753]
 [40.053]
 [55.553]
 [55.553]
 [52.852]] [[1.855]
 [1.918]
 [2.123]
 [2.123]
 [2.119]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.512313108903935
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.296]
 [1.374]
 [1.296]
 [1.296]
 [1.296]] [[42.838]
 [50.948]
 [42.838]
 [42.838]
 [42.838]] [[1.97 ]
 [2.291]
 [1.97 ]
 [1.97 ]
 [1.97 ]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8779678
siam score:  -0.8773605
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.7029],
        [0.0000],
        [0.6337],
        [0.5886],
        [0.7455],
        [0.8641],
        [0.5531],
        [0.5886],
        [0.7494]], dtype=torch.float64)
0.0 0.0
0.0 0.7028507940283059
0.9801 0.9801
0.0 0.6337220901333425
0.0 0.5886348568637572
0.0 0.745464794875442
0.0 0.8640558994324297
0.0 0.5531174866214599
0.0 0.5886348568637572
0.0 0.7494143468398902
actions average: 
K:  2  action  0 :  tensor([    0.9165,     0.0112,     0.0016,     0.0004,     0.0703],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9986,     0.0001,     0.0002,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0000,     0.9709,     0.0050,     0.0238],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0087,     0.0001,     0.0009,     0.9487,     0.0417],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1031, 0.1509, 0.0018, 0.1457, 0.5985], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.393]
 [1.307]
 [1.341]
 [1.09 ]
 [1.221]] [[26.861]
 [25.073]
 [27.085]
 [32.868]
 [28.75 ]] [[0.813]
 [1.674]
 [1.768]
 [1.689]
 [1.698]]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
printing an ep nov before normalisation:  33.13513517379761
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([    0.7147,     0.0004,     0.0004,     0.1060,     0.1784],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0009,     0.9980,     0.0002,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9699,     0.0003,     0.0297],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0751,     0.0004,     0.0004,     0.7688,     0.1553],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1209, 0.0010, 0.2205, 0.0668, 0.5907], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8805408
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 25.23278844033145
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.796]
 [0.765]
 [0.852]
 [0.156]
 [0.894]] [[30.898]
 [37.55 ]
 [41.011]
 [34.112]
 [41.956]] [[1.062]
 [1.217]
 [1.402]
 [0.512]
 [1.471]]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.41473661150251
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8731447
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.564]
 [0.543]
 [0.712]
 [0.56 ]
 [0.568]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.564]
 [0.543]
 [0.712]
 [0.56 ]
 [0.568]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.750117328080556
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.661]
 [0.767]
 [0.007]
 [0.662]] [[23.1  ]
 [24.911]
 [30.194]
 [20.507]
 [23.126]] [[1.335]
 [1.39 ]
 [1.826]
 [0.462]
 [1.28 ]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.697]
 [0.622]
 [1.176]
 [0.103]
 [0.771]] [[26.058]
 [22.348]
 [31.229]
 [23.892]
 [25.472]] [[1.127]
 [0.943]
 [1.757]
 [0.47 ]
 [1.184]]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.624227924392265
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8716257
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  34.428967390221246
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  58.3786153628681
siam score:  -0.8698204
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.924521446228027
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.68085605829767
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.036]
 [1.284]
 [1.036]
 [1.036]
 [1.086]] [[57.062]
 [58.019]
 [57.062]
 [57.062]
 [68.368]] [[2.223]
 [2.501]
 [2.223]
 [2.223]
 [2.616]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  27.84251692894385
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.127]
 [1.37 ]
 [1.127]
 [1.277]] [[26.817]
 [26.817]
 [42.989]
 [26.817]
 [43.098]] [[1.542]
 [1.542]
 [2.261]
 [1.542]
 [2.171]]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8636681
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.52218983584785
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.317]
 [1.317]
 [1.317]
 [1.317]
 [1.317]] [[32.583]
 [32.583]
 [32.583]
 [32.583]
 [32.583]] [[2.3]
 [2.3]
 [2.3]
 [2.3]
 [2.3]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.43179429963213
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8126,     0.0033,     0.0001,     0.0795,     0.1046],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9986,     0.0001,     0.0009,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0016,     0.0001,     0.9853,     0.0012,     0.0118],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0011,     0.0002,     0.0005,     0.9491,     0.0492],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1440, 0.0008, 0.0770, 0.1692, 0.6090], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.603]
 [1.042]
 [0.604]
 [0.431]] [[22.325]
 [24.285]
 [23.614]
 [20.572]
 [23.063]] [[1.365]
 [1.191]
 [1.597]
 [1.012]
 [0.96 ]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.214909395718195
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.07608093449732678
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.957449708666122
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[60.674]
 [60.674]
 [60.674]
 [60.674]
 [60.674]] [[0.682]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 54.12713962811306
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.01692157539248
printing an ep nov before normalisation:  49.44418579756263
siam score:  -0.8568778
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3581 1.0 1.0
siam score:  -0.8610591
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  12.864549915726545
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.721]
 [0.721]
 [0.888]
 [0.721]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.721]
 [0.721]
 [0.721]
 [0.888]
 [0.721]]
printing an ep nov before normalisation:  47.28905301984935
printing an ep nov before normalisation:  67.07139508472012
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7962,     0.0029,     0.0004,     0.1022,     0.0983],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0022,     0.9952,     0.0002,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9809,     0.0004,     0.0184],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0016,     0.0002,     0.0038,     0.9119,     0.0825],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0872, 0.0072, 0.0600, 0.0545, 0.7911], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  95.88214764547665
Printing some Q and Qe and total Qs values:  [[0.658]
 [0.687]
 [0.774]
 [0.727]
 [0.732]] [[36.557]
 [34.573]
 [40.241]
 [40.042]
 [37.922]] [[1.331]
 [1.294]
 [1.57 ]
 [1.516]
 [1.451]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.414740914564625
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.761197566986084
maxi score, test score, baseline:  0.3661 1.0 1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.619]
 [0.61 ]
 [0.646]
 [0.666]
 [0.665]] [[31.315]
 [31.639]
 [31.784]
 [31.625]
 [33.594]] [[2.374]
 [2.399]
 [2.451]
 [2.454]
 [2.665]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  35.496981143951416
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actions average: 
K:  4  action  0 :  tensor([    0.8770,     0.0033,     0.0001,     0.0032,     0.1165],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9995,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0002,     0.9819,     0.0048,     0.0126],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0391,     0.0004,     0.0115,     0.9003,     0.0487],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0812, 0.0049, 0.1607, 0.1314, 0.6218], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.27196134839739
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.18808262573285
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4887],
        [0.4242],
        [0.0000],
        [0.4141],
        [0.0000],
        [0.0000],
        [0.3810],
        [0.4903],
        [0.3157],
        [0.3914]], dtype=torch.float64)
0.0 0.4887431689300588
0.0 0.4241975161349733
0.0 0.0
0.0 0.41411500798678214
0.0 0.0
0.9801 0.9801
0.0 0.3810054706468075
0.0 0.4903069894790887
0.0 0.315718855202995
0.0 0.39139541353508217
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8529845
printing an ep nov before normalisation:  63.72872381277539
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.18741226196289
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8471701
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3661 1.0 1.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.06335411587712
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3681 1.0 1.0
maxi score, test score, baseline:  0.3681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.7612,     0.0004,     0.0013,     0.0210,     0.2162],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9904,     0.0012,     0.0008,     0.0070],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9851,     0.0016,     0.0131],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0480,     0.0001,     0.0004,     0.8463,     0.1053],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.1252,     0.0006,     0.1817,     0.0484,     0.6441],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  42.396185687095326
siam score:  -0.84588796
maxi score, test score, baseline:  0.3681 1.0 1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  58.607540917281135
maxi score, test score, baseline:  0.3701 1.0 1.0
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.838620315835605
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3721 1.0 1.0
printing an ep nov before normalisation:  58.18857830833831
printing an ep nov before normalisation:  54.39680994138855
printing an ep nov before normalisation:  43.81772873851191
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.31154582581709
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  59.18238699746406
printing an ep nov before normalisation:  7.602566496885004
printing an ep nov before normalisation:  72.36894286223519
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.853]
 [0.853]
 [1.092]
 [0.853]
 [0.853]] [[38.546]
 [38.546]
 [56.547]
 [38.546]
 [38.546]] [[1.601]
 [1.601]
 [2.257]
 [1.601]
 [1.601]]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.0015419760984514141
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.15907928088786
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9548,     0.0008,     0.0007,     0.0001,     0.0436],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9973,     0.0010,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0000,     0.9654,     0.0011,     0.0331],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0276,     0.0002,     0.0034,     0.8720,     0.0968],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0798, 0.0030, 0.0780, 0.1557, 0.6835], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.83957565
printing an ep nov before normalisation:  30.28794417647843
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.462877726445944
actions average: 
K:  0  action  0 :  tensor([    0.8329,     0.0001,     0.0002,     0.0586,     0.1082],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9987,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9901,     0.0004,     0.0093],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0664,     0.0003,     0.0013,     0.8656,     0.0665],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0571, 0.0017, 0.2249, 0.0675, 0.6488], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.876914572388394
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  40.83327293395996
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.949665162103035
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.70085452766803
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3741 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 44.239182472229004
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.202814296947615
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.84799683
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.64931752628515
using explorer policy with actor:  1
siam score:  -0.8508688
maxi score, test score, baseline:  0.3761 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8564,     0.0016,     0.0006,     0.0278,     0.1136],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9983,     0.0002,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0000,     0.9719,     0.0023,     0.0253],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0000,     0.0006,     0.9990,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1121, 0.0024, 0.1666, 0.0310, 0.6879], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8454149
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.7818415816325341
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3821 1.0 1.0
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  57.16975159215229
printing an ep nov before normalisation:  44.26533651630759
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.8381,     0.0001,     0.0006,     0.0742,     0.0869],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9943,     0.0004,     0.0013,     0.0039],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0004,     0.9472,     0.0005,     0.0517],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0308,     0.0000,     0.0034,     0.8379,     0.1278],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0756, 0.0161, 0.1343, 0.0495, 0.7246], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.64377879011197
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.175]
 [1.236]
 [1.411]
 [1.302]
 [1.302]] [[45.602]
 [60.047]
 [54.865]
 [52.988]
 [52.988]] [[2.095]
 [2.808]
 [2.749]
 [2.556]
 [2.556]]
Printing some Q and Qe and total Qs values:  [[1.236]
 [1.236]
 [1.236]
 [1.236]
 [1.236]] [[60.047]
 [60.047]
 [60.047]
 [60.047]
 [60.047]] [[2.808]
 [2.808]
 [2.808]
 [2.808]
 [2.808]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.42637862557669
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.798]
 [1.201]
 [0.798]
 [0.798]
 [0.798]] [[70.945]
 [70.809]
 [70.945]
 [70.945]
 [70.945]] [[2.536]
 [2.935]
 [2.536]
 [2.536]
 [2.536]]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.523]
 [0.523]
 [0.523]
 [0.652]
 [0.523]] [[36.667]
 [36.667]
 [36.667]
 [40.032]
 [36.667]] [[0.523]
 [0.523]
 [0.523]
 [0.652]
 [0.523]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.20837239901801
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  70.82337755156425
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([0.7989, 0.0347, 0.0036, 0.0251, 0.1377], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9941,     0.0006,     0.0043,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9590,     0.0005,     0.0403],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0409, 0.0009, 0.0021, 0.8377, 0.1184], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0650, 0.1054, 0.0103, 0.1619, 0.6573], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8679402
Printing some Q and Qe and total Qs values:  [[0.695]
 [0.703]
 [0.967]
 [0.854]
 [0.762]] [[52.066]
 [52.515]
 [43.714]
 [48.204]
 [50.706]] [[1.898]
 [1.925]
 [1.805]
 [1.889]
 [1.905]]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.78678704522531
printing an ep nov before normalisation:  62.01842229639289
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.06830720057117
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.925]
 [1.446]
 [1.332]
 [1.388]
 [1.34 ]] [[25.222]
 [25.736]
 [23.328]
 [28.667]
 [23.622]] [[1.933]
 [2.474]
 [2.264]
 [2.534]
 [2.284]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3981 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.72 ]
 [1.123]
 [0.708]
 [0.708]] [[73.005]
 [74.909]
 [51.712]
 [73.005]
 [73.005]] [[1.008]
 [1.029]
 [1.322]
 [1.008]
 [1.008]]
printing an ep nov before normalisation:  25.869349070957732
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.516101261240784
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.34696205544738
printing an ep nov before normalisation:  66.47469611437427
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.448]
 [0.431]
 [0.431]] [[52.441]
 [52.441]
 [58.525]
 [52.441]
 [52.441]] [[0.431]
 [0.431]
 [0.448]
 [0.431]
 [0.431]]
siam score:  -0.85304487
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.912]
 [0.644]
 [0.786]
 [0.786]] [[47.466]
 [43.393]
 [37.119]
 [47.466]
 [47.466]] [[0.786]
 [0.912]
 [0.644]
 [0.786]
 [0.786]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]] [[84.981]
 [84.981]
 [84.981]
 [84.981]
 [84.981]] [[0.451]
 [0.451]
 [0.451]
 [0.451]
 [0.451]]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.939]
 [0.98 ]
 [0.939]
 [0.939]
 [0.939]] [[49.984]
 [41.266]
 [49.984]
 [49.984]
 [49.984]] [[0.939]
 [0.98 ]
 [0.939]
 [0.939]
 [0.939]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.35035313546733
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.931]
 [0.566]
 [0.566]
 [0.566]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.566]
 [0.931]
 [0.566]
 [0.566]
 [0.566]]
printing an ep nov before normalisation:  60.70068559332973
printing an ep nov before normalisation:  32.04680453344128
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.748]
 [0.539]
 [0.539]
 [0.539]] [[24.038]
 [52.158]
 [24.038]
 [24.038]
 [24.038]] [[1.025]
 [2.106]
 [1.025]
 [1.025]
 [1.025]]
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.707438426088714
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8430102
maxi score, test score, baseline:  0.4381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4381 1.0 1.0
printing an ep nov before normalisation:  81.11420075165601
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.124]
 [1.124]
 [1.124]
 [1.124]] [[45.63]
 [45.63]
 [45.63]
 [45.63]
 [45.63]] [[2.134]
 [2.134]
 [2.134]
 [2.134]
 [2.134]]
Printing some Q and Qe and total Qs values:  [[0.059]
 [0.066]
 [0.068]
 [0.057]
 [0.052]] [[8.707]
 [8.803]
 [8.782]
 [8.517]
 [8.725]] [[0.226]
 [0.239]
 [0.239]
 [0.213]
 [0.221]]
printing an ep nov before normalisation:  35.684571266174316
printing an ep nov before normalisation:  31.092872619628906
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.008]
 [0.775]
 [0.473]
 [0.438]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.008]
 [0.775]
 [0.473]
 [0.438]]
siam score:  -0.8513124
maxi score, test score, baseline:  0.4401 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8520374
Printing some Q and Qe and total Qs values:  [[1.352]
 [1.419]
 [1.352]
 [1.352]
 [1.352]] [[46.489]
 [65.004]
 [46.489]
 [46.489]
 [46.489]] [[1.67 ]
 [1.959]
 [1.67 ]
 [1.67 ]
 [1.67 ]]
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4421 1.0 1.0
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4461 1.0 1.0
printing an ep nov before normalisation:  56.67590706515301
printing an ep nov before normalisation:  45.98749005763927
maxi score, test score, baseline:  0.4461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.12902627180353
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.441]
 [1.253]
 [1.253]
 [1.253]] [[30.622]
 [45.516]
 [30.622]
 [30.622]
 [30.622]] [[1.676]
 [2.255]
 [1.676]
 [1.676]
 [1.676]]
siam score:  -0.8604241
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.937]
 [0.992]
 [0.95 ]
 [0.937]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.937]
 [0.937]
 [0.992]
 [0.95 ]
 [0.937]]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.76 ]
 [0.684]
 [1.037]
 [0.004]
 [0.767]] [[22.491]
 [25.121]
 [38.138]
 [22.343]
 [23.188]] [[0.882]
 [0.839]
 [1.354]
 [0.124]
 [0.898]]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.092]
 [1.092]
 [1.172]
 [1.092]
 [1.092]] [[74.617]
 [74.617]
 [84.861]
 [74.617]
 [74.617]] [[2.783]
 [2.783]
 [3.172]
 [2.783]
 [2.783]]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.821233759250084
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.595]
 [0.743]
 [0.595]
 [0.625]] [[35.417]
 [43.323]
 [48.695]
 [43.323]
 [42.512]] [[0.527]
 [0.595]
 [0.743]
 [0.595]
 [0.625]]
maxi score, test score, baseline:  0.4481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.00021397641342256414
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.572209088708455
Printing some Q and Qe and total Qs values:  [[1.339]
 [1.339]
 [1.339]
 [1.339]
 [1.339]] [[29.177]
 [29.177]
 [29.177]
 [29.177]
 [29.177]] [[1.508]
 [1.508]
 [1.508]
 [1.508]
 [1.508]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.892]
 [0.892]
 [0.818]
 [0.928]
 [0.892]] [[57.451]
 [57.451]
 [55.7  ]
 [59.486]
 [57.451]] [[1.481]
 [1.481]
 [1.387]
 [1.542]
 [1.481]]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.875]
 [0.772]
 [0.854]
 [0.797]] [[29.186]
 [47.966]
 [31.478]
 [31.9  ]
 [29.075]] [[1.058]
 [1.661]
 [1.165]
 [1.258]
 [1.132]]
printing an ep nov before normalisation:  69.21686676137205
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.29330240781997
Printing some Q and Qe and total Qs values:  [[0.807]
 [0.807]
 [1.123]
 [0.807]
 [0.807]] [[51.727]
 [51.727]
 [48.968]
 [51.727]
 [51.727]] [[1.491]
 [1.491]
 [1.75 ]
 [1.491]
 [1.491]]
printing an ep nov before normalisation:  68.04435990696966
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.70890251552186
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.933]
 [0.933]
 [0.933]
 [0.933]] [[37.122]
 [37.122]
 [37.122]
 [37.122]
 [37.122]] [[1.8]
 [1.8]
 [1.8]
 [1.8]
 [1.8]]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.41516351699829
printing an ep nov before normalisation:  45.965437979386074
maxi score, test score, baseline:  0.4501 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4501 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8454074
maxi score, test score, baseline:  0.4501 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.84539205
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.200083654719585
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.016]
 [1.016]
 [1.159]
 [1.016]
 [1.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.016]
 [1.016]
 [1.159]
 [1.016]
 [1.016]]
maxi score, test score, baseline:  0.4501 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4501 1.0 1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.166]
 [1.377]
 [1.359]
 [1.202]
 [1.202]] [[29.033]
 [14.586]
 [17.202]
 [19.794]
 [19.794]] [[2.125]
 [1.56 ]
 [1.683]
 [1.665]
 [1.665]]
siam score:  -0.8540319
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.276948680181363
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.8725,     0.0008,     0.0004,     0.0395,     0.0868],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9996,     0.0001,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0148,     0.9152,     0.0018,     0.0679],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0000,     0.0028,     0.9800,     0.0169],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0401, 0.0078, 0.0908, 0.1491, 0.7121], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.728544235229492
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.4521 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4541 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4541 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.801]
 [0.959]
 [0.801]
 [0.801]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.801]
 [0.801]
 [0.959]
 [0.801]
 [0.801]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.46717166900635
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4561 1.0 1.0
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4561 1.0 1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8536069
printing an ep nov before normalisation:  34.525299072265625
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8539717
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.307]
 [1.376]
 [1.211]
 [1.211]] [[41.459]
 [40.406]
 [36.815]
 [41.459]
 [41.459]] [[2.374]
 [2.413]
 [2.288]
 [2.374]
 [2.374]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.58271030447622
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00010224921709323098
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.19 ]
 [1.275]
 [1.19 ]
 [1.19 ]
 [1.19 ]] [[29.641]
 [28.099]
 [29.641]
 [29.641]
 [29.641]] [[2.482]
 [2.447]
 [2.482]
 [2.482]
 [2.482]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4581 1.0 1.0
printing an ep nov before normalisation:  37.999320089045796
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.922195434570312
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.85221726
printing an ep nov before normalisation:  47.76103364939815
printing an ep nov before normalisation:  46.22158420778267
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[44.88]
 [44.88]
 [44.88]
 [44.88]
 [44.88]] [[2.5]
 [2.5]
 [2.5]
 [2.5]
 [2.5]]
printing an ep nov before normalisation:  61.921468047880836
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.8088595020315
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4601 1.0 1.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  66.45799941339162
printing an ep nov before normalisation:  47.86455915500466
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.84357929229736
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.38496638551598
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.92746985307759
maxi score, test score, baseline:  0.4661 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.27211633117625
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  66.67441748143116
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.9728527343477
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [1.109]
 [1.062]
 [0.984]] [[58.475]
 [58.475]
 [61.084]
 [65.041]
 [58.475]] [[2.49 ]
 [2.49 ]
 [2.712]
 [2.811]
 [2.49 ]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  63.578821520691115
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.813026428222656
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.088]
 [1.367]
 [1.301]
 [1.321]
 [1.207]] [[23.747]
 [24.506]
 [24.058]
 [30.379]
 [23.465]] [[1.374]
 [1.67 ]
 [1.594]
 [1.758]
 [1.486]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.25370169567894
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
printing an ep nov before normalisation:  57.75999893355566
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.744]
 [0.867]
 [0.744]
 [0.744]
 [0.744]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.744]
 [0.867]
 [0.744]
 [0.744]
 [0.744]]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  61.07451653480667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
printing an ep nov before normalisation:  0.00016002846223273082
printing an ep nov before normalisation:  0.0011008243274091
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  56.126190548168715
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.416]
 [1.309]
 [1.309]
 [1.325]] [[34.261]
 [48.86 ]
 [34.261]
 [34.261]
 [42.267]] [[1.453]
 [1.678]
 [1.453]
 [1.453]
 [1.533]]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.977]
 [0.945]
 [0.994]
 [1.064]
 [1.043]] [[39.955]
 [36.315]
 [39.568]
 [57.992]
 [39.71 ]] [[1.413]
 [1.298]
 [1.421]
 [1.908]
 [1.474]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.84473056
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.126]
 [1.188]
 [1.126]
 [1.126]
 [1.126]] [[63.674]
 [63.079]
 [63.674]
 [63.674]
 [63.674]] [[2.353]
 [2.401]
 [2.353]
 [2.353]
 [2.353]]
printing an ep nov before normalisation:  59.70702601905761
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [0.999]
 [0.999]
 [0.999]] [[56.182]
 [56.182]
 [56.182]
 [56.182]
 [56.182]] [[2.54]
 [2.54]
 [2.54]
 [2.54]
 [2.54]]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.019]
 [1.004]
 [0.893]
 [0.721]] [[27.985]
 [25.54 ]
 [37.711]
 [28.514]
 [24.601]] [[1.565]
 [0.553]
 [2.081]
 [1.56 ]
 [1.213]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.84763044
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.7794135002163
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.45330238342285
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85642487
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.71 ]
 [0.004]
 [0.856]
 [0.705]
 [0.674]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.71 ]
 [0.004]
 [0.856]
 [0.705]
 [0.674]]
siam score:  -0.8558093
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.853436
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.87831115722656
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8019,     0.0002,     0.0012,     0.0300,     0.1666],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9995,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9683,     0.0001,     0.0314],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0333,     0.0003,     0.0014,     0.8136,     0.1514],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0909, 0.0018, 0.1627, 0.0962, 0.6483], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.461700153443225
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.94562834041815
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
line 256 mcts: sample exp_bonus 47.37487784857231
printing an ep nov before normalisation:  56.02038330819243
Printing some Q and Qe and total Qs values:  [[1.23]
 [1.23]
 [1.28]
 [1.23]
 [1.23]] [[46.792]
 [46.792]
 [40.302]
 [46.792]
 [46.792]] [[1.818]
 [1.818]
 [1.744]
 [1.818]
 [1.818]]
Printing some Q and Qe and total Qs values:  [[0.888]
 [0.888]
 [0.888]
 [0.888]
 [0.888]] [[34.803]
 [34.803]
 [34.803]
 [34.803]
 [34.803]] [[1.209]
 [1.209]
 [1.209]
 [1.209]
 [1.209]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.898144721984863
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00015929930441416218
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  35.27430341609896
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.05016782371702
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  25.932929512109524
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8527086
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.922]
 [0.922]
 [0.995]
 [0.922]
 [0.922]] [[67.668]
 [67.668]
 [65.458]
 [67.668]
 [67.668]] [[2.172]
 [2.172]
 [2.194]
 [2.172]
 [2.172]]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.977]
 [0.993]
 [0.891]
 [0.971]] [[43.099]
 [48.774]
 [56.705]
 [43.099]
 [58.195]] [[1.344]
 [1.579]
 [1.803]
 [1.344]
 [1.82 ]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  55.72329621169621
Printing some Q and Qe and total Qs values:  [[1.011]
 [1.011]
 [1.011]
 [1.011]
 [1.011]] [[48.818]
 [48.818]
 [48.818]
 [48.818]
 [48.818]] [[2.21]
 [2.21]
 [2.21]
 [2.21]
 [2.21]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4821 1.0 1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.16848321499859
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8603361
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.61452659581989
printing an ep nov before normalisation:  0.0014682985252534309
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4821 1.0 1.0
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.219]
 [1.219]
 [1.219]
 [1.219]
 [1.219]] [[47.307]
 [47.307]
 [47.307]
 [47.307]
 [47.307]] [[2.393]
 [2.393]
 [2.393]
 [2.393]
 [2.393]]
Printing some Q and Qe and total Qs values:  [[0.863]
 [0.863]
 [0.878]
 [0.863]
 [0.833]] [[44.722]
 [58.239]
 [39.835]
 [44.722]
 [56.293]] [[0.863]
 [0.863]
 [0.878]
 [0.863]
 [0.833]]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.49822540066715
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0006266021352985263
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([0.6948, 0.0827, 0.0036, 0.0384, 0.1804], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9981,     0.0005,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0011,     0.0011,     0.9956,     0.0004,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0293, 0.0010, 0.0750, 0.7119, 0.1827], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0032, 0.2077, 0.1492, 0.0015, 0.6384], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.85190916
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.05772319004247
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
printing an ep nov before normalisation:  23.36458444595337
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  74.81924885496335
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
printing an ep nov before normalisation:  47.95829099752908
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  0  action  0 :  tensor([    0.9975,     0.0000,     0.0005,     0.0011,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9976,     0.0001,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0000,     0.9756,     0.0020,     0.0220],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0548,     0.0002,     0.0008,     0.8213,     0.1229],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0806, 0.0007, 0.0522, 0.1902, 0.6763], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.82 ]
 [1.089]
 [0.696]
 [0.783]] [[32.574]
 [31.315]
 [65.901]
 [39.664]
 [36.43 ]] [[0.462]
 [1.231]
 [2.397]
 [1.323]
 [1.326]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.851]
 [0.869]
 [0.851]
 [0.72 ]
 [0.74 ]] [[50.195]
 [61.052]
 [50.195]
 [28.389]
 [34.449]] [[1.502]
 [1.756]
 [1.502]
 [0.894]
 [1.047]]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8529138
printing an ep nov before normalisation:  66.95402025538209
printing an ep nov before normalisation:  70.79490098637847
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.4217209815979
printing an ep nov before normalisation:  77.39940940833381
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  66.0378447160704
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
printing an ep nov before normalisation:  56.66115042264841
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  67.94523715972662
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.72365856170654
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.298]
 [1.298]
 [1.378]
 [1.269]
 [1.298]] [[35.903]
 [35.903]
 [49.144]
 [53.597]
 [35.903]] [[2.195]
 [2.195]
 [2.78 ]
 [2.841]
 [2.195]]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.8404,     0.0004,     0.0027,     0.0456,     0.1110],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9957,     0.0001,     0.0005,     0.0026],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0001,     0.9373,     0.0004,     0.0619],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0000,     0.0001,     0.9966,     0.0033],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0953,     0.0005,     0.1291,     0.0054,     0.7698],
       grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84942734
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8512334
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.15 ]
 [1.324]
 [0.023]
 [0.839]] [[28.853]
 [27.205]
 [54.228]
 [32.106]
 [30.069]] [[1.128]
 [0.335]
 [2.116]
 [0.318]
 [1.088]]
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.58815002441406
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.199]
 [1.199]
 [1.199]
 [1.17 ]
 [1.226]] [[27.38 ]
 [27.38 ]
 [27.38 ]
 [16.964]
 [23.054]] [[2.556]
 [2.556]
 [2.556]
 [1.667]
 [2.226]]
printing an ep nov before normalisation:  19.265711307525635
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  75.50308185285817
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.789]
 [0.956]
 [0.702]
 [0.719]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.789]
 [0.789]
 [0.956]
 [0.702]
 [0.719]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.944]
 [0.941]
 [0.944]
 [0.945]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.944]
 [0.944]
 [0.941]
 [0.944]
 [0.945]]
Printing some Q and Qe and total Qs values:  [[0.587]
 [0.587]
 [0.796]
 [0.587]
 [0.587]] [[30.48 ]
 [30.48 ]
 [41.029]
 [30.48 ]
 [30.48 ]] [[0.587]
 [0.587]
 [0.796]
 [0.587]
 [0.587]]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
printing an ep nov before normalisation:  34.5762857558628
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.71099041164171
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.17293125103779
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.7135,     0.0007,     0.0004,     0.0917,     0.1938],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9700,     0.0002,     0.0011,     0.0284],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0002,     0.9429,     0.0007,     0.0559],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0623,     0.0001,     0.0002,     0.8437,     0.0937],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1153, 0.0023, 0.0441, 0.0841, 0.7542], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20023
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.47476509666216
printing an ep nov before normalisation:  23.15507411956787
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([    0.6358,     0.0044,     0.0004,     0.1061,     0.2532],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9984,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0014,     0.0001,     0.9463,     0.0016,     0.0505],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0555,     0.0005,     0.0001,     0.8538,     0.0900],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0803,     0.0006,     0.0604,     0.0066,     0.8521],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.04627551884485
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.923]
 [0.911]
 [1.16 ]
 [0.923]
 [0.923]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.923]
 [0.911]
 [1.16 ]
 [0.923]
 [0.923]]
printing an ep nov before normalisation:  26.594736282785163
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.871249636014303
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.968059062957764
line 256 mcts: sample exp_bonus 43.83714186889126
maxi score, test score, baseline:  0.4861 1.0 1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.95869605632395
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.43]
 [1.43]
 [1.43]
 [1.43]
 [1.43]] [[61.238]
 [61.238]
 [61.238]
 [61.238]
 [61.238]] [[2.097]
 [2.097]
 [2.097]
 [2.097]
 [2.097]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.023]
 [1.023]
 [1.023]
 [0.984]
 [1.023]] [[47.597]
 [47.597]
 [47.597]
 [43.891]
 [47.597]] [[2.183]
 [2.183]
 [2.183]
 [1.984]
 [2.183]]
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.203]
 [1.041]
 [1.041]
 [1.041]] [[50.625]
 [47.114]
 [50.625]
 [50.625]
 [50.625]] [[2.708]
 [2.666]
 [2.708]
 [2.708]
 [2.708]]
printing an ep nov before normalisation:  36.524405513429066
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.02272154716549
siam score:  -0.8477078
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([    0.7218,     0.0005,     0.0013,     0.0012,     0.2752],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0010,     0.9982,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9843,     0.0001,     0.0154],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0143,     0.0002,     0.0028,     0.9245,     0.0582],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0616, 0.0018, 0.0886, 0.1305, 0.7174], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.801]
 [0.675]
 [0.692]
 [0.692]
 [0.692]] [[47.282]
 [17.183]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.168]
 [0.771]
 [0.634]
 [0.634]
 [0.634]]
printing an ep nov before normalisation:  62.57623547219758
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.358359816319446
printing an ep nov before normalisation:  0.0036696650045087154
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  17.647709068667154
maxi score, test score, baseline:  0.4861 1.0 1.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  86.61216503812565
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Starting evaluation
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.014]
 [0.874]
 [0.018]
 [0.668]] [[34.356]
 [39.199]
 [49.134]
 [34.633]
 [39.569]] [[0.759]
 [0.014]
 [0.874]
 [0.018]
 [0.668]]
printing an ep nov before normalisation:  58.398780098052285
printing an ep nov before normalisation:  49.06921357912877
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.8273,     0.0038,     0.0004,     0.0004,     0.1681],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9998,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0038, 0.0010, 0.9465, 0.0049, 0.0437], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0152,     0.0002,     0.0040,     0.9332,     0.0474],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0641, 0.0024, 0.1180, 0.1020, 0.7135], grad_fn=<DivBackward0>)
actions average: 
K:  2  action  0 :  tensor([    0.9771,     0.0001,     0.0023,     0.0044,     0.0161],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9984,     0.0004,     0.0001,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9719,     0.0015,     0.0264],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0288,     0.0001,     0.0001,     0.8714,     0.0996],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0779,     0.0006,     0.0294,     0.0632,     0.8290],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.944]
 [0.965]
 [0.944]
 [0.944]
 [0.944]] [[38.069]
 [54.447]
 [38.069]
 [38.069]
 [38.069]] [[0.944]
 [0.965]
 [0.944]
 [0.944]
 [0.944]]
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0008075985908817529
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5181 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([0.7643, 0.0015, 0.0011, 0.0692, 0.1638], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9973,     0.0002,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9685,     0.0001,     0.0312],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0272,     0.0004,     0.0048,     0.8881,     0.0796],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0537, 0.0019, 0.1019, 0.0981, 0.7443], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 24.334078550338745
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9250,     0.0001,     0.0069,     0.0002,     0.0678],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0010,     0.9559,     0.0136,     0.0001,     0.0294],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0015,     0.0001,     0.9454,     0.0264,     0.0266],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0056,     0.0004,     0.0022,     0.9434,     0.0484],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0965, 0.0016, 0.0506, 0.1155, 0.7358], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.436111722673694
printing an ep nov before normalisation:  44.408210118611656
siam score:  -0.8492639
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.62154681742359
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.8149, 0.0296, 0.0024, 0.0460, 0.1072], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9996,     0.0002,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0008,     0.0027,     0.9256,     0.0092,     0.0618],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0355,     0.0002,     0.0015,     0.8492,     0.1137],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0067, 0.0047, 0.1867, 0.0398, 0.7620], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.681850846121044
Printing some Q and Qe and total Qs values:  [[0.786]
 [0.786]
 [0.786]
 [0.829]
 [0.786]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.786]
 [0.786]
 [0.786]
 [0.829]
 [0.786]]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.841844452752007
printing an ep nov before normalisation:  78.85747156129857
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  95.723341626233
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.33114957081665
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.47519303184951
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.29665961883226
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5221 1.0 1.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8423192
printing an ep nov before normalisation:  18.29873561859131
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.397]
 [1.428]
 [1.397]
 [1.397]
 [1.397]] [[75.676]
 [78.672]
 [75.676]
 [75.676]
 [75.676]] [[2.349]
 [2.428]
 [2.349]
 [2.349]
 [2.349]]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.79303550720215
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  56.73232348255545
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5241 1.0 1.0
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5261 1.0 1.0
printing an ep nov before normalisation:  62.09091813089329
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.73112487792969
Printing some Q and Qe and total Qs values:  [[1.445]
 [1.479]
 [1.448]
 [1.445]
 [1.445]] [[47.371]
 [55.515]
 [52.326]
 [47.371]
 [47.371]] [[2.524]
 [2.743]
 [2.639]
 [2.524]
 [2.524]]
printing an ep nov before normalisation:  35.3812575340271
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5261 1.0 1.0
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8402021
siam score:  -0.8396982
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.167 0.583 0.042 0.083 0.125]
printing an ep nov before normalisation:  34.31926250457764
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.935]
 [0.891]
 [0.913]
 [0.837]] [[62.523]
 [66.642]
 [68.822]
 [62.523]
 [69.209]] [[0.913]
 [0.935]
 [0.891]
 [0.913]
 [0.837]]
actions average: 
K:  4  action  0 :  tensor([    0.8744,     0.0002,     0.0004,     0.0007,     0.1244],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9971,     0.0002,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0012,     0.0001,     0.9548,     0.0044,     0.0396],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0320,     0.0000,     0.0003,     0.9297,     0.0380],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0548, 0.0041, 0.2537, 0.0021, 0.6852], grad_fn=<DivBackward0>)
siam score:  -0.83778167
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.955]
 [0.974]
 [0.955]
 [0.955]
 [0.955]] [[30.025]
 [45.313]
 [30.025]
 [30.025]
 [30.025]] [[0.955]
 [0.974]
 [0.955]
 [0.955]
 [0.955]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  79.7875925962518
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.28510111199316
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.2373336473380805
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  69.5593190234296
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.8019, 0.0012, 0.0012, 0.0493, 0.1465], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9992,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0003,     0.9787,     0.0002,     0.0207],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0550,     0.0005,     0.0006,     0.7945,     0.1494],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1956, 0.0023, 0.0362, 0.0911, 0.6747], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.785]
 [0.745]
 [0.848]
 [0.741]
 [0.735]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.785]
 [0.745]
 [0.848]
 [0.741]
 [0.735]]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5281 1.0 1.0
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.27607345581055
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.1397060071167
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
siam score:  -0.8492957
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8484197
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9659,     0.0155,     0.0003,     0.0072,     0.0110],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9871,     0.0003,     0.0002,     0.0119],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0007,     0.0000,     0.9805,     0.0012,     0.0176],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0271,     0.0007,     0.0041,     0.8761,     0.0920],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2442, 0.0025, 0.0954, 0.0706, 0.5873], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84838974
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.52322569868937
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.448496765190455
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.125 0.042 0.167 0.625 0.042]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 41.49770736694336
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.98647030645619
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  92.44995220341231
actions average: 
K:  0  action  0 :  tensor([    0.8118,     0.0002,     0.0005,     0.0023,     0.1852],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9983,     0.0000,     0.0002,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0007,     0.9593,     0.0003,     0.0394],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0387,     0.0000,     0.0008,     0.8649,     0.0956],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0953, 0.0034, 0.0851, 0.1205, 0.6957], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.256]
 [1.256]
 [1.256]
 [1.256]
 [1.256]] [[34.648]
 [34.648]
 [34.648]
 [34.648]
 [34.648]] [[59.015]
 [59.015]
 [59.015]
 [59.015]
 [59.015]]
maxi score, test score, baseline:  0.5361 1.0 1.0
printing an ep nov before normalisation:  74.70970562986123
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.10123450964721
UNIT TEST: sample policy line 217 mcts : [0.042 0.75  0.042 0.125 0.042]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.06080913543701
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  11.459326612416874
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5381 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([    0.7760,     0.0031,     0.0006,     0.0527,     0.1676],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0034,     0.9756,     0.0002,     0.0000,     0.0207],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0010,     0.0011,     0.9547,     0.0010,     0.0422],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0367,     0.0004,     0.0003,     0.9155,     0.0471],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0361, 0.0024, 0.0561, 0.0128, 0.8926], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.88070425225886
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.38 ]
 [1.418]
 [1.376]
 [1.38 ]
 [1.38 ]] [[54.126]
 [58.989]
 [58.294]
 [54.126]
 [54.126]] [[2.44 ]
 [2.618]
 [2.557]
 [2.44 ]
 [2.44 ]]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8875,     0.0051,     0.0006,     0.0001,     0.1067],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9996,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0007,     0.9837,     0.0036,     0.0118],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0883,     0.0001,     0.0002,     0.7567,     0.1548],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0720, 0.0055, 0.0991, 0.0488, 0.7746], grad_fn=<DivBackward0>)
siam score:  -0.8430649
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([    0.5879,     0.0001,     0.0015,     0.1498,     0.2607],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9893,     0.0076,     0.0001,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0002,     0.9699,     0.0010,     0.0285],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0157,     0.0002,     0.0003,     0.8804,     0.1033],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0886,     0.0003,     0.0424,     0.1129,     0.7558],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8383075
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.668366149119663
maxi score, test score, baseline:  0.5381 1.0 1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.96930408477783
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.627377444112476
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.66594559198938
printing an ep nov before normalisation:  52.00446229622785
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.11860341379305
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.832455
maxi score, test score, baseline:  0.5381 1.0 1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.07601331642945297
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.59427833557129
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  33.4990671293565
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8261052
maxi score, test score, baseline:  0.5381 1.0 1.0
actions average: 
K:  3  action  0 :  tensor([0.8007, 0.0015, 0.0027, 0.0020, 0.1931], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9995,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0011,     0.0000,     0.9825,     0.0003,     0.0160],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0487,     0.0009,     0.0006,     0.8114,     0.1384],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0826, 0.0032, 0.0744, 0.0583, 0.7815], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5381 1.0 1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8279738
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.82603955
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
