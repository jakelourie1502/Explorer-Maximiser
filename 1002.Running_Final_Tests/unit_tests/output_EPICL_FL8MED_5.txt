append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 25}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lakeGym_Image.gridWorld'>
same_env_each_time:True
env_size:[8, 8]
observable_size:[8, 8]
game_modes:1
env_map:[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
max_steps:100
actions_size:5
optimal_score:1
total_frames:205000
exp_gamma:0.95
atari_env:False
memory_size:60
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(1, 64)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  8.801642060279846
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.   ]
 [-0.   ]
 [-0.   ]] [[9.731]
 [9.731]
 [7.971]
 [7.962]
 [7.959]] [[1.332]
 [1.332]
 [0.874]
 [0.871]
 [0.871]]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  0.0009979599959809673
actions average: 
K:  0  action  0 :  tensor([0.1854, 0.2613, 0.2012, 0.2100, 0.1421], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0832, 0.4211, 0.2018, 0.1663, 0.1276], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0936, 0.1831, 0.4400, 0.1741, 0.1093], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2157, 0.1414, 0.2540, 0.2299, 0.1591], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1617, 0.1521, 0.3132, 0.2334, 0.1396], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([0.1972, 0.2296, 0.2454, 0.1939, 0.1339], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0816, 0.4795, 0.1629, 0.1577, 0.1184], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0777, 0.1588, 0.5076, 0.1719, 0.0841], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2087, 0.1681, 0.2797, 0.2188, 0.1247], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1800, 0.1657, 0.2763, 0.2454, 0.1326], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.06706802973741784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 2 threads
Frames:  771 train batches done:  27 episodes:  62
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.10374470657127098
siam score:  -0.10721224470960972
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  771 train batches done:  71 episodes:  62
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.38913712
siam score:  -0.42439613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([0.3609, 0.1614, 0.0210, 0.2304, 0.2263], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0472, 0.8729, 0.0286, 0.0067, 0.0446], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0267, 0.0898, 0.6024, 0.1933, 0.0878], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1861, 0.0269, 0.3256, 0.2870, 0.1743], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2819, 0.1884, 0.0640, 0.2094, 0.2563], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.4948718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.48438117
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.698]
 [33.601]
 [33.601]
 [33.601]
 [33.601]] [[0.411]
 [0.164]
 [0.164]
 [0.164]
 [0.164]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.038]
 [28.038]
 [28.038]
 [28.038]
 [28.038]] [[28.038]
 [28.038]
 [28.038]
 [28.038]
 [28.038]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.47210297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.46679639816284
siam score:  -0.4918623
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  60.93103545052665
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5574085
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.06345502860171
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5590351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6152178
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.02379608154297
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.902838561833676
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5839109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6669, 0.0413, 0.0048, 0.1285, 0.1585], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1545, 0.7493, 0.0160, 0.0099, 0.0704], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0132, 0.0393, 0.6561, 0.1752, 0.1162], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2432, 0.0047, 0.0189, 0.5367, 0.1965], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.4917, 0.0428, 0.0213, 0.2314, 0.2129], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.63470054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6369388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.65053016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.899020397826526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.  0.  0.4 0.6 0. ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5725, 0.0373, 0.0007, 0.1842, 0.2053], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0213,     0.9652,     0.0060,     0.0000,     0.0075],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0041,     0.9606,     0.0145,     0.0202],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1467, 0.0219, 0.1893, 0.4419, 0.2002], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2712, 0.0058, 0.0280, 0.3085, 0.3866], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.5716,     0.0077,     0.0006,     0.2008,     0.2194],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0339, 0.9253, 0.0060, 0.0064, 0.0284], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0035, 0.0022, 0.9398, 0.0376, 0.0169], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2041, 0.0013, 0.0467, 0.5370, 0.2109], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3800, 0.0405, 0.0804, 0.2105, 0.2886], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  116.13988824140722
printing an ep nov before normalisation:  16.256046926723684
printing an ep nov before normalisation:  103.70346905889203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.029122839833917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.060075759887695
actions average: 
K:  4  action  0 :  tensor([0.2748, 0.2743, 0.0830, 0.1027, 0.2652], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0093, 0.9662, 0.0044, 0.0017, 0.0184], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0008,     0.9787,     0.0068,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1666, 0.0043, 0.0835, 0.3739, 0.3716], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2034, 0.2355, 0.1268, 0.1502, 0.2841], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[86.976]
 [90.348]
 [95.204]
 [86.976]
 [93.27 ]] [[1.356]
 [1.432]
 [1.541]
 [1.356]
 [1.497]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.662775435612048
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.4871, 0.1197, 0.0036, 0.1223, 0.2673], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0442, 0.9178, 0.0025, 0.0026, 0.0329], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0012,     0.9844,     0.0078,     0.0062],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1111, 0.0384, 0.1498, 0.3745, 0.3263], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1959, 0.1852, 0.0636, 0.2162, 0.3391], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.55182147800275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.023900985717773
printing an ep nov before normalisation:  39.34719243134701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.929]
 [36.355]
 [39.522]
 [35.929]
 [35.929]] [[1.668]
 [1.697]
 [1.908]
 [1.668]
 [1.668]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  36.929178696737864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.595]
 [57.838]
 [54.828]
 [52.595]
 [52.595]] [[0.278]
 [0.333]
 [0.301]
 [0.278]
 [0.278]]
printing an ep nov before normalisation:  10.30825492507887
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.14764303971775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.52665615081787
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.010967552358586
actions average: 
K:  3  action  0 :  tensor([0.6613, 0.0380, 0.0030, 0.1155, 0.1822], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0184, 0.9463, 0.0171, 0.0024, 0.0158], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0016,     0.0007,     0.8423,     0.0769,     0.0785],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2462, 0.0070, 0.0057, 0.5705, 0.1705], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.3597, 0.0704, 0.0769, 0.2682, 0.2248], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  20.04645586013794
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.784767659984993
actions average: 
K:  0  action  0 :  tensor([0.5643, 0.0849, 0.0007, 0.1485, 0.2016], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0092, 0.8544, 0.0594, 0.0228, 0.0542], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0064,     0.9608,     0.0236,     0.0084],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1561, 0.0008, 0.1013, 0.5367, 0.2051], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2161, 0.0206, 0.1617, 0.3220, 0.2795], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.20284843444824
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.648033989800346
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.19500150456508436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.5089528278746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  3.3891321726332535
printing an ep nov before normalisation:  40.61482614912082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.847]
 [40.847]
 [40.847]
 [42.849]
 [40.847]] [[0.287]
 [0.287]
 [0.287]
 [0.314]
 [0.287]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.75970114400604
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7438, 0.0436, 0.0083, 0.0657, 0.1385], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0516,     0.9292,     0.0012,     0.0009,     0.0170],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0107, 0.0204, 0.8604, 0.0553, 0.0532], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.3102, 0.0573, 0.0088, 0.3061, 0.3176], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.4482, 0.0119, 0.0010, 0.2499, 0.2890], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.50594292424148
printing an ep nov before normalisation:  23.58943462371826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.624]
 [31.436]
 [32.019]
 [49.624]
 [49.624]] [[0.649]
 [0.328]
 [0.339]
 [0.649]
 [0.649]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.31593176034674
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.45573668855872
actions average: 
K:  0  action  0 :  tensor([0.5629, 0.0011, 0.0020, 0.2392, 0.1948], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.1357,     0.7878,     0.0002,     0.0008,     0.0754],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0015,     0.9776,     0.0139,     0.0069],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1629, 0.0027, 0.0337, 0.6101, 0.1906], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3295, 0.0008, 0.0353, 0.3795, 0.2548], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.09824059503432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  9174 train batches done:  1074 episodes:  870
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.86652166709996
siam score:  -0.64595294
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.61479015255859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  65.59018935290126
actions average: 
K:  1  action  0 :  tensor([0.6216, 0.0133, 0.0138, 0.1342, 0.2171], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0159,     0.9731,     0.0008,     0.0001,     0.0102],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9966,     0.0017,     0.0017],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.2169, 0.0007, 0.0051, 0.4859, 0.2914], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2887, 0.0512, 0.0469, 0.2704, 0.3428], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.09698916588804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[13.789]
 [13.789]
 [27.731]
 [13.789]
 [13.789]] [[0.429]
 [0.429]
 [1.254]
 [0.429]
 [0.429]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.859]
 [28.274]
 [36.329]
 [28.54 ]
 [42.962]] [[1.225]
 [0.69 ]
 [1.008]
 [0.701]
 [1.269]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.4636, 0.0451, 0.0078, 0.1795, 0.3040], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0286, 0.9341, 0.0091, 0.0015, 0.0268], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0381,     0.9114,     0.0203,     0.0296],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0941,     0.0002,     0.0234,     0.6377,     0.2446],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1861, 0.0028, 0.1166, 0.2909, 0.4035], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  23.500757806426122
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.397226333618164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([0.5387, 0.0791, 0.0027, 0.1455, 0.2340], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0035,     0.9929,     0.0000,     0.0002,     0.0035],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0106, 0.0023, 0.8201, 0.0965, 0.0705], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1129, 0.0007, 0.0526, 0.5814, 0.2524], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2317, 0.0595, 0.0175, 0.3169, 0.3745], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.221155285835266
Starting evaluation
actions average: 
K:  4  action  0 :  tensor([0.4502, 0.0177, 0.0070, 0.2173, 0.3078], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0681, 0.8132, 0.0335, 0.0121, 0.0731], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9899,     0.0052,     0.0046],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1209, 0.0021, 0.0725, 0.6599, 0.1445], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1874, 0.0020, 0.1435, 0.3641, 0.3031], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.71482691750502
siam score:  -0.6947017
actions average: 
K:  0  action  0 :  tensor([0.5909, 0.0065, 0.0007, 0.1336, 0.2684], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0775,     0.9164,     0.0000,     0.0002,     0.0059],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0124, 0.0088, 0.8920, 0.0572, 0.0297], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.2006,     0.0004,     0.0005,     0.6006,     0.1979],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3155, 0.0013, 0.0020, 0.3527, 0.3286], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.937416315078735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.84467688807042
siam score:  -0.70016634
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.305]
 [78.472]
 [74.653]
 [67.826]
 [67.046]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.62454178883304
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.021]
 [16.011]
 [34.935]
 [50.43 ]
 [14.263]] [[0.123]
 [0.123]
 [0.667]
 [1.112]
 [0.072]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.31781352303193
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.418324632039571
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7082895
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.06649959590812
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7008158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5525, 0.0008, 0.0012, 0.1530, 0.2926], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0016,     0.9975,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0052,     0.9928,     0.0007,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2041, 0.0036, 0.0008, 0.4807, 0.3108], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2759, 0.0261, 0.0022, 0.2100, 0.4858], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6991618
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.146]
 [21.146]
 [21.146]
 [21.146]
 [21.146]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.72532068585055
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.68990415
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6899107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6930825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6971416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.125 0.375 0.083]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.53614044189453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  104.50641843376414
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.04184681321233
printing an ep nov before normalisation:  60.12424124720012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.887397029491268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -1.2933933346289966e-09
0.0 -1.62586874093522e-09
0.0 0.0
0.0 -1.877664371228399e-09
0.0 -1.8983446182695727e-09
0.0 0.0
0.0 -1.777454692315525e-09
0.0 0.0
0.0 0.0
0.0 0.0
siam score:  -0.69835967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.05679903944467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.75096866905338
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.43059575402339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.20791635864482
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.08893851772882044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.25271951533594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.36206969099149
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.69805247
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.263]
 [19.263]
 [28.786]
 [19.221]
 [19.263]] [[0.144]
 [0.144]
 [0.216]
 [0.144]
 [0.144]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.82017815305338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  13425 train batches done:  1570 episodes:  1229
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.5616,     0.1196,     0.0004,     0.1397,     0.1787],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0025, 0.8972, 0.0580, 0.0021, 0.0401], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9773,     0.0141,     0.0081],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1333, 0.0009, 0.0197, 0.6446, 0.2016], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2824, 0.0569, 0.0374, 0.2376, 0.3857], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.944]
 [48.769]
 [55.215]
 [51.749]
 [52.753]] [[0.612]
 [0.832]
 [1.04 ]
 [0.928]
 [0.961]]
printing an ep nov before normalisation:  47.24149703979492
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.661]
 [26.883]
 [24.003]
 [22.4  ]
 [25.737]] [[1.432]
 [1.383]
 [1.203]
 [1.103]
 [1.312]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7071, 0.0022, 0.0008, 0.0810, 0.2089], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0312,     0.9612,     0.0000,     0.0001,     0.0075],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9546,     0.0252,     0.0202],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1001, 0.0053, 0.0015, 0.6471, 0.2460], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3610, 0.0103, 0.0015, 0.2113, 0.4160], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.4964873790741
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.6744, 0.0142, 0.0073, 0.1221, 0.1819], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9959,     0.0001,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0049,     0.8891,     0.0441,     0.0618],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1578, 0.0008, 0.0015, 0.5488, 0.2910], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3195, 0.0481, 0.0443, 0.2454, 0.3427], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.038804054260254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
actions average: 
K:  4  action  0 :  tensor([    0.7400,     0.0028,     0.0002,     0.1090,     0.1480],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0387,     0.9526,     0.0015,     0.0001,     0.0071],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0085,     0.9875,     0.0015,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0963, 0.0008, 0.0184, 0.6146, 0.2698], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.3178, 0.1282, 0.0383, 0.1466, 0.3691], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.93533134460449
actions average: 
K:  2  action  0 :  tensor([    0.4923,     0.0694,     0.0003,     0.1408,     0.2972],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9968,     0.0018,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9909,     0.0048,     0.0042],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1272,     0.0004,     0.1111,     0.4998,     0.2614],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2221, 0.0335, 0.0330, 0.2926, 0.4187], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
siam score:  -0.70968413
printing an ep nov before normalisation:  57.53343720432148
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.31466883599697
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.6087, 0.0225, 0.0017, 0.1695, 0.1976], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0306,     0.9504,     0.0001,     0.0001,     0.0188],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9830,     0.0096,     0.0071],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1247, 0.0553, 0.0844, 0.4698, 0.2658], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2475, 0.0014, 0.0439, 0.3294, 0.3778], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.155708024929424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7086675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  78.70988968432864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.451167616095674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.037398532445735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.70128936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
printing an ep nov before normalisation:  18.274978261062795
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.5077, 0.0093, 0.0008, 0.2940, 0.1882], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0258,     0.9530,     0.0011,     0.0003,     0.0198],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0042,     0.9592,     0.0114,     0.0249],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0568, 0.0009, 0.0200, 0.6885, 0.2338], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2749, 0.0008, 0.0016, 0.4208, 0.3019], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.99002465103983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.61 ]
 [27.544]
 [31.469]
 [31.469]
 [31.469]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.493621953297236
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.68945783
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.96528148651123
printing an ep nov before normalisation:  46.674304416795515
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.172]
 [63.828]
 [70.694]
 [63.828]
 [63.828]] [[0.806]
 [0.714]
 [0.86 ]
 [0.714]
 [0.714]]
siam score:  -0.7053471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.611885047848602
printing an ep nov before normalisation:  54.90920238542641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.484]
 [58.156]
 [60.431]
 [49.704]
 [34.142]] [[0.107]
 [0.216]
 [0.23 ]
 [0.164]
 [0.067]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 45.52226611526509
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  51.92227704184396
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.743813573382475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.7679,     0.0437,     0.0003,     0.0736,     0.1145],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9845,     0.0013,     0.0000,     0.0134],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9871,     0.0052,     0.0077],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0986,     0.0004,     0.0260,     0.5609,     0.3141],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1629, 0.0793, 0.1196, 0.2437, 0.3945], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.154967478441684
siam score:  -0.700618
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.152420625319586
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.69554853
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.495]
 [50.384]
 [31.495]
 [31.097]
 [31.495]] [[0.347]
 [0.743]
 [0.347]
 [0.339]
 [0.347]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.378]
 [49.634]
 [49.625]
 [38.968]
 [49.625]] [[0.501]
 [1.333]
 [1.333]
 [0.787]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.79077813875787
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6987792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.31275473926409
siam score:  -0.6955504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.24552347331798
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.538627285598025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.26610857594631
printing an ep nov before normalisation:  75.26990539217596
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.441541416043044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.357301536674257
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.752]
 [73.797]
 [58.255]
 [57.675]
 [55.093]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 0.0
0.0 2.8635369832848057e-08
0.0 2.701312510839322e-08
0.0 1.709608605372292e-08
0.0 0.0
0.0 0.0
0.0 0.0
0.0 3.303366311106172e-08
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([0.6040, 0.0133, 0.0017, 0.1357, 0.2453], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0111,     0.9684,     0.0008,     0.0003,     0.0193],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0628,     0.8745,     0.0252,     0.0374],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1238,     0.0002,     0.0018,     0.6824,     0.1918],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2878, 0.0725, 0.0041, 0.1518, 0.4838], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.355]
 [38.681]
 [38.327]
 [29.264]
 [43.633]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.22312381481834
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.41989803314209
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.283306052205425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.49510737827846
actions average: 
K:  2  action  0 :  tensor([0.7032, 0.0404, 0.0114, 0.0860, 0.1591], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0073,     0.9854,     0.0005,     0.0001,     0.0067],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9689,     0.0263,     0.0048],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1838, 0.0010, 0.0043, 0.5545, 0.2564], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.3773, 0.0011, 0.0023, 0.2999, 0.3194], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.74451923423557
printing an ep nov before normalisation:  48.991451992616845
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.322]
 [46.796]
 [60.803]
 [36.668]
 [34.402]] [[0.367]
 [0.501]
 [0.75 ]
 [0.32 ]
 [0.28 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.958]
 [60.869]
 [76.767]
 [40.958]
 [57.502]] [[0.403]
 [0.776]
 [1.074]
 [0.403]
 [0.713]]
siam score:  -0.70591474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.89675960417254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.4395695763628
siam score:  -0.71218795
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.14935111999512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71293914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.48879562498278
siam score:  -0.7093863
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.22197805923845
siam score:  -0.7059812
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71273947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.248042982496703
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.727524853219144
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.12998001916068
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.761926194305865
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.2401655076924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  205000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.104]
 [51.448]
 [53.442]
 [54.798]
 [52.377]] [[0.864]
 [1.042]
 [1.124]
 [1.18 ]
 [1.08 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.91321473308517
printing an ep nov before normalisation:  55.7719283880065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.298847507709276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.6348,     0.0005,     0.0009,     0.1900,     0.1738],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0575, 0.8850, 0.0048, 0.0012, 0.0515], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0075,     0.9820,     0.0029,     0.0075],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0242,     0.0002,     0.1505,     0.7206,     0.1046],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1137, 0.0014, 0.3432, 0.2940, 0.2477], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
STARTED EXPV TRAINING ON FRAME NO.  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.77821350097656
printing an ep nov before normalisation:  55.97344215183371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  27.756058792452563
printing an ep nov before normalisation:  25.338870365588583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.8717603789435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.54971307095587
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.64737484910806
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  3  action  0 :  tensor([    0.7144,     0.0228,     0.0007,     0.0690,     0.1930],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0045,     0.9902,     0.0001,     0.0000,     0.0052],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0005,     0.9907,     0.0032,     0.0056],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1264, 0.0008, 0.0044, 0.6048, 0.2636], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.4577, 0.0017, 0.0009, 0.1324, 0.4074], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.6087, 0.0043, 0.0011, 0.1310, 0.2549], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0778,     0.8001,     0.0001,     0.0004,     0.1216],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9975,     0.0011,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1680, 0.0012, 0.0065, 0.4960, 0.3284], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2153, 0.0699, 0.0150, 0.2715, 0.4284], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.70513636
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7166,     0.0032,     0.0005,     0.0876,     0.1920],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0088,     0.9536,     0.0012,     0.0007,     0.0357],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9715,     0.0128,     0.0154],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0743,     0.0003,     0.1731,     0.5036,     0.2487],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2439, 0.0007, 0.0559, 0.2426, 0.4569], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.859]
 [36.984]
 [36.984]
 [36.984]
 [36.984]] [[1.   ]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6778,     0.0006,     0.0005,     0.1767,     0.1445],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0054,     0.9872,     0.0000,     0.0002,     0.0072],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9988,     0.0009,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1906,     0.0005,     0.0034,     0.5862,     0.2192],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2715, 0.0006, 0.0100, 0.3675, 0.3503], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.29014950095062
siam score:  -0.70197713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.906]
 [24.906]
 [24.906]
 [24.906]
 [24.906]] [[0.731]
 [0.731]
 [0.731]
 [0.731]
 [0.731]]
siam score:  -0.7052444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.34972898942864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71459734
printing an ep nov before normalisation:  55.350422859191895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.69761085510254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  97.21665033226124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.03343224445318
printing an ep nov before normalisation:  47.709293365478516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  38.55746632838163
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.491]
 [30.491]
 [30.177]
 [30.491]
 [30.491]] [[1.359]
 [1.359]
 [1.333]
 [1.359]
 [1.359]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.243]
 [60.976]
 [42.516]
 [39.243]
 [39.243]] [[0.554]
 [1.138]
 [0.642]
 [0.554]
 [0.554]]
line 256 mcts: sample exp_bonus 49.95808207111579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.44 ]
 [48.763]
 [57.002]
 [48.763]
 [48.763]] [[1.069]
 [1.052]
 [1.267]
 [1.052]
 [1.052]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6893784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.41593647003174
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.9978289604187
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.82638019356125
printing an ep nov before normalisation:  65.8511757135436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.097418389641351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.793]
 [27.427]
 [43.225]
 [28.137]
 [27.13 ]] [[0.197]
 [0.274]
 [0.611]
 [0.289]
 [0.268]]
printing an ep nov before normalisation:  61.167593857498204
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.3757666437318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.344152107453574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.233]
 [31.528]
 [33.988]
 [27.293]
 [30.208]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.780327402539086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7044403
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.49981457634355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 0.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7172694
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.59477361043294
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.40118224097384
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.12140884963404
line 256 mcts: sample exp_bonus 38.73167334554264
siam score:  -0.7198595
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.131]
 [16.131]
 [16.131]
 [16.131]
 [16.131]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  35.186099227418765
printing an ep nov before normalisation:  26.62071650546173
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.284]
 [22.243]
 [30.881]
 [25.941]
 [17.432]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.753547543319783
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  80.75693585677362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.04246346880124
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.53601913258254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.9640347917590475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.096]
 [38.096]
 [45.653]
 [38.096]
 [38.096]] [[1.503]
 [1.503]
 [2.   ]
 [1.503]
 [1.503]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.483]
 [39.493]
 [64.778]
 [32.147]
 [29.861]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.48003625869751
printing an ep nov before normalisation:  49.1170393432101
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.91625118255615
siam score:  -0.7232483
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.264509201049805
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.33562056002201
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.413]
 [40.702]
 [46.803]
 [40.702]
 [40.702]] [[1.156]
 [1.284]
 [1.625]
 [1.284]
 [1.284]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.434355603982446
siam score:  -0.7254668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  25359 train batches done:  2969 episodes:  2225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.81339808872768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.625737054007395
printing an ep nov before normalisation:  63.23965105101336
printing an ep nov before normalisation:  62.034419734765116
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.617124693734308
printing an ep nov before normalisation:  66.91795554727065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.550165600246853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.075]
 [48.091]
 [46.924]
 [30.282]
 [29.935]] [[0.28 ]
 [0.846]
 [0.816]
 [0.388]
 [0.379]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.44529592186072
printing an ep nov before normalisation:  45.36458030483895
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.785]
 [82.488]
 [81.86 ]
 [83.623]
 [85.189]] [[1.182]
 [1.5  ]
 [1.487]
 [1.523]
 [1.555]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.370247634897
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.994]
 [34.994]
 [34.994]
 [34.994]
 [34.994]] [[1.336]
 [1.336]
 [1.336]
 [1.336]
 [1.336]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.2958930203605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.5293, 0.0007, 0.0015, 0.2103, 0.2582], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9968,     0.0000,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9973,     0.0011,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1382, 0.0024, 0.0008, 0.5787, 0.2798], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2400, 0.0025, 0.0019, 0.3061, 0.4495], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7339,     0.0202,     0.0002,     0.0559,     0.1898],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9985,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0003,     0.9965,     0.0011,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0963,     0.0006,     0.0367,     0.6385,     0.2279],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1466, 0.0687, 0.0112, 0.4460, 0.3276], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.92570680854078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.7552118007268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7415, 0.0066, 0.0050, 0.0653, 0.1816], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0013,     0.9978,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9662,     0.0188,     0.0149],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1795,     0.0005,     0.0015,     0.5706,     0.2480],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2082, 0.0036, 0.0060, 0.3255, 0.4568], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7186628
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([0.6164, 0.0380, 0.0008, 0.1179, 0.2269], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0013,     0.9979,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0018,     0.9804,     0.0038,     0.0141],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1101, 0.0013, 0.0385, 0.6323, 0.2177], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2321, 0.0341, 0.0107, 0.2548, 0.4683], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.54407024296992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.78333394257751
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.333412901907394
actions average: 
K:  1  action  0 :  tensor([    0.6967,     0.0001,     0.0005,     0.1252,     0.1775],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0022,     0.9952,     0.0002,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9939,     0.0024,     0.0037],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.1137,     0.0005,     0.0004,     0.5613,     0.3241],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2692, 0.0015, 0.0047, 0.3019, 0.4227], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.028753247486176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.061]
 [59.061]
 [59.061]
 [66.035]
 [59.061]] [[1.332]
 [1.332]
 [1.332]
 [1.667]
 [1.332]]
using explorer policy with actor:  1
siam score:  -0.7139862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7153704
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.823]
 [40.823]
 [39.704]
 [36.333]
 [27.091]] [[2.083]
 [2.083]
 [2.   ]
 [1.749]
 [1.06 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.35990729534916
printing an ep nov before normalisation:  18.296401814811716
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.388046109438424
siam score:  -0.7139121
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.74054064449154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.6709,     0.0003,     0.0006,     0.1166,     0.2116],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0063,     0.9838,     0.0025,     0.0001,     0.0073],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0013,     0.9936,     0.0036,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1103,     0.0006,     0.0008,     0.6831,     0.2052],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2516, 0.0241, 0.0165, 0.2953, 0.4124], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.756255646796724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  90.01409720798941
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.141]
 [78.357]
 [79.08 ]
 [78.74 ]
 [73.141]] [[1.622]
 [1.751]
 [1.769]
 [1.761]
 [1.622]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.585]
 [62.585]
 [89.419]
 [62.585]
 [62.585]] [[0.212]
 [0.212]
 [0.333]
 [0.212]
 [0.212]]
printing an ep nov before normalisation:  54.25895933792817
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.93862957457151
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.8089358573562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.71354806
printing an ep nov before normalisation:  42.663120452015306
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6464,     0.0178,     0.0005,     0.0920,     0.2434],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0029, 0.9487, 0.0013, 0.0174, 0.0296], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0037,     0.9673,     0.0078,     0.0210],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1661, 0.0007, 0.0006, 0.6068, 0.2258], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.3310, 0.0065, 0.0006, 0.2535, 0.4084], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.542 0.333 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.234]
 [40.794]
 [49.511]
 [32.824]
 [33.644]] [[0.464]
 [0.612]
 [0.846]
 [0.399]
 [0.421]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.083 0.    0.583 0.333 0.   ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.284]
 [63.284]
 [63.284]
 [71.079]
 [63.284]] [[1.731]
 [1.731]
 [1.731]
 [1.945]
 [1.731]]
actions average: 
K:  4  action  0 :  tensor([0.6185, 0.0319, 0.0024, 0.1576, 0.1895], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0699,     0.8886,     0.0000,     0.0000,     0.0414],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9802,     0.0056,     0.0140],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.1720,     0.0004,     0.0707,     0.5366,     0.2202],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2105, 0.0060, 0.0055, 0.3048, 0.4731], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.804997024809605
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.26253960590714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.40029275417328
actions average: 
K:  3  action  0 :  tensor([    0.8041,     0.0059,     0.0002,     0.0509,     0.1388],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0419,     0.8855,     0.0000,     0.0055,     0.0671],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9747,     0.0097,     0.0155],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1126, 0.0007, 0.0035, 0.5689, 0.3142], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1630, 0.0069, 0.0027, 0.3422, 0.4851], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.43251753742157
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.087801541425385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.433]
 [46.071]
 [45.384]
 [44.88 ]
 [43.995]] [[1.188]
 [1.29 ]
 [1.27 ]
 [1.256]
 [1.232]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7102972
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.28752728341342
printing an ep nov before normalisation:  29.974040985107422
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  50.49033698148025
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  13.493711948394775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.236]
 [43.373]
 [61.664]
 [54.439]
 [52.956]] [[0.735]
 [0.74 ]
 [1.366]
 [1.119]
 [1.068]]
printing an ep nov before normalisation:  36.21482710365148
printing an ep nov before normalisation:  41.99075358254569
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  45.82602295353944
printing an ep nov before normalisation:  42.967250275744995
printing an ep nov before normalisation:  56.5686437043853
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 71.71762239233915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.6214721505145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7105746
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.1835556119052
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0. ]
 [0. ]
 [0. ]
 [1.5]
 [0. ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0. ]
 [0. ]
 [0. ]
 [1.5]
 [0. ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.71436954
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.309]
 [64.309]
 [77.05 ]
 [80.729]
 [64.309]] [[0.405]
 [0.405]
 [0.53 ]
 [0.566]
 [0.405]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7092261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.99956893293297
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.6177,     0.2243,     0.0001,     0.0536,     0.1042],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0042,     0.9210,     0.0635,     0.0004,     0.0110],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0002,     0.9972,     0.0006,     0.0018],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1441, 0.0007, 0.0136, 0.4548, 0.3868], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1069, 0.0382, 0.0081, 0.3695, 0.4773], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.125 0.542 0.125 0.167 0.042]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337396830680685, 0.08337396830680685, 0.08337396830680685, 0.5831301584659657, 0.08337396830680685, 0.08337396830680685]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 38.932444837513856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337400827578101, 0.08337400827578101, 0.08337400827578101, 0.5831299586210951, 0.08337400827578101, 0.08337400827578101]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337400827578101, 0.08337400827578101, 0.08337400827578101, 0.5831299586210951, 0.08337400827578101, 0.08337400827578101]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833740282602537, 0.0833740282602537, 0.0833740282602537, 0.5831298586987315, 0.0833740282602537, 0.0833740282602537]
printing an ep nov before normalisation:  42.059006690979004
using explorer policy with actor:  1
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337404824471681, 0.08337404824471681, 0.08337404824471681, 0.5831297587764162, 0.08337404824471681, 0.08337404824471681]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337408821361425, 0.08337408821361425, 0.08337408821361425, 0.5831295589319289, 0.08337408821361425, 0.08337408821361425]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337408821361425, 0.08337408821361425, 0.08337408821361425, 0.5831295589319289, 0.08337408821361425, 0.08337408821361425]
printing an ep nov before normalisation:  10.905551969673866
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337410819804858, 0.08337410819804858, 0.08337410819804858, 0.5831294590097572, 0.08337410819804858, 0.08337410819804858]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337416815129405, 0.08337416815129405, 0.08337416815129405, 0.5831291592435298, 0.08337416815129405, 0.08337416815129405]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337420812007641, 0.08337420812007641, 0.08337420812007641, 0.5831289593996182, 0.08337420812007641, 0.08337420812007641]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337436799482226, 0.08337436799482226, 0.08337436799482226, 0.5831281600258887, 0.08337436799482226, 0.08337436799482226]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337436799482226, 0.08337436799482226, 0.08337436799482226, 0.5831281600258887, 0.08337436799482226, 0.08337436799482226]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337438797912235, 0.08337438797912235, 0.08337438797912235, 0.5831280601043883, 0.08337438797912235, 0.08337438797912235]
printing an ep nov before normalisation:  53.62143516540527
printing an ep nov before normalisation:  42.93993881770543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337440796341283, 0.08337440796341283, 0.08337440796341283, 0.583127960182936, 0.08337440796341283, 0.08337440796341283]
from probs:  [0.08337440796341283, 0.08337440796341283, 0.08337440796341283, 0.583127960182936, 0.08337440796341283, 0.08337440796341283]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337444793196504, 0.08337444793196504, 0.08337444793196504, 0.5831277603401749, 0.08337444793196504, 0.08337444793196504]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337448790047888, 0.08337448790047888, 0.08337448790047888, 0.5831275604976056, 0.08337448790047888, 0.08337448790047888]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  30.95116324774331
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337450788472142, 0.08337450788472142, 0.08337450788472142, 0.5831274605763929, 0.08337450788472142, 0.08337450788472142]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337452786895437, 0.08337452786895437, 0.08337452786895437, 0.5831273606552282, 0.08337452786895437, 0.08337452786895437]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337452786895437, 0.08337452786895437, 0.08337452786895437, 0.5831273606552282, 0.08337452786895437, 0.08337452786895437]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337454785317773, 0.08337454785317773, 0.08337454785317773, 0.5831272607341114, 0.08337454785317773, 0.08337454785317773]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337454785317773, 0.08337454785317773, 0.08337454785317773, 0.5831272607341114, 0.08337454785317773, 0.08337454785317773]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337454785317773, 0.08337454785317773, 0.08337454785317773, 0.5831272607341114, 0.08337454785317773, 0.08337454785317773]
siam score:  -0.6888492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337454785317773, 0.08337454785317773, 0.08337454785317773, 0.5831272607341114, 0.08337454785317773, 0.08337454785317773]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337456783739149, 0.08337456783739149, 0.08337456783739149, 0.5831271608130425, 0.08337456783739149, 0.08337456783739149]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337458782159568, 0.08337458782159568, 0.08337458782159568, 0.5831270608920215, 0.08337458782159568, 0.08337458782159568]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337458782159568, 0.08337458782159568, 0.08337458782159568, 0.5831270608920215, 0.08337458782159568, 0.08337458782159568]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337458782159568, 0.08337458782159568, 0.08337458782159568, 0.5831270608920215, 0.08337458782159568, 0.08337458782159568]
siam score:  -0.6943044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337460780579029, 0.08337460780579029, 0.08337460780579029, 0.5831269609710487, 0.08337460780579029, 0.08337460780579029]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337460780579029, 0.08337460780579029, 0.08337460780579029, 0.5831269609710487, 0.08337460780579029, 0.08337460780579029]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337462778997527, 0.08337462778997527, 0.08337462778997527, 0.5831268610501236, 0.08337462778997527, 0.08337462778997527]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833746477741507, 0.0833746477741507, 0.0833746477741507, 0.5831267611292466, 0.0833746477741507, 0.0833746477741507]
from probs:  [0.0833746677583165, 0.0833746677583165, 0.0833746677583165, 0.5831266612084174, 0.0833746677583165, 0.0833746677583165]
siam score:  -0.6875758
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.292 0.208 0.083]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833747676790018, 0.0833747676790018, 0.0833747676790018, 0.5831261616049911, 0.0833747676790018, 0.0833747676790018]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0833747676790018, 0.0833747676790018, 0.0833747676790018, 0.5831261616049911, 0.0833747676790018, 0.0833747676790018]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337482763129789, 0.08337482763129789, 0.08337482763129789, 0.5831258618435107, 0.08337482763129789, 0.08337482763129789]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[22.125]
 [22.125]
 [28.838]
 [32.407]
 [22.125]] [[0.764]
 [0.764]
 [0.996]
 [1.12 ]
 [0.764]]
printing an ep nov before normalisation:  55.95569290334842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337490756755843, 0.08337490756755843, 0.08337490756755843, 0.583125462162208, 0.08337490756755843, 0.08337490756755843]
printing an ep nov before normalisation:  78.61913391623567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337492755159959, 0.08337492755159959, 0.08337492755159959, 0.5831253622420022, 0.08337492755159959, 0.08337492755159959]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337492755159959, 0.08337492755159959, 0.08337492755159959, 0.5831253622420022, 0.08337492755159959, 0.08337492755159959]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
from probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
siam score:  -0.68053436
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.942]
 [31.994]
 [38.929]
 [22.649]
 [22.91 ]] [[0.135]
 [0.226]
 [0.33 ]
 [0.086]
 [0.09 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
using another actor
from probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337494753563116, 0.08337494753563116, 0.08337494753563116, 0.5831252623218443, 0.08337494753563116, 0.08337494753563116]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337504745564515, 0.08337504745564515, 0.08337504745564515, 0.5831247627217742, 0.08337504745564515, 0.08337504745564515]
from probs:  [0.08337504745564515, 0.08337504745564515, 0.08337504745564515, 0.5831247627217742, 0.08337504745564515, 0.08337504745564515]
printing an ep nov before normalisation:  26.914015854010813
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833750674396192, 0.0833750674396192, 0.0833750674396192, 0.583124662801904, 0.0833750674396192, 0.0833750674396192]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337510740753849, 0.08337510740753849, 0.08337510740753849, 0.5831244629623075, 0.08337510740753849, 0.08337510740753849]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337516735934551, 0.08337516735934551, 0.08337516735934551, 0.5831241632032723, 0.08337516735934551, 0.08337516735934551]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337516735934551, 0.08337516735934551, 0.08337516735934551, 0.5831241632032723, 0.08337516735934551, 0.08337516735934551]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337516735934551, 0.08337516735934551, 0.08337516735934551, 0.5831241632032723, 0.08337516735934551, 0.08337516735934551]
printing an ep nov before normalisation:  40.05398750305176
from probs:  [0.08337518734326203, 0.08337518734326203, 0.08337518734326203, 0.5831240632836899, 0.08337518734326203, 0.08337518734326203]
siam score:  -0.69108856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337518734326203, 0.08337518734326203, 0.08337518734326203, 0.5831240632836899, 0.08337518734326203, 0.08337518734326203]
siam score:  -0.69069993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337520732716892, 0.08337520732716892, 0.08337520732716892, 0.5831239633641553, 0.08337520732716892, 0.08337520732716892]
actions average: 
K:  3  action  0 :  tensor([0.6085, 0.0009, 0.0016, 0.0648, 0.3241], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0347,     0.9271,     0.0090,     0.0001,     0.0291],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0050, 0.0127, 0.9235, 0.0147, 0.0441], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0867, 0.0007, 0.0015, 0.5753, 0.3357], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1986, 0.0128, 0.0984, 0.3027, 0.3875], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.57728385925293
printing an ep nov before normalisation:  64.19374379706474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337532723040898, 0.08337532723040898, 0.08337532723040898, 0.583123363847955, 0.08337532723040898, 0.08337532723040898]
printing an ep nov before normalisation:  77.86963995383188
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337536719807898, 0.08337536719807898, 0.08337536719807898, 0.5831231640096051, 0.08337536719807898, 0.08337536719807898]
siam score:  -0.7025984
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337536719807898, 0.08337536719807898, 0.08337536719807898, 0.5831231640096051, 0.08337536719807898, 0.08337536719807898]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337536719807898, 0.08337536719807898, 0.08337536719807898, 0.5831231640096051, 0.08337536719807898, 0.08337536719807898]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833754071657106, 0.0833754071657106, 0.0833754071657106, 0.583122964171447, 0.0833754071657106, 0.0833754071657106]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337542714951203, 0.08337542714951203, 0.08337542714951203, 0.58312286425244, 0.08337542714951203, 0.08337542714951203]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337546711708611, 0.08337546711708611, 0.08337546711708611, 0.5831226644145695, 0.08337546711708611, 0.08337546711708611]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.432]
 [42.581]
 [24.432]
 [24.432]
 [24.432]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337548710085876, 0.08337548710085876, 0.08337548710085876, 0.5831225644957062, 0.08337548710085876, 0.08337548710085876]
siam score:  -0.7073262
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.099]
 [76.099]
 [76.099]
 [76.099]
 [76.099]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  12.066333293914795
from probs:  [0.08337550708462184, 0.08337550708462184, 0.08337550708462184, 0.5831224645768909, 0.08337550708462184, 0.08337550708462184]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337552706837532, 0.08337552706837532, 0.08337552706837532, 0.5831223646581235, 0.08337552706837532, 0.08337552706837532]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337552706837532, 0.08337552706837532, 0.08337552706837532, 0.5831223646581235, 0.08337552706837532, 0.08337552706837532]
Printing some Q and Qe and total Qs values:  [[0. ]
 [0. ]
 [1.5]
 [0. ]
 [0. ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0. ]
 [0. ]
 [1.5]
 [0. ]
 [0. ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833755670358535, 0.0833755670358535, 0.0833755670358535, 0.5831221648207324, 0.0833755670358535, 0.0833755670358535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833755670358535, 0.0833755670358535, 0.0833755670358535, 0.5831221648207324, 0.0833755670358535, 0.0833755670358535]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833755670358535, 0.0833755670358535, 0.0833755670358535, 0.5831221648207324, 0.0833755670358535, 0.0833755670358535]
from probs:  [0.0833755670358535, 0.0833755670358535, 0.0833755670358535, 0.5831221648207324, 0.0833755670358535, 0.0833755670358535]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337560700329333, 0.08337560700329333, 0.08337560700329333, 0.5831219649835333, 0.08337560700329333, 0.08337560700329333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337560700329333, 0.08337560700329333, 0.08337560700329333, 0.5831219649835333, 0.08337560700329333, 0.08337560700329333]
printing an ep nov before normalisation:  35.29789795497029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337562698699888, 0.08337562698699888, 0.08337562698699888, 0.5831218650650057, 0.08337562698699888, 0.08337562698699888]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337564697069481, 0.08337564697069481, 0.08337564697069481, 0.583121765146526, 0.08337564697069481, 0.08337564697069481]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337566695438116, 0.08337566695438116, 0.08337566695438116, 0.5831216652280942, 0.08337566695438116, 0.08337566695438116]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337568693805794, 0.08337568693805794, 0.08337568693805794, 0.5831215653097105, 0.08337568693805794, 0.08337568693805794]
printing an ep nov before normalisation:  62.854620774765145
actions average: 
K:  3  action  0 :  tensor([0.5803, 0.0043, 0.0009, 0.1525, 0.2620], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0058, 0.9254, 0.0052, 0.0012, 0.0624], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0375,     0.9513,     0.0057,     0.0053],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1008, 0.0029, 0.0136, 0.5938, 0.2889], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1719, 0.0008, 0.0607, 0.2701, 0.4966], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757069217251, 0.0833757069217251, 0.0833757069217251, 0.5831214653913744, 0.0833757069217251, 0.0833757069217251]
printing an ep nov before normalisation:  39.99598503112793
printing an ep nov before normalisation:  0.0774808791607029
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757069217251, 0.0833757069217251, 0.0833757069217251, 0.5831214653913744, 0.0833757069217251, 0.0833757069217251]
actions average: 
K:  0  action  0 :  tensor([0.5570, 0.0027, 0.0008, 0.1682, 0.2713], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0063,     0.9905,     0.0000,     0.0001,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0074, 0.0123, 0.9538, 0.0026, 0.0239], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1406,     0.0002,     0.0005,     0.6419,     0.2168],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.3810, 0.0144, 0.0016, 0.1929, 0.4101], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337572690538268, 0.08337572690538268, 0.08337572690538268, 0.5831213654730866, 0.08337572690538268, 0.08337572690538268]
printing an ep nov before normalisation:  66.31128543364792
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833757468890307, 0.0833757468890307, 0.0833757468890307, 0.5831212655548467, 0.0833757468890307, 0.0833757468890307]
using another actor
siam score:  -0.71659577
actions average: 
K:  1  action  0 :  tensor([    0.8084,     0.0236,     0.0003,     0.0283,     0.1394],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9991,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0006,     0.9932,     0.0034,     0.0028],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1198, 0.0022, 0.0325, 0.6218, 0.2236], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2297, 0.0032, 0.0036, 0.3859, 0.3776], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.67235491010878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833758667907173, 0.0833758667907173, 0.0833758667907173, 0.5831206660464135, 0.0833758667907173, 0.0833758667907173]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833758667907173, 0.0833758667907173, 0.0833758667907173, 0.5831206660464135, 0.0833758667907173, 0.0833758667907173]
siam score:  -0.7013617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337592674143116, 0.08337592674143116, 0.08337592674143116, 0.5831203662928442, 0.08337592674143116, 0.08337592674143116]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337596670852578, 0.08337596670852578, 0.08337596670852578, 0.5831201664573712, 0.08337596670852578, 0.08337596670852578]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833759866920587, 0.0833759866920587, 0.0833759866920587, 0.5831200665397065, 0.0833759866920587, 0.0833759866920587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833759866920587, 0.0833759866920587, 0.0833759866920587, 0.5831200665397065, 0.0833759866920587, 0.0833759866920587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833759866920587, 0.0833759866920587, 0.0833759866920587, 0.5831200665397065, 0.0833759866920587, 0.0833759866920587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833759866920587, 0.0833759866920587, 0.0833759866920587, 0.5831200665397065, 0.0833759866920587, 0.0833759866920587]
siam score:  -0.6936206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337602665909581, 0.08337602665909581, 0.08337602665909581, 0.5831198667045211, 0.08337602665909581, 0.08337602665909581]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.74419019387564
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337608660957951, 0.08337608660957951, 0.08337608660957951, 0.5831195669521024, 0.08337608660957951, 0.08337608660957951]
printing an ep nov before normalisation:  39.896652587578885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337608660957951, 0.08337608660957951, 0.08337608660957951, 0.5831195669521024, 0.08337608660957951, 0.08337608660957951]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.006]
 [44.999]
 [66.432]
 [63.273]
 [62.729]] [[1.096]
 [0.809]
 [1.22 ]
 [1.159]
 [1.149]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337610659305489, 0.08337610659305489, 0.08337610659305489, 0.5831194670347255, 0.08337610659305489, 0.08337610659305489]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833761265765207, 0.0833761265765207, 0.0833761265765207, 0.5831193671173965, 0.0833761265765207, 0.0833761265765207]
using another actor
siam score:  -0.6977005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337620651028804, 0.08337620651028804, 0.08337620651028804, 0.58311896744856, 0.08337620651028804, 0.08337620651028804]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
printing an ep nov before normalisation:  48.19939612598181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
from probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
printing an ep nov before normalisation:  42.586439203157795
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0833762264937059, 0.0833762264937059, 0.0833762264937059, 0.5831188675314706, 0.0833762264937059, 0.0833762264937059]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337624647711415, 0.08337624647711415, 0.08337624647711415, 0.5831187676144292, 0.08337624647711415, 0.08337624647711415]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.481]
 [51.481]
 [62.94 ]
 [51.481]
 [51.481]] [[0.247]
 [0.247]
 [0.333]
 [0.247]
 [0.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337624647711415, 0.08337624647711415, 0.08337624647711415, 0.5831187676144292, 0.08337624647711415, 0.08337624647711415]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337624647711415, 0.08337624647711415, 0.08337624647711415, 0.5831187676144292, 0.08337624647711415, 0.08337624647711415]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337624647711415, 0.08337624647711415, 0.08337624647711415, 0.5831187676144292, 0.08337624647711415, 0.08337624647711415]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337626646051284, 0.08337626646051284, 0.08337626646051284, 0.5831186676974358, 0.08337626646051284, 0.08337626646051284]
using explorer policy with actor:  0
printing an ep nov before normalisation:  23.835724337345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337632641065135, 0.08337632641065135, 0.08337632641065135, 0.5831183679467433, 0.08337632641065135, 0.08337632641065135]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.22266784942632
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[65.947]
 [65.947]
 [74.146]
 [69.985]
 [65.947]] [[1.027]
 [1.027]
 [1.195]
 [1.11 ]
 [1.027]]
from probs:  [0.08337648627726545, 0.08337648627726545, 0.08337648627726545, 0.5831175686136727, 0.08337648627726545, 0.08337648627726545]
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 15.335146634054835
printing an ep nov before normalisation:  44.955017502869026
using another actor
using another actor
printing an ep nov before normalisation:  53.93806878881339
siam score:  -0.7219707
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337662616004937, 0.08337662616004937, 0.08337662616004937, 0.583116869199753, 0.08337662616004937, 0.08337662616004937]
siam score:  -0.73279434
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  46.23464107513428
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  44.49288845062256
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([0.5914, 0.0372, 0.0012, 0.1411, 0.2291], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9992,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.1317, 0.0009, 0.0013, 0.6057, 0.2604], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2350, 0.0016, 0.0042, 0.2502, 0.5089], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  43.97341011977762
using explorer policy with actor:  1
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  31.796445082216472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  34.80471240149604
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.55]
 [50.55]
 [50.55]
 [50.55]
 [50.55]] [[1.22]
 [1.22]
 [1.22]
 [1.22]
 [1.22]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  2  action  0 :  tensor([0.7020, 0.0031, 0.0007, 0.1481, 0.1460], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0413,     0.9359,     0.0031,     0.0000,     0.0196],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0177,     0.9547,     0.0062,     0.0214],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0468, 0.0022, 0.0086, 0.7174, 0.2250], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0786, 0.2047, 0.0589, 0.2221, 0.4357], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
using another actor
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  73.3493142624968
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  50.56044961573672
printing an ep nov before normalisation:  45.107324018722394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actions average: 
K:  0  action  0 :  tensor([0.6551, 0.0012, 0.0010, 0.1051, 0.2376], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9969,     0.0002,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0006,     0.9508,     0.0031,     0.0453],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0959, 0.0006, 0.0048, 0.5268, 0.3718], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2309, 0.0082, 0.0623, 0.1224, 0.5762], grad_fn=<DivBackward0>)
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.73070955
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[0]
 [0]
 [0]
 [0]
 [0]] [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]]
siam score:  -0.7250156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([0.7099, 0.0707, 0.0013, 0.0524, 0.1657], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9580,     0.0133,     0.0003,     0.0278],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0006,     0.9965,     0.0009,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1172, 0.0022, 0.0151, 0.5416, 0.3239], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1717, 0.0030, 0.0251, 0.2498, 0.5504], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
siam score:  -0.7203272
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  48.39884945368061
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0001],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0001],
        [    0.0001],
        [    0.0000],
        [    0.0001],
        [    0.0000]], dtype=torch.float64)
0.0 0.00012550683470563686
0.0 0.00014171429952163764
0.0 0.0
0.0 0.0
0.0 0.0
0.0 7.156513423011063e-05
0.0 5.774648785378105e-05
0.970299 0.970299
0.0 0.00012550683470563686
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.452]
 [42.408]
 [55.586]
 [44.143]
 [46.798]] [[0.115]
 [0.164]
 [0.272]
 [0.178]
 [0.2  ]]
siam score:  -0.7170897
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
UNIT TEST: sample policy line 217 mcts : [0.042 0.083 0.75  0.083 0.042]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
printing an ep nov before normalisation:  58.289683048285724
printing an ep nov before normalisation:  46.575974619530484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706, 0.08337664614326587, 0.08337664614326587]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.18247064823107
printing an ep nov before normalisation:  16.827843388836072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  1
siam score:  -0.73824096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
actions average: 
K:  2  action  0 :  tensor([    0.7792,     0.0195,     0.0001,     0.0708,     0.1304],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0052,     0.9582,     0.0317,     0.0000,     0.0049],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0026,     0.8232,     0.0993,     0.0749],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1280, 0.0007, 0.0056, 0.6160, 0.2496], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1993, 0.0349, 0.0477, 0.1257, 0.5926], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.007]
 [0.   ]
 [0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.   ]
 [0.   ]
 [0.007]
 [0.   ]
 [0.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.7034,     0.0010,     0.0005,     0.0897,     0.2054],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0027,     0.9957,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9986,     0.0007,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0678,     0.0003,     0.0261,     0.7222,     0.1836],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0726, 0.0023, 0.0268, 0.2993, 0.5990], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
actions average: 
K:  1  action  0 :  tensor([    0.7448,     0.0094,     0.0002,     0.0528,     0.1928],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0035,     0.9708,     0.0002,     0.0000,     0.0254],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0010,     0.9912,     0.0027,     0.0051],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.1255, 0.0008, 0.0470, 0.5921, 0.2346], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1347, 0.0182, 0.0874, 0.0714, 0.6884], grad_fn=<DivBackward0>)
using another actor
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  61.89258424537234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7504035
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
printing an ep nov before normalisation:  52.012851372702514
maxi score, test score, baseline:  0.0001 0.0 0.0001
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  86.76070249491438
printing an ep nov before normalisation:  45.56617734020628
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.08337165004067144, 0.08337165004067144, 0.08337165004067144, 0.5831417497966429, 0.08337165004067144, 0.08337165004067144]
actions average: 
K:  4  action  0 :  tensor([    0.9279,     0.0474,     0.0000,     0.0014,     0.0234],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0154,     0.9629,     0.0087,     0.0001,     0.0129],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0010,     0.9883,     0.0013,     0.0092],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0041,     0.0002,     0.0991,     0.7568,     0.1398],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0660, 0.0500, 0.1860, 0.2590, 0.4389], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.96074718872777
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.896]
 [0.409]
 [0.43 ]
 [0.295]] [[22.62 ]
 [19.453]
 [25.596]
 [22.62 ]
 [24.678]] [[1.634]
 [1.927]
 [1.775]
 [1.634]
 [1.612]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05124936625817207, 0.05124936625817207, 0.2459862688579299, 0.3581547247553905, 0.24211090761216358, 0.05124936625817207]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05124936625817207, 0.05124936625817207, 0.2459862688579299, 0.3581547247553905, 0.24211090761216358, 0.05124936625817207]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051298862868251284, 0.051298862868251284, 0.24573085025590927, 0.3585014029407509, 0.24187115819858598, 0.051298862868251284]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051298862868251284, 0.051298862868251284, 0.24573085025590927, 0.3585014029407509, 0.24187115819858598, 0.051298862868251284]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051298862868251284, 0.051298862868251284, 0.24573085025590927, 0.3585014029407509, 0.24187115819858598, 0.051298862868251284]
from probs:  [0.051324017134947754, 0.051324017134947754, 0.24536036074993997, 0.35867758542109546, 0.24199000242412122, 0.051324017134947754]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051324017134947754, 0.051324017134947754, 0.24536036074993997, 0.35867758542109546, 0.24199000242412122, 0.051324017134947754]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051348207151440825, 0.051348207151440825, 0.24547624191472714, 0.3588470142164864, 0.24163212241446408, 0.051348207151440825]
actions average: 
K:  2  action  0 :  tensor([    0.8333,     0.0235,     0.0002,     0.0519,     0.0912],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0154,     0.9482,     0.0104,     0.0003,     0.0257],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0040,     0.9444,     0.0157,     0.0357],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0881,     0.0002,     0.0189,     0.8388,     0.0539],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1442, 0.0375, 0.0453, 0.1818, 0.5912], grad_fn=<DivBackward0>)
from probs:  [0.051348207151440825, 0.051348207151440825, 0.24547624191472714, 0.3588470142164864, 0.24163212241446408, 0.051348207151440825]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05139825710711221, 0.05139825710711221, 0.2447395328427775, 0.3591975680782914, 0.2418681277575944, 0.05139825710711221]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05142239749464434, 0.05142239749464434, 0.24485471644807189, 0.3593666492685011, 0.24151144179949413, 0.05142239749464434]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05142239749464434, 0.05142239749464434, 0.24485471644807189, 0.3593666492685011, 0.24151144179949413, 0.05142239749464434]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051446441541993154, 0.051446441541993154, 0.24496944037517615, 0.3595350556844206, 0.24115617931442382, 0.051446441541993154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.051446441541993154, 0.051446441541993154, 0.24496944037517615, 0.3595350556844206, 0.24115617931442382, 0.051446441541993154]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  17.859100680234025
from probs:  [0.05149533304102938, 0.05149533304102938, 0.24471723942134324, 0.3598774956240104, 0.24091926583155823, 0.05149533304102938]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  68.81577087616934
actions average: 
K:  3  action  0 :  tensor([0.8044, 0.0038, 0.0051, 0.0639, 0.1228], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0393,     0.9333,     0.0030,     0.0000,     0.0245],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0051,     0.9713,     0.0055,     0.0176],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0720,     0.0002,     0.0179,     0.6977,     0.2121],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1974, 0.0040, 0.0149, 0.1102, 0.6734], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  9.66013948970174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05169083206749911, 0.05169083206749911, 0.24277475444648097, 0.3612467863214499, 0.24090596302957165, 0.05169083206749911]
printing an ep nov before normalisation:  75.19225610424331
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05173897486838826, 0.05173897486838826, 0.24253023561432904, 0.36158398231979616, 0.24066885746071015, 0.05173897486838826]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05176319704913267, 0.05176319704913267, 0.24217501482985962, 0.36175363639615626, 0.24078175762658602, 0.05176319704913267]
printing an ep nov before normalisation:  69.4337463238049
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.001]
 [0.006]
 [0.004]] [[31.616]
 [34.34 ]
 [36.895]
 [45.212]
 [34.831]] [[0.638]
 [0.709]
 [0.77 ]
 [0.993]
 [0.72 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05178697156812729, 0.05178697156812729, 0.24228646999930922, 0.3619201550140916, 0.2404324602822174, 0.05178697156812729]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05181111982401057, 0.05181111982401057, 0.24193254843359788, 0.3620892913148571, 0.2405448007795133, 0.05181111982401057]
from probs:  [0.0518825293165489, 0.0518825293165489, 0.2418011844214131, 0.36258944906810675, 0.2399617785608333, 0.0518825293165489]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05195402018740562, 0.05195402018740562, 0.24120958990404281, 0.36309017680155725, 0.23983817273218316, 0.05195402018740562]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[73.958]
 [73.958]
 [73.958]
 [73.958]
 [73.958]] [[1.335]
 [1.335]
 [1.335]
 [1.335]
 [1.335]]
from probs:  [0.05200136650160622, 0.05200136650160622, 0.24097007259524483, 0.3634217941439227, 0.23960403375601375, 0.05200136650160622]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05202478665314829, 0.05202478665314829, 0.24107881826173272, 0.3635858307440955, 0.23926099103472706, 0.05202478665314829]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05202478665314829, 0.05202478665314829, 0.24107881826173272, 0.3635858307440955, 0.23926099103472706, 0.05202478665314829]
printing an ep nov before normalisation:  55.499699731918795
printing an ep nov before normalisation:  66.78918139720014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05204811548526986, 0.05204811548526986, 0.2411871399091348, 0.3637492277357993, 0.23891928589925626, 0.05204811548526986]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.003]
 [0.   ]
 [0.001]
 [0.001]] [[26.666]
 [33.035]
 [33.67 ]
 [33.006]
 [32.591]] [[0.847]
 [1.348]
 [1.396]
 [1.345]
 [1.312]]
siam score:  -0.8114468
printing an ep nov before normalisation:  54.135043151110594
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.856329917907715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05218888574039327, 0.05218888574039327, 0.24046979419503947, 0.364735193775106, 0.23822835480867463, 0.05218888574039327]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05218888574039327, 0.05218888574039327, 0.24046979419503947, 0.364735193775106, 0.23822835480867463, 0.05218888574039327]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.052235973512359854, 0.052235973512359854, 0.23978335166271048, 0.3650650002671448, 0.23844372753306511, 0.052235973512359854]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05230492774701196, 0.05230492774701196, 0.24010051135149785, 0.36554796119929445, 0.2374367442081718, 0.05230492774701196]
from probs:  [0.05230492774701196, 0.05230492774701196, 0.24010051135149785, 0.36554796119929445, 0.2374367442081718, 0.05230492774701196]
printing an ep nov before normalisation:  44.309343510763796
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05232773575303101, 0.05232773575303101, 0.24020541832533873, 0.3657077102836403, 0.237103664131928, 0.05232773575303101]
printing an ep nov before normalisation:  71.87426477627176
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.059756473181187396, 0.059756473181187396, 0.20249850853509843, 0.41775349784879634, 0.2004785740725431, 0.059756473181187396]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.003]
 [0.003]
 [0.002]
 [0.002]] [[17.353]
 [12.565]
 [34.917]
 [ 9.674]
 [ 8.792]] [[0.002]
 [0.003]
 [0.003]
 [0.002]
 [0.002]]
printing an ep nov before normalisation:  36.448778736825304
printing an ep nov before normalisation:  52.76360052467769
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05977680840113344, 0.05977680840113344, 0.20222673832414634, 0.4178959322275879, 0.20054690424486551, 0.05977680840113344]
actor:  0 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059817560496610356, 0.059817560496610356, 0.2016864129204961, 0.4181782817193457, 0.2006826238703271, 0.059817560496610356]
from probs:  [0.059817560496610356, 0.059817560496610356, 0.2016864129204961, 0.4181782817193457, 0.2006826238703271, 0.059817560496610356]
printing an ep nov before normalisation:  34.02442932128906
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059837365097583355, 0.059837365097583355, 0.20175329278957493, 0.41831699844755416, 0.20041761347012088, 0.059837365097583355]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.   ]
 [0.002]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.   ]
 [0.002]
 [0.003]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05985746604896297, 0.05985746604896297, 0.20148476654043215, 0.418457790893363, 0.2004850444193159, 0.05985746604896297]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  31.293561410955455
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.059937058830000514, 0.059937058830000514, 0.200752047541361, 0.4190152800439697, 0.20042149592466765, 0.059937058830000514]
from probs:  [0.059937058830000514, 0.059937058830000514, 0.200752047541361, 0.4190152800439697, 0.20042149592466765, 0.059937058830000514]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.05999628429776345, 0.05999628429776345, 0.20029051801398864, 0.4194301110787325, 0.20029051801398864, 0.05999628429776345]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[27.576]
 [ 0.   ]
 [ 0.   ]
 [51.952]
 [26.644]] [[ 0.128]
 [-0.082]
 [-0.082]
 [ 0.314]
 [ 0.121]]
printing an ep nov before normalisation:  50.35619258880615
printing an ep nov before normalisation:  78.14684799625142
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0600354948616635, 0.0600354948616635, 0.2004216219131352, 0.41970475236753396, 0.1997671411343404, 0.0600354948616635]
printing an ep nov before normalisation:  31.97851646364912
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06005516527905213, 0.06005516527905213, 0.20015927897022645, 0.4198425292379878, 0.1998326959546293, 0.06005516527905213]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.23552295139858
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06011343328090044, 0.06011343328090044, 0.20035378399904977, 0.42025065392510796, 0.19905526223314096, 0.06011343328090044]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06011343328090044, 0.06011343328090044, 0.20035378399904977, 0.42025065392510796, 0.19905526223314096, 0.06011343328090044]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06015234371238261, 0.06015234371238261, 0.20015655866720267, 0.4205231930060894, 0.19886321718955996, 0.06015234371238261]
printing an ep nov before normalisation:  75.78017373867547
printing an ep nov before normalisation:  28.581507205963135
using explorer policy with actor:  1
from probs:  [0.060171907416773515, 0.060171907416773515, 0.19989606052272682, 0.4206602224301331, 0.19892799479681955, 0.060171907416773515]
printing an ep nov before normalisation:  43.81511448706092
siam score:  -0.8098385
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06022932969478656, 0.06022932969478656, 0.200087117968534, 0.4210624234410551, 0.1981624695060513, 0.06022932969478656]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06024885258244345, 0.06024885258244345, 0.1998274737701159, 0.4211991669737644, 0.19822680150878935, 0.06024885258244345]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06032617092240725, 0.06032617092240725, 0.19879918027400342, 0.4217407253300733, 0.1984815816287016, 0.06032617092240725]
actions average: 
K:  3  action  0 :  tensor([    0.8055,     0.0485,     0.0003,     0.0360,     0.1097],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9993,     0.0004,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0036,     0.9644,     0.0007,     0.0313],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0277, 0.0009, 0.0358, 0.8332, 0.1025], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1199, 0.0378, 0.0515, 0.0926, 0.6982], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06040244777186634, 0.06040244777186634, 0.1984163840938134, 0.42227498880801156, 0.19810128378257608, 0.06040244777186634]
printing an ep nov before normalisation:  0.06425394978805343
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  49.78098311495083
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.012]
 [0.009]
 [0.009]] [[33.027]
 [51.448]
 [63.116]
 [51.448]
 [51.448]] [[0.162]
 [0.51 ]
 [0.733]
 [0.51 ]
 [0.51 ]]
actions average: 
K:  4  action  0 :  tensor([    0.7736,     0.0000,     0.0002,     0.1106,     0.1155],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0010,     0.9920,     0.0059,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0023,     0.9291,     0.0154,     0.0529],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0474,     0.0000,     0.0019,     0.8732,     0.0774],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1774, 0.0146, 0.1936, 0.1147, 0.4997], grad_fn=<DivBackward0>)
siam score:  -0.8164719
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06044039715026766, 0.06044039715026766, 0.19791207492764434, 0.42254079641587783, 0.19822593720567486, 0.06044039715026766]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06045926023102416, 0.06045926023102416, 0.19766140350881156, 0.422672918484383, 0.198287897313733, 0.06045926023102416]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06049702240190013, 0.06049702240190013, 0.19747373912099073, 0.42293741484061387, 0.198097778832695, 0.06049702240190013]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06053440467676306, 0.06053440467676306, 0.19697716074398772, 0.4231992503034463, 0.19822037492227676, 0.06053440467676306]
printing an ep nov before normalisation:  14.220581935007194
using another actor
from probs:  [0.06053440467676306, 0.06053440467676306, 0.19697716074398772, 0.4231992503034463, 0.19822037492227676, 0.06053440467676306]
siam score:  -0.8174954
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06057149663724117, 0.06057149663724117, 0.1964844388429902, 0.42345905232659115, 0.19834201891869518, 0.06057149663724117]
printing an ep nov before normalisation:  21.06648151723398
using another actor
from probs:  [0.06057149663724117, 0.06057149663724117, 0.1964844388429902, 0.42345905232659115, 0.19834201891869518, 0.06057149663724117]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.47448514274886
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.   ]
 [0.002]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.   ]
 [0.002]
 [0.003]]
printing an ep nov before normalisation:  57.011844135844605
siam score:  -0.81986237
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.060627774158214055, 0.060627774158214055, 0.1963622513609282, 0.42385323515267714, 0.19790119101175258, 0.060627774158214055]
printing an ep nov before normalisation:  59.722439448038735
from probs:  [0.06066448286958135, 0.06066448286958135, 0.19587499995546212, 0.4241103527964289, 0.198021198639365, 0.06066448286958135]
actions average: 
K:  2  action  0 :  tensor([0.7551, 0.0084, 0.0010, 0.0603, 0.1752], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0009,     0.9978,     0.0010,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9462,     0.0028,     0.0509],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0298, 0.0050, 0.0132, 0.8472, 0.1048], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0785, 0.0056, 0.0388, 0.2332, 0.6439], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.006]
 [0.004]
 [0.004]
 [0.004]] [[37.549]
 [52.287]
 [37.549]
 [37.549]
 [37.549]] [[0.984]
 [1.673]
 [0.984]
 [0.984]
 [0.984]]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06072034386609593, 0.06072034386609593, 0.1957542244553988, 0.42450161817367793, 0.19758312577263543, 0.06072034386609593]
using another actor
from probs:  [0.060757252467632424, 0.060757252467632424, 0.19557313522099026, 0.42476013590169853, 0.19739497147441404, 0.060757252467632424]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06079346709692986, 0.06079346709692986, 0.19509299724641543, 0.4250137928623347, 0.19751280860046025, 0.06079346709692986]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06079346709692986, 0.06079346709692986, 0.19509299724641543, 0.4250137928623347, 0.19751280860046025, 0.06079346709692986]
printing an ep nov before normalisation:  56.321491492473264
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06082940537642659, 0.06082940537642659, 0.19461652315122216, 0.4252655141949698, 0.1976297465245284, 0.06082940537642659]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[40.1  ]
 [52.202]
 [62.273]
 [60.741]
 [58.868]] [[0.003]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
from probs:  [0.06082940537642659, 0.06082940537642659, 0.19461652315122216, 0.4252655141949698, 0.1976297465245284, 0.06082940537642659]
printing an ep nov before normalisation:  42.306357622146606
printing an ep nov before normalisation:  12.510043524102059
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06092095854680019, 0.06092095854680019, 0.19432220802350653, 0.42590677711506875, 0.19700813922102414, 0.06092095854680019]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06092095854680019, 0.06092095854680019, 0.19432220802350653, 0.42590677711506875, 0.19700813922102414, 0.06092095854680019]
printing an ep nov before normalisation:  53.550272947936925
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06093872226299163, 0.06093872226299163, 0.1940869632100363, 0.42603119893978814, 0.19706567106120063, 0.06093872226299163]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  59.408051627022886
siam score:  -0.838949
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.001]] [[23.244]
 [27.18 ]
 [29.101]
 [27.14 ]
 [25.246]] [[0.581]
 [0.853]
 [0.985]
 [0.849]
 [0.718]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.41800354897722
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06093872226299163, 0.06093872226299163, 0.1940869632100363, 0.42603119893978814, 0.19706567106120063, 0.06093872226299163]
printing an ep nov before normalisation:  23.33270109824865
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06093872226299163, 0.06093872226299163, 0.1940869632100363, 0.42603119893978814, 0.19706567106120063, 0.06093872226299163]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06095721871001515, 0.06095721871001515, 0.19414596207159077, 0.4261607530074557, 0.19682162879090817, 0.06095721871001515]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06095721871001515, 0.06095721871001515, 0.19414596207159077, 0.4261607530074557, 0.19682162879090817, 0.06095721871001515]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06097492594189868, 0.06097492594189868, 0.19391155376542818, 0.4262847792009579, 0.19687888920791768, 0.06097492594189868]
actions average: 
K:  2  action  0 :  tensor([0.7534, 0.0014, 0.0019, 0.0972, 0.1462], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9700,     0.0113,     0.0000,     0.0185],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9878,     0.0012,     0.0109],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0758, 0.0007, 0.0099, 0.7072, 0.2063], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1678, 0.0030, 0.0509, 0.2036, 0.5747], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06097492594189868, 0.06097492594189868, 0.19391155376542818, 0.4262847792009579, 0.19687888920791768, 0.06097492594189868]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[46.261]
 [44.654]
 [50.086]
 [50.542]
 [48.179]] [[0.711]
 [0.66 ]
 [0.832]
 [0.846]
 [0.771]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.002]
 [0.003]
 [0.003]] [[18.111]
 [18.111]
 [56.914]
 [18.111]
 [18.111]] [[0.192]
 [0.192]
 [0.964]
 [0.192]
 [0.192]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06102764768843932, 0.06102764768843932, 0.19321362337259212, 0.42665405643080595, 0.19704937713128404, 0.06102764768843932]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  79.68464527923324
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  29.07772574287415
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06118971704627716, 0.06118971704627716, 0.19287057788084838, 0.42778923360972343, 0.19577103737059662, 0.06118971704627716]
printing an ep nov before normalisation:  82.4911297925632
from probs:  [0.06118971704627716, 0.06118971704627716, 0.19287057788084838, 0.42778923360972343, 0.19577103737059662, 0.06118971704627716]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06118971704627716, 0.06118971704627716, 0.19287057788084838, 0.42778923360972343, 0.19577103737059662, 0.06118971704627716]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06118971704627716, 0.06118971704627716, 0.19287057788084838, 0.42778923360972343, 0.19577103737059662, 0.06118971704627716]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06122511557701425, 0.06122511557701425, 0.19269897178508072, 0.4280371743975197, 0.19558850708635692, 0.06122511557701425]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  37.67496207473009
printing an ep nov before normalisation:  49.95272107984726
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06127668735625011, 0.06127668735625011, 0.19201803692611943, 0.4283983969484432, 0.19575350405668715, 0.06127668735625011]
printing an ep nov before normalisation:  80.50633488980135
actions average: 
K:  4  action  0 :  tensor([    0.7619,     0.0038,     0.0004,     0.0527,     0.1812],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9976,     0.0018,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0016,     0.9851,     0.0008,     0.0124],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0088,     0.0001,     0.0134,     0.8776,     0.1001],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0832, 0.0044, 0.0992, 0.2299, 0.5833], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.253926638945416
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06132882187071228, 0.06132882187071228, 0.19162482867101752, 0.42876356104757296, 0.19562514466927244, 0.06132882187071228]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  36.34521961212158
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06139623715747099, 0.06139623715747099, 0.19073502710475082, 0.429235755767535, 0.19584050565530137, 0.06139623715747099]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.061432254170323435, 0.061432254170323435, 0.19084708342466666, 0.42948802856967566, 0.19536812549468738, 0.061432254170323435]
using another actor
using explorer policy with actor:  0
UNIT TEST: sample policy line 217 mcts : [0. 0. 1. 0. 0.]
using explorer policy with actor:  1
siam score:  -0.8426512
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06150141553816416, 0.06150141553816416, 0.1905178853459067, 0.42997245330907674, 0.19500541473052382, 0.06150141553816416]
printing an ep nov before normalisation:  42.341562588243676
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.061519206926595286, 0.061519206926595286, 0.1905730801867414, 0.4300970689575726, 0.19477223007590017, 0.061519206926595286]
from probs:  [0.061519206926595286, 0.061519206926595286, 0.1905730801867414, 0.4300970689575726, 0.19477223007590017, 0.061519206926595286]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.833 0.042 0.042]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06153693154798073, 0.06153693154798073, 0.19062806789385672, 0.4302212169518026, 0.19453992051039842, 0.06153693154798073]
printing an ep nov before normalisation:  43.196192904689894
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06164124927110982, 0.06164124927110982, 0.1906806052970361, 0.43095188621731084, 0.19344376067232358, 0.06164124927110982]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.06165787155526375, 0.06165787155526375, 0.19046207015747418, 0.4310683131464032, 0.19349600203033138, 0.06165787155526375]
printing an ep nov before normalisation:  27.009036299917753
printing an ep nov before normalisation:  60.798172335011216
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048452891354886646, 0.048452891354886646, 0.24992803345683368, 0.3385770959816904, 0.1515531772919171, 0.16303591055978545]
actions average: 
K:  2  action  0 :  tensor([    0.8582,     0.0002,     0.0001,     0.0582,     0.0833],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0051,     0.9886,     0.0000,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9750,     0.0014,     0.0233],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0558, 0.0018, 0.0054, 0.6366, 0.3004], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1035, 0.0058, 0.1090, 0.1976, 0.5841], grad_fn=<DivBackward0>)
from probs:  [0.04850398069213713, 0.04850398069213713, 0.24935526609160447, 0.33893493937976693, 0.1514936823544172, 0.1632081507899372]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04852770472516681, 0.04852770472516681, 0.24947750011550096, 0.33910110885959005, 0.15134914992489196, 0.16301683164968345]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048540786265111595, 0.048540786265111595, 0.24954490051341027, 0.3391927354681515, 0.15139002519046188, 0.16279076629775305]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04855381326907298, 0.04855381326907298, 0.24961201992398957, 0.3392839800920823, 0.15143073005003876, 0.16256564339574334]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04855381326907298, 0.04855381326907298, 0.24961201992398957, 0.3392839800920823, 0.15143073005003876, 0.16256564339574334]
from probs:  [0.04855381326907298, 0.04855381326907298, 0.24961201992398957, 0.3392839800920823, 0.15143073005003876, 0.16256564339574334]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04856678607737634, 0.04856678607737634, 0.24967886010070717, 0.33937484511511273, 0.1514712655670212, 0.1623414570624062]
actions average: 
K:  1  action  0 :  tensor([    0.8242,     0.0013,     0.0007,     0.0581,     0.1158],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0053,     0.9790,     0.0003,     0.0000,     0.0154],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0008,     0.9651,     0.0044,     0.0296],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0990,     0.0005,     0.0167,     0.7181,     0.1656],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2229, 0.0109, 0.0023, 0.1405, 0.6234], grad_fn=<DivBackward0>)
from probs:  [0.04865071376917884, 0.04865071376917884, 0.24886513533088253, 0.33996269714145777, 0.15151511467746942, 0.1623556253118326]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04868363928086015, 0.04868363928086015, 0.2486216893225182, 0.3401933162415976, 0.15161784724722224, 0.1621998686269417]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04875980175322845, 0.04875980175322845, 0.248190796861519, 0.34072677859176587, 0.15163752438837202, 0.16192529665188637]
printing an ep nov before normalisation:  27.95081081558976
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04875980175322845, 0.04875980175322845, 0.248190796861519, 0.34072677859176587, 0.15163752438837202, 0.16192529665188637]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04877965731016001, 0.04877965731016001, 0.24788408465780304, 0.3408658522291524, 0.15169938772559072, 0.1619913607671338]
printing an ep nov before normalisation:  79.49681815683478
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048822837539337874, 0.048822837539337874, 0.2476973005169992, 0.34116829811650007, 0.15161660004888855, 0.16187212623893654]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048822837539337874, 0.048822837539337874, 0.2476973005169992, 0.34116829811650007, 0.15161660004888855, 0.16187212623893654]
from probs:  [0.048822837539337874, 0.048822837539337874, 0.2476973005169992, 0.34116829811650007, 0.15161660004888855, 0.16187212623893654]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048842582542621873, 0.048842582542621873, 0.24739249080054082, 0.34130659740653657, 0.15167803080700815, 0.16193771590067085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.048842582542621873, 0.048842582542621873, 0.24739249080054082, 0.34130659740653657, 0.15167803080700815, 0.16193771590067085]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.002]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.002]
 [0.003]]
printing an ep nov before normalisation:  54.39873953856612
maxi score, test score, baseline:  0.0021 0.05 0.05
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04894206249761261, 0.04894206249761261, 0.2462897521615874, 0.34200338164861527, 0.15155456920314575, 0.16226817199142646]
siam score:  -0.81630355
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04897816660836578, 0.04897816660836578, 0.24647183522749608, 0.34225626450777435, 0.15145129655225417, 0.16186427049574395]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
printing an ep nov before normalisation:  39.921579360961914
actions average: 
K:  4  action  0 :  tensor([    0.7602,     0.0001,     0.0003,     0.0766,     0.1629],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0044,     0.9930,     0.0016,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9221,     0.0052,     0.0721],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0297,     0.0002,     0.0112,     0.7906,     0.1683],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1007, 0.0027, 0.1460, 0.1253, 0.6253], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04902754641752232, 0.04902754641752232, 0.24592532475139584, 0.3426021339133278, 0.15138967594883523, 0.16202777255139664]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04904690123404991, 0.04904690123404991, 0.24562708797889185, 0.34273770023084377, 0.15144955081452197, 0.16209185850764266]
maxi score, test score, baseline:  0.0021 0.05 0.05
using another actor
from probs:  [0.04906619371337794, 0.04906619371337794, 0.24532981175515767, 0.3428728299219222, 0.15150923283769602, 0.1621557380584681]
from probs:  [0.04909819842853718, 0.04909819842853718, 0.2450973991539549, 0.3430969995166638, 0.15160824064894396, 0.16200096382336304]
printing an ep nov before normalisation:  73.77885114784605
printing an ep nov before normalisation:  76.84308270058115
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049108687975466345, 0.049108687975466345, 0.24514987673322003, 0.34317047111209686, 0.15142663617888685, 0.1620356400248636]
siam score:  -0.81529784
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04911913828311032, 0.04911913828311032, 0.2452021580044165, 0.3432436678650696, 0.15124571105462395, 0.16207018650966917]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04911913828311032, 0.04911913828311032, 0.2452021580044165, 0.3432436678650696, 0.15124571105462395, 0.16207018650966917]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.004]
 [0.002]
 [0.002]
 [0.002]] [[34.642]
 [29.783]
 [44.595]
 [27.421]
 [26.465]] [[0.387]
 [0.289]
 [0.591]
 [0.239]
 [0.22 ]]
printing an ep nov before normalisation:  57.776828248085124
printing an ep nov before normalisation:  21.236480560031765
printing an ep nov before normalisation:  17.941792252145685
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04915497464107183, 0.04915497464107183, 0.2453814417550112, 0.3434946753119809, 0.1511437829123501, 0.16167015073851412]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04915497464107183, 0.04915497464107183, 0.2453814417550112, 0.3434946753119809, 0.1511437829123501, 0.16167015073851412]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04915497464107183, 0.04915497464107183, 0.2453814417550112, 0.3434946753119809, 0.1511437829123501, 0.16167015073851412]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04915497464107183, 0.04915497464107183, 0.2453814417550112, 0.3434946753119809, 0.1511437829123501, 0.16167015073851412]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04915497464107183, 0.04915497464107183, 0.2453814417550112, 0.3434946753119809, 0.1511437829123501, 0.16167015073851412]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049165357896684594, 0.049165357896684594, 0.24543338757527672, 0.3435674024145728, 0.15096412847686724, 0.16170436573991404]
printing an ep nov before normalisation:  51.85043366361695
from probs:  [0.049165357896684594, 0.049165357896684594, 0.24543338757527672, 0.3435674024145728, 0.15096412847686724, 0.16170436573991404]
deleting a thread, now have 1 threads
Frames:  45744 train batches done:  5357 episodes:  3878
printing an ep nov before normalisation:  22.049681749881245
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049177998321151974, 0.049177998321151974, 0.24549662566186056, 0.34365593933221494, 0.15100301250201745, 0.1614884258616031]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049177998321151974, 0.049177998321151974, 0.24549662566186056, 0.34365593933221494, 0.15100301250201745, 0.1614884258616031]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049177998321151974, 0.049177998321151974, 0.24549662566186056, 0.34365593933221494, 0.15100301250201745, 0.1614884258616031]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049177998321151974, 0.049177998321151974, 0.24549662566186056, 0.34365593933221494, 0.15100301250201745, 0.1614884258616031]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  54.10148288181737
printing an ep nov before normalisation:  49.48125169497285
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.049177998321151974, 0.049177998321151974, 0.24549662566186056, 0.34365593933221494, 0.15100301250201745, 0.1614884258616031]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.04424271821163072, 0.04424271821163072, 0.22080618336801436, 0.3090879159462062, 0.13582127897738988, 0.24579918528512806]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04424271821163072, 0.04424271821163072, 0.22080618336801436, 0.3090879159462062, 0.13582127897738988, 0.24579918528512806]
actions average: 
K:  0  action  0 :  tensor([    0.8755,     0.0145,     0.0001,     0.0015,     0.1084],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0025, 0.0025, 0.9433, 0.0046, 0.0471], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0952,     0.0001,     0.0004,     0.8100,     0.0943],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2969, 0.0059, 0.1749, 0.0958, 0.4264], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04424271821163072, 0.04424271821163072, 0.22080618336801436, 0.3090879159462062, 0.13582127897738988, 0.24579918528512806]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04424271821163072, 0.04424271821163072, 0.22080618336801436, 0.3090879159462062, 0.13582127897738988, 0.24579918528512806]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04424271821163072, 0.04424271821163072, 0.22080618336801436, 0.3090879159462062, 0.13582127897738988, 0.24579918528512806]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.028]
 [0.02 ]
 [0.02 ]] [[30.159]
 [30.159]
 [36.977]
 [30.159]
 [30.159]] [[1.317]
 [1.317]
 [2.028]
 [1.317]
 [1.317]]
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.255]
 [0.156]
 [0.372]] [[23.925]
 [23.925]
 [23.983]
 [28.336]
 [23.925]] [[1.503]
 [1.503]
 [1.391]
 [1.672]
 [1.503]]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.044262995560163734, 0.044262995560163734, 0.22090762780332832, 0.30922994392491066, 0.13588365543732378, 0.24545278171410972]
from probs:  [0.044262995560163734, 0.044262995560163734, 0.22090762780332832, 0.30922994392491066, 0.13588365543732378, 0.24545278171410972]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.005]
 [0.005]
 [0.002]
 [0.004]] [[34.75 ]
 [34.213]
 [37.373]
 [37.599]
 [35.984]] [[1.047]
 [1.017]
 [1.208]
 [1.219]
 [1.123]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04428319926368001, 0.04428319926368001, 0.22100870380402549, 0.30937145607419825, 0.13594580535307085, 0.24510763624134535]
from probs:  [0.04428319926368001, 0.04428319926368001, 0.22100870380402549, 0.30937145607419825, 0.13594580535307085, 0.24510763624134535]
from probs:  [0.04430332972265776, 0.04430332972265776, 0.22110941337363635, 0.3095124551991256, 0.13600772995656782, 0.24476374202535456]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  12.518742570807332
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04413106650434248, 0.04413106650434248, 0.20017189822154025, 0.27819231408013917, 0.1250651078514368, 0.3083085468381988]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.04413106650434248, 0.04413106650434248, 0.20017189822154025, 0.27819231408013917, 0.1250651078514368, 0.3083085468381988]
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04413106650434248, 0.04413106650434248, 0.20017189822154025, 0.27819231408013917, 0.1250651078514368, 0.3083085468381988]
actions average: 
K:  2  action  0 :  tensor([0.6844, 0.0094, 0.0008, 0.0807, 0.2246], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0052,     0.9072,     0.0062,     0.0000,     0.0814],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0018,     0.9150,     0.0227,     0.0605],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0000,     0.0045,     0.9859,     0.0087],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0496, 0.0047, 0.2664, 0.0460, 0.6333], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 47.92784878764453
siam score:  -0.824971
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04413844615126288, 0.04413844615126288, 0.20020545020441438, 0.27823895223099016, 0.12491846895310114, 0.3083602363089686]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04404501883879055, 0.04404501883879055, 0.20048371769524106, 0.2787030671234663, 0.1250174302551728, 0.3077057472485387]
using another actor
printing an ep nov before normalisation:  56.95567715135089
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04399845381591335, 0.04399845381591335, 0.2006224087453573, 0.27893438621007927, 0.12506675346883464, 0.30737954394390204]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04400580819797881, 0.04400580819797881, 0.2006560224786732, 0.2789811296190204, 0.12492017507850277, 0.30743105642784607]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04395934272624556, 0.04395934272624556, 0.20079448256119697, 0.27921205247867265, 0.12496922900462955, 0.3071055505030097]
printing an ep nov before normalisation:  37.06949234008789
using another actor
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
siam score:  -0.8337442
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04388133694000423, 0.04388133694000423, 0.20113774552062852, 0.2797659498109407, 0.12477455123044882, 0.30655907955797357]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  46.66881561279297
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04649674938054836, 0.04649674938054836, 0.1856893359199396, 0.3248819224593309, 0.11795084924676563, 0.27848439361286714]
maxi score, test score, baseline:  0.0041 0.05 0.05
from probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.02 ]
 [0.008]
 [0.024]] [[50.315]
 [50.315]
 [69.434]
 [70.677]
 [50.315]] [[0.521]
 [0.521]
 [0.802]
 [0.809]
 [0.521]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.776161248335555
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
printing an ep nov before normalisation:  22.82760350681252
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04650354446249912, 0.04650354446249912, 0.1857165311996935, 0.3249295179368879, 0.11782167291393068, 0.2785251890244898]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.005]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.005]
 [0.008]
 [0.008]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046551237379090005, 0.046551237379090005, 0.1859074078099193, 0.32526357824074864, 0.11794271813258861, 0.2777838210585634]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04658175253731659, 0.04658175253731659, 0.1860295355886157, 0.3254773186399148, 0.11787407516068012, 0.2774555655361562]
siam score:  -0.8431163
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046605404061856065, 0.046605404061856065, 0.18612419372977096, 0.32564298339768594, 0.11793402863645468, 0.27708798611237623]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046605404061856065, 0.046605404061856065, 0.18612419372977096, 0.32564298339768594, 0.11793402863645468, 0.27708798611237623]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046605404061856065, 0.046605404061856065, 0.18612419372977096, 0.32564298339768594, 0.11793402863645468, 0.27708798611237623]
from probs:  [0.046605404061856065, 0.046605404061856065, 0.18612419372977096, 0.32564298339768594, 0.11793402863645468, 0.27708798611237623]
printing an ep nov before normalisation:  51.22685673622304
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04663575372431042, 0.04663575372431042, 0.1862456591612202, 0.32585556459813003, 0.11786529731457053, 0.27676197147745846]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04663575372431042, 0.04663575372431042, 0.1862456591612202, 0.32585556459813003, 0.11786529731457053, 0.27676197147745846]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04664250623962287, 0.04664250623962287, 0.18627268408075692, 0.325902861921891, 0.11773730147441618, 0.27680214004369]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046666011802653964, 0.046666011802653964, 0.18636675805470385, 0.3260675043067538, 0.1177967380002354, 0.27643697603299916]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.013]
 [0.014]
 [0.007]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.013]
 [0.014]
 [0.007]
 [0.009]]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  28.95015778493131
printing an ep nov before normalisation:  28.836106006716957
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04669279536127512, 0.04669279536127512, 0.1864739512239379, 0.3262551070866007, 0.11728933872625635, 0.2765960122406547]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04671624865286431, 0.04671624865286431, 0.18656781599709873, 0.3264193833413332, 0.11734835337217463, 0.2762319499836648]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04671624865286431, 0.04671624865286431, 0.18656781599709873, 0.3264193833413332, 0.11734835337217463, 0.2762319499836648]
printing an ep nov before normalisation:  61.75178347077761
siam score:  -0.879998
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046722888488443554, 0.046722888488443554, 0.18659438994976124, 0.32646589141107896, 0.11722263720886579, 0.2762713044534069]
printing an ep nov before normalisation:  32.78118192068651
printing an ep nov before normalisation:  41.0812542713189
siam score:  -0.8775166
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04626081902724796, 0.04626081902724796, 0.17339007770624557, 0.3005193363852432, 0.11033806634529111, 0.32323088150872425]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04626081902724796, 0.04626081902724796, 0.17339007770624557, 0.3005193363852432, 0.11033806634529111, 0.32323088150872425]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04626677169416422, 0.04626677169416422, 0.17341243613151378, 0.30055810056886334, 0.1102233433427304, 0.3232725765685641]
printing an ep nov before normalisation:  51.692852221195956
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04626677169416422, 0.04626677169416422, 0.17341243613151378, 0.30055810056886334, 0.1102233433427304, 0.3232725765685641]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.046100124937668886, 0.046100124937668886, 0.17389045786192828, 0.30168079078618765, 0.11012333782557437, 0.322105163650972]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04610601629837435, 0.04610601629837435, 0.17391272750734824, 0.3017194387163221, 0.11000937190286131, 0.32214642927671966]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04601729322945272, 0.04601729322945272, 0.17412833105621842, 0.30223936888298414, 0.11007281214283558, 0.3215249014590564]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04601729322945272, 0.04601729322945272, 0.17412833105621842, 0.30223936888298414, 0.11007281214283558, 0.3215249014590564]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04601729322945272, 0.04601729322945272, 0.17412833105621842, 0.30223936888298414, 0.11007281214283558, 0.3215249014590564]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.062]
 [0.062]
 [0.062]
 [0.062]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04592891200181879, 0.04592891200181879, 0.17434310390574886, 0.3027572958096789, 0.11013600795378382, 0.32090576832715084]
siam score:  -0.88986784
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04592891200181879, 0.04592891200181879, 0.17434310390574886, 0.3027572958096789, 0.11013600795378382, 0.32090576832715084]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04592891200181879, 0.04592891200181879, 0.17434310390574886, 0.3027572958096789, 0.11013600795378382, 0.32090576832715084]
from probs:  [0.04588484896169711, 0.04588484896169711, 0.17445018031710063, 0.30301551167250423, 0.11016751463939889, 0.3205970954476022]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0458408706436497, 0.0458408706436497, 0.17455705084761328, 0.30327323105157683, 0.11019896074563149, 0.3202890160678791]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0458408706436497, 0.0458408706436497, 0.17455705084761328, 0.30327323105157683, 0.11019896074563149, 0.3202890160678791]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0458408706436497, 0.0458408706436497, 0.17455705084761328, 0.30327323105157683, 0.11019896074563149, 0.3202890160678791]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0458408706436497, 0.0458408706436497, 0.17455705084761328, 0.30327323105157683, 0.11019896074563149, 0.3202890160678791]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04579697680356261, 0.04579697680356261, 0.17466371609050152, 0.30353045537744044, 0.11023034644703206, 0.31998152847790073]
printing an ep nov before normalisation:  49.81931251710314
printing an ep nov before normalisation:  30.018704116857293
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04838859988734957, 0.04838859988734957, 0.16428798155113253, 0.338137054046807, 0.10633829071924106, 0.2944594739081202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04838859988734957, 0.04838859988734957, 0.16428798155113253, 0.338137054046807, 0.10633829071924106, 0.2944594739081202]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04843890064290996, 0.04843890064290996, 0.1644590942720322, 0.33848938471571555, 0.10644899745747109, 0.29372472226896124]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04843890064290996, 0.04843890064290996, 0.1644590942720322, 0.33848938471571555, 0.10644899745747109, 0.29372472226896124]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.054]
 [0.164]
 [0.14 ]
 [0.164]
 [0.164]] [[33.692]
 [47.004]
 [48.504]
 [47.004]
 [47.004]] [[0.696]
 [1.574]
 [1.637]
 [1.574]
 [1.574]]
printing an ep nov before normalisation:  16.927497177283506
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04846393085458666, 0.04846393085458666, 0.16454424185236133, 0.3386647083490234, 0.10650408635347403, 0.29335910173596785]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04846393085458666, 0.04846393085458666, 0.16454424185236133, 0.3386647083490234, 0.10650408635347403, 0.29335910173596785]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0484888814650648, 0.0484888814650648, 0.1646291186459494, 0.33883947441727624, 0.10655900005550709, 0.29299464395113767]
from probs:  [0.0484888814650648, 0.0484888814650648, 0.1646291186459494, 0.33883947441727624, 0.10655900005550709, 0.29299464395113767]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04851375285346368, 0.04851375285346368, 0.16471372594248146, 0.33901368557600814, 0.10661373939797257, 0.2926313433766103]
printing an ep nov before normalisation:  40.06286213434911
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04856325946850152, 0.04856325946850152, 0.16488213716229877, 0.3393604537029946, 0.10672269831540016, 0.2919081918823033]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04856325946850152, 0.04856325946850152, 0.16488213716229877, 0.3393604537029946, 0.10672269831540016, 0.2919081918823033]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.04856325946850152, 0.04856325946850152, 0.16488213716229877, 0.3393604537029946, 0.10672269831540016, 0.2919081918823033]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.052261951992802695, 0.052261951992802695, 0.15659818411439266, 0.3652706483575726, 0.1044300680535977, 0.26917719548883173]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.052261951992802695, 0.052261951992802695, 0.15659818411439266, 0.3652706483575726, 0.1044300680535977, 0.26917719548883173]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.012]
 [0.036]
 [0.017]
 [0.022]] [[27.02 ]
 [31.312]
 [41.177]
 [34.539]
 [29.999]] [[0.031]
 [0.012]
 [0.036]
 [0.017]
 [0.022]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.052261951992802695, 0.052261951992802695, 0.15659818411439266, 0.3652706483575726, 0.1044300680535977, 0.26917719548883173]
using another actor
maxi score, test score, baseline:  0.0061 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.054]
 [0.003]
 [0.046]] [[44.302]
 [44.302]
 [60.156]
 [35.144]
 [44.302]] [[0.046]
 [0.046]
 [0.054]
 [0.003]
 [0.046]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.052261951992802695, 0.052261951992802695, 0.15659818411439266, 0.3652706483575726, 0.1044300680535977, 0.26917719548883173]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.65510046331159
printing an ep nov before normalisation:  57.266710809524305
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8860721
printing an ep nov before normalisation:  19.779117107391357
using another actor
from probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
line 256 mcts: sample exp_bonus 36.369094639182805
from probs:  [0.04960486942641395, 0.04960486942641395, 0.14862292079196263, 0.3466590235230599, 0.09911389510918828, 0.3063944217229613]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
printing an ep nov before normalisation:  2.3568000528939592e-06
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
printing an ep nov before normalisation:  17.434917089361843
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
printing an ep nov before normalisation:  32.36197471618652
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.049631206346643614, 0.049631206346643614, 0.14870197135538518, 0.34684350137286835, 0.09916658885101441, 0.3060255257274448]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.7992,     0.0010,     0.0003,     0.0500,     0.1495],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0019,     0.9828,     0.0108,     0.0000,     0.0046],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9231,     0.0261,     0.0506],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0001,     0.0032,     0.9374,     0.0590],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.1353,     0.0002,     0.0789,     0.1641,     0.6215],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.83362728700389
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04731708142787482, 0.04731708142787482, 0.18897560821266426, 0.3306341349974537, 0.09453659035613797, 0.2912195035779944]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.046069447862427546, 0.046069447862427546, 0.18039666768936025, 0.3147238875162929, 0.09084518780473844, 0.3218953612647534]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.046069447862427546, 0.046069447862427546, 0.18039666768936025, 0.3147238875162929, 0.09084518780473844, 0.3218953612647534]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.046069447862427546, 0.046069447862427546, 0.18039666768936025, 0.3147238875162929, 0.09084518780473844, 0.3218953612647534]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.046069447862427546, 0.046069447862427546, 0.18039666768936025, 0.3147238875162929, 0.09084518780473844, 0.3218953612647534]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.046069447862427546, 0.046069447862427546, 0.18039666768936025, 0.3147238875162929, 0.09084518780473844, 0.3218953612647534]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.083 0.208 0.333 0.125 0.25 ]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04835738204076465, 0.04835738204076465, 0.1724568422574282, 0.3379227892129795, 0.0897238687796525, 0.30318173566841045]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04835738204076465, 0.04835738204076465, 0.1724568422574282, 0.3379227892129795, 0.0897238687796525, 0.30318173566841045]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04835738204076465, 0.04835738204076465, 0.1724568422574282, 0.3379227892129795, 0.0897238687796525, 0.30318173566841045]
using explorer policy with actor:  1
siam score:  -0.8949095
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04843279685540239, 0.04843279685540239, 0.1727263286474585, 0.33845103770353335, 0.0898639741194211, 0.30209306581878226]
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04843279685540239, 0.04843279685540239, 0.1727263286474585, 0.33845103770353335, 0.0898639741194211, 0.30209306581878226]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.082]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [0.07 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.082]
 [1.5  ]
 [1.5  ]
 [1.5  ]
 [0.07 ]]
printing an ep nov before normalisation:  59.39302407681982
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.047109127438330775, 0.047109127438330775, 0.16582045557000505, 0.3241022264122374, 0.08667957014888887, 0.32917949299220717]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04706748916973796, 0.04706748916973796, 0.16591514580268238, 0.3243786879799416, 0.08668337471405275, 0.3288878131638474]
actions average: 
K:  4  action  0 :  tensor([    0.8366,     0.0042,     0.0001,     0.0230,     0.1361],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0018,     0.9842,     0.0005,     0.0000,     0.0134],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9755,     0.0017,     0.0227],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0533,     0.0001,     0.0018,     0.8789,     0.0660],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0400, 0.0091, 0.1696, 0.1411, 0.6402], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04702592465633507, 0.04702592465633507, 0.16600966830753788, 0.3246546598424749, 0.08668717254006934, 0.3285966499972477]
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.101]
 [0.087]
 [0.087]
 [0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.101]
 [0.087]
 [0.087]
 [0.087]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.0492689414709687, 0.0492689414709687, 0.15990906608722014, 0.34430927378097254, 0.0861489830097192, 0.3110947941801506]
from probs:  [0.0492689414709687, 0.0492689414709687, 0.15990906608722014, 0.34430927378097254, 0.0861489830097192, 0.3110947941801506]
printing an ep nov before normalisation:  38.01164249858626
maxi score, test score, baseline:  0.0081 0.05 0.05
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04929501864818817, 0.04929501864818817, 0.1599938619002364, 0.3444919339869834, 0.08619463306553758, 0.3107295337508662]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04929501864818817, 0.04929501864818817, 0.1599938619002364, 0.3444919339869834, 0.08619463306553758, 0.3107295337508662]
printing an ep nov before normalisation:  39.706013202667236
maxi score, test score, baseline:  0.0081 0.05 0.05
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.047851798654947826, 0.047851798654947826, 0.15444134262687564, 0.33209058258008867, 0.08338164664559045, 0.3343828308375495]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.047810784832625644, 0.047810784832625644, 0.15452342221593868, 0.3323778178547937, 0.08338166396039665, 0.3340955263036197]
printing an ep nov before normalisation:  37.78689309668465
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.047810784832625644, 0.047810784832625644, 0.15452342221593868, 0.3323778178547937, 0.08338166396039665, 0.3340955263036197]
printing an ep nov before normalisation:  39.674027781762405
maxi score, test score, baseline:  0.0081 0.05 0.05
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04998606521024229, 0.04998606521024229, 0.14976855833988306, 0.3493335445991647, 0.0832468962534559, 0.31767887038701165]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.04998606521024229, 0.04998606521024229, 0.14976855833988306, 0.3493335445991647, 0.0832468962534559, 0.31767887038701165]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.05001289876700228, 0.05001289876700228, 0.14984910035690027, 0.3495215035366963, 0.08329163263030162, 0.3173119659420973]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.05001289876700228, 0.05001289876700228, 0.14984910035690027, 0.3495215035366963, 0.08329163263030162, 0.3173119659420973]
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.05001289876700228, 0.05001289876700228, 0.14984910035690027, 0.3495215035366963, 0.08329163263030162, 0.3173119659420973]
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([0.7875, 0.0059, 0.0010, 0.0978, 0.1078], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9987,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9664,     0.0099,     0.0233],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0411,     0.0007,     0.0099,     0.8662,     0.0821],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1113, 0.0598, 0.1762, 0.0422, 0.6105], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.05001289876700228, 0.05001289876700228, 0.14984910035690027, 0.3495215035366963, 0.08329163263030162, 0.3173119659420973]
siam score:  -0.9020906
siam score:  -0.90109116
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.05003965350301362, 0.05003965350301362, 0.14992940579021985, 0.3497089103646323, 0.08333623759874903, 0.3169461392403715]
actions average: 
K:  0  action  0 :  tensor([    0.7119,     0.0004,     0.0007,     0.0890,     0.1980],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9946,     0.0001,     0.0000,     0.0046],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9571,     0.0015,     0.0411],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1205,     0.0000,     0.0003,     0.7397,     0.1395],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1589, 0.0007, 0.1374, 0.1457, 0.5573], grad_fn=<DivBackward0>)
using another actor
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
maxi score, test score, baseline:  0.0101 0.05 0.05
siam score:  -0.89659846
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.047728968348102016, 0.047728968348102016, 0.15468715880638673, 0.33295080957019463, 0.08338169850086358, 0.33352239642635106]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  51.59083843231201
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
siam score:  -0.89607435
using explorer policy with actor:  1
siam score:  -0.8968425
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
from probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
from probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
siam score:  -0.90084577
maxi score, test score, baseline:  0.0101 0.05 0.05
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.050066329765058495, 0.050066329765058495, 0.15000947568072265, 0.3165813855401629, 0.08338071173694656, 0.34989576751205104]
printing an ep nov before normalisation:  30.389549732208252
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  69.51299200702479
Printing some Q and Qe and total Qs values:  [[0.372]
 [0.372]
 [0.372]
 [0.372]
 [0.372]] [[63.454]
 [63.454]
 [63.454]
 [63.454]
 [63.454]] [[1.372]
 [1.372]
 [1.372]
 [1.372]
 [1.372]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.046944834971493654, 0.046944834971493654, 0.20310374587824104, 0.29679909242228947, 0.07817661715284313, 0.328030874603639]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.046944834971493654, 0.046944834971493654, 0.20310374587824104, 0.29679909242228947, 0.07817661715284313, 0.328030874603639]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.046944834971493654, 0.046944834971493654, 0.20310374587824104, 0.29679909242228947, 0.07817661715284313, 0.328030874603639]
printing an ep nov before normalisation:  36.62003092893027
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.046944834971493654, 0.046944834971493654, 0.20310374587824104, 0.29679909242228947, 0.07817661715284313, 0.328030874603639]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.90287995
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0101 0.05 0.05
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.086]
 [0.001]
 [0.039]
 [0.047]] [[35.089]
 [42.586]
 [49.918]
 [30.496]
 [29.421]] [[0.055]
 [0.086]
 [0.001]
 [0.039]
 [0.047]]
from probs:  [0.04768793945666284, 0.04768793945666284, 0.19046241210866743, 0.27612709569987015, 0.10479772851746467, 0.333236884760672]
siam score:  -0.90533984
printing an ep nov before normalisation:  37.678641390804316
printing an ep nov before normalisation:  35.23256233819076
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.90473956
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04768816454227875, 0.04768816454227875, 0.19046236709154427, 0.2761268886211035, 0.10479784556198495, 0.3332365696408098]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04768816454227875, 0.04768816454227875, 0.19046236709154427, 0.2761268886211035, 0.10479784556198495, 0.3332365696408098]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04961640163769926, 0.04961640163769926, 0.18467439974804625, 0.26570919861425435, 0.10363960088183807, 0.34674399748046264]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[31.822]
 [31.822]
 [31.822]
 [31.822]
 [31.822]] [[0.669]
 [0.669]
 [0.669]
 [0.669]
 [0.669]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04961640163769926, 0.04961640163769926, 0.18467439974804625, 0.26570919861425446, 0.10363960088183807, 0.34674399748046264]
printing an ep nov before normalisation:  43.6646842956543
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04961640163769926, 0.04961640163769926, 0.18467439974804625, 0.26570919861425446, 0.10363960088183807, 0.34674399748046264]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.15 0.15
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04865540934187217, 0.0768877675535455, 0.19433437771410653, 0.24289403383818461, 0.0972150654659503, 0.3400133460863409]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04865540934187217, 0.0768877675535455, 0.19433437771410653, 0.24289403383818461, 0.0972150654659503, 0.3400133460863409]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
printing an ep nov before normalisation:  77.64415757261787
printing an ep nov before normalisation:  61.871105393185324
Printing some Q and Qe and total Qs values:  [[1.194]
 [1.41 ]
 [1.311]
 [1.194]
 [1.194]] [[38.566]
 [41.278]
 [42.994]
 [38.566]
 [38.566]] [[2.753]
 [3.155]
 [3.175]
 [2.753]
 [2.753]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
printing an ep nov before normalisation:  69.86599483448893
Printing some Q and Qe and total Qs values:  [[0.92 ]
 [0.992]
 [0.92 ]
 [0.92 ]
 [0.646]] [[50.507]
 [39.446]
 [50.507]
 [50.507]
 [45.59 ]] [[2.774]
 [2.44 ]
 [2.774]
 [2.774]
 [2.319]]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.2608389088527112, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
Printing some Q and Qe and total Qs values:  [[0.212]
 [0.212]
 [0.254]
 [0.212]
 [0.212]] [[41.078]
 [41.078]
 [53.448]
 [41.078]
 [41.078]] [[0.823]
 [0.823]
 [1.217]
 [0.823]
 [0.823]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
from probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
printing an ep nov before normalisation:  26.521660372549032
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04750432704198397, 0.07506693451107017, 0.1897273815824688, 0.26083890885271116, 0.09491201188881225, 0.3319504361229536]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9125012
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.103]
 [0.372]
 [0.167]
 [0.205]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.027]
 [0.103]
 [0.372]
 [0.167]
 [0.205]]
siam score:  -0.9028623
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04912814473486837, 0.07544273919721127, 0.18491145216055777, 0.25280310587340243, 0.09438924721009817, 0.34332531082386203]
siam score:  -0.90527755
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048043051608079215, 0.07377485530842356, 0.1808191587018561, 0.26933656343104057, 0.09230175397267151, 0.33572461697792905]
from probs:  [0.048043051608079215, 0.07377485530842356, 0.1808191587018561, 0.26933656343104057, 0.09230175397267151, 0.33572461697792905]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048043051608079215, 0.07377485530842356, 0.1808191587018561, 0.26933656343104057, 0.09230175397267151, 0.33572461697792905]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048043051608079215, 0.07377485530842356, 0.1808191587018561, 0.26933656343104057, 0.09230175397267151, 0.33572461697792905]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04700498031965288, 0.07217924813459267, 0.17690420224474215, 0.2851535538489833, 0.09030472096134931, 0.3284532944906796]
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.064]
 [0.032]
 [0.036]
 [0.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.042]
 [0.064]
 [0.032]
 [0.036]
 [0.038]]
maxi score, test score, baseline:  0.0161 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.121]
 [0.258]
 [0.08 ]
 [0.121]] [[53.754]
 [53.754]
 [63.63 ]
 [62.69 ]
 [53.754]] [[1.141]
 [1.141]
 [1.586]
 [1.378]
 [1.141]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04700498031965288, 0.07217924813459267, 0.17690420224474215, 0.2851535538489833, 0.09030472096134931, 0.3284532944906796]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04700498031965288, 0.07217924813459267, 0.17690420224474215, 0.2851535538489833, 0.09030472096134931, 0.3284532944906796]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.89980018034611
printing an ep nov before normalisation:  46.693265438079834
maxi score, test score, baseline:  0.0161 0.15 0.15
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([0.7792, 0.0127, 0.0016, 0.0498, 0.1566], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0004,     0.9657,     0.0013,     0.0324],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0648, 0.0018, 0.0015, 0.8003, 0.1317], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1542, 0.0344, 0.0576, 0.0799, 0.6740], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04506070227066145, 0.0691347350796898, 0.16957159995895607, 0.3148343139286332, 0.08656433483342632, 0.31483431392863315]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04506070227066145, 0.0691347350796898, 0.16957159995895607, 0.3148343139286332, 0.08656433483342632, 0.31483431392863315]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04506070227066145, 0.0691347350796898, 0.16957159995895607, 0.3148343139286332, 0.08656433483342632, 0.31483431392863315]
printing an ep nov before normalisation:  30.32949924468994
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04658777010918871, 0.0697020909094732, 0.16613503728826012, 0.32553139352702204, 0.08643685916887918, 0.3056068489971767]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04658777010918871, 0.0697020909094732, 0.16613503728826012, 0.32553139352702204, 0.08643685916887918, 0.3056068489971767]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04568194565204359, 0.06829309460817798, 0.18243417453874441, 0.31918640342544524, 0.08475401104824382, 0.2996503707273451]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04568194565204359, 0.06829309460817798, 0.18243417453874441, 0.31918640342544524, 0.08475401104824382, 0.2996503707273451]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.04568194565204359, 0.06829309460817798, 0.18243417453874441, 0.31918640342544524, 0.08475401104824382, 0.2996503707273451]
printing an ep nov before normalisation:  80.53753699701429
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.047099543357322915, 0.0688601214893316, 0.17870751989971148, 0.32911663594815554, 0.08470182236943394, 0.2915143569360445]
Printing some Q and Qe and total Qs values:  [[0.182]
 [0.182]
 [0.633]
 [0.182]
 [0.182]] [[72.818]
 [72.818]
 [67.924]
 [72.818]
 [72.818]] [[1.311]
 [1.311]
 [1.659]
 [1.311]
 [1.311]]
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.16 ]
 [0.172]
 [0.119]
 [0.16 ]] [[58.924]
 [58.924]
 [67.037]
 [68.892]
 [58.924]] [[0.827]
 [0.827]
 [0.993]
 [0.976]
 [0.827]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04709954335732291, 0.0688601214893316, 0.17870751989971145, 0.32911663594815554, 0.08470182236943392, 0.29151435693604455]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04709954335732291, 0.0688601214893316, 0.17870751989971145, 0.32911663594815554, 0.08470182236943392, 0.29151435693604455]
printing an ep nov before normalisation:  51.89700126647949
line 256 mcts: sample exp_bonus 34.944287031605086
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
printing an ep nov before normalisation:  27.18635082244873
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
from probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
actions average: 
K:  0  action  0 :  tensor([    0.6798,     0.0004,     0.0003,     0.1819,     0.1376],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0046,     0.9926,     0.0001,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9441,     0.0026,     0.0533],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0434, 0.0016, 0.0013, 0.7996, 0.1540], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0752, 0.0038, 0.0727, 0.1509, 0.6974], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
from probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048414355140704676, 0.06938603488638251, 0.17525107424256414, 0.3383268559449548, 0.08465341774123596, 0.28396826204415787]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
from probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
printing an ep nov before normalisation:  70.45981128762674
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
printing an ep nov before normalisation:  38.36696275442543
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04963716896631613, 0.06987514989213188, 0.17203647760564977, 0.3468926328046979, 0.08460840000612574, 0.2769501707250786]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8946011
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
printing an ep nov before normalisation:  39.159768818544
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
actions average: 
K:  0  action  0 :  tensor([    0.7037,     0.0004,     0.0010,     0.0693,     0.2256],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0010,     0.9979,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9936,     0.0003,     0.0061],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1243,     0.0000,     0.0008,     0.7436,     0.1313],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0461, 0.0014, 0.0869, 0.1473, 0.7184], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048658862141037415, 0.0882432992827479, 0.1686392911175619, 0.34003990394116834, 0.0829389847057587, 0.27147965881172575]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04784050234918417, 0.08675736839202776, 0.16579752332504313, 0.334307553290556, 0.08154250834228673, 0.28375454430090213]
actions average: 
K:  2  action  0 :  tensor([    0.8416,     0.0009,     0.0003,     0.0010,     0.1562],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9991,     0.0003,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9603,     0.0005,     0.0392],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0484,     0.0002,     0.0043,     0.8461,     0.1009],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0418, 0.0140, 0.0081, 0.0706, 0.8655], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.19793276415394
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04784050234918417, 0.08675736839202776, 0.16579752332504313, 0.334307553290556, 0.08154250834228673, 0.28375454430090213]
maxi score, test score, baseline:  0.0161 0.15 0.15
actions average: 
K:  0  action  0 :  tensor([    0.7998,     0.0003,     0.0004,     0.0475,     0.1519],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9993,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9617,     0.0005,     0.0378],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1132,     0.0001,     0.0010,     0.7845,     0.1012],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1183, 0.0031, 0.0720, 0.1428, 0.6639], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.28112183058039
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.34395556321462
printing an ep nov before normalisation:  49.99212632389655
printing an ep nov before normalisation:  41.28806129966288
from probs:  [0.049003459854601215, 0.08656753566988232, 0.16312312218142516, 0.3424540201235772, 0.0816090776622652, 0.27724278450824913]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.049003459854601215, 0.08656753566988232, 0.16312312218142516, 0.3424540201235772, 0.0816090776622652, 0.27724278450824913]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.049003459854601215, 0.08656753566988232, 0.16312312218142516, 0.3424540201235772, 0.0816090776622652, 0.27724278450824913]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.049003459854601215, 0.08656753566988232, 0.16312312218142516, 0.3424540201235772, 0.0816090776622652, 0.27724278450824913]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.049003459854601215, 0.08656753566988232, 0.16312312218142516, 0.3424540201235772, 0.0816090776622652, 0.27724278450824913]
printing an ep nov before normalisation:  26.227975303037788
maxi score, test score, baseline:  0.0161 0.15 0.15
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
siam score:  -0.89784133
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
siam score:  -0.897396
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
printing an ep nov before normalisation:  62.78978559009732
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.048218901148651044, 0.08517992637790492, 0.16050649579512435, 0.33695843023958244, 0.08030107104764342, 0.2888351753910938]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04745913490846952, 0.08383616626586869, 0.17376018778135952, 0.33163650387247195, 0.07903439812669201, 0.2842736090451382]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04745913490846952, 0.08383616626586869, 0.17376018778135952, 0.33163650387247195, 0.07903439812669201, 0.2842736090451382]
maxi score, test score, baseline:  0.0161 0.15 0.15
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04855864651482497, 0.08382178947417261, 0.17099227886968, 0.3393385233576056, 0.07916705460353872, 0.27812170718017815]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04856257556601462, 0.08374750836660608, 0.17100614171207293, 0.33936604516290314, 0.07917346710252919, 0.2781442620898739]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04856257556601462, 0.08374750836660608, 0.17100614171207293, 0.33936604516290314, 0.07917346710252919, 0.2781442620898739]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04856257556601462, 0.08374750836660608, 0.17100614171207293, 0.33936604516290314, 0.07917346710252919, 0.2781442620898739]
printing an ep nov before normalisation:  30.07108449935913
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04783194143514252, 0.08248605328880693, 0.1684282506858946, 0.33424817590567873, 0.07798101874783053, 0.28902455993664666]
maxi score, test score, baseline:  0.0161 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.077]
 [0.043]
 [0.052]
 [0.062]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.077]
 [0.043]
 [0.052]
 [0.062]]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04783194143514252, 0.08248605328880693, 0.1684282506858946, 0.33424817590567873, 0.07798101874783053, 0.28902455993664666]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04783194143514252, 0.08248605328880693, 0.1684282506858946, 0.33424817590567873, 0.07798101874783053, 0.28902455993664666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.3241158287913
maxi score, test score, baseline:  0.0161 0.15 0.15
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04816885291828629, 0.08132282272431485, 0.16354466784326568, 0.3366083902307347, 0.09143478351515355, 0.278920482768245]
maxi score, test score, baseline:  0.0161 0.15 0.15
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.047485394717848024, 0.0801676447952206, 0.17543640377076164, 0.33182097039098934, 0.09013573106881924, 0.2749538552563611]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.91804314
using another actor
from probs:  [0.04682111237499483, 0.07904487781304237, 0.17297715406495098, 0.32716787168600847, 0.10289046423719755, 0.27109851982380573]
maxi score, test score, baseline:  0.0161 0.15 0.15
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04682111237499483, 0.07904487781304237, 0.17297715406495098, 0.32716787168600847, 0.10289046423719755, 0.27109851982380573]
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04682111237499483, 0.07904487781304237, 0.17297715406495098, 0.32716787168600847, 0.10289046423719755, 0.27109851982380573]
siam score:  -0.9180505
maxi score, test score, baseline:  0.0161 0.15 0.15
probs:  [0.04682111237499483, 0.07904487781304237, 0.17297715406495098, 0.32716787168600847, 0.10289046423719755, 0.27109851982380573]
actor:  0 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.83462905883789
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.8540,     0.0000,     0.0003,     0.0429,     0.1027],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0121,     0.9648,     0.0035,     0.0000,     0.0197],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0009,     0.9693,     0.0044,     0.0254],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0160,     0.0001,     0.0021,     0.8806,     0.1012],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1469, 0.0070, 0.0642, 0.0788, 0.7031], grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([    0.8262,     0.0012,     0.0003,     0.0719,     0.1004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9986,     0.0008,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9774,     0.0009,     0.0217],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0662,     0.0002,     0.0011,     0.7844,     0.1481],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.1687,     0.0005,     0.1685,     0.1231,     0.5392],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.036174677619044
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.046175210026005775, 0.0779531766125537, 0.18440936467748928, 0.3226435193289727, 0.10146887188659919, 0.26734985746837936]
from probs:  [0.046175210026005775, 0.0779531766125537, 0.18440936467748928, 0.3226435193289727, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.046175210026005775, 0.0779531766125537, 0.18440936467748928, 0.3226435193289727, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.046175210026005775, 0.0779531766125537, 0.18440936467748928, 0.3226435193289727, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04617521002600578, 0.07795317661255372, 0.18440936467748925, 0.3226435193289728, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04617521002600578, 0.07795317661255372, 0.18440936467748925, 0.3226435193289728, 0.10146887188659919, 0.26734985746837936]
printing an ep nov before normalisation:  46.692228741839585
actions average: 
K:  2  action  0 :  tensor([    0.8558,     0.0009,     0.0007,     0.0461,     0.0965],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0021,     0.9971,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0048,     0.9590,     0.0007,     0.0355],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1045,     0.0001,     0.0214,     0.7930,     0.0809],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1937, 0.0148, 0.0691, 0.1326, 0.5898], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  23.995649814605713
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04617521002600578, 0.07795317661255372, 0.18440936467748925, 0.3226435193289728, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04617521002600578, 0.07795317661255372, 0.18440936467748925, 0.3226435193289728, 0.10146887188659919, 0.26734985746837936]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04617521002600578, 0.07795317661255372, 0.18440936467748925, 0.3226435193289728, 0.10146887188659919, 0.26734985746837936]
printing an ep nov before normalisation:  46.439767103883774
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04717616262500209, 0.07809887231876907, 0.18168994979288847, 0.32965511567756345, 0.10098167749215663, 0.26239822209362024]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04717616262500209, 0.07809887231876907, 0.18168994979288847, 0.32965511567756345, 0.10098167749215663, 0.26239822209362024]
actions average: 
K:  2  action  0 :  tensor([    0.7240,     0.0059,     0.0003,     0.1170,     0.1528],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0013,     0.9945,     0.0025,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9387,     0.0163,     0.0448],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0356, 0.0021, 0.0062, 0.8090, 0.1471], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1112, 0.0030, 0.1378, 0.0295, 0.7185], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04717616262500209, 0.07809887231876907, 0.18168994979288847, 0.32965511567756345, 0.10098167749215663, 0.26239822209362024]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.052]
 [0.052]
 [0.052]
 [0.052]]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
maxi score, test score, baseline:  0.018099999999999998 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.04812464896303091, 0.0782369311908808, 0.17911307665417792, 0.3362991898835544, 0.10052002003948972, 0.25770613326886616]
printing an ep nov before normalisation:  34.270837903022766
maxi score, test score, baseline:  0.0201 0.15 0.15
printing an ep nov before normalisation:  22.315612350042656
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.15 0.15
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.047722788105170076, 0.09046735395141944, 0.1719669928316016, 0.3334844589759625, 0.09742046999574269, 0.25893793614010363]
siam score:  -0.9209448
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.047722788105170076, 0.09046735395141944, 0.1719669928316016, 0.3334844589759625, 0.09742046999574269, 0.25893793614010363]
maxi score, test score, baseline:  0.0201 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.35 ]
 [0.391]
 [0.685]
 [0.35 ]] [[43.383]
 [43.383]
 [51.592]
 [62.299]
 [43.383]] [[0.713]
 [0.713]
 [0.885]
 [1.349]
 [0.713]]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.047722788105170076, 0.09046735395141944, 0.1719669928316016, 0.3334844589759625, 0.09742046999574269, 0.25893793614010363]
using explorer policy with actor:  1
siam score:  -0.9242537
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.015]
 [0.127]
 [0.084]
 [0.076]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.08 ]
 [0.015]
 [0.127]
 [0.084]
 [0.076]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.04800652398592666, 0.08921432164477294, 0.17976158903381115, 0.33547212045403824, 0.09591745673061194, 0.251627988150839]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.04800652398592666, 0.08921432164477294, 0.17976158903381115, 0.33547212045403824, 0.09591745673061194, 0.251627988150839]
printing an ep nov before normalisation:  11.984465040630699
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
maxi score, test score, baseline:  0.0201 0.15 0.15
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.00644927633622
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.15 0.15
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
from probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.048834098343254265, 0.08907746184168747, 0.17750554590224468, 0.3412692064318688, 0.0956237156374326, 0.24768997184351216]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.049623821207025194, 0.08894686171534134, 0.17535268939228135, 0.34680114600853984, 0.09534340963802743, 0.2439320720387847]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.049628278536460034, 0.0888648684631478, 0.1753684703881854, 0.34683236836781095, 0.09535198466436016, 0.2439540295800356]
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.049628278536460034, 0.0888648684631478, 0.1753684703881854, 0.34683236836781095, 0.09535198466436016, 0.2439540295800356]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
maxi score, test score, baseline:  0.0201 0.15 0.15
probs:  [0.04906845602579662, 0.087861253583537, 0.184688076287657, 0.3429109665931608, 0.0942749961130834, 0.24119625139676543]
printing an ep nov before normalisation:  33.08152243073848
actor:  0 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04906845602579662, 0.087861253583537, 0.184688076287657, 0.3429109665931608, 0.0942749961130834, 0.24119625139676543]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  41.992383832122705
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04906845602579662, 0.087861253583537, 0.184688076287657, 0.3429109665931608, 0.0942749961130834, 0.24119625139676543]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04982686809245181, 0.08776186762953457, 0.1824476264740931, 0.3482235744511447, 0.0940337875529989, 0.23770627579977693]
using explorer policy with actor:  0
printing an ep nov before normalisation:  37.78908729553223
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04982686809245181, 0.08776186762953457, 0.1824476264740931, 0.3482235744511447, 0.0940337875529989, 0.23770627579977693]
siam score:  -0.9209685
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04928324085952402, 0.08680324768781834, 0.18045318473124106, 0.34441561457088743, 0.10393738413940613, 0.2351073280111231]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
maxi score, test score, baseline:  0.022099999999999998 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
maxi score, test score, baseline:  0.0241 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
probs:  [0.04928324085952401, 0.08680324768781836, 0.18045318473124106, 0.3444156145708874, 0.10393738413940612, 0.2351073280111231]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.026099999999999998 0.15 0.15
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843418, 0.3406900680364025, 0.10281409920673802, 0.23256462765928226]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843415, 0.3406900680364025, 0.10281409920673804, 0.23256462765928226]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843415, 0.3406900680364025, 0.10281409920673804, 0.23256462765928226]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843415, 0.3406900680364025, 0.10281409920673804, 0.23256462765928226]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843415, 0.3406900680364025, 0.10281409920673804, 0.23256462765928226]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04875137901817791, 0.08586537457096517, 0.18931445150843415, 0.3406900680364025, 0.10281409920673804, 0.23256462765928226]
actions average: 
K:  2  action  0 :  tensor([    0.7231,     0.0014,     0.0004,     0.1021,     0.1729],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9989,     0.0001,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0211,     0.9349,     0.0013,     0.0426],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0772,     0.0003,     0.0033,     0.8330,     0.0862],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2228, 0.0027, 0.1124, 0.0609, 0.6012], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.91594315
Printing some Q and Qe and total Qs values:  [[0.449]
 [0.449]
 [0.782]
 [0.449]
 [0.449]] [[60.725]
 [60.725]
 [56.302]
 [60.725]
 [60.725]] [[1.66 ]
 [1.66 ]
 [1.858]
 [1.66 ]
 [1.66 ]]
maxi score, test score, baseline:  0.0281 0.15 0.15
siam score:  -0.91680926
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.048967128210896514, 0.08491445568822552, 0.1851116398100339, 0.34220146088596176, 0.10133040190287244, 0.23747491350200986]
maxi score, test score, baseline:  0.0281 0.15 0.15
printing an ep nov before normalisation:  29.62024688720703
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04846060734673899, 0.0840350776319538, 0.1831929844736092, 0.33865341961999806, 0.10028075239553523, 0.2453771585321647]
using another actor
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04846060734673899, 0.0840350776319538, 0.1831929844736092, 0.33865341961999806, 0.10028075239553523, 0.2453771585321647]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04846060734673899, 0.0840350776319538, 0.1831929844736092, 0.33865341961999806, 0.10028075239553523, 0.2453771585321647]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04846060734673899, 0.0840350776319538, 0.1831929844736092, 0.33865341961999806, 0.10028075239553523, 0.2453771585321647]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04846060734673899, 0.0840350776319538, 0.1831929844736092, 0.33865341961999806, 0.10028075239553523, 0.2453771585321647]
printing an ep nov before normalisation:  32.532639503479004
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0281 0.15 0.15
printing an ep nov before normalisation:  57.68475019694416
actions average: 
K:  2  action  0 :  tensor([    0.8185,     0.0011,     0.0002,     0.0027,     0.1775],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9992,     0.0001,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9353,     0.0011,     0.0633],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0326,     0.0002,     0.0004,     0.8768,     0.0899],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0736, 0.0336, 0.1653, 0.0805, 0.6470], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.74668589918484
using another actor
printing an ep nov before normalisation:  67.2322353174784
printing an ep nov before normalisation:  64.44047627785102
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04797216603138629, 0.0830266552916581, 0.19160209336047332, 0.33523202068956043, 0.09926856864891737, 0.24289849597800436]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04797216603138629, 0.0830266552916581, 0.19160209336047332, 0.33523202068956043, 0.09926856864891737, 0.24289849597800436]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04797216603138629, 0.0830266552916581, 0.19160209336047332, 0.33523202068956043, 0.09926856864891737, 0.24289849597800436]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04748597228730073, 0.08218419793393138, 0.1998111828760093, 0.33182636538622345, 0.09826104248353693, 0.24043123903299823]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04748597228730073, 0.08218419793393138, 0.1998111828760093, 0.33182636538622345, 0.09826104248353693, 0.24043123903299823]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
printing an ep nov before normalisation:  81.53858612732768
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
UNIT TEST: sample policy line 217 mcts : [0.042 0.417 0.167 0.042 0.333]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.04820048386703831, 0.0822077448136017, 0.19749235942245158, 0.33683144327417064, 0.0979644423855094, 0.23730352623722845]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.049622519741000175, 0.07439951098552142, 0.19770413618718718, 0.34673662910546404, 0.09125235553127517, 0.240284848449552]
printing an ep nov before normalisation:  18.723944425582886
printing an ep nov before normalisation:  27.551779747009277
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.049622519741000175, 0.07439951098552142, 0.19770413618718718, 0.34673662910546404, 0.09125235553127517, 0.240284848449552]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.049622519741000175, 0.07439951098552142, 0.19770413618718718, 0.34673662910546404, 0.09125235553127517, 0.240284848449552]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.049622519741000175, 0.07439951098552142, 0.19770413618718718, 0.34673662910546404, 0.09125235553127517, 0.240284848449552]
printing an ep nov before normalisation:  32.50468919093404
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05032624321606983, 0.07458651796734046, 0.19531966332680092, 0.3516673058424443, 0.09108790164970526, 0.23701236799763914]
from probs:  [0.05032624321606983, 0.07458651796734046, 0.19531966332680092, 0.3516673058424443, 0.09108790164970526, 0.23701236799763914]
Printing some Q and Qe and total Qs values:  [[0.21 ]
 [0.21 ]
 [0.318]
 [0.21 ]
 [0.21 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.21 ]
 [0.21 ]
 [0.318]
 [0.21 ]
 [0.21 ]]
printing an ep nov before normalisation:  37.24236340186657
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.050322902018411074, 0.0746042643067152, 0.19531953429305357, 0.35164402846415255, 0.09110320484565429, 0.2370060660720133]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099784359510527, 0.07478324219005276, 0.19303281892425403, 0.3563730405527752, 0.09094518040642838, 0.2338678743313843]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099784359510527, 0.07478324219005276, 0.19303281892425403, 0.3563730405527752, 0.09094518040642838, 0.2338678743313843]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actions average: 
K:  3  action  0 :  tensor([    0.8136,     0.0003,     0.0001,     0.0448,     0.1411],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0031,     0.9617,     0.0045,     0.0006,     0.0301],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0013,     0.9527,     0.0003,     0.0454],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0679,     0.0000,     0.0104,     0.8816,     0.0401],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0789, 0.0186, 0.0659, 0.0426, 0.7939], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099784359510527, 0.07478324219005276, 0.19303281892425403, 0.3563730405527752, 0.09094518040642838, 0.2338678743313843]
from probs:  [0.05099784359510527, 0.07478324219005276, 0.19303281892425403, 0.3563730405527752, 0.09094518040642838, 0.2338678743313843]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099784359510527, 0.07478324219005276, 0.19303281892425403, 0.3563730405527752, 0.09094518040642838, 0.2338678743313843]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05100187953558398, 0.07470988213113954, 0.19304812487406148, 0.3564013070133106, 0.09095238603703079, 0.23388642040887372]
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.027]
 [0.02 ]
 [0.035]
 [0.022]] [[18.352]
 [17.78 ]
 [19.731]
 [20.   ]
 [16.205]] [[1.239]
 [1.166]
 [1.407]
 [1.457]
 [0.96 ]]
using another actor
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099852343766673, 0.0747271716320382, 0.1930483270292529, 0.3563779219305062, 0.09096733021596964, 0.23388072575456623]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3885],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0153],
        [0.0637],
        [0.7892],
        [0.0000],
        [0.0000],
        [0.0843]], dtype=torch.float64)
0.0 0.38847602916294277
0.0 0.0
0.0 0.0
0.0 0.0
0.0 0.015292213505800893
0.0 0.06374987813269219
0.0 0.7892492524270444
0.0 0.0
0.0 0.0
0.0 0.08428603738330236
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099852343766673, 0.0747271716320382, 0.1930483270292529, 0.3563779219305062, 0.09096733021596964, 0.23388072575456623]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05099852343766673, 0.0747271716320382, 0.1930483270292529, 0.3563779219305062, 0.09096733021596964, 0.23388072575456623]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05226883007610064, 0.07506623326451815, 0.18874380407318925, 0.3652783846231255, 0.09066903710100242, 0.22797371086206397]
actions average: 
K:  3  action  0 :  tensor([    0.9702,     0.0004,     0.0000,     0.0009,     0.0285],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0086,     0.9819,     0.0014,     0.0000,     0.0081],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0005,     0.9793,     0.0037,     0.0165],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0595,     0.0006,     0.0005,     0.8087,     0.1307],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2118, 0.0220, 0.0407, 0.1842, 0.5413], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05226883007610064, 0.07506623326451815, 0.18874380407318925, 0.3652783846231255, 0.09066903710100242, 0.22797371086206397]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05226883007610064, 0.07506623326451815, 0.18874380407318925, 0.3652783846231255, 0.09066903710100242, 0.22797371086206397]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.052867312522039286, 0.07522597613997123, 0.1867158044159411, 0.3694716797859624, 0.09052850158961417, 0.2251907255464719]
siam score:  -0.9047058
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.052867312522039286, 0.07522597613997123, 0.1867158044159411, 0.3694716797859624, 0.09052850158961417, 0.2251907255464719]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.052867312522039286, 0.07522597613997123, 0.1867158044159411, 0.3694716797859624, 0.09052850158961417, 0.2251907255464719]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.052867312522039286, 0.07522597613997123, 0.1867158044159411, 0.3694716797859624, 0.09052850158961417, 0.2251907255464719]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.053439780935512854, 0.07539550991263284, 0.18476567611958866, 0.3734828256531596, 0.09040710135280325, 0.22250910602630283]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.053439780935512854, 0.07539550991263284, 0.18476567611958866, 0.3734828256531596, 0.09040710135280325, 0.22250910602630283]
printing an ep nov before normalisation:  24.834445126280706
printing an ep nov before normalisation:  23.11537504196167
siam score:  -0.9013445
maxi score, test score, baseline:  0.0281 0.15 0.15
printing an ep nov before normalisation:  16.770164966583252
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.053439780935512854, 0.07539550991263284, 0.18476567611958866, 0.3734828256531596, 0.09040710135280325, 0.22250910602630283]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.053439780935512854, 0.07539550991263284, 0.18476567611958866, 0.3734828256531596, 0.09040710135280325, 0.22250910602630283]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.0539943091648752, 0.07554321074787164, 0.18288680886596084, 0.37736815133873436, 0.09027664578368781, 0.21993087409887008]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.0539943091648752, 0.07554321074787164, 0.18288680886596084, 0.37736815133873436, 0.09027664578368781, 0.21993087409887008]
maxi score, test score, baseline:  0.0281 0.15 0.15
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.0539943091648752, 0.07554321074787164, 0.18288680886596084, 0.37736815133873436, 0.09027664578368781, 0.21993087409887008]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.0539943091648752, 0.07554321074787164, 0.18288680886596084, 0.37736815133873436, 0.09027664578368781, 0.21993087409887008]
printing an ep nov before normalisation:  32.336318526116884
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.7916,     0.0082,     0.0002,     0.0369,     0.1631],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9870,     0.0068,     0.0000,     0.0058],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9296,     0.0386,     0.0318],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.1306,     0.0002,     0.0010,     0.6376,     0.2306],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1260, 0.0136, 0.0340, 0.1748, 0.6516], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.0539943091648752, 0.07554321074787164, 0.18288680886596084, 0.37736815133873436, 0.09027664578368781, 0.21993087409887008]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05452866105386088, 0.07568553753330036, 0.1810763036239035, 0.38111211063508754, 0.09015093680063804, 0.2174464503532097]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05452866105386088, 0.07568553753330036, 0.1810763036239035, 0.38111211063508754, 0.09015093680063804, 0.2174464503532097]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05452866105386088, 0.07568553753330036, 0.1810763036239035, 0.38111211063508754, 0.09015093680063804, 0.2174464503532097]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05403813657801377, 0.07500422561584491, 0.17944460403220372, 0.3776765379675673, 0.08933917951612946, 0.22449731629024094]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05455751615397038, 0.07515231609720444, 0.1777431567446634, 0.38131556995099375, 0.08923341187234587, 0.22199802918082218]
maxi score, test score, baseline:  0.0281 0.15 0.15
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.15 0.15
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.188]
 [0.016]
 [0.019]
 [0.051]] [[ 0.   ]
 [47.49 ]
 [41.416]
 [37.165]
 [30.281]] [[0.027]
 [0.188]
 [0.016]
 [0.019]
 [0.051]]
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05506251629699542, 0.07523307515024501, 0.17611276349496405, 0.38485384295547354, 0.08913731371975174, 0.2196004883825702]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0281 0.15 0.15
probs:  [0.05506251629699542, 0.07523307515024501, 0.17611276349496405, 0.38485384295547354, 0.08913731371975174, 0.2196004883825702]
actor:  0 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05506251629699542, 0.07523307515024501, 0.17611276349496405, 0.38485384295547354, 0.08913731371975174, 0.2196004883825702]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05506251629699542, 0.07523307515024501, 0.17611276349496405, 0.38485384295547354, 0.08913731371975174, 0.2196004883825702]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05506251629699542, 0.07523307515024501, 0.17611276349496405, 0.38485384295547354, 0.08913731371975174, 0.2196004883825702]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05453052495327696, 0.08418301107067952, 0.1744083979188888, 0.38112781556161096, 0.08827530723442119, 0.2174749432611226]
printing an ep nov before normalisation:  55.665970410278575
printing an ep nov before normalisation:  38.02592234153296
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  17.342618850744998
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05501905356302005, 0.08416918070820699, 0.17286600997020352, 0.3845506672801521, 0.0881921470462241, 0.2152029414321932]
from probs:  [0.05501905356302005, 0.08416918070820699, 0.17286600997020352, 0.3845506672801521, 0.0881921470462241, 0.2152029414321932]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05501581717338825, 0.08418227345836195, 0.17286857228607147, 0.3845280975660263, 0.0882047621740896, 0.2152004773420624]
printing an ep nov before normalisation:  30.81434352028552
printing an ep nov before normalisation:  35.84448943571688
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05455469567994173, 0.08347609647869675, 0.17141725630604873, 0.3812984260883899, 0.09586003518440595, 0.21339349026251694]
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.335]
 [0.611]
 [0.353]
 [0.335]] [[52.35 ]
 [52.35 ]
 [64.135]
 [55.097]
 [52.35 ]] [[0.972]
 [0.972]
 [1.522]
 [1.055]
 [0.972]]
printing an ep nov before normalisation:  39.79439437389374
Printing some Q and Qe and total Qs values:  [[0.624]
 [0.624]
 [0.927]
 [0.624]
 [0.624]] [[64.885]
 [64.885]
 [63.947]
 [64.885]
 [64.885]] [[1.204]
 [1.204]
 [1.494]
 [1.204]
 [1.204]]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05455469567994173, 0.08347609647869675, 0.17141725630604873, 0.3812984260883899, 0.09586003518440595, 0.21339349026251694]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.054048046850287676, 0.09200247910525047, 0.16982264934630603, 0.3777498834644462, 0.09496884506377562, 0.21140809616993408]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.054048046850287676, 0.09200247910525047, 0.16982264934630603, 0.3777498834644462, 0.09496884506377562, 0.21140809616993408]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.054048046850287676, 0.09200247910525047, 0.16982264934630603, 0.3777498834644462, 0.09496884506377562, 0.21140809616993408]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
actions average: 
K:  3  action  0 :  tensor([    0.8452,     0.0040,     0.0003,     0.0247,     0.1258],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9994,     0.0001,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9753,     0.0029,     0.0217],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0434,     0.0000,     0.0006,     0.8940,     0.0619],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2305, 0.0393, 0.0871, 0.0744, 0.5686], grad_fn=<DivBackward0>)
siam score:  -0.89394563
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
from probs:  [0.05553842673897393, 0.08619456762627817, 0.16842656573748388, 0.3881417356003556, 0.08932910458685006, 0.21236959971005823]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056483854164641877, 0.08609849295492353, 0.16553676700314232, 0.3947674535871581, 0.08912653814180377, 0.20798689414833044]
maxi score, test score, baseline:  0.0301 0.15 0.15
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.88465358767867
Printing some Q and Qe and total Qs values:  [[1.247]
 [1.247]
 [1.247]
 [1.247]
 [1.247]] [[46.96]
 [46.96]
 [46.96]
 [46.96]
 [46.96]] [[2.387]
 [2.387]
 [2.387]
 [2.387]
 [2.387]]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8999429
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05646819020873915, 0.0852446959118091, 0.17110221402244785, 0.39465997403772535, 0.08830304364641915, 0.2042218821728594]
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05646819020873915, 0.0852446959118091, 0.17110221402244785, 0.39465997403772535, 0.08830304364641915, 0.2042218821728594]
siam score:  -0.8989254
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05646819020873915, 0.0852446959118091, 0.17110221402244785, 0.39465997403772535, 0.08830304364641915, 0.2042218821728594]
printing an ep nov before normalisation:  31.760190465941406
siam score:  -0.8993274
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647997968064002, 0.08505340034814474, 0.17113799347356778, 0.3947425384296387, 0.0883214953416897, 0.20426459272631903]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647997968064002, 0.08505340034814474, 0.17113799347356778, 0.3947425384296387, 0.0883214953416897, 0.20426459272631903]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647997968064002, 0.08505340034814474, 0.17113799347356778, 0.3947425384296387, 0.0883214953416897, 0.20426459272631903]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647997968064002, 0.08505340034814474, 0.17113799347356778, 0.3947425384296387, 0.0883214953416897, 0.20426459272631903]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647997968064002, 0.08505340034814474, 0.17113799347356778, 0.3947425384296387, 0.0883214953416897, 0.20426459272631903]
printing an ep nov before normalisation:  46.80655002593994
using another actor
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647336765327146, 0.08507858498825444, 0.17114335989085908, 0.39469642705590763, 0.08834592760750778, 0.20426233280419961]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647336765327146, 0.08507858498825444, 0.17114335989085908, 0.39469642705590763, 0.08834592760750778, 0.20426233280419961]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647336765327146, 0.08507858498825444, 0.17114335989085908, 0.39469642705590763, 0.08834592760750778, 0.20426233280419961]
UNIT TEST: sample policy line 217 mcts : [0.292 0.417 0.125 0.125 0.042]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05647336765327146, 0.08507858498825444, 0.17114335989085908, 0.39469642705590763, 0.08834592760750778, 0.20426233280419961]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056911624576446426, 0.08505068709930878, 0.16971293782231683, 0.39776776677592446, 0.08826478462459986, 0.20229219910140364]
maxi score, test score, baseline:  0.0301 0.15 0.15
printing an ep nov before normalisation:  24.65359971001794
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056911624576446426, 0.08505068709930878, 0.16971293782231683, 0.39776776677592446, 0.08826478462459986, 0.20229219910140364]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05645254714454274, 0.08436409341026976, 0.16834181392231218, 0.39455272284258525, 0.08755220359364325, 0.20873661908664667]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05645254714454274, 0.08436409341026976, 0.16834181392231218, 0.39455272284258525, 0.08755220359364325, 0.20873661908664667]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05645254714454274, 0.08436409341026976, 0.16834181392231218, 0.39455272284258525, 0.08755220359364325, 0.20873661908664667]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056000833842085325, 0.08368851347283715, 0.16699268442496132, 0.3913892519077323, 0.09486521630549918, 0.2070635000468847]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056000833842085325, 0.08368851347283715, 0.16699268442496132, 0.3913892519077323, 0.09486521630549918, 0.2070635000468847]
printing an ep nov before normalisation:  32.87856578826904
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056000833842085325, 0.08368851347283715, 0.16699268442496132, 0.3913892519077323, 0.09486521630549918, 0.2070635000468847]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056006453460854866, 0.08359641040849157, 0.16700946850770718, 0.3914286076241695, 0.09487474522027287, 0.207084314778504]
printing an ep nov before normalisation:  15.75409700538424
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056006453460854866, 0.08359641040849157, 0.16700946850770718, 0.3914286076241695, 0.09487474522027287, 0.207084314778504]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.056006453460854866, 0.08359641040849157, 0.16700946850770718, 0.3914286076241695, 0.09487474522027287, 0.207084314778504]
printing an ep nov before normalisation:  33.49685716158893
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555618394275579, 0.08293224789447264, 0.17363273208643792, 0.38831485479928934, 0.09412083478538186, 0.20543749100686037]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555618394275579, 0.08293224789447264, 0.17363273208643792, 0.38831485479928934, 0.09412083478538186, 0.20543749100686037]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555618394275579, 0.08293224789447264, 0.17363273208643792, 0.38831485479928934, 0.09412083478538186, 0.20543749100686037]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.62659444977052
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555618394275579, 0.08293224789447264, 0.17363273208643792, 0.38831485479928934, 0.09412083478538186, 0.20543749100686037]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555618394275579, 0.08293224789447264, 0.17363273208643792, 0.38831485479928934, 0.09412083478538186, 0.20543749100686037]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([    0.6721,     0.0028,     0.0003,     0.1356,     0.1892],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9681,     0.0048,     0.0000,     0.0267],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9856,     0.0012,     0.0132],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0928, 0.0018, 0.0054, 0.8268, 0.0732], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1601, 0.0089, 0.0041, 0.1067, 0.7203], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0301 0.15 0.15
printing an ep nov before normalisation:  32.2755915770475
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8847818
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05469350703639447, 0.08163513673573873, 0.18656798349534615, 0.3822336872394832, 0.09264844569816041, 0.20222123979487713]
printing an ep nov before normalisation:  47.84071922302246
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05469350703639447, 0.08163513673573873, 0.18656798349534615, 0.3822336872394832, 0.09264844569816041, 0.20222123979487713]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.446407004045575
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.054714653256066934, 0.0810378868371383, 0.18356219091244605, 0.3823837263643444, 0.09944538745202752, 0.19885615517797678]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.054714653256066934, 0.0810378868371383, 0.18356219091244605, 0.3823837263643444, 0.09944538745202752, 0.19885615517797678]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05514642505861233, 0.08107298022353969, 0.18205229255489608, 0.3854094293022915, 0.09920308869484613, 0.1971157841658143]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05514642505861233, 0.08107298022353969, 0.18205229255489608, 0.3854094293022915, 0.09920308869484615, 0.1971157841658143]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05514642505861233, 0.08107298022353969, 0.18205229255489608, 0.3854094293022915, 0.09920308869484615, 0.1971157841658143]
from probs:  [0.05514642505861233, 0.08107298022353969, 0.18205229255489608, 0.3854094293022915, 0.09920308869484615, 0.1971157841658143]
printing an ep nov before normalisation:  31.32683753967285
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555596659544532, 0.08113031000423529, 0.18059007229156163, 0.388305471054815, 0.09898759420599788, 0.1954268864889369]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555596659544532, 0.08113031000423529, 0.18059007229156163, 0.388305471054815, 0.09898759420599788, 0.1954268864889369]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.0555596659544532, 0.08113031000423529, 0.18059007229156163, 0.388305471054815, 0.09898759420599788, 0.1954268864889369]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055559117963448514, 0.08106838656773616, 0.18060962989014395, 0.3883018049128125, 0.09901627541695272, 0.195444785248906]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055559117963448514, 0.08106838656773616, 0.18060962989014395, 0.3883018049128125, 0.09901627541695272, 0.195444785248906]
printing an ep nov before normalisation:  30.301129262938147
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.903888256626864
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055960118898789046, 0.08112473255560203, 0.1791906046035626, 0.39111206329403236, 0.09880660303131548, 0.19380587761669846]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055960118898789046, 0.08112473255560203, 0.1791906046035626, 0.39111206329403236, 0.09880660303131548, 0.19380587761669846]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055960118898789046, 0.08112473255560203, 0.1791906046035626, 0.39111206329403236, 0.09880660303131548, 0.19380587761669846]
maxi score, test score, baseline:  0.0301 0.15 0.15
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.055960118898789046, 0.08112473255560203, 0.1791906046035626, 0.39111206329403236, 0.09880660303131548, 0.19380587761669846]
maxi score, test score, baseline:  0.0301 0.15 0.15
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  81.31919289449966
printing an ep nov before normalisation:  62.66118843377981
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05595767193326627, 0.08049546687309445, 0.18370531215231845, 0.391096576871574, 0.09788823709607479, 0.1908567350736721]
using another actor
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05595767193326627, 0.08049546687309445, 0.18370531215231845, 0.391096576871574, 0.09788823709607479, 0.1908567350736721]
maxi score, test score, baseline:  0.0301 0.15 0.15
probs:  [0.05595767193326627, 0.08049546687309445, 0.18370531215231845, 0.391096576871574, 0.09788823709607479, 0.1908567350736721]
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05595767193326627, 0.08049546687309445, 0.18370531215231845, 0.391096576871574, 0.09788823709607479, 0.1908567350736721]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [0.05596261599391424, 0.08041409046608484, 0.18372157011100565, 0.3911312023211928, 0.09789689471368682, 0.19087362639411556]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [0.05596261599391424, 0.08041409046608484, 0.18372157011100565, 0.3911312023211928, 0.09789689471368682, 0.19087362639411556]
maxi score, test score, baseline:  0.032100000000000004 0.15 0.15
probs:  [0.05596261599391424, 0.08041409046608484, 0.18372157011100565, 0.3911312023211928, 0.09789689471368682, 0.19087362639411556]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  68.62247553987197
printing an ep nov before normalisation:  34.21527065338161
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.055527918307914925, 0.08756896544450768, 0.18229211818401544, 0.38808682161923513, 0.09713568917633836, 0.18938848726798857]
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]] [[39.217]
 [39.217]
 [39.217]
 [39.217]
 [39.217]] [[1.725]
 [1.725]
 [1.725]
 [1.725]
 [1.725]]
Printing some Q and Qe and total Qs values:  [[1.258]
 [1.474]
 [1.338]
 [1.083]
 [1.186]] [[37.536]
 [39.072]
 [38.129]
 [32.336]
 [38.678]] [[1.75 ]
 [2.   ]
 [1.843]
 [1.46 ]
 [1.703]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.76901146714936
printing an ep nov before normalisation:  29.91760289970687
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.055118921443663545, 0.09366891651183663, 0.18521568809109806, 0.38522399313629296, 0.09555679272601067, 0.18521568809109806]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05550341171144406, 0.09352868861881775, 0.1838293834210201, 0.38791826457218087, 0.09539086825551707, 0.1838293834210201]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.055128978357100844, 0.09289711449471424, 0.1825871631146971, 0.3852959209303578, 0.09474670139457753, 0.18934412170855247]
printing an ep nov before normalisation:  32.337880268625256
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05550564038351126, 0.09276998778141342, 0.18126366185979156, 0.3879353174531168, 0.09459490306259068, 0.18793048945957627]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.055138635437487545, 0.09215599703671438, 0.18006314049544744, 0.3853649890886602, 0.1005914571690425, 0.18668578077264786]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.055138635437487545, 0.09215599703671438, 0.18006314049544744, 0.3853649890886602, 0.1005914571690425, 0.18668578077264786]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05550282538094995, 0.0920593473462374, 0.1788014070220819, 0.38791713911272596, 0.10038300748809041, 0.18533627364991453]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05550282538094995, 0.0920593473462374, 0.1788014070220819, 0.38791713911272596, 0.10038300748809041, 0.18533627364991453]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.88459885
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.056212894075771416, 0.09183782681798092, 0.17636939261647208, 0.39289278799787264, 0.09994937071715426, 0.18273772777474862]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.056212894075771416, 0.09183782681798092, 0.17636939261647208, 0.39289278799787264, 0.09994937071715426, 0.18273772777474862]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.056212894075771416, 0.09183782681798092, 0.17636939261647208, 0.39289278799787264, 0.09994937071715426, 0.18273772777474862]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.023]
 [0.026]
 [0.023]
 [0.023]] [[64.469]
 [64.469]
 [54.179]
 [64.469]
 [64.469]] [[1.023]
 [1.023]
 [0.743]
 [1.023]
 [1.023]]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05620796334741162, 0.09185560815183084, 0.17637252548254997, 0.3928583995433742, 0.09996574640225907, 0.18273975707257425]
printing an ep nov before normalisation:  46.80506244009817
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05620796334741162, 0.09185560815183084, 0.17637252548254997, 0.3928583995433742, 0.09996574640225907, 0.18273975707257425]
siam score:  -0.8810591
printing an ep nov before normalisation:  40.722511511389435
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
printing an ep nov before normalisation:  40.1591505599965
actions average: 
K:  4  action  0 :  tensor([0.6709, 0.0449, 0.0015, 0.0800, 0.2028], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0034,     0.9728,     0.0000,     0.0000,     0.0238],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0478,     0.9097,     0.0029,     0.0396],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0000,     0.0582,     0.8888,     0.0526],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1235, 0.0692, 0.1747, 0.1048, 0.5278], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.056203055254056014, 0.09187330785868025, 0.1763756439668912, 0.39282416895253314, 0.09998204691313523, 0.18274177705470424]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.056203055254056014, 0.09187330785868025, 0.1763756439668912, 0.39282416895253314, 0.09998204691313523, 0.18274177705470424]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05584808331167481, 0.09129251145436384, 0.1815857023256022, 0.390338085612664, 0.09934991497009296, 0.1815857023256022]
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05584808331167481, 0.09129251145436384, 0.1815857023256022, 0.390338085612664, 0.09934991497009296, 0.1815857023256022]
actions average: 
K:  0  action  0 :  tensor([    0.7574,     0.0007,     0.0039,     0.1138,     0.1242],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0220,     0.9681,     0.0042,     0.0000,     0.0057],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9541,     0.0088,     0.0370],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1237,     0.0006,     0.0026,     0.7164,     0.1567],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0506,     0.0007,     0.0665,     0.1630,     0.7193],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05619202498088695, 0.09119344988003138, 0.18035810860892787, 0.3927481602239083, 0.09915014769731767, 0.18035810860892787]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  74.37431062606088
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05549974817027186, 0.09006893628301212, 0.1904717382866207, 0.3878997114071185, 0.09792737588638734, 0.1781324899665896]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
line 256 mcts: sample exp_bonus 63.897702455389876
printing an ep nov before normalisation:  26.451694517536133
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.525]
 [0.301]
 [0.241]
 [0.249]] [[34.344]
 [32.444]
 [39.11 ]
 [38.347]
 [34.344]] [[0.249]
 [0.525]
 [0.301]
 [0.241]
 [0.249]]
printing an ep nov before normalisation:  45.59576976608959
maxi score, test score, baseline:  0.034100000000000005 0.15 0.15
probs:  [0.05515998217504637, 0.08951703063547732, 0.1893036939135115, 0.385520116984372, 0.09732724559904563, 0.1831719306925471]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.007]
 [0.018]
 [0.006]
 [0.014]] [[0.883]
 [0.828]
 [0.946]
 [0.969]
 [0.906]] [[0.011]
 [0.007]
 [0.018]
 [0.006]
 [0.014]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.042100000000000005 0.15 0.15
probs:  [0.05482436080825949, 0.08897185739752544, 0.19424426203322917, 0.3831695499947867, 0.09673443598855432, 0.18205553377764486]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.05482446824043287, 0.08897193202869612, 0.19424423554306533, 0.3831693420289806, 0.0967345031632381, 0.1820555189955869]
maxi score, test score, baseline:  0.0461 0.3 0.3
probs:  [0.05482446824043287, 0.08897193202869612, 0.19424423554306533, 0.3831693420289806, 0.0967345031632381, 0.1820555189955869]
maxi score, test score, baseline:  0.0461 0.3 0.3
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.183080196380615
printing an ep nov before normalisation:  59.58767664404585
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05482446824043287, 0.08897193202869612, 0.19424423554306533, 0.3831693420289806, 0.0967345031632381, 0.1820555189955869]
from probs:  [0.05482446824043287, 0.08897193202869612, 0.19424423554306533, 0.3831693420289806, 0.0967345031632381, 0.1820555189955869]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05482446824043287, 0.08897193202869612, 0.19424423554306533, 0.3831693420289806, 0.0967345031632381, 0.1820555189955869]
printing an ep nov before normalisation:  47.525694491369016
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0548301456749161, 0.08887742840989281, 0.19426438598223789, 0.38320910456480917, 0.09674453122994302, 0.18207440413820097]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0548301456749161, 0.08887742840989281, 0.19426438598223789, 0.38320910456480917, 0.09674453122994302, 0.18207440413820097]
printing an ep nov before normalisation:  38.57916593551636
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
siam score:  -0.87371504
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0548357993247397, 0.0887833206981516, 0.19428445200452488, 0.38324870052216226, 0.09675451728574425, 0.18209321016467728]
printing an ep nov before normalisation:  60.35694995028008
siam score:  -0.8739776
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0548357993247397, 0.0887833206981516, 0.19428445200452488, 0.38324870052216226, 0.09675451728574425, 0.18209321016467728]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0548357993247397, 0.0887833206981516, 0.19428445200452488, 0.38324870052216226, 0.09675451728574425, 0.18209321016467728]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.979909215654647
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
from probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
printing an ep nov before normalisation:  36.13207296999853
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05450410989259856, 0.08824579023226457, 0.1991658747693872, 0.3809256771621512, 0.09616865341348761, 0.1809898945301108]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054845420563531255, 0.0882045152663159, 0.19777841183157943, 0.38331740475511406, 0.09603122216383471, 0.17982302541962453]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05451965040497909, 0.08768011667023652, 0.20255107454677393, 0.3810358298131069, 0.0954602213869741, 0.1787531071779295]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05451965040497909, 0.08768011667023652, 0.20255107454677393, 0.3810358298131069, 0.0954602213869741, 0.1787531071779295]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053837576362355, 0.09911339940288147, 0.200012500570565, 0.37625883289245315, 0.09426470117743214, 0.17651298959431327]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053837576362355, 0.09911339940288147, 0.200012500570565, 0.37625883289245315, 0.09426470117743214, 0.17651298959431327]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05383346012809873, 0.09912844715776219, 0.2000116222863658, 0.37623013566889996, 0.09428051425684529, 0.17651582050202788]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053829362552927, 0.09914342670045853, 0.20001074798347046, 0.37620156853121817, 0.09429625565482178, 0.17651863857710406]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053494417924279766, 0.10475861572636773, 0.1987639561775955, 0.3738557373237347, 0.09370888748991202, 0.1754183853581103]
actions average: 
K:  2  action  0 :  tensor([    0.6885,     0.0550,     0.0005,     0.0405,     0.2155],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9988,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0007,     0.8887,     0.0251,     0.0852],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0821,     0.0001,     0.0005,     0.8171,     0.1003],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1371, 0.0574, 0.1079, 0.1407, 0.5570], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05318061152665369, 0.10416499988566404, 0.2034115934465205, 0.3716580792958022, 0.09318113720043934, 0.1744035786449202]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05318061152665369, 0.10416499988566405, 0.20341159344652054, 0.3716580792958022, 0.09318113720043936, 0.17440357864492023]
printing an ep nov before normalisation:  35.248470227939066
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053526985827709435, 0.1039263461133942, 0.20203411962984436, 0.3740850673274396, 0.09306851942136729, 0.17335896168024512]
printing an ep nov before normalisation:  59.843275211831376
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053526985827709435, 0.1039263461133942, 0.20203411962984436, 0.3740850673274396, 0.09306851942136729, 0.17335896168024512]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.053526985827709435, 0.1039263461133942, 0.20203411962984436, 0.3740850673274396, 0.09306851942136729, 0.17335896168024512]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.0541469971709824, 0.10620110481249154, 0.20104541687250432, 0.3783965695050485, 0.08872302020522631, 0.17148689143374696]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.042 0.    0.042 0.917 0.   ]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
siam score:  -0.89728594
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
siam score:  -0.89686006
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.941]
 [1.352]
 [1.338]
 [0.872]
 [1.234]] [[12.858]
 [10.614]
 [10.575]
 [19.789]
 [11.03 ]] [[1.215]
 [1.516]
 [1.501]
 [1.484]
 [1.419]]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055159185553291606, 0.1028160998349119, 0.2012984767465742, 0.3854525148740727, 0.08466758593249175, 0.1706061370586578]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
printing an ep nov before normalisation:  39.348473243860866
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([    0.8696,     0.0028,     0.0003,     0.0030,     0.1243],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0066,     0.9009,     0.0118,     0.0002,     0.0806],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9785,     0.0019,     0.0195],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0001,     0.0074,     0.9232,     0.0691],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.2038,     0.0004,     0.1513,     0.0659,     0.5785],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055501422050822195, 0.10258012294757018, 0.19986762962135127, 0.38785134847881986, 0.08465180193451566, 0.16954767496692086]
printing an ep nov before normalisation:  47.40016181720456
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05516742817100972, 0.10196213935337105, 0.20469019659695953, 0.38551254225531684, 0.08414196615805467, 0.1685257274652881]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05516742817100972, 0.10196213935337105, 0.20469019659695953, 0.38551254225531684, 0.08414196615805467, 0.1685257274652881]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05516742817100972, 0.10196213935337105, 0.20469019659695953, 0.38551254225531684, 0.08414196615805467, 0.1685257274652881]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.169]
 [0.169]
 [0.003]
 [0.093]
 [0.098]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.169]
 [0.169]
 [0.003]
 [0.093]
 [0.098]]
siam score:  -0.8985372
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054845948196004655, 0.10121189593683248, 0.2034952856532697, 0.38326136517777426, 0.0836512326369333, 0.17353427239918556]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518383758426131, 0.10100048047306236, 0.20207210239458595, 0.3856296762769716, 0.08364786118014353, 0.17246604209097532]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
from probs:  [0.05518383758426131, 0.10100048047306236, 0.20207210239458595, 0.3856296762769716, 0.08364786118014353, 0.17246604209097532]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518383758426131, 0.10100048047306236, 0.20207210239458595, 0.3856296762769716, 0.08364786118014353, 0.17246604209097532]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
printing an ep nov before normalisation:  50.11387547117594
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
Printing some Q and Qe and total Qs values:  [[1.008]
 [1.008]
 [0.986]
 [1.008]
 [1.008]] [[29.496]
 [29.496]
 [31.425]
 [29.496]
 [29.496]] [[1.929]
 [1.929]
 [1.997]
 [1.929]
 [1.929]]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05485953228703977, 0.10040626754420706, 0.20676880256517743, 0.3833587016456121, 0.0831558732088732, 0.17145082274909051]
printing an ep nov before normalisation:  18.554675330686795
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055191363077206325, 0.10020792171210009, 0.205332367345643, 0.38568452251358554, 0.08315832674800448, 0.17042549860346057]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.055191363077206325, 0.10020792171210009, 0.205332367345643, 0.38568452251358554, 0.08315832674800448, 0.17042549860346057]
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.239]
 [0.468]
 [0.06 ]
 [0.155]] [[38.529]
 [41.57 ]
 [54.952]
 [48.455]
 [46.675]] [[0.612]
 [0.786]
 [1.437]
 [0.824]
 [0.863]]
siam score:  -0.8758804
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05552379013978524, 0.09988045639332914, 0.2039592826084407, 0.38801447002042744, 0.0831730658693244, 0.16944893496869318]
printing an ep nov before normalisation:  17.14889249886997
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05552379013978524, 0.09988045639332914, 0.2039592826084407, 0.38801447002042744, 0.0831730658693244, 0.16944893496869318]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05552379013978524, 0.09988045639332914, 0.2039592826084407, 0.38801447002042744, 0.0831730658693244, 0.16944893496869318]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05552379013978524, 0.09988045639332914, 0.2039592826084407, 0.38801447002042744, 0.0831730658693244, 0.16944893496869318]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8690419
printing an ep nov before normalisation:  60.30860736892037
printing an ep nov before normalisation:  32.42433626805422
printing an ep nov before normalisation:  33.5076940926016
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518143961274861, 0.10533252168601459, 0.2027273692415021, 0.3856173643469651, 0.082705809953542, 0.16843549515922776]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518143961274861, 0.10533252168601459, 0.2027273692415021, 0.3856173643469651, 0.082705809953542, 0.16843549515922776]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518143961274861, 0.10533252168601459, 0.2027273692415021, 0.3856173643469651, 0.082705809953542, 0.16843549515922776]
siam score:  -0.8687662
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518143961274861, 0.10533252168601459, 0.2027273692415021, 0.3856173643469651, 0.082705809953542, 0.16843549515922776]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05518143961274861, 0.10533252168601459, 0.2027273692415021, 0.3856173643469651, 0.082705809953542, 0.16843549515922776]
printing an ep nov before normalisation:  21.39828521723416
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054847201163714415, 0.10561352520495665, 0.20410514538581245, 0.3832750090684263, 0.08273201192339659, 0.16942710725369362]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054847201163714415, 0.10561352520495665, 0.20410514538581245, 0.3832750090684263, 0.08273201192339659, 0.16942710725369362]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054847201163714415, 0.10561352520495665, 0.20410514538581245, 0.3832750090684263, 0.08273201192339659, 0.16942710725369362]
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.054847201163714415, 0.10561352520495665, 0.20410514538581245, 0.3832750090684263, 0.08273201192339659, 0.16942710725369362]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05451700840530526, 0.11100702819620156, 0.20287419751233415, 0.380962790968298, 0.08223353742926191, 0.1684054374885992]
using another actor
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05451700840530526, 0.11100702819620156, 0.20287419751233415, 0.380962790968298, 0.08223353742926191, 0.1684054374885992]
printing an ep nov before normalisation:  35.0583815574646
maxi score, test score, baseline:  0.048100000000000004 0.3 0.3
probs:  [0.05451700840530526, 0.11100702819620156, 0.20287419751233415, 0.380962790968298, 0.08223353742926191, 0.1684054374885992]
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05451700840530526, 0.11100702819620156, 0.20287419751233415, 0.380962790968298, 0.08223353742926191, 0.1684054374885992]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05451700840530526, 0.11100702819620156, 0.20287419751233415, 0.380962790968298, 0.08223353742926191, 0.1684054374885992]
printing an ep nov before normalisation:  63.21524908389295
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05452642966038913, 0.11085313757873029, 0.20290931964513578, 0.38102876454275125, 0.08224776019836402, 0.16843458837462957]
printing an ep nov before normalisation:  20.511360607725493
from probs:  [0.054518406074109786, 0.11087819557746, 0.2029067833692319, 0.38097283612337846, 0.0822813927938423, 0.16844238606197767]
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.292 0.292 0.333]
actions average: 
K:  2  action  0 :  tensor([    0.7562,     0.0045,     0.0006,     0.0001,     0.2387],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9984,     0.0000,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0010,     0.9694,     0.0006,     0.0290],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0577,     0.0003,     0.0117,     0.8074,     0.1229],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1190, 0.0023, 0.0400, 0.1166, 0.7221], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.88128245
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054846184061071813, 0.1105656054276822, 0.2015485510016543, 0.383270192631279, 0.08229372368221305, 0.16747574319609962]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054855513224308775, 0.11041406062746072, 0.2015828945380077, 0.38333552171368634, 0.08230773295396857, 0.16750427694256798]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054855513224308775, 0.11041406062746072, 0.2015828945380077, 0.38333552171368634, 0.08230773295396857, 0.16750427694256798]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054855513224308775, 0.11041406062746072, 0.2015828945380077, 0.38333552171368634, 0.08230773295396857, 0.16750427694256798]
printing an ep nov before normalisation:  0.2513513862672312
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using another actor
from probs:  [0.054855513224308775, 0.11041406062746072, 0.2015828945380077, 0.38333552171368634, 0.08230773295396857, 0.16750427694256798]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
printing an ep nov before normalisation:  27.506960641343657
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054546212758008604, 0.10979073299368411, 0.20044426382282918, 0.3811695915934456, 0.0874909339661939, 0.16655826486583858]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.054546212758008604, 0.10979073299368411, 0.20044426382282918, 0.3811695915934456, 0.0874909339661939, 0.16655826486583858]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05455540037288606, 0.10964053392678796, 0.20047808627587566, 0.38123392945893375, 0.08750568428646434, 0.16658636567905225]
maxi score, test score, baseline:  0.050100000000000006 0.3 0.3
probs:  [0.05455540037288606, 0.10964053392678796, 0.20047808627587566, 0.38123392945893375, 0.08750568428646434, 0.16658636567905225]
actions average: 
K:  4  action  0 :  tensor([    0.9183,     0.0002,     0.0000,     0.0444,     0.0370],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9987,     0.0001,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0012,     0.9446,     0.0065,     0.0477],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0152, 0.1013, 0.0079, 0.8661, 0.0094], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1263, 0.0606, 0.1946, 0.0684, 0.5502], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05454753672211265, 0.10966542174181922, 0.20047631671811855, 0.3811791149239531, 0.08753706783947199, 0.16659454205452456]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054241743521011104, 0.10904988144464389, 0.20496567268764954, 0.37903774361970316, 0.0870458827014196, 0.16565907602557287]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054241743521011104, 0.10904988144464389, 0.20496567268764954, 0.37903774361970316, 0.0870458827014196, 0.16565907602557287]
printing an ep nov before normalisation:  28.939428329467773
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05425078841628323, 0.10890105967155225, 0.20499991291695996, 0.37910108210831545, 0.0870604112066869, 0.16568674568020225]
actions average: 
K:  2  action  0 :  tensor([    0.9458,     0.0002,     0.0001,     0.0317,     0.0222],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0035,     0.9909,     0.0000,     0.0000,     0.0056],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0009,     0.9533,     0.0009,     0.0449],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0662,     0.0002,     0.0021,     0.7867,     0.1448],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1319, 0.0248, 0.0068, 0.0913, 0.7452], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0545743228366386, 0.10861732228734426, 0.2036483286092807, 0.3813686521204346, 0.08701936630508597, 0.1647720078412158]
maxi score, test score, baseline:  0.0521 0.3 0.3
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05428226440281234, 0.1078717552544509, 0.20808050357874674, 0.3793234504389885, 0.08655325096825257, 0.1638887753567488]
UNIT TEST: sample policy line 217 mcts : [0.167 0.458 0.167 0.083 0.125]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05428226440281234, 0.1078717552544509, 0.20808050357874674, 0.3793234504389885, 0.08655325096825257, 0.1638887753567488]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05428226440281234, 0.1078717552544509, 0.20808050357874674, 0.3793234504389885, 0.08655325096825257, 0.1638887753567488]
Printing some Q and Qe and total Qs values:  [[0.121]
 [0.236]
 [0.064]
 [0.121]
 [0.104]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.121]
 [0.236]
 [0.064]
 [0.121]
 [0.104]]
maxi score, test score, baseline:  0.0521 0.3 0.3
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05460898489434499, 0.10746007303089743, 0.20675050467143805, 0.3816132788955282, 0.08653234739237602, 0.1630348111154154]
actions average: 
K:  1  action  0 :  tensor([0.5242, 0.0015, 0.0012, 0.1698, 0.3032], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9982,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0023,     0.9604,     0.0011,     0.0359],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0143,     0.0000,     0.0004,     0.9437,     0.0415],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1668, 0.0452, 0.0863, 0.1239, 0.5778], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054625215904449444, 0.10662342278443092, 0.2096878403277799, 0.3817287770483092, 0.08603341705989949, 0.16130132687513105]
Printing some Q and Qe and total Qs values:  [[0.988]
 [1.329]
 [1.242]
 [0.868]
 [1.213]] [[17.717]
 [12.09 ]
 [13.055]
 [20.215]
 [14.091]] [[1.575]
 [1.431]
 [1.427]
 [1.67 ]
 [1.488]]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054625215904449444, 0.10662342278443092, 0.2096878403277799, 0.3817287770483092, 0.08603341705989949, 0.16130132687513105]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054625215904449444, 0.10662342278443092, 0.2096878403277799, 0.3817287770483092, 0.08603341705989949, 0.16130132687513105]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.054625215904449444, 0.10662342278443092, 0.2096878403277799, 0.3817287770483092, 0.08603341705989949, 0.16130132687513105]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
maxi score, test score, baseline:  0.0521 0.3 0.3
using explorer policy with actor:  1
siam score:  -0.8795676
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
siam score:  -0.87738097
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
siam score:  -0.8714362
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.195]
 [0.317]
 [0.195]
 [0.195]] [[ 0.   ]
 [ 0.   ]
 [53.961]
 [ 0.   ]
 [ 0.   ]] [[-0.129]
 [-0.129]
 [ 1.156]
 [-0.129]
 [-0.129]]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0549236155697613, 0.10640015277207819, 0.2083396581496276, 0.3838204271180839, 0.0860348797776732, 0.16048126661277587]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05522304811226242, 0.10615766885196803, 0.2070240115974346, 0.38591903457996035, 0.08600679016807902, 0.15966944669029548]
siam score:  -0.8701229
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05551624181401413, 0.10592023721692088, 0.20573577726053763, 0.38797391640787343, 0.08597928582085983, 0.15887454147979413]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05551624181401413, 0.10592023721692088, 0.20573577726053763, 0.38797391640787343, 0.08597928582085983, 0.15887454147979413]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.055229138701741745, 0.10537184201112638, 0.209849754821783, 0.38596336734438763, 0.08553426362935616, 0.15805163349160511]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.055229138701741745, 0.10537184201112638, 0.209849754821783, 0.38596336734438763, 0.08553426362935616, 0.15805163349160511]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.055229138701741745, 0.10537184201112638, 0.209849754821783, 0.38596336734438763, 0.08553426362935616, 0.15805163349160511]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.055229138701741745, 0.10537184201112638, 0.209849754821783, 0.38596336734438763, 0.08553426362935616, 0.15805163349160511]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05551775819330783, 0.10514612462107205, 0.20855235919488366, 0.3879861734078795, 0.08551202944882935, 0.15728555513402767]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05551775819330783, 0.10514612462107205, 0.20855235919488366, 0.3879861734078795, 0.08551202944882935, 0.15728555513402767]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05523504280110824, 0.1046100767059874, 0.21258897552240785, 0.3860063426667696, 0.0850762055633184, 0.15648335674040847]
siam score:  -0.87159777
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05523504280110824, 0.1046100767059874, 0.21258897552240785, 0.3860063426667696, 0.0850762055633184, 0.15648335674040847]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05495519904339869, 0.10407947360828358, 0.21150998358809733, 0.3840466217265318, 0.08971941549037885, 0.15568930654330967]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05495519904339869, 0.10407947360828358, 0.21150998358809733, 0.3840466217265318, 0.08971941549037885, 0.15568930654330967]
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05495519904339869, 0.10407947360828358, 0.21150998358809733, 0.3840466217265318, 0.08971941549037885, 0.15568930654330967]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05439516646266519, 0.10820215652466096, 0.2143734367276152, 0.3801247633843567, 0.08880424986644743, 0.15410022703425466]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.4301996251477
printing an ep nov before normalisation:  68.78020324196669
Printing some Q and Qe and total Qs values:  [[0.216]
 [0.216]
 [0.332]
 [0.354]
 [0.216]] [[38.066]
 [38.066]
 [42.527]
 [41.741]
 [38.066]] [[1.303]
 [1.303]
 [1.666]
 [1.644]
 [1.303]]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05467652154717782, 0.10797721561237263, 0.21306520494595907, 0.38209685030434126, 0.08877723041773684, 0.1534069771724124]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05467652154717782, 0.10797721561237263, 0.21306520494595907, 0.38209685030434126, 0.08877723041773684, 0.1534069771724124]
printing an ep nov before normalisation:  31.240767237390813
printing an ep nov before normalisation:  32.90524375859255
printing an ep nov before normalisation:  34.42244918169574
maxi score, test score, baseline:  0.0521 0.3 0.3
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05467652154717782, 0.10797721561237263, 0.21306520494595907, 0.38209685030434126, 0.08877723041773684, 0.1534069771724124]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05467652154717783, 0.10797721561237261, 0.21306520494595907, 0.3820968503043413, 0.08877723041773684, 0.1534069771724124]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.05467652154717783, 0.10797721561237261, 0.21306520494595907, 0.3820968503043413, 0.08877723041773684, 0.1534069771724124]
printing an ep nov before normalisation:  38.77354471242148
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  78.17344682297998
printing an ep nov before normalisation:  29.206271171569824
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0552342597222987, 0.10994342745023135, 0.21253155778312588, 0.3859759652401731, 0.08499890524117941, 0.15131588456299155]
maxi score, test score, baseline:  0.0521 0.3 0.3
probs:  [0.0552342597222987, 0.10994342745023135, 0.21253155778312588, 0.3859759652401731, 0.08499890524117941, 0.15131588456299155]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.36683037912453
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  76.56717480382314
printing an ep nov before normalisation:  33.360090531559074
maxi score, test score, baseline:  0.0541 0.3 0.3
printing an ep nov before normalisation:  20.804154632081485
maxi score, test score, baseline:  0.0541 0.3 0.3
probs:  [0.05551849730842837, 0.10967490717344301, 0.21122653123072455, 0.387968295814194, 0.08498241367110354, 0.15062935480210646]
maxi score, test score, baseline:  0.0541 0.3 0.3
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0541 0.3 0.3
probs:  [0.05580640614902434, 0.10926215725031559, 0.20998288284895167, 0.3899862954742219, 0.08498051297029174, 0.14998174530719488]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0541 0.3 0.3
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.026]
 [0.032]
 [0.059]
 [0.051]] [[30.334]
 [32.361]
 [32.696]
 [31.152]
 [31.349]] [[1.102]
 [1.236]
 [1.264]
 [1.184]
 [1.189]]
maxi score, test score, baseline:  0.0541 0.3 0.3
probs:  [0.05608866706262913, 0.10885807339470155, 0.20876331488566732, 0.39196470815925133, 0.08497858970081326, 0.14934664679693735]
maxi score, test score, baseline:  0.0541 0.3 0.3
probs:  [0.05608866706262913, 0.10885807339470155, 0.20876331488566732, 0.39196470815925133, 0.08497858970081326, 0.14934664679693735]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.056100000000000004 0.3 0.3
probs:  [0.05608866706262913, 0.10885807339470155, 0.20876331488566732, 0.39196470815925133, 0.08497858970081326, 0.14934664679693735]
printing an ep nov before normalisation:  39.56162929534912
actor:  0 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05608866706262913, 0.10885807339470155, 0.20876331488566732, 0.39196470815925133, 0.08497858970081326, 0.14934664679693735]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.2018441131536
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.056072901336370835, 0.10810527000861432, 0.21139413631442894, 0.3918563934643968, 0.08458282047931644, 0.14798847839687268]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.056072901336370835, 0.10810527000861432, 0.21139413631442894, 0.3918563934643968, 0.08458282047931644, 0.14798847839687268]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05580117161337137, 0.10758079049342628, 0.21522159233993454, 0.3899535788989925, 0.08417260242064109, 0.14727026423363423]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581018072545329, 0.10743647218908085, 0.21525640151418954, 0.3900166661074937, 0.08418620306921143, 0.1472940763945713]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581018072545329, 0.10743647218908085, 0.21525640151418954, 0.3900166661074937, 0.08418620306921143, 0.1472940763945713]
printing an ep nov before normalisation:  34.65939631731863
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05554099122433704, 0.106917677516493, 0.2190472953491234, 0.38813163969991543, 0.08377981986848991, 0.14658257634164118]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05554099122433704, 0.106917677516493, 0.2190472953491234, 0.38813163969991543, 0.08377981986848991, 0.14658257634164118]
printing an ep nov before normalisation:  49.057090857932515
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.055291962573861056, 0.10611934860435152, 0.21806339217973786, 0.3863877916655595, 0.08821314114781847, 0.14592436382867152]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.055291962573861056, 0.10611934860435152, 0.21806339217973786, 0.3863877916655595, 0.08821314114781847, 0.14592436382867152]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05555953337477871, 0.10590250002189422, 0.21677964111032683, 0.388263228138208, 0.08816695083941593, 0.14532814651537632]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05555953337477871, 0.10590250002189422, 0.21677964111032683, 0.388263228138208, 0.08816695083941593, 0.14532814651537632]
from probs:  [0.05555953337477871, 0.10590250002189422, 0.21677964111032683, 0.388263228138208, 0.08816695083941593, 0.14532814651537632]
actions average: 
K:  3  action  0 :  tensor([    0.9448,     0.0002,     0.0003,     0.0255,     0.0291],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0022,     0.9948,     0.0008,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0007,     0.0005,     0.9389,     0.0011,     0.0589],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0040,     0.0003,     0.0302,     0.9129,     0.0525],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0726, 0.0063, 0.2454, 0.0034, 0.6723], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.055822052064146435, 0.10568974584444767, 0.2155201290594587, 0.39010325374303345, 0.08812163266874204, 0.14474318662017166]
Printing some Q and Qe and total Qs values:  [[1.043]
 [1.208]
 [0.947]
 [1.043]
 [1.043]] [[34.594]
 [26.688]
 [35.945]
 [34.594]
 [34.594]] [[2.296]
 [1.992]
 [2.28 ]
 [2.296]
 [2.296]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.38505697250366
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.415]
 [0.415]
 [0.661]
 [0.415]
 [0.415]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.415]
 [0.415]
 [0.661]
 [0.415]
 [0.415]]
actions average: 
K:  0  action  0 :  tensor([0.7216, 0.0014, 0.0013, 0.1242, 0.1516], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9991,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0004,     0.9538,     0.0014,     0.0443],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0046, 0.0013, 0.0027, 0.9105, 0.0809], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1065, 0.0112, 0.1472, 0.0357, 0.6994], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581085317210291, 0.10501691654152312, 0.21793264331644366, 0.3900267419218472, 0.08769927139884101, 0.1435135736492421]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
from probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
from probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
maxi score, test score, baseline:  0.058100000000000006 0.3 0.3
probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05581111058831057, 0.10490635463902301, 0.21795906689470512, 0.3900287974415204, 0.08774413567008814, 0.14355053476635254]
printing an ep nov before normalisation:  39.04326599019555
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.055553172103456346, 0.10442097301173178, 0.22157893564616107, 0.3882225313655576, 0.08733826131664718, 0.142886126556446]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8799457
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.258]
 [0.006]
 [0.099]
 [0.147]] [[25.68 ]
 [31.758]
 [32.476]
 [21.299]
 [50.888]] [[1.09 ]
 [1.548]
 [1.339]
 [0.765]
 [2.578]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05605900160715652, 0.10403806890918847, 0.21906533982643425, 0.3917678814813237, 0.08726603172138707, 0.14180367645451]
maxi score, test score, baseline:  0.0601 0.3 0.3
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05579498007309325, 0.10816119388966111, 0.2180563710388147, 0.38991924709937775, 0.08689786036101654, 0.14117034753803645]
siam score:  -0.8805765
maxi score, test score, baseline:  0.0601 0.3 0.3
probs:  [0.05579498007309325, 0.10816119388966111, 0.2180563710388147, 0.38991924709937775, 0.08689786036101654, 0.14117034753803645]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using another actor
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05579498007309325, 0.10816119388966111, 0.2180563710388147, 0.38991924709937775, 0.08689786036101654, 0.14117034753803645]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05554417210202357, 0.10767442741825183, 0.2215767528400003, 0.3881628985282037, 0.08650690498470028, 0.14053484412682027]
printing an ep nov before normalisation:  17.574079315892792
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05554417210202357, 0.10767442741825183, 0.2215767528400003, 0.3881628985282037, 0.08650690498470028, 0.14053484412682027]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  77.03655856965432
maxi score, test score, baseline:  0.0621 0.3 0.3
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05529561423851107, 0.10719202794088672, 0.22506555176261483, 0.38642230692510426, 0.08611945703936004, 0.13990504209352317]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0621 0.3 0.3
maxi score, test score, baseline:  0.0621 0.3 0.3
printing an ep nov before normalisation:  82.55364164601149
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05554511363008601, 0.10698026461976096, 0.22380611178187604, 0.3881709993943366, 0.08609498972819288, 0.1394025208457476]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05579021692259564, 0.10677223250027135, 0.22256886272868695, 0.38988888046446685, 0.08607095352318223, 0.1389088538607969]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056031039288170284, 0.10656783382974341, 0.22135322322934295, 0.3915767573544456, 0.08604733712990235, 0.13842380916839547]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056031039288170284, 0.10656783382974341, 0.2213532232293429, 0.3915767573544456, 0.08604733712990234, 0.13842380916839545]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056025656397046146, 0.10590892516932501, 0.21920994638547345, 0.39154046686665195, 0.08996205602458966, 0.1373529491569137]
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056025656397046146, 0.10590892516932501, 0.21920994638547345, 0.39154046686665195, 0.08996205602458966, 0.1373529491569137]
printing an ep nov before normalisation:  55.53574838845568
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056259320861195374, 0.10571627206749175, 0.2180489883433865, 0.39317816314233045, 0.08990568971001286, 0.13689156587558315]
maxi score, test score, baseline:  0.0621 0.3 0.3
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056259320861195374, 0.10571627206749175, 0.2180489883433865, 0.39317816314233045, 0.08990568971001286, 0.13689156587558315]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.380764484405518
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.056489025234520436, 0.10552688400548207, 0.21690770594734368, 0.3947881041236055, 0.08985027867858517, 0.13643800201046327]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.139]
 [0.035]
 [0.075]
 [0.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.139]
 [0.035]
 [0.075]
 [0.074]]
actions average: 
K:  3  action  0 :  tensor([    0.7490,     0.0010,     0.0002,     0.0994,     0.1503],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9837,     0.0001,     0.0000,     0.0145],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9763,     0.0037,     0.0199],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0000,     0.0620,     0.8860,     0.0519],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1319, 0.0021, 0.0720, 0.2430, 0.5509], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0621 0.3 0.3
probs:  [0.05625116174770199, 0.10508204419346835, 0.2202101276976584, 0.3931223662980553, 0.08947160582906571, 0.1358626942340502]
actor:  0 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05625116174770199, 0.10508204419346835, 0.2202101276976584, 0.3931223662980553, 0.08947160582906571, 0.1358626942340502]
printing an ep nov before normalisation:  24.025495550447026
maxi score, test score, baseline:  0.0641 0.3 0.3
probs:  [0.05625116174770199, 0.10508204419346835, 0.2202101276976584, 0.3931223662980553, 0.08947160582906571, 0.1358626942340502]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.056259404417005054, 0.10495069541801272, 0.22024245285566493, 0.3931800890121433, 0.08948472795686424, 0.13588263034030965]
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.056259404417005054, 0.10495069541801272, 0.22024245285566493, 0.3931800890121433, 0.08948472795686424, 0.13588263034030965]
printing an ep nov before normalisation:  53.986975614446266
maxi score, test score, baseline:  0.0661 0.3 0.3
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.056259404417005054, 0.10495069541801272, 0.220242452855665, 0.3931800890121433, 0.08948472795686425, 0.13588263034030967]
printing an ep nov before normalisation:  39.19972003535711
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.056259404417005054, 0.10495069541801272, 0.220242452855665, 0.3931800890121433, 0.08948472795686425, 0.13588263034030967]
actions average: 
K:  4  action  0 :  tensor([    0.7831,     0.0003,     0.0001,     0.1744,     0.0423],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9922,     0.0013,     0.0000,     0.0059],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0010,     0.9499,     0.0064,     0.0426],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0270,     0.0003,     0.0014,     0.8104,     0.1609],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0826, 0.0082, 0.0512, 0.0419, 0.8161], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 21.79232039322493
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0661 0.3 0.3
probs:  [0.05602233158746118, 0.1087285038814461, 0.2193127277396502, 0.3915198880924332, 0.0891073138143753, 0.13530923488463412]
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 42.089835264784746
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
maxi score, test score, baseline:  0.06810000000000001 0.3 0.3
probs:  [0.05676191462534356, 0.1064854076956244, 0.21996337955571496, 0.39667682578540914, 0.08635077386984868, 0.13376169846805933]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05676191462534356, 0.1064854076956244, 0.21996337955571496, 0.39667682578540914, 0.08635077386984868, 0.13376169846805933]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05676191462534356, 0.1064854076956244, 0.21996337955571496, 0.39667682578540914, 0.08635077386984868, 0.13376169846805933]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05676191462534356, 0.1064854076956244, 0.21996337955571496, 0.39667682578540914, 0.08635077386984868, 0.13376169846805933]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05676191462534356, 0.1064854076956244, 0.21996337955571496, 0.39667682578540914, 0.08635077386984868, 0.13376169846805933]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
siam score:  -0.87967086
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05628599960191111, 0.11055626390720447, 0.2201934375337634, 0.3933425780580143, 0.0860028536274689, 0.1336188672716379]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
siam score:  -0.88338065
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05628599960191111, 0.11055626390720447, 0.2201934375337634, 0.3933425780580143, 0.0860028536274689, 0.1336188672716379]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
from probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
siam score:  -0.8877294
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
maxi score, test score, baseline:  0.07010000000000001 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
actor:  0 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05651852905869225, 0.11032279888685997, 0.21901856715909376, 0.39497251314499504, 0.08598021775511953, 0.13318737399523942]
printing an ep nov before normalisation:  33.726438374945026
printing an ep nov before normalisation:  24.880213351811875
maxi score, test score, baseline:  0.0721 0.3 0.3
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05627739027465339, 0.10985155942637394, 0.2223556829444038, 0.3932839336479197, 0.08561308238159115, 0.13261835132505803]
Printing some Q and Qe and total Qs values:  [[0.563]
 [0.563]
 [0.504]
 [0.563]
 [0.563]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.563]
 [0.563]
 [0.504]
 [0.563]
 [0.563]]
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05627739027465339, 0.10985155942637394, 0.2223556829444038, 0.3932839336479197, 0.08561308238159115, 0.13261835132505803]
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05628667407767147, 0.10970447773512784, 0.22239242887452437, 0.3933489436772839, 0.08562721703231684, 0.13264025860307568]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  80.41074332849848
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.7560,     0.0005,     0.0003,     0.0745,     0.1687],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0038,     0.9624,     0.0001,     0.0000,     0.0337],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9808,     0.0007,     0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0292,     0.0001,     0.0245,     0.8596,     0.0866],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0584, 0.0029, 0.1623, 0.1405, 0.6358], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.05627816384837204, 0.10901937285186816, 0.224499787485139, 0.39329095186059626, 0.08524707687538685, 0.1316646470786376]
maxi score, test score, baseline:  0.0721 0.3 0.3
actions average: 
K:  1  action  0 :  tensor([    0.6627,     0.0019,     0.0005,     0.0803,     0.2546],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9993,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0070,     0.9582,     0.0006,     0.0339],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0424,     0.0001,     0.0227,     0.8111,     0.1237],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0485, 0.0014, 0.1356, 0.0447, 0.7699], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.056287293795187786, 0.10887459534575103, 0.2245362726051278, 0.3933548847689951, 0.08526091756993738, 0.13168603591500086]
maxi score, test score, baseline:  0.0721 0.3 0.3
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0721 0.3 0.3
probs:  [0.056514045459346046, 0.1086610020999406, 0.22335417499879914, 0.39494429283692856, 0.08524505576323156, 0.1312814288417541]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056514045459346046, 0.1086610020999406, 0.22335417499879914, 0.39494429283692856, 0.08524505576323156, 0.1312814288417541]
maxi score, test score, baseline:  0.0741 0.3 0.3
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05627888090705079, 0.10820833244585558, 0.22659078845821026, 0.3932975347776157, 0.08489005408671563, 0.13073440932455213]
actions average: 
K:  3  action  0 :  tensor([    0.5653,     0.1027,     0.0003,     0.0660,     0.2657],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0012,     0.9888,     0.0002,     0.0000,     0.0098],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0014,     0.0001,     0.9431,     0.0066,     0.0487],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0101,     0.0000,     0.0028,     0.9393,     0.0477],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0020,     0.0004,     0.2463,     0.1726,     0.5788],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05627888090705079, 0.10820833244585558, 0.22659078845821026, 0.3932975347776157, 0.08489005408671563, 0.13073440932455213]
maxi score, test score, baseline:  0.0741 0.3 0.3
printing an ep nov before normalisation:  47.32332163001921
printing an ep nov before normalisation:  49.826102356876625
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05627888090705079, 0.10820833244585558, 0.22659078845821026, 0.3932975347776157, 0.08489005408671563, 0.13073440932455213]
maxi score, test score, baseline:  0.0741 0.3 0.3
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.05702548482799117, 0.10583233685125783, 0.22745901209323835, 0.39850341221606705, 0.08207127198883395, 0.12910848202261183]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876651, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876651, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876651, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
actions average: 
K:  1  action  0 :  tensor([    0.7771,     0.0001,     0.0004,     0.0249,     0.1975],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0135,     0.9518,     0.0081,     0.0001,     0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0005,     0.9597,     0.0088,     0.0309],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0756,     0.0003,     0.0016,     0.7900,     0.1325],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1556, 0.0026, 0.1574, 0.2123, 0.4721], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876646, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876646, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.418]
 [0.09 ]
 [0.133]
 [0.133]] [[40.457]
 [46.778]
 [48.41 ]
 [40.457]
 [40.457]] [[0.98 ]
 [1.443]
 [1.161]
 [0.98 ]
 [0.98 ]]
maxi score, test score, baseline:  0.0741 0.3 0.3
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.057014566971314976, 0.10586539525328739, 0.22744669728876646, 0.3984272887703392, 0.08211319452942976, 0.1291328571868622]
maxi score, test score, baseline:  0.0741 0.3 0.3
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056772268864741605, 0.1054150067052509, 0.22647840745422193, 0.396730670921423, 0.08602029009378113, 0.12858335596058137]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05699683472051573, 0.10522882802730363, 0.225269956340574, 0.3983049571748896, 0.08599788249832013, 0.12820154123839705]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0741 0.3 0.3
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05676946248822403, 0.10880395570042051, 0.2243697348701794, 0.3967128458331025, 0.08565454799758285, 0.1276894531104909]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0565322002801117, 0.10834870904124023, 0.22761623772522158, 0.3950514826635293, 0.08529627952766008, 0.127155090762237]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0565322002801117, 0.10834870904124023, 0.22761623772522158, 0.3950514826635293, 0.08529627952766008, 0.127155090762237]
actions average: 
K:  2  action  0 :  tensor([0.7990, 0.0009, 0.0013, 0.0238, 0.1751], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9989,     0.0002,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9599,     0.0014,     0.0386],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1345,     0.0005,     0.0017,     0.7360,     0.1274],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1832, 0.0083, 0.0719, 0.2324, 0.5043], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0565322002801117, 0.10834870904124023, 0.22761623772522158, 0.3950514826635293, 0.08529627952766008, 0.127155090762237]
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.334]
 [1.109]
 [0.98 ]
 [0.98 ]] [[34.066]
 [31.28 ]
 [32.91 ]
 [34.066]
 [34.066]] [[1.59 ]
 [1.849]
 [1.679]
 [1.59 ]
 [1.59 ]]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056755075199629, 0.10814121930251, 0.2264181654395519, 0.3966138834792115, 0.08528025291885863, 0.126791403660239]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.05676447985884158, 0.10799319582099226, 0.22645575044943125, 0.3966797373070734, 0.08529439549431346, 0.1268124410693481]
actions average: 
K:  2  action  0 :  tensor([0.6845, 0.0373, 0.0011, 0.0309, 0.2462], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0046, 0.9730, 0.0138, 0.0019, 0.0067], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9881,     0.0015,     0.0103],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0614, 0.0019, 0.0029, 0.7816, 0.1522], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1436, 0.0544, 0.0692, 0.0700, 0.6628], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.056530143563413515, 0.10754687856347167, 0.22965386782341804, 0.3950388546645335, 0.0849420043374422, 0.126288251047721]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056530143563413515, 0.1075468785634717, 0.2296538678234181, 0.3950388546645335, 0.0849420043374422, 0.126288251047721]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056297738611700794, 0.10710423974606978, 0.2287080407016341, 0.3934114957843858, 0.08871010388129526, 0.12576838127491435]
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056297738611700794, 0.10710423974606978, 0.2287080407016341, 0.3934114957843858, 0.08871010388129526, 0.12576838127491435]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[1.033]
 [1.213]
 [1.033]
 [1.033]
 [1.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.033]
 [1.213]
 [1.033]
 [1.033]
 [1.033]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.20596767503271
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.0562793181621726, 0.10650672252778771, 0.23064935770090012, 0.39328454149815884, 0.08834357187829879, 0.12493648823268197]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056497895824118044, 0.10631999680568702, 0.22946087943403692, 0.3948167706842399, 0.08830341129361971, 0.12460104595829839]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056712974156479884, 0.10613626047800821, 0.22829142816710454, 0.3963244695735286, 0.08826389366175116, 0.12427097396312771]
printing an ep nov before normalisation:  53.02317142486572
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0741 0.3 0.3
maxi score, test score, baseline:  0.0741 0.3 0.3
probs:  [0.056712974156479884, 0.10613626047800821, 0.22829142816710454, 0.3963244695735286, 0.08826389366175116, 0.12427097396312771]
printing an ep nov before normalisation:  28.198140210773808
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.056712974156479884, 0.10613626047800821, 0.22829142816710454, 0.3963244695735286, 0.08826389366175116, 0.12427097396312771]
maxi score, test score, baseline:  0.0741 0.3 0.3
printing an ep nov before normalisation:  54.92576962182105
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8937062
printing an ep nov before normalisation:  36.68926677165346
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.655480628429395
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.0564774266239, 0.10574411700761836, 0.23135362969152243, 0.3946753823416103, 0.08794916394998184, 0.12380028038536697]
printing an ep nov before normalisation:  40.88046981876468
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05625369429042626, 0.10532475581390481, 0.23440313630933263, 0.39310872885599124, 0.0876004632036735, 0.12330922152667165]
actions average: 
K:  4  action  0 :  tensor([    0.8197,     0.0535,     0.0006,     0.0040,     0.1221],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9990,     0.0003,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0009,     0.9647,     0.0005,     0.0339],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0281,     0.0008,     0.0123,     0.8164,     0.1425],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0978, 0.0078, 0.0600, 0.0813, 0.7531], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.461]
 [0.461]
 [0.461]
 [0.637]
 [0.461]] [[60.653]
 [60.653]
 [60.653]
 [65.49 ]
 [60.653]] [[1.239]
 [1.239]
 [1.239]
 [1.528]
 [1.239]]
maxi score, test score, baseline:  0.0781 0.3 0.3
probs:  [0.05625369429042626, 0.10532475581390481, 0.23440313630933263, 0.39310872885599124, 0.0876004632036735, 0.12330922152667165]
printing an ep nov before normalisation:  23.55287188510964
Printing some Q and Qe and total Qs values:  [[0.546]
 [0.546]
 [0.675]
 [0.546]
 [0.339]] [[26.639]
 [26.639]
 [25.503]
 [26.639]
 [30.404]] [[0.546]
 [0.546]
 [0.675]
 [0.546]
 [0.339]]
printing an ep nov before normalisation:  32.802828128556555
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.05624395042428483, 0.10535523216248228, 0.23438947343034072, 0.393040795523349, 0.08763700049430823, 0.12333354796523506]
printing an ep nov before normalisation:  54.57478811138617
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.024]
 [0.856]
 [1.07 ]
 [0.955]
 [0.827]] [[30.819]
 [23.775]
 [28.418]
 [34.497]
 [29.557]] [[1.918]
 [1.458]
 [1.864]
 [2.002]
 [1.669]]
maxi score, test score, baseline:  0.0861 0.3 0.3
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.056475125098839896, 0.10490222320371738, 0.2332733862969597, 0.3946611626763864, 0.08763075883259908, 0.1230573438914976]
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.056475125098839896, 0.10490222320371738, 0.2332733862969597, 0.3946611626763864, 0.08763075883259908, 0.1230573438914976]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.05625870009501339, 0.10833798553062447, 0.2323778296792013, 0.3931456704308754, 0.08729465631793457, 0.12258515794635079]
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.05625870009501339, 0.10833798553062451, 0.23237782967920131, 0.3931456704308754, 0.08729465631793457, 0.1225851579463508]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.05625870009501339, 0.10833798553062451, 0.23237782967920131, 0.3931456704308754, 0.08729465631793457, 0.1225851579463508]
printing an ep nov before normalisation:  40.37528772651826
maxi score, test score, baseline:  0.0861 0.3 0.3
probs:  [0.05625870009501339, 0.10833798553062451, 0.23237782967920131, 0.3931456704308754, 0.08729465631793457, 0.1225851579463508]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0881 0.3 0.3
probs:  [0.056469719201785874, 0.10814359873326274, 0.2312178673224764, 0.3946248700810887, 0.08726407917798472, 0.12227986548340163]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0881 0.3 0.3
probs:  [0.056677478371628655, 0.10795221492794778, 0.2300758246896171, 0.3960812182576139, 0.0872339744101781, 0.12197928934301462]
printing an ep nov before normalisation:  30.81172521272056
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  64.8101885441372
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8919319
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0901 0.3 0.3
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.7551,     0.0004,     0.0013,     0.0358,     0.2075],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9995,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9540,     0.0003,     0.0456],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0739,     0.0005,     0.0056,     0.7544,     0.1656],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1232, 0.0012, 0.1360, 0.1988, 0.5407], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0901 0.3 0.3
probs:  [0.05748627930523279, 0.10707692995089578, 0.2257137931059715, 0.4017506786037881, 0.0871315640970522, 0.1208407549370596]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [0.05748627930523279, 0.10707692995089578, 0.2257137931059715, 0.4017506786037881, 0.0871315640970522, 0.1208407549370596]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.895]
 [1.092]
 [0.895]
 [0.895]
 [0.418]] [[46.991]
 [36.311]
 [46.991]
 [46.991]
 [43.667]] [[1.562]
 [1.503]
 [1.562]
 [1.562]
 [1.005]]
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [0.0572720964223271, 0.10667755259188812, 0.2286028490526216, 0.40025085287815776, 0.08680667197935255, 0.12038997707565309]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [0.056854177087086975, 0.10950234307689913, 0.23063598561665039, 0.3973243530490027, 0.08617273384194517, 0.1195104073284156]
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [0.056854177087086975, 0.10950234307689913, 0.23063598561665039, 0.3973243530490027, 0.08617273384194517, 0.1195104073284156]
printing an ep nov before normalisation:  56.638413031498224
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.182774775096178
maxi score, test score, baseline:  0.0921 0.3 0.3
probs:  [0.05687227753485206, 0.10921838540500609, 0.23070954226762128, 0.3974511022870146, 0.08620019025081374, 0.1195485022546924]
maxi score, test score, baseline:  0.0921 0.3 0.3
actions average: 
K:  2  action  0 :  tensor([0.6939, 0.0032, 0.0008, 0.1102, 0.1919], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9745,     0.0108,     0.0000,     0.0139],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0012,     0.9493,     0.0128,     0.0366],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0446,     0.0005,     0.0273,     0.8019,     0.1257],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2935, 0.0262, 0.0600, 0.1571, 0.4632], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8833551
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  59.46248692498203
printing an ep nov before normalisation:  45.7571509263164
maxi score, test score, baseline:  0.0941 0.3 0.3
probs:  [0.05770213487735983, 0.10653427207681938, 0.2321831174092749, 0.4032458503372279, 0.0834329148632289, 0.11690171043608925]
from probs:  [0.05770213487735983, 0.10653427207681938, 0.2321831174092749, 0.4032458503372279, 0.0834329148632289, 0.11690171043608925]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.057891681030043746, 0.1063631668598917, 0.23108402847592358, 0.40457466198844866, 0.08343242548654056, 0.11665403615915172]
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.057891681030043746, 0.1063631668598917, 0.23108402847592358, 0.40457466198844866, 0.08343242548654056, 0.11665403615915172]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0961 0.3 0.3
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.057679092611766535, 0.10597217206573581, 0.23391168409609747, 0.40308605627561744, 0.08312583063174266, 0.11622516431904004]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.05725857624428768, 0.10519875405983489, 0.23950500606308334, 0.40014147880502465, 0.08251936224709525, 0.11537682258067414]
printing an ep nov before normalisation:  73.84165266572147
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.05726750391041571, 0.10505901680088002, 0.23954241557296213, 0.40020399291048786, 0.08253223772038017, 0.11539483308487407]
Printing some Q and Qe and total Qs values:  [[0.808]
 [1.21 ]
 [0.495]
 [0.485]
 [1.092]] [[32.548]
 [37.598]
 [40.582]
 [41.794]
 [39.733]] [[1.369]
 [1.998]
 [1.418]
 [1.463]
 [1.977]]
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.05726750391041571, 0.10505901680088002, 0.23954241557296213, 0.40020399291048786, 0.08253223772038017, 0.11539483308487407]
printing an ep nov before normalisation:  27.131112598316008
printing an ep nov before normalisation:  28.075515222950834
maxi score, test score, baseline:  0.0961 0.3 0.3
probs:  [0.05726750391041571, 0.10505901680088002, 0.23954241557296213, 0.40020399291048786, 0.08253223772038017, 0.11539483308487407]
actor:  0 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.001550389708881994
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 0.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.721]
 [0.535]
 [0.535]] [[59.805]
 [59.805]
 [65.308]
 [59.805]
 [59.805]] [[1.86 ]
 [1.86 ]
 [2.218]
 [1.86 ]
 [1.86 ]]
printing an ep nov before normalisation:  22.50666071243336
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0981 0.3 0.3
probs:  [0.0574340433097147, 0.10714311383868606, 0.23813277690253146, 0.401374600505326, 0.08198842389116272, 0.11392704155257903]
printing an ep nov before normalisation:  22.3445525214796
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.0981 0.3 0.3
maxi score, test score, baseline:  0.0981 0.3 0.3
probs:  [0.05762584947469931, 0.10683616235393152, 0.23707862480795622, 0.4027191254280007, 0.08201092209982945, 0.11372931583558263]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.78415221184875
printing an ep nov before normalisation:  41.98117256164551
using another actor
printing an ep nov before normalisation:  36.91686528959316
printing an ep nov before normalisation:  31.886821696719483
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.81339013396058
printing an ep nov before normalisation:  59.62798881440981
maxi score, test score, baseline:  0.0981 0.3 0.3
probs:  [0.05779581215573292, 0.10670184257329579, 0.23598812842226038, 0.40391096984378844, 0.08205885711919296, 0.11354438988572947]
maxi score, test score, baseline:  0.0981 0.3 0.3
probs:  [0.05779581215573292, 0.10670184257329579, 0.23598812842226038, 0.40391096984378844, 0.08205885711919296, 0.11354438988572947]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0981 0.3 0.3
probs:  [0.05760346901867841, 0.10967936504241706, 0.235201395198988, 0.4025641020744158, 0.08178558056317908, 0.11316608810232177]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9075132
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
from probs:  [0.057781953907065364, 0.10949707308062875, 0.2341494985616153, 0.40381529991073584, 0.081796534084854, 0.11295964045510062]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.10010000000000001 0.3 0.3
probs:  [0.057781953907065364, 0.10949707308062875, 0.2341494985616153, 0.40381529991073584, 0.081796534084854, 0.11295964045510062]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  78.18810231469324
actor:  0 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1021 0.3 0.3
maxi score, test score, baseline:  0.1021 0.3 0.3
probs:  [0.05795798275801493, 0.10931728954342897, 0.23311207652511812, 0.4050492806663531, 0.0818073368808314, 0.11275603362625368]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.51 ]
 [0.515]
 [0.51 ]
 [0.51 ]] [[38.203]
 [38.203]
 [51.111]
 [38.203]
 [38.203]] [[0.975]
 [0.975]
 [1.226]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  0.1021 0.3 0.3
probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
maxi score, test score, baseline:  0.1021 0.3 0.3
from probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
printing an ep nov before normalisation:  34.6517596926008
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1021 0.3 0.3
probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
maxi score, test score, baseline:  0.1021 0.3 0.3
probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
Printing some Q and Qe and total Qs values:  [[0.586]
 [0.586]
 [0.905]
 [0.586]
 [0.586]] [[31.22 ]
 [31.22 ]
 [32.649]
 [31.22 ]
 [31.22 ]] [[0.586]
 [0.586]
 [0.905]
 [0.586]
 [0.586]]
maxi score, test score, baseline:  0.1021 0.3 0.3
probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
actor:  0 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([    0.8831,     0.0082,     0.0004,     0.0503,     0.0580],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0013,     0.9765,     0.0130,     0.0000,     0.0092],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9469,     0.0113,     0.0417],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0085,     0.0006,     0.0019,     0.8750,     0.1140],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1862, 0.0770, 0.0451, 0.1551, 0.5366], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1041 0.3 0.3
probs:  [0.058121511732918414, 0.10916831500563141, 0.23207632256253907, 0.4061959999348249, 0.0818554244374297, 0.11258242632665662]
printing an ep nov before normalisation:  35.71632493927266
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Starting evaluation
printing an ep nov before normalisation:  39.746201038360596
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1041 0.3 0.3
probs:  [0.05790070370817415, 0.10825937392424662, 0.23624670344930981, 0.40465108440334047, 0.08131467297160175, 0.11162746154332723]
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.281]
 [0.347]
 [0.296]
 [0.312]] [[39.975]
 [42.772]
 [56.028]
 [49.36 ]
 [48.276]] [[0.339]
 [0.281]
 [0.347]
 [0.296]
 [0.312]]
printing an ep nov before normalisation:  44.828944071707
maxi score, test score, baseline:  0.1041 0.3 0.3
probs:  [0.05790070370817415, 0.10825937392424662, 0.23624670344930981, 0.40465108440334047, 0.08131467297160175, 0.11162746154332723]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.795]
 [0.942]
 [0.877]
 [0.835]
 [0.872]] [[29.216]
 [25.264]
 [28.339]
 [31.257]
 [27.437]] [[0.795]
 [0.942]
 [0.877]
 [0.835]
 [0.872]]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.7065,     0.0003,     0.0002,     0.1203,     0.1728],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0010,     0.9974,     0.0001,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9664,     0.0008,     0.0326],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0315,     0.0003,     0.0326,     0.7838,     0.1517],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1050, 0.0205, 0.2508, 0.0735, 0.5502], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  43.03641684060462
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.339]
 [0.339]
 [0.425]
 [0.339]
 [0.339]] [[49.022]
 [49.022]
 [52.764]
 [49.022]
 [49.022]] [[0.339]
 [0.339]
 [0.425]
 [0.339]
 [0.339]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.028]
 [1.185]
 [1.202]
 [1.055]
 [1.15 ]] [[37.622]
 [29.625]
 [32.521]
 [39.381]
 [34.531]] [[2.623]
 [2.293]
 [2.486]
 [2.757]
 [2.556]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.89988714
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.232]
 [0.174]
 [0.252]
 [0.258]
 [0.249]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.232]
 [0.174]
 [0.252]
 [0.258]
 [0.249]]
Printing some Q and Qe and total Qs values:  [[0.42 ]
 [0.42 ]
 [0.627]
 [0.42 ]
 [0.42 ]] [[ 0.   ]
 [ 0.   ]
 [48.597]
 [ 0.   ]
 [ 0.   ]] [[-0.254]
 [-0.254]
 [ 1.526]
 [-0.254]
 [-0.254]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.999]
 [0.914]
 [0.914]
 [0.914]] [[63.943]
 [45.487]
 [63.943]
 [63.943]
 [63.943]] [[2.247]
 [1.803]
 [2.247]
 [2.247]
 [2.247]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.70595932006836
Printing some Q and Qe and total Qs values:  [[0.378]
 [0.39 ]
 [0.07 ]
 [0.201]
 [0.206]] [[54.27 ]
 [57.367]
 [52.033]
 [40.066]
 [40.495]] [[0.576]
 [0.609]
 [0.253]
 [0.308]
 [0.316]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.308]
 [0.538]
 [0.308]
 [0.308]
 [0.308]] [[34.833]
 [53.744]
 [34.833]
 [34.833]
 [34.833]] [[0.761]
 [1.459]
 [0.761]
 [0.761]
 [0.761]]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.963]
 [0.963]
 [0.963]
 [0.963]
 [0.963]] [[46.837]
 [46.837]
 [46.837]
 [46.837]
 [46.837]] [[2.296]
 [2.296]
 [2.296]
 [2.296]
 [2.296]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.914]
 [0.914]
 [1.002]
 [0.914]
 [1.057]] [[18.197]
 [18.197]
 [32.465]
 [18.197]
 [27.032]] [[1.355]
 [1.355]
 [2.37 ]
 [1.355]
 [2.072]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9019756
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8378,     0.0053,     0.0003,     0.0794,     0.0773],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9974,     0.0002,     0.0001,     0.0015],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0037,     0.9429,     0.0008,     0.0526],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0543,     0.0001,     0.0149,     0.8021,     0.1286],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2616, 0.0174, 0.0758, 0.1264, 0.5188], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  28.866136751398233
Printing some Q and Qe and total Qs values:  [[0.51 ]
 [0.528]
 [0.585]
 [0.51 ]
 [0.51 ]] [[69.98 ]
 [71.53 ]
 [69.786]
 [69.98 ]
 [69.98 ]] [[1.942]
 [2.017]
 [2.011]
 [1.942]
 [1.942]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 46.14143960267543
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  50.953479625396945
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.235]
 [0.213]
 [0.235]
 [0.235]
 [0.235]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.235]
 [0.213]
 [0.235]
 [0.235]
 [0.235]]
printing an ep nov before normalisation:  28.948693026718587
printing an ep nov before normalisation:  22.592326396845625
Printing some Q and Qe and total Qs values:  [[1.255]
 [1.309]
 [1.255]
 [1.255]
 [1.255]] [[25.532]
 [19.285]
 [23.25 ]
 [25.532]
 [25.532]] [[2.588]
 [2.162]
 [2.412]
 [2.588]
 [2.588]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.88779724
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.41266821223828
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.5214725304236
printing an ep nov before normalisation:  8.701280462728763
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.507]
 [0.171]
 [0.19 ]
 [0.205]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.179]
 [0.507]
 [0.171]
 [0.19 ]
 [0.205]]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.050283608400008
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  53.86134519125722
printing an ep nov before normalisation:  78.92549834572343
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [1.04 ]
 [0.737]
 [0.737]] [[63.183]
 [63.183]
 [59.395]
 [63.183]
 [63.183]] [[1.554]
 [1.554]
 [1.786]
 [1.554]
 [1.554]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.215077051178916
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.220888029324517
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.552]
 [0.327]
 [0.588]
 [0.552]
 [0.558]] [[27.304]
 [24.354]
 [23.647]
 [27.304]
 [20.881]] [[0.552]
 [0.327]
 [0.588]
 [0.552]
 [0.558]]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8923024
printing an ep nov before normalisation:  19.068613688401786
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.439]
 [0.079]
 [0.253]
 [0.273]] [[46.8  ]
 [53.909]
 [61.596]
 [61.772]
 [62.631]] [[0.661]
 [1.109]
 [0.969]
 [1.149]
 [1.193]]
printing an ep nov before normalisation:  67.7416071891471
printing an ep nov before normalisation:  28.606106104875725
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8007,     0.0268,     0.0000,     0.0410,     0.1315],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9998,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0399,     0.9483,     0.0003,     0.0115],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0409,     0.0004,     0.0081,     0.8328,     0.1178],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1306, 0.0527, 0.0060, 0.0396, 0.7711], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.08273643581738
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 28.187403678894043
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  11.580434976731041
siam score:  -0.9024803
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.817]
 [0.817]
 [0.817]
 [0.817]
 [0.817]]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 64.30711487841023
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.903]
 [0.903]
 [1.017]
 [0.903]
 [0.886]] [[66.858]
 [66.858]
 [55.754]
 [66.858]
 [60.419]] [[2.57 ]
 [2.57 ]
 [2.312]
 [2.57 ]
 [2.337]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.639]
 [0.639]
 [0.639]
 [0.639]
 [0.639]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  41.68597048442972
printing an ep nov before normalisation:  57.66332305562494
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  52.362198545920705
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.037]
 [1.208]
 [1.196]
 [1.041]
 [1.037]] [[31.5  ]
 [27.148]
 [26.774]
 [32.38 ]
 [31.5  ]] [[2.222]
 [2.08 ]
 [2.041]
 [2.29 ]
 [2.222]]
siam score:  -0.89488584
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.142]
 [1.328]
 [1.155]
 [1.144]
 [1.134]] [[19.316]
 [16.134]
 [16.614]
 [21.646]
 [19.56 ]] [[2.087]
 [2.117]
 [1.968]
 [2.202]
 [2.09 ]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.364415129021296
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  18.31031797494644
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.92717563354711
siam score:  -0.89386207
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.81255762412639
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.85020417203142
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.499]
 [1.399]
 [1.399]
 [1.399]
 [1.399]] [[30.758]
 [28.74 ]
 [28.74 ]
 [28.74 ]
 [28.74 ]] [[2.643]
 [2.425]
 [2.425]
 [2.425]
 [2.425]]
printing an ep nov before normalisation:  57.88029606570709
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  49.307379814376965
actions average: 
K:  0  action  0 :  tensor([    0.7164,     0.0002,     0.0005,     0.1296,     0.1533],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9989,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0040,     0.0015,     0.9526,     0.0009,     0.0410],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0327,     0.0001,     0.0087,     0.9071,     0.0514],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2234, 0.0017, 0.1323, 0.0921, 0.5505], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.17795276641846
printing an ep nov before normalisation:  34.18695404088511
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.89024574
maxi score, test score, baseline:  0.1541 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.701691655559273
printing an ep nov before normalisation:  47.01734329226086
Printing some Q and Qe and total Qs values:  [[0.635]
 [0.635]
 [0.635]
 [1.045]
 [0.635]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.635]
 [0.635]
 [0.635]
 [1.045]
 [0.635]]
actions average: 
K:  4  action  0 :  tensor([    0.7998,     0.0019,     0.0001,     0.0656,     0.1327],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9988,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0047,     0.9327,     0.0029,     0.0592],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0507,     0.0007,     0.0019,     0.7457,     0.2010],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0390, 0.0019, 0.0023, 0.1542, 0.8027], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.03836631774902
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.78388256282649
printing an ep nov before normalisation:  61.58113641515882
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.142]
 [1.114]
 [1.114]
 [1.114]] [[41.974]
 [42.618]
 [41.974]
 [41.974]
 [41.974]] [[2.308]
 [2.369]
 [2.308]
 [2.308]
 [2.308]]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.225]
 [1.353]
 [1.292]
 [1.254]
 [1.275]] [[34.775]
 [29.083]
 [33.945]
 [33.675]
 [35.528]] [[2.711]
 [2.596]
 [2.742]
 [2.693]
 [2.793]]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.895921
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.95724926456793
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.517189652199015
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.955365515508866
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.69728744929127
printing an ep nov before normalisation:  26.68119416453977
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.23358380252079
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8963467
printing an ep nov before normalisation:  22.5253095101593
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.022422075271606
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.187]
 [1.187]
 [1.187]
 [1.197]
 [1.187]] [[37.466]
 [37.466]
 [37.466]
 [35.572]
 [37.466]] [[2.288]
 [2.288]
 [2.288]
 [2.197]
 [2.288]]
siam score:  -0.8974896
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.678916494183213
Printing some Q and Qe and total Qs values:  [[0.151]
 [0.889]
 [0.113]
 [0.889]
 [0.126]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.151]
 [0.889]
 [0.113]
 [0.889]
 [0.126]]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.559]
 [0.559]
 [0.579]
 [0.559]
 [0.559]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.559]
 [0.559]
 [0.579]
 [0.559]
 [0.559]]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.74962701645411
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.90089417
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  20.375371401846483
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.274]
 [0.282]
 [0.057]
 [0.25 ]
 [0.264]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.274]
 [0.282]
 [0.057]
 [0.25 ]
 [0.264]]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.202740907669067
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  21.16381265592555
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  74.55383088572245
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.157]
 [0.637]
 [0.484]
 [0.484]] [[31.883]
 [34.354]
 [50.679]
 [31.883]
 [31.883]] [[0.805]
 [0.541]
 [1.432]
 [0.805]
 [0.805]]
printing an ep nov before normalisation:  13.102848529815674
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.    0.292 0.25  0.083 0.375]
siam score:  -0.89199585
maxi score, test score, baseline:  0.1581 1.0 1.0
siam score:  -0.89090025
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.5988218502267
printing an ep nov before normalisation:  47.23151967450434
Printing some Q and Qe and total Qs values:  [[1.515]
 [0.623]
 [0.623]
 [1.515]
 [1.515]] [[20.603]
 [15.315]
 [18.353]
 [20.603]
 [20.603]] [[2.083]
 [1.033]
 [1.124]
 [2.083]
 [2.083]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.16390728604009
printing an ep nov before normalisation:  37.081488097542206
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.260170459747314
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.907160061440834
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.66505153006044
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.82 ]
 [1.057]
 [0.82 ]
 [0.82 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.82 ]
 [0.82 ]
 [1.057]
 [0.82 ]
 [0.82 ]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.253]
 [1.384]
 [1.278]
 [1.236]
 [1.253]] [[21.875]
 [25.332]
 [24.872]
 [24.571]
 [21.875]] [[1.881]
 [2.202]
 [2.07 ]
 [2.012]
 [1.881]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
printing an ep nov before normalisation:  41.73504997822685
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 53.82693165585804
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  17.095648413286906
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.823]
 [1.175]
 [0.503]
 [0.349]
 [1.036]] [[47.633]
 [49.341]
 [60.973]
 [59.718]
 [53.862]] [[2.157]
 [2.594]
 [2.503]
 [2.287]
 [2.681]]
printing an ep nov before normalisation:  31.365879758842546
printing an ep nov before normalisation:  55.09238952813801
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.8840, 0.0027, 0.0013, 0.0398, 0.0723], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0049,     0.9916,     0.0000,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0121,     0.9297,     0.0012,     0.0569],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0241,     0.0002,     0.0020,     0.8662,     0.1075],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1392, 0.0302, 0.0173, 0.0780, 0.7352], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.502145623140365
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.58952534395721
printing an ep nov before normalisation:  26.959697448079574
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.640907229451866
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.146860122680664
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  0.0009252123879832652
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1661 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.89048904
actions average: 
K:  3  action  0 :  tensor([    0.9099,     0.0024,     0.0002,     0.0019,     0.0856],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0032,     0.9902,     0.0001,     0.0000,     0.0066],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9460,     0.0008,     0.0528],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0000,     0.0123,     0.9727,     0.0150],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0378, 0.0275, 0.0293, 0.2478, 0.6576], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.44 ]
 [0.44 ]
 [0.824]
 [0.44 ]
 [0.44 ]] [[18.661]
 [18.661]
 [24.27 ]
 [18.661]
 [18.661]] [[0.44 ]
 [0.44 ]
 [0.824]
 [0.44 ]
 [0.44 ]]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.872]
 [0.749]
 [0.749]
 [0.749]] [[15.177]
 [15.555]
 [15.177]
 [15.177]
 [15.177]] [[0.749]
 [0.872]
 [0.749]
 [0.749]
 [0.749]]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.319]
 [1.349]
 [1.319]
 [1.319]
 [1.319]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.319]
 [1.349]
 [1.319]
 [1.319]
 [1.319]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  23.106393278036577
printing an ep nov before normalisation:  27.8456201751042
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.53 ]
 [0.823]
 [0.53 ]
 [0.53 ]] [[49.925]
 [49.925]
 [51.39 ]
 [49.925]
 [49.925]] [[0.98 ]
 [0.98 ]
 [1.299]
 [0.98 ]
 [0.98 ]]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.627138137817383
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.35776401669679
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.157]
 [1.33 ]
 [1.158]
 [1.   ]
 [1.157]] [[28.748]
 [35.046]
 [30.631]
 [26.06 ]
 [28.748]] [[1.537]
 [1.849]
 [1.579]
 [1.322]
 [1.537]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  32.05522537231445
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  53.712354130712114
printing an ep nov before normalisation:  15.708091855049135
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.03694172770525
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.276]
 [0.315]
 [0.02 ]
 [0.224]
 [0.24 ]] [[30.862]
 [40.97 ]
 [38.007]
 [31.07 ]
 [30.465]] [[0.276]
 [0.315]
 [0.02 ]
 [0.224]
 [0.24 ]]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.586]
 [0.542]
 [0.542]] [[56.524]
 [56.524]
 [60.809]
 [56.524]
 [56.524]] [[1.218]
 [1.218]
 [1.343]
 [1.218]
 [1.218]]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.207]
 [0.093]
 [0.093]
 [0.093]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.093]
 [0.207]
 [0.093]
 [0.093]
 [0.093]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.75161157124104
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([    0.7171,     0.0005,     0.0005,     0.0967,     0.1852],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0330,     0.9440,     0.0005,     0.0000,     0.0224],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9759,     0.0005,     0.0233],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0208, 0.0016, 0.0044, 0.8708, 0.1024], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2191, 0.0008, 0.1192, 0.2068, 0.4541], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  33.97424790009294
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.02684554433565
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8724948
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8734528
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.03689849896888
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.109]
 [1.261]
 [1.109]
 [1.109]] [[32.003]
 [32.003]
 [24.693]
 [32.003]
 [32.003]] [[1.776]
 [1.776]
 [1.637]
 [1.776]
 [1.776]]
printing an ep nov before normalisation:  35.19516679023016
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86625266
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  27.245311132170833
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.638]
 [0.756]
 [0.638]
 [0.638]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.638]
 [0.638]
 [0.756]
 [0.638]
 [0.638]]
siam score:  -0.868348
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  47.9108664864515
Printing some Q and Qe and total Qs values:  [[1.283]
 [1.275]
 [1.176]
 [1.075]
 [1.283]] [[43.235]
 [37.356]
 [44.016]
 [50.856]
 [43.235]] [[2.181]
 [1.969]
 [2.101]
 [2.236]
 [2.181]]
printing an ep nov before normalisation:  58.900830246548324
printing an ep nov before normalisation:  27.99889888236251
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
deleting a thread, now have 1 threads
Frames:  79471 train batches done:  9312 episodes:  5740
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.869993
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.380555933260286
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  10.633015186471653
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8813551
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.14755022820001
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.40727796897072
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.20594210241359
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.8627,     0.0002,     0.0005,     0.0603,     0.0762],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0038,     0.9742,     0.0001,     0.0001,     0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9341,     0.0014,     0.0643],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0796,     0.0002,     0.0015,     0.8233,     0.0953],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0433, 0.0035, 0.2223, 0.1186, 0.6123], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.608528017997742
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.6907, 0.0664, 0.0008, 0.0522, 0.1898], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9996,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9698,     0.0065,     0.0234],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0432,     0.0001,     0.0033,     0.8752,     0.0782],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1617, 0.0171, 0.2524, 0.1482, 0.4205], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
Starting evaluation
printing an ep nov before normalisation:  25.123873023376092
printing an ep nov before normalisation:  24.426040538188992
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.73 ]
 [0.583]
 [0.583]] [[76.942]
 [76.942]
 [90.741]
 [76.942]
 [76.942]] [[0.583]
 [0.583]
 [0.73 ]
 [0.583]
 [0.583]]
printing an ep nov before normalisation:  46.89632113580458
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  62.4365865887914
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.352419360280038
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  21.320138879987024
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.996760810177022
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.8703512431266
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.282532691955566
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  24.66054677963257
printing an ep nov before normalisation:  14.519505338627496
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
line 256 mcts: sample exp_bonus 44.13239376502172
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.869446
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8710707
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.33330339624971
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.8272,     0.0007,     0.0008,     0.0063,     0.1650],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0057,     0.9791,     0.0077,     0.0000,     0.0075],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0036,     0.0002,     0.9601,     0.0059,     0.0302],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0813,     0.0002,     0.0242,     0.8198,     0.0744],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1980, 0.0017, 0.2048, 0.0603, 0.5353], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]] [[36.14]
 [36.14]
 [36.14]
 [36.14]
 [36.14]] [[0.607]
 [0.607]
 [0.607]
 [0.607]
 [0.607]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.54038565188141
actions average: 
K:  0  action  0 :  tensor([0.7353, 0.0036, 0.0015, 0.1220, 0.1375], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9998,     0.0001,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9630,     0.0003,     0.0364],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0232,     0.0005,     0.0090,     0.8841,     0.0833],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2641, 0.0556, 0.0054, 0.1375, 0.5374], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.6971,     0.0411,     0.0003,     0.1032,     0.1584],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0009,     0.9978,     0.0001,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0050,     0.9405,     0.0004,     0.0539],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0057,     0.0000,     0.0180,     0.9608,     0.0156],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0699, 0.0493, 0.1014, 0.1493, 0.6301], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.86724743989406
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 54.48482021857006
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87751955
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87358886
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.53078928832167
printing an ep nov before normalisation:  29.283872726102224
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  48.65354438735988
printing an ep nov before normalisation:  30.87205410003662
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.366]
 [1.329]
 [1.122]
 [1.261]] [[28.077]
 [22.778]
 [23.94 ]
 [24.085]
 [23.248]] [[2.862]
 [2.539]
 [2.614]
 [2.422]
 [2.479]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.13399934768677
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  3  action  0 :  tensor([    0.9937,     0.0001,     0.0005,     0.0031,     0.0026],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0045,     0.9902,     0.0003,     0.0000,     0.0050],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9903,     0.0004,     0.0093],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0611,     0.0003,     0.0011,     0.8225,     0.1150],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1550, 0.0017, 0.0552, 0.0632, 0.7249], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.91 ]
 [1.293]
 [1.208]
 [0.854]
 [1.011]] [[24.872]
 [21.005]
 [25.941]
 [27.956]
 [23.564]] [[1.949]
 [1.969]
 [2.347]
 [2.181]
 [1.927]]
actions average: 
K:  2  action  0 :  tensor([0.7559, 0.0012, 0.0008, 0.0735, 0.1686], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0003,     0.9745,     0.0025,     0.0226],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0239, 0.0036, 0.0844, 0.7687, 0.1195], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1677, 0.0036, 0.1021, 0.1397, 0.5868], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.97164097227575
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8269,     0.0002,     0.0004,     0.0634,     0.1091],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9983,     0.0003,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9622,     0.0016,     0.0359],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0180,     0.0003,     0.0007,     0.9166,     0.0644],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1033, 0.0021, 0.1853, 0.1847, 0.5246], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.625997528984925
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.57904693048889
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.14625072479248
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.172619227551838
printing an ep nov before normalisation:  18.85878564190945
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.62790193801129
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  77.80413575846936
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.437771267973524
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.807592512582495
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]] [[66.522]
 [66.522]
 [66.522]
 [66.522]
 [66.522]] [[2.327]
 [2.327]
 [2.327]
 [2.327]
 [2.327]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9048,     0.0135,     0.0002,     0.0004,     0.0811],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0005,     0.9679,     0.0004,     0.0307],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0492,     0.0003,     0.0044,     0.8348,     0.1113],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0832, 0.0103, 0.1430, 0.1137, 0.6498], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87380266
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.042 0.208 0.083 0.25  0.417]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.26 ]
 [1.316]
 [1.26 ]
 [1.26 ]
 [1.26 ]] [[16.566]
 [21.944]
 [16.566]
 [16.566]
 [16.566]] [[2.535]
 [3.006]
 [2.535]
 [2.535]
 [2.535]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.29163882654927
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.022]
 [1.173]
 [1.022]
 [0.927]
 [1.022]] [[24.07 ]
 [23.295]
 [24.07 ]
 [31.436]
 [24.07 ]] [[2.089]
 [2.206]
 [2.089]
 [2.32 ]
 [2.089]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8749098
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.99773924080627
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.5161841952538
maxi score, test score, baseline:  0.2481 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.39662638988983
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  59.15893664704615
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.209]
 [1.31 ]
 [1.209]
 [1.209]
 [1.209]] [[42.695]
 [42.989]
 [42.695]
 [42.695]
 [42.695]] [[1.767]
 [1.875]
 [1.767]
 [1.767]
 [1.767]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.738]
 [0.746]
 [0.308]
 [0.738]] [[42.503]
 [42.503]
 [51.017]
 [45.059]
 [42.503]] [[1.302]
 [1.302]
 [1.541]
 [0.941]
 [1.302]]
printing an ep nov before normalisation:  48.14435298975662
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.28604440509265
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.6547,     0.0005,     0.0004,     0.1695,     0.1750],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9995,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9985,     0.0008,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.1248,     0.0003,     0.0006,     0.7615,     0.1128],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1380, 0.0062, 0.1608, 0.1681, 0.5268], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.857]
 [0.857]
 [0.928]
 [0.857]
 [0.857]] [[62.869]
 [62.869]
 [62.364]
 [62.869]
 [62.869]] [[1.174]
 [1.174]
 [1.24 ]
 [1.174]
 [1.174]]
printing an ep nov before normalisation:  40.491090003191076
printing an ep nov before normalisation:  25.171808795201095
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.38777748538889
line 256 mcts: sample exp_bonus 16.37156776157425
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.682]
 [1.002]
 [0.682]
 [0.682]] [[30.822]
 [30.822]
 [58.237]
 [30.822]
 [30.822]] [[0.964]
 [0.964]
 [1.763]
 [0.964]
 [0.964]]
Printing some Q and Qe and total Qs values:  [[1.296]
 [1.297]
 [1.128]
 [1.06 ]
 [1.296]] [[47.144]
 [42.908]
 [49.622]
 [59.952]
 [47.144]] [[2.122]
 [1.956]
 [2.053]
 [2.393]
 [2.122]]
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.703]
 [0.153]
 [0.638]
 [0.338]] [[35.807]
 [45.021]
 [42.896]
 [35.807]
 [41.899]] [[1.064]
 [1.383]
 [0.774]
 [1.064]
 [0.932]]
maxi score, test score, baseline:  0.2501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.335]
 [1.433]
 [1.148]
 [1.335]
 [1.335]] [[48.172]
 [52.816]
 [52.102]
 [48.172]
 [48.172]] [[2.203]
 [2.433]
 [2.128]
 [2.203]
 [2.203]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88400894
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.254]
 [1.4  ]
 [1.303]
 [1.185]
 [1.322]] [[31.576]
 [28.389]
 [29.777]
 [26.645]
 [30.038]] [[1.846]
 [1.932]
 [1.861]
 [1.684]
 [1.885]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.737]
 [0.77 ]
 [0.828]
 [0.737]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.737]
 [0.737]
 [0.77 ]
 [0.828]
 [0.737]]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.30757582683619
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.677241802215576
maxi score, test score, baseline:  0.2501 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.234]
 [1.234]
 [1.234]
 [1.194]
 [1.234]] [[15.485]
 [15.485]
 [15.485]
 [13.206]
 [15.485]] [[3.005]
 [3.005]
 [3.005]
 [2.527]
 [3.005]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.0007537111946476216
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8750,     0.0150,     0.0040,     0.0001,     0.1058],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9990,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0023,     0.9450,     0.0016,     0.0510],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0871,     0.0007,     0.0042,     0.7487,     0.1593],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1850, 0.0059, 0.0214, 0.0691, 0.7187], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2521 1.0 1.0
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2521 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]] [[57.723]
 [57.723]
 [57.723]
 [57.723]
 [57.723]] [[0.578]
 [0.578]
 [0.578]
 [0.578]
 [0.578]]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.007]
 [1.092]
 [1.007]
 [1.007]
 [1.007]] [[60.135]
 [42.959]
 [60.135]
 [60.135]
 [60.135]] [[1.317]
 [1.255]
 [1.317]
 [1.317]
 [1.317]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.22837079598021
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.14896732165698
printing an ep nov before normalisation:  12.96771764755249
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.002423079392727
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 60.24613862865596
line 256 mcts: sample exp_bonus 25.07202284676688
printing an ep nov before normalisation:  34.658029079437256
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.75128009298322
maxi score, test score, baseline:  0.2541 1.0 1.0
printing an ep nov before normalisation:  27.77801812052476
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.517]
 [0.709]
 [0.709]
 [0.709]
 [0.709]] [[35.324]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[1.359]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.63 ]
 [0.63 ]
 [0.681]
 [0.63 ]
 [0.63 ]] [[66.991]
 [66.991]
 [71.097]
 [66.991]
 [66.991]] [[0.63 ]
 [0.63 ]
 [0.681]
 [0.63 ]
 [0.63 ]]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.879655
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8754652
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2581 1.0 1.0
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  39.362288138161716
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.16870421551026
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
actions average: 
K:  3  action  0 :  tensor([0.7244, 0.0030, 0.0026, 0.1019, 0.1681], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9989,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0001,     0.9652,     0.0004,     0.0340],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0203,     0.0001,     0.0070,     0.8946,     0.0780],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1850, 0.0569, 0.1081, 0.1222, 0.5277], grad_fn=<DivBackward0>)
siam score:  -0.87146914
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.824]
 [0.749]
 [0.749]
 [0.749]] [[54.728]
 [41.368]
 [54.728]
 [54.728]
 [54.728]] [[0.749]
 [0.824]
 [0.749]
 [0.749]
 [0.749]]
line 256 mcts: sample exp_bonus 41.181864738464355
printing an ep nov before normalisation:  17.448978424072266
printing an ep nov before normalisation:  22.1041459310276
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.33208190080187
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.63328729877284
printing an ep nov before normalisation:  28.140220980839377
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.723]
 [0.723]
 [0.723]
 [0.723]
 [0.723]]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87372804
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.313780096519526
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.959351747996152
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.2138737052734
printing an ep nov before normalisation:  43.79766529111011
printing an ep nov before normalisation:  39.29418987698025
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8706183
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2641 1.0 1.0
printing an ep nov before normalisation:  54.74815572388637
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.845]
 [0.809]
 [0.737]
 [0.737]] [[21.255]
 [24.585]
 [31.848]
 [21.255]
 [21.255]] [[0.737]
 [0.845]
 [0.809]
 [0.737]
 [0.737]]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.105]
 [1.105]
 [1.252]
 [1.105]
 [1.105]] [[24.473]
 [24.473]
 [25.913]
 [24.473]
 [24.473]] [[1.394]
 [1.394]
 [1.568]
 [1.394]
 [1.394]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.269082314599324
siam score:  -0.88470167
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.88460124
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2661 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.072]
 [1.077]
 [1.072]
 [1.072]
 [0.86 ]] [[28.645]
 [36.474]
 [28.645]
 [28.645]
 [35.988]] [[1.27 ]
 [1.41 ]
 [1.27 ]
 [1.27 ]
 [1.185]]
maxi score, test score, baseline:  0.2661 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.78835695022474
using explorer policy with actor:  1
siam score:  -0.883768
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.971]
 [1.116]
 [0.971]
 [0.971]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.971]
 [0.971]
 [1.116]
 [0.971]
 [0.971]]
printing an ep nov before normalisation:  47.36393292744954
Printing some Q and Qe and total Qs values:  [[0.862]
 [0.862]
 [1.048]
 [0.862]
 [0.862]] [[31.643]
 [31.643]
 [33.511]
 [31.643]
 [31.643]] [[1.069]
 [1.069]
 [1.278]
 [1.069]
 [1.069]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.745]
 [0.783]
 [0.783]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.783]
 [0.783]
 [0.745]
 [0.783]
 [0.783]]
siam score:  -0.88065386
printing an ep nov before normalisation:  3.4078007507332586
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  51.09959849920818
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.153913497924805
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.7992, 0.0381, 0.0010, 0.0768, 0.0848], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9984,     0.0005,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0002,     0.9869,     0.0009,     0.0120],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0569,     0.0004,     0.0002,     0.8916,     0.0509],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1127, 0.0009, 0.0037, 0.1050, 0.7776], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.01627826690674
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8718327
maxi score, test score, baseline:  0.2701 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.312]
 [1.312]
 [1.312]
 [1.312]] [[37.475]
 [37.475]
 [37.475]
 [37.475]
 [37.475]] [[3.259]
 [3.259]
 [3.259]
 [3.259]
 [3.259]]
siam score:  -0.8699826
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.56341470788593
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.93631458282471
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2701 1.0 1.0
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.85319653456798
printing an ep nov before normalisation:  28.132776899694136
maxi score, test score, baseline:  0.2701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.128824362538694
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2721 1.0 1.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.78456271544651
maxi score, test score, baseline:  0.2721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2741 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.0212622692137
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.47005803480236
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.646451950073242
maxi score, test score, baseline:  0.2761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.849]
 [0.217]
 [0.416]
 [0.416]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.416]
 [0.849]
 [0.217]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2781 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.011543706857466
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28209999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.690977593935195
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8804381
printing an ep nov before normalisation:  11.115612539114933
siam score:  -0.87944543
printing an ep nov before normalisation:  21.00175380706787
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.641]
 [0.381]
 [0.437]
 [0.434]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.444]
 [0.641]
 [0.381]
 [0.437]
 [0.434]]
siam score:  -0.88000345
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.20842226083265
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.719360438614455
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
printing an ep nov before normalisation:  64.34274621765994
siam score:  -0.8799732
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.596]
 [0.596]
 [0.681]
 [0.596]
 [0.596]] [[26.625]
 [26.625]
 [40.892]
 [26.625]
 [26.625]] [[0.596]
 [0.596]
 [0.681]
 [0.596]
 [0.596]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88406616
using explorer policy with actor:  1
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.587157329824084
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7454, 0.0350, 0.0036, 0.0864, 0.1296], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9990,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9749,     0.0007,     0.0241],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0259,     0.0004,     0.0207,     0.8675,     0.0856],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0939, 0.0030, 0.1567, 0.0777, 0.6688], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.899]
 [0.899]
 [0.9  ]
 [0.899]
 [0.899]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.899]
 [0.899]
 [0.9  ]
 [0.899]
 [0.899]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.101]
 [1.127]
 [1.101]
 [1.101]
 [1.101]] [[33.555]
 [14.152]
 [33.555]
 [33.555]
 [33.555]] [[3.101]
 [1.602]
 [3.101]
 [3.101]
 [3.101]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.79 ]
 [0.745]
 [0.79 ]
 [0.79 ]] [[20.911]
 [20.911]
 [43.033]
 [20.911]
 [20.911]] [[0.983]
 [0.983]
 [1.326]
 [0.983]
 [0.983]]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.498136043548584
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.7592, 0.0009, 0.0088, 0.0777, 0.1534], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9937,     0.0013,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9688,     0.0003,     0.0306],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0330,     0.0004,     0.0087,     0.8311,     0.1267],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0649, 0.0070, 0.0889, 0.0553, 0.7839], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8803241
printing an ep nov before normalisation:  0.002031767567132192
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.29009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.71633762783474
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.482]
 [1.243]
 [0.976]
 [1.147]] [[20.156]
 [14.188]
 [19.514]
 [24.139]
 [20.156]] [[2.454]
 [2.402]
 [2.509]
 [2.542]
 [2.454]]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8822774
printing an ep nov before normalisation:  33.116226411078515
siam score:  -0.88187075
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.699]
 [0.71 ]
 [0.754]
 [0.754]] [[48.293]
 [50.08 ]
 [43.463]
 [48.293]
 [48.293]] [[1.893]
 [1.907]
 [1.665]
 [1.893]
 [1.893]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2961 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2961 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [1.038]
 [0.994]
 [0.994]] [[62.251]
 [62.251]
 [66.836]
 [62.251]
 [62.251]] [[2.31 ]
 [2.31 ]
 [2.488]
 [2.31 ]
 [2.31 ]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.518]
 [0.378]
 [0.392]
 [0.382]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.421]
 [0.518]
 [0.378]
 [0.392]
 [0.382]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.001]] [[12.679]
 [ 6.496]
 [ 7.965]
 [12.704]
 [12.388]] [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.001]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.59355915865141
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.60117497036259
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.697661176672156
printing an ep nov before normalisation:  31.43473700593707
siam score:  -0.87778455
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.732347430039702
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2981 1.0 1.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87745935
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.142123224472666
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.011712773324923
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.40697848624271
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8664174
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.891860829719175
siam score:  -0.867791
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.7405629156258442
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.04277786731396
Printing some Q and Qe and total Qs values:  [[0.549]
 [0.549]
 [0.757]
 [0.549]
 [0.549]] [[20.82 ]
 [20.82 ]
 [20.683]
 [20.82 ]
 [20.82 ]] [[0.549]
 [0.549]
 [0.757]
 [0.549]
 [0.549]]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.85773295
printing an ep nov before normalisation:  62.15691262117984
maxi score, test score, baseline:  0.3041 1.0 1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8574654
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 51.372855297513766
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.389]
 [1.389]
 [1.389]
 [1.389]
 [1.389]] [[13.979]
 [13.979]
 [13.979]
 [13.979]
 [13.979]] [[1.91]
 [1.91]
 [1.91]
 [1.91]
 [1.91]]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.372890115250364
printing an ep nov before normalisation:  36.82381623973978
printing an ep nov before normalisation:  35.2633034202865
printing an ep nov before normalisation:  39.48084868498
printing an ep nov before normalisation:  0.007735551329091095
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.6854, 0.0035, 0.0010, 0.0012, 0.3089], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9982,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0000,     0.9727,     0.0002,     0.0268],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0309,     0.0007,     0.0172,     0.8796,     0.0716],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1175, 0.0023, 0.3476, 0.0448, 0.4878], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
printing an ep nov before normalisation:  35.81379345544576
maxi score, test score, baseline:  0.34609999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9952,     0.0008,     0.0002,     0.0008,     0.0030],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0047,     0.9717,     0.0000,     0.0024,     0.0211],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9719,     0.0097,     0.0182],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0622,     0.0000,     0.0026,     0.8973,     0.0378],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.2322, 0.0025, 0.1427, 0.1336, 0.4889], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [1.172]
 [0.929]
 [0.929]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.929]
 [0.929]
 [1.172]
 [0.929]
 [0.929]]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.70545105576426
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.12662903510502
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34809999999999997 1.0 1.0
printing an ep nov before normalisation:  61.18588959774142
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8674341
printing an ep nov before normalisation:  34.54013273912281
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8696355
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.187]
 [1.187]
 [1.187]
 [1.187]
 [1.187]] [[38.152]
 [38.152]
 [38.152]
 [38.152]
 [38.152]] [[2.55]
 [2.55]
 [2.55]
 [2.55]
 [2.55]]
printing an ep nov before normalisation:  21.482651999965586
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.456936221973478
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.8384, 0.0022, 0.0011, 0.0755, 0.0827], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9993,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0005,     0.9424,     0.0047,     0.0522],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0307,     0.0002,     0.0042,     0.8539,     0.1111],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0471, 0.0027, 0.2133, 0.0018, 0.7351], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.083456730665276
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.657]
 [0.704]
 [0.657]
 [0.657]] [[52.25 ]
 [52.25 ]
 [62.139]
 [52.25 ]
 [52.25 ]] [[0.657]
 [0.657]
 [0.704]
 [0.657]
 [0.657]]
maxi score, test score, baseline:  0.35209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.109]
 [1.109]
 [1.109]
 [1.155]
 [1.109]] [[36.567]
 [36.567]
 [36.567]
 [35.118]
 [36.567]] [[2.547]
 [2.547]
 [2.547]
 [2.488]
 [2.547]]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.23553545932784
siam score:  -0.8615753
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.167 0.25  0.167 0.167 0.25 ]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.04765837533134
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  75.21315514175276
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
using explorer policy with actor:  1
siam score:  -0.8719654
actions average: 
K:  0  action  0 :  tensor([0.7883, 0.0038, 0.0138, 0.0422, 0.1520], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9987,     0.0001,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9425,     0.0171,     0.0403],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0668,     0.0008,     0.0008,     0.7909,     0.1408],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0992, 0.0014, 0.1712, 0.1169, 0.6112], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.5975044383591
Printing some Q and Qe and total Qs values:  [[1.031]
 [1.269]
 [1.031]
 [1.031]
 [1.031]] [[26.287]
 [34.55 ]
 [26.287]
 [26.287]
 [26.287]] [[1.621]
 [2.291]
 [1.621]
 [1.621]
 [1.621]]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.565]
 [0.639]
 [0.565]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.565]
 [0.565]
 [0.639]
 [0.565]
 [0.565]]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.959]
 [0.959]
 [1.129]
 [0.959]
 [0.959]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.959]
 [0.959]
 [1.129]
 [0.959]
 [0.959]]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  49.372921092223216
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [1.197]
 [1.177]
 [1.209]
 [1.148]] [[21.467]
 [20.485]
 [16.003]
 [19.276]
 [16.039]] [[1.849]
 [2.032]
 [1.678]
 [1.954]
 [1.651]]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.833]
 [0.875]
 [0.833]
 [0.833]
 [0.833]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.833]
 [0.875]
 [0.833]
 [0.833]
 [0.833]]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
maxi score, test score, baseline:  0.35409999999999997 1.0 1.0
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.35609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3581 1.0 1.0
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.543300151338613
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.18418977824737
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  4  action  0 :  tensor([    0.9406,     0.0042,     0.0002,     0.0003,     0.0548],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9735,     0.0004,     0.0259],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0062,     0.0004,     0.0011,     0.9335,     0.0588],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0975, 0.0501, 0.2198, 0.1159, 0.5167], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 14.322683636328694
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3601 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.02408007305651
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8614918
printing an ep nov before normalisation:  24.59800248019374
siam score:  -0.86066836
siam score:  -0.8591524
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.476]
 [1.499]
 [1.476]
 [1.476]
 [1.476]] [[21.455]
 [24.359]
 [21.455]
 [21.455]
 [21.455]] [[2.318]
 [2.456]
 [2.318]
 [2.318]
 [2.318]]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8601044
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.86059797
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.996]
 [1.197]
 [1.062]
 [1.062]
 [1.062]] [[33.651]
 [24.254]
 [28.221]
 [28.221]
 [28.221]] [[2.313]
 [1.957]
 [2.057]
 [2.057]
 [2.057]]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.89035987854004
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.52092665740504
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3661 1.0 1.0
maxi score, test score, baseline:  0.3661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.00408229911396
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3701 1.0 1.0
printing an ep nov before normalisation:  51.26266295885916
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3701 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.142]
 [1.216]
 [1.038]
 [1.038]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.038]
 [1.142]
 [1.216]
 [1.038]
 [1.038]]
maxi score, test score, baseline:  0.3701 1.0 1.0
printing an ep nov before normalisation:  36.65602207183838
maxi score, test score, baseline:  0.3701 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3701 1.0 1.0
printing an ep nov before normalisation:  32.50588600554534
maxi score, test score, baseline:  0.3701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.794320000542534
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.738]
 [0.79 ]
 [0.124]
 [0.672]] [[39.727]
 [36.398]
 [51.885]
 [59.077]
 [34.778]] [[1.15 ]
 [1.107]
 [1.635]
 [1.19 ]
 [0.992]]
actions average: 
K:  0  action  0 :  tensor([    0.8425,     0.0003,     0.0005,     0.0016,     0.1551],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0109, 0.9444, 0.0013, 0.0068, 0.0366], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0000,     0.9463,     0.0043,     0.0490],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0854,     0.0002,     0.0009,     0.7782,     0.1352],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2057, 0.0017, 0.0735, 0.1310, 0.5880], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85217613
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.625268394778026
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.54026913231056
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.835]
 [0.835]
 [0.835]
 [0.835]
 [0.835]] [[45.823]
 [45.823]
 [45.823]
 [45.823]
 [45.823]] [[31.399]
 [31.399]
 [31.399]
 [31.399]
 [31.399]]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.708776830337374
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3721 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3721 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3761 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.11061730521076
printing an ep nov before normalisation:  25.432552113993157
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8393165
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3841 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3841 1.0 1.0
maxi score, test score, baseline:  0.3841 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.8880,     0.0006,     0.0005,     0.0299,     0.0809],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9978,     0.0013,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9761,     0.0011,     0.0226],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0037,     0.0007,     0.0040,     0.8372,     0.1544],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0717, 0.0042, 0.1518, 0.0494, 0.7229], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.3841 1.0 1.0
line 256 mcts: sample exp_bonus 43.478784792119455
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.22470011671211
maxi score, test score, baseline:  0.3841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.639]
 [0.675]
 [0.639]
 [0.639]
 [0.639]] [[52.922]
 [48.834]
 [52.922]
 [52.922]
 [52.922]] [[0.639]
 [0.675]
 [0.639]
 [0.639]
 [0.639]]
printing an ep nov before normalisation:  65.13728405038836
printing an ep nov before normalisation:  46.398459014228145
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.906978046871515
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([0.8389, 0.0017, 0.0010, 0.0067, 0.1517], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9990,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0005,     0.9713,     0.0015,     0.0264],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0020,     0.0002,     0.0248,     0.9026,     0.0703],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1042, 0.0208, 0.2486, 0.0698, 0.5566], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.3713116645813
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.51204206476195
Printing some Q and Qe and total Qs values:  [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.416]
 [0.416]
 [0.416]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 18.210863653927984
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  18.61016273498535
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actions average: 
K:  2  action  0 :  tensor([0.8791, 0.0032, 0.0010, 0.0630, 0.0537], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9732,     0.0003,     0.0263],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0013,     0.0002,     0.0128,     0.9437,     0.0421],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1354, 0.0073, 0.2731, 0.1301, 0.4541], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.038]
 [1.08 ]
 [1.08 ]
 [1.08 ]
 [1.08 ]] [[14.515]
 [23.414]
 [23.414]
 [23.414]
 [23.414]] [[2.371]
 [3.969]
 [3.969]
 [3.969]
 [3.969]]
maxi score, test score, baseline:  0.3861 1.0 1.0
printing an ep nov before normalisation:  23.82160523541117
printing an ep nov before normalisation:  28.827969532235098
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.9990,     0.0002,     0.0001,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9953,     0.0001,     0.0000,     0.0033],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0001,     0.9114,     0.0113,     0.0766],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0628,     0.0001,     0.0007,     0.8469,     0.0895],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0729, 0.0045, 0.2082, 0.0664, 0.6480], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  50.13756275177002
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.074]
 [1.074]
 [1.074]
 [1.074]
 [1.074]]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.07247051138857
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.846995
printing an ep nov before normalisation:  25.20263194536013
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8755,     0.0002,     0.0003,     0.0399,     0.0840],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0013,     0.9751,     0.0002,     0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0000,     0.0033,     0.9659,     0.0304],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1175, 0.0524, 0.2134, 0.0854, 0.5314], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3861 1.0 1.0
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0001292180348855254
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  1  action  0 :  tensor([    0.8589,     0.0008,     0.0025,     0.0761,     0.0617],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9939,     0.0002,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9690,     0.0007,     0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0000,     0.0029,     0.9957,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1618, 0.0007, 0.1797, 0.1576, 0.5002], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.45018897461853
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.15323144277862
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.750418889800017
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3901 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.147]
 [1.147]
 [1.196]
 [1.147]
 [1.147]] [[26.216]
 [26.216]
 [27.543]
 [26.216]
 [26.216]] [[1.241]
 [1.241]
 [1.302]
 [1.241]
 [1.241]]
maxi score, test score, baseline:  0.3921 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8573186
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.973222808275203
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.399]
 [0.432]
 [0.342]
 [0.323]
 [0.387]] [[45.957]
 [48.246]
 [ 0.   ]
 [35.861]
 [43.794]] [[0.399]
 [0.432]
 [0.342]
 [0.323]
 [0.387]]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  0  action  0 :  tensor([    0.8723,     0.0008,     0.0005,     0.0288,     0.0976],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9995,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0001,     0.9586,     0.0011,     0.0399],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0376,     0.0001,     0.0006,     0.9206,     0.0411],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1227, 0.0588, 0.1199, 0.0982, 0.6004], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.289]
 [1.371]
 [1.289]
 [1.22 ]
 [1.289]] [[33.982]
 [35.666]
 [33.982]
 [38.028]
 [33.982]] [[2.433]
 [2.611]
 [2.433]
 [2.595]
 [2.433]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8858,     0.0010,     0.0005,     0.0407,     0.0720],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9985,     0.0001,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9864,     0.0019,     0.0115],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0743,     0.0003,     0.0015,     0.8400,     0.0838],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1070, 0.0041, 0.0902, 0.0876, 0.7111], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.37336335227078
printing an ep nov before normalisation:  38.41216545342042
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.034197546820057
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  58.35771850219894
printing an ep nov before normalisation:  38.91699065741741
printing an ep nov before normalisation:  33.924739360809326
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.025]
 [1.025]
 [1.153]
 [1.025]
 [1.002]] [[36.374]
 [36.374]
 [50.435]
 [36.374]
 [45.06 ]] [[1.099]
 [1.099]
 [1.31 ]
 [1.099]
 [1.128]]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.31564202733812
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  66.2708136431895
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3981 1.0 1.0
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3981 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.86504394
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.849]
 [1.103]
 [1.011]
 [0.849]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.849]
 [0.849]
 [1.103]
 [1.011]
 [0.849]]
printing an ep nov before normalisation:  32.25159049034119
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.707732677459717
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86900467
printing an ep nov before normalisation:  36.145203184758465
printing an ep nov before normalisation:  27.67447678813849
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86404747
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8627231
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.995598793029785
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.25647216003391
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.759]
 [0.759]
 [0.807]
 [0.759]
 [0.759]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.759]
 [0.759]
 [0.807]
 [0.759]
 [0.759]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 49.89026154513099
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8687,     0.0023,     0.0001,     0.0531,     0.0758],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0013, 0.9815, 0.0059, 0.0035, 0.0078], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0004,     0.9380,     0.0010,     0.0597],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0675,     0.0001,     0.0007,     0.8897,     0.0420],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0416, 0.0010, 0.0783, 0.2084, 0.6707], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.017857686539962
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  26.220489340843326
actions average: 
K:  0  action  0 :  tensor([    0.8404,     0.0606,     0.0005,     0.0394,     0.0592],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9984,     0.0006,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9952,     0.0041,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0384,     0.0006,     0.0270,     0.7672,     0.1668],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1494, 0.0013, 0.0559, 0.2295, 0.5638], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  54.23023318491977
printing an ep nov before normalisation:  39.01636600494385
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4021 1.0 1.0
actions average: 
K:  3  action  0 :  tensor([    0.8269,     0.0005,     0.0002,     0.0912,     0.0812],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9902,     0.0047,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0000,     0.9585,     0.0023,     0.0389],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0267,     0.0027,     0.0002,     0.8673,     0.1030],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0666, 0.0074, 0.0023, 0.1364, 0.7873], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 28.429505690271455
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.184127533489324
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  22.22252368927002
siam score:  -0.8653238
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  50.91214340297463
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8642795
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.94309224658639
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.454663506209087
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([    0.7134,     0.0003,     0.0007,     0.1797,     0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0136,     0.9796,     0.0001,     0.0000,     0.0066],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0018,     0.9737,     0.0024,     0.0218],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0085,     0.0002,     0.0524,     0.8934,     0.0456],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1138, 0.1451, 0.2496, 0.0129, 0.4785], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  24.145028854988574
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.625 0.042 0.208 0.083]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.1948184967041
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.176]
 [0.263]
 [0.294]
 [0.189]
 [0.197]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.176]
 [0.263]
 [0.294]
 [0.189]
 [0.197]]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 39.43306982986423
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.16244606747101
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.7187, 0.0116, 0.0007, 0.0441, 0.2248], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9910,     0.0051,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9384,     0.0069,     0.0546],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0009,     0.0000,     0.0011,     0.9912,     0.0068],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.1263,     0.0181,     0.0005,     0.2166,     0.6385],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.943]
 [1.249]
 [0.943]
 [0.943]
 [1.11 ]] [[29.288]
 [32.417]
 [29.288]
 [29.288]
 [33.429]] [[1.556]
 [2.001]
 [1.556]
 [1.556]
 [1.908]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.69548589802119
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85188395
printing an ep nov before normalisation:  25.391881369780783
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]] [[33.621]
 [33.621]
 [33.621]
 [33.621]
 [33.621]] [[45.631]
 [45.631]
 [45.631]
 [45.631]
 [45.631]]
siam score:  -0.85366803
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.000321995504464212
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.902502415987207
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.61044116523705
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84977376
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.804463386535645
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.197]
 [1.247]
 [1.197]
 [1.197]] [[49.425]
 [49.425]
 [51.632]
 [49.425]
 [49.425]] [[1.67 ]
 [1.67 ]
 [1.753]
 [1.67 ]
 [1.67 ]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.93029594421387
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.373746176344035
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.73507942474779
printing an ep nov before normalisation:  28.906217440016775
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.24 ]
 [1.317]
 [1.24 ]
 [1.24 ]
 [1.24 ]] [[23.586]
 [25.453]
 [23.586]
 [23.586]
 [23.586]] [[1.817]
 [1.997]
 [1.817]
 [1.817]
 [1.817]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8662725
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8664668
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.161]
 [1.161]
 [1.161]
 [1.161]
 [1.161]] [[14.615]
 [14.615]
 [14.615]
 [14.615]
 [14.615]] [[1.394]
 [1.394]
 [1.394]
 [1.394]
 [1.394]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7595,     0.0008,     0.0003,     0.0724,     0.1669],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0270,     0.9605,     0.0013,     0.0001,     0.0111],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0014, 0.0017, 0.9079, 0.0111, 0.0779], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0043,     0.0004,     0.0035,     0.9289,     0.0629],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0063, 0.0026, 0.2093, 0.1793, 0.6024], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.50471447607198
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.610366285858056
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  0  action  0 :  tensor([    0.8255,     0.0005,     0.0034,     0.0685,     0.1020],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.9977,     0.0003,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9915,     0.0014,     0.0070],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0600,     0.0005,     0.0037,     0.8193,     0.1164],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1658, 0.0046, 0.0790, 0.1835, 0.5671], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.47571974850571
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4201 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  46.448490855518564
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.814466451813157
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.711]
 [0.778]
 [0.711]
 [0.711]] [[78.995]
 [78.995]
 [59.023]
 [78.995]
 [78.995]] [[0.711]
 [0.711]
 [0.778]
 [0.711]
 [0.711]]
siam score:  -0.8603066
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4221 1.0 1.0
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.959484141873624
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.10149687228017
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.25430893816064
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4241 1.0 1.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.962]
 [1.071]
 [1.186]
 [1.071]
 [1.071]] [[31.842]
 [21.032]
 [29.994]
 [21.032]
 [21.032]] [[2.285]
 [1.507]
 [2.357]
 [1.507]
 [1.507]]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.879]
 [0.946]
 [0.971]
 [0.946]
 [0.946]] [[40.18 ]
 [42.911]
 [48.309]
 [42.911]
 [42.911]] [[1.483]
 [1.63 ]
 [1.813]
 [1.63 ]
 [1.63 ]]
printing an ep nov before normalisation:  19.651904883595005
maxi score, test score, baseline:  0.4241 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.131]
 [1.365]
 [1.242]
 [1.069]
 [1.223]] [[26.12 ]
 [21.168]
 [20.44 ]
 [27.888]
 [20.608]] [[2.396]
 [2.275]
 [2.1  ]
 [2.462]
 [2.093]]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.316]
 [1.271]
 [0.209]
 [1.272]] [[29.471]
 [27.594]
 [31.389]
 [37.976]
 [31.87 ]] [[1.929]
 [2.017]
 [2.203]
 [1.542]
 [2.233]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.119552220718038
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.062]
 [1.247]
 [1.062]
 [1.062]
 [1.062]] [[29.974]
 [40.7  ]
 [29.974]
 [29.974]
 [29.974]] [[1.472]
 [1.975]
 [1.472]
 [1.472]
 [1.472]]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.44194319220099
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Starting evaluation
line 256 mcts: sample exp_bonus 54.2368848378026
printing an ep nov before normalisation:  42.25443643466212
printing an ep nov before normalisation:  34.393595666854154
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  7.142628021483688e-05
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.8574,     0.0011,     0.0004,     0.0003,     0.1408],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9775,     0.0150,     0.0000,     0.0067],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0020,     0.9770,     0.0156,     0.0053],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0002,     0.0014,     0.8600,     0.1377],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2250, 0.0120, 0.1079, 0.0646, 0.5905], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.46615511578973
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.13412325674881
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4621 1.0 1.0
printing an ep nov before normalisation:  61.13995170799854
Printing some Q and Qe and total Qs values:  [[1.359]
 [1.359]
 [1.359]
 [1.359]
 [1.359]] [[40.214]
 [40.214]
 [40.214]
 [40.214]
 [40.214]] [[2.476]
 [2.476]
 [2.476]
 [2.476]
 [2.476]]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.776]
 [0.803]
 [0.889]
 [0.776]
 [0.776]] [[57.632]
 [51.427]
 [48.502]
 [57.632]
 [57.632]] [[0.776]
 [0.803]
 [0.889]
 [0.776]
 [0.776]]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4641 1.0 1.0
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4661 1.0 1.0
maxi score, test score, baseline:  0.4661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4661 1.0 1.0
printing an ep nov before normalisation:  35.691778662147264
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.96279558835208
maxi score, test score, baseline:  0.4681 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.104]
 [1.104]
 [1.104]
 [1.104]
 [1.104]] [[22.625]
 [22.625]
 [22.625]
 [22.625]
 [22.625]] [[46.354]
 [46.354]
 [46.354]
 [46.354]
 [46.354]]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  28.453781889668758
Printing some Q and Qe and total Qs values:  [[1.283]
 [1.283]
 [1.368]
 [1.216]
 [1.283]] [[29.222]
 [29.222]
 [28.252]
 [35.669]
 [29.222]] [[1.998]
 [1.998]
 [2.024]
 [2.318]
 [1.998]]
printing an ep nov before normalisation:  15.459082126617432
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.3  ]
 [1.3  ]
 [1.3  ]
 [1.209]
 [1.3  ]] [[24.096]
 [24.096]
 [24.096]
 [23.313]
 [24.096]] [[2.539]
 [2.539]
 [2.539]
 [2.408]
 [2.539]]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.7105579953662
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.4681 1.0 1.0
printing an ep nov before normalisation:  8.600904529525906
maxi score, test score, baseline:  0.4681 1.0 1.0
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.78324031829834
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.182]
 [1.272]
 [1.182]
 [1.182]
 [1.182]] [[49.996]
 [45.564]
 [49.996]
 [49.996]
 [49.996]] [[2.68 ]
 [2.517]
 [2.68 ]
 [2.68 ]
 [2.68 ]]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.04384775322066
printing an ep nov before normalisation:  44.15055084879231
Printing some Q and Qe and total Qs values:  [[1.208]
 [1.424]
 [1.19 ]
 [1.16 ]
 [1.208]] [[33.815]
 [34.626]
 [35.246]
 [31.526]
 [33.815]] [[2.709]
 [2.985]
 [2.796]
 [2.493]
 [2.709]]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.686416612087346
printing an ep nov before normalisation:  22.81476723501413
Printing some Q and Qe and total Qs values:  [[1.118]
 [1.235]
 [1.118]
 [0.275]
 [1.118]] [[42.738]
 [43.04 ]
 [42.738]
 [36.344]
 [42.738]] [[1.357]
 [1.477]
 [1.357]
 [0.453]
 [1.357]]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47009999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
siam score:  -0.85999525
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.789]
 [0.786]
 [0.721]
 [0.722]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.677]
 [0.789]
 [0.786]
 [0.721]
 [0.722]]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  25.300238132476807
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
printing an ep nov before normalisation:  35.422756336412036
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.75271833819216
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.   ]
 [1.   ]
 [0.982]
 [1.   ]
 [1.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.   ]
 [1.   ]
 [0.982]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
printing an ep nov before normalisation:  39.56220247978001
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.84648361476303
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([0.7414, 0.0016, 0.0013, 0.0628, 0.1929], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9999,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0001,     0.9729,     0.0043,     0.0226],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0909,     0.0011,     0.0003,     0.8134,     0.0943],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2163, 0.0016, 0.0813, 0.0901, 0.6107], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8432683
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.999]
 [0.999]
 [0.999]
 [0.999]
 [1.124]] [[25.395]
 [25.395]
 [25.395]
 [25.395]
 [20.923]] [[3.813]
 [3.813]
 [3.813]
 [3.813]
 [3.124]]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.94289493560791
printing an ep nov before normalisation:  23.07096242904663
printing an ep nov before normalisation:  31.47271156311035
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
printing an ep nov before normalisation:  19.997094137559582
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.47809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.815]
 [0.788]
 [0.877]
 [0.815]
 [0.815]] [[71.45 ]
 [71.183]
 [60.218]
 [71.45 ]
 [71.45 ]] [[0.815]
 [0.788]
 [0.877]
 [0.815]
 [0.815]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.06 ]
 [1.233]
 [1.06 ]
 [1.06 ]
 [1.06 ]] [[54.386]
 [44.627]
 [54.386]
 [54.386]
 [54.386]] [[1.393]
 [1.47 ]
 [1.393]
 [1.393]
 [1.393]]
maxi score, test score, baseline:  0.48009999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.585736751556396
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4841 1.0 1.0
maxi score, test score, baseline:  0.4841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.940619406601414
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.728]
 [0.819]
 [0.802]
 [0.728]
 [0.728]] [[71.692]
 [67.177]
 [68.188]
 [71.692]
 [71.692]] [[0.728]
 [0.819]
 [0.802]
 [0.728]
 [0.728]]
maxi score, test score, baseline:  0.4861 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.727]
 [0.836]
 [0.851]
 [0.727]
 [0.688]] [[16.351]
 [31.107]
 [33.641]
 [16.351]
 [35.803]] [[0.727]
 [0.836]
 [0.851]
 [0.727]
 [0.688]]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4881 1.0 1.0
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.304]
 [0.221]
 [1.096]
 [1.096]] [[40.175]
 [50.54 ]
 [48.129]
 [40.175]
 [40.175]] [[1.712]
 [2.288]
 [1.119]
 [1.712]
 [1.712]]
maxi score, test score, baseline:  0.4881 1.0 1.0
printing an ep nov before normalisation:  35.860083568401244
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.301]
 [1.375]
 [1.301]
 [1.301]
 [1.301]] [[35.961]
 [36.851]
 [35.961]
 [35.961]
 [35.961]] [[2.144]
 [2.265]
 [2.144]
 [2.144]
 [2.144]]
printing an ep nov before normalisation:  20.46473979949951
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.43 ]
 [0.733]
 [0.43 ]
 [0.43 ]
 [0.43 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.43 ]
 [0.733]
 [0.43 ]
 [0.43 ]
 [0.43 ]]
siam score:  -0.850598
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.715919377227145
Printing some Q and Qe and total Qs values:  [[0.933]
 [0.993]
 [0.933]
 [0.933]
 [0.933]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.933]
 [0.993]
 [0.933]
 [0.933]
 [0.933]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.418]
 [1.234]
 [1.219]
 [0.959]
 [0.959]] [[25.64 ]
 [28.166]
 [39.106]
 [43.17 ]
 [43.17 ]] [[1.084]
 [2.026]
 [2.552]
 [2.493]
 [2.493]]
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4941 1.0 1.0
maxi score, test score, baseline:  0.4941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.186]
 [1.186]
 [1.186]
 [1.205]] [[21.885]
 [21.885]
 [21.885]
 [21.885]
 [20.121]] [[1.574]
 [1.574]
 [1.574]
 [1.574]
 [1.538]]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.7874,     0.0178,     0.0007,     0.0258,     0.1683],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0020,     0.9911,     0.0009,     0.0000,     0.0060],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9849,     0.0040,     0.0108],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0206,     0.0001,     0.0002,     0.8891,     0.0899],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1079, 0.0850, 0.0524, 0.2721, 0.4826], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([0.7929, 0.0022, 0.0019, 0.0540, 0.1489], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0074,     0.9898,     0.0000,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9906,     0.0005,     0.0088],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0000,     0.0024,     0.9933,     0.0043],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0460, 0.0089, 0.2026, 0.1376, 0.6050], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.256789500934374
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.154]
 [1.154]
 [1.154]
 [1.154]] [[27.953]
 [27.953]
 [27.953]
 [27.953]
 [27.953]] [[38.415]
 [38.415]
 [38.415]
 [38.415]
 [38.415]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4961 1.0 1.0
printing an ep nov before normalisation:  18.182023760641062
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.0275296813373
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.160780289983705
printing an ep nov before normalisation:  24.26496982574463
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8504845
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.25 ]
 [1.363]
 [1.307]
 [0.223]
 [1.25 ]] [[33.629]
 [38.331]
 [34.8  ]
 [35.42 ]
 [33.629]] [[1.613]
 [1.83 ]
 [1.696]
 [0.626]
 [1.613]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.223]
 [1.246]
 [1.268]
 [1.223]
 [1.223]] [[52.368]
 [47.821]
 [52.668]
 [52.368]
 [52.368]] [[1.474]
 [1.458]
 [1.521]
 [1.474]
 [1.474]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.55221351979982
maxi score, test score, baseline:  0.4961 1.0 1.0
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.88542175292969
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.35620935678696
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.261]
 [1.308]
 [1.257]
 [1.261]
 [1.261]] [[29.389]
 [22.004]
 [29.354]
 [29.389]
 [29.389]] [[2.594]
 [2.121]
 [2.588]
 [2.594]
 [2.594]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.852]
 [0.852]
 [0.97 ]
 [0.054]
 [0.852]] [[46.53 ]
 [46.53 ]
 [51.469]
 [53.808]
 [46.53 ]] [[1.992]
 [1.992]
 [2.364]
 [1.568]
 [1.992]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.7529,     0.0012,     0.0007,     0.0918,     0.1533],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9971,     0.0000,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0005,     0.9805,     0.0001,     0.0188],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0581,     0.0001,     0.0161,     0.8556,     0.0701],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1087, 0.0015, 0.3307, 0.0654, 0.4936], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.161]
 [1.183]
 [1.205]
 [1.003]
 [0.94 ]] [[39.984]
 [21.715]
 [27.855]
 [26.373]
 [26.253]] [[1.896]
 [1.775]
 [2.181]
 [1.886]
 [1.815]]
Printing some Q and Qe and total Qs values:  [[1.01 ]
 [1.01 ]
 [1.073]
 [1.01 ]
 [1.01 ]] [[ 0.   ]
 [ 0.   ]
 [51.033]
 [ 0.   ]
 [ 0.   ]] [[0.634]
 [0.634]
 [1.806]
 [0.634]
 [0.634]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85837173
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.0248409253023
Printing some Q and Qe and total Qs values:  [[1.344]
 [1.344]
 [1.344]
 [1.344]
 [1.344]] [[32.046]
 [32.046]
 [32.046]
 [32.046]
 [32.046]] [[2.621]
 [2.621]
 [2.621]
 [2.621]
 [2.621]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.947]
 [1.207]
 [1.023]
 [1.023]
 [1.023]] [[36.283]
 [27.275]
 [33.518]
 [33.518]
 [33.518]] [[2.947]
 [2.518]
 [2.811]
 [2.811]
 [2.811]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4961 1.0 1.0
printing an ep nov before normalisation:  25.617175102233887
printing an ep nov before normalisation:  37.86198212039956
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.905]
 [0.905]
 [1.082]
 [0.905]
 [0.905]] [[41.304]
 [41.304]
 [42.784]
 [41.304]
 [41.304]] [[1.991]
 [1.991]
 [2.242]
 [1.991]
 [1.991]]
maxi score, test score, baseline:  0.4961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.163]
 [1.361]
 [1.324]
 [1.163]
 [1.223]] [[37.294]
 [41.913]
 [40.791]
 [37.294]
 [37.016]] [[1.533]
 [1.822]
 [1.763]
 [1.533]
 [1.589]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5001 1.0 1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  66.03224120557816
actions average: 
K:  3  action  0 :  tensor([    0.9665,     0.0193,     0.0016,     0.0003,     0.0124],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9972,     0.0000,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0251,     0.9117,     0.0009,     0.0620],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0005,     0.0006,     0.9539,     0.0447],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0024, 0.0352, 0.1159, 0.0465, 0.7999], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85716975
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.217082749472546
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8571892
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([    0.8785,     0.0013,     0.0002,     0.0289,     0.0910],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9972,     0.0000,     0.0002,     0.0025],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9868,     0.0005,     0.0127],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0524,     0.0002,     0.0005,     0.8232,     0.1237],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1670, 0.0007, 0.0010, 0.1709, 0.6605], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5021 1.0 1.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
siam score:  -0.8567281
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.459]
 [0.529]
 [0.503]
 [0.501]
 [0.459]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.459]
 [0.529]
 [0.503]
 [0.501]
 [0.459]]
printing an ep nov before normalisation:  37.88677857813712
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.487]
 [0.514]
 [0.445]
 [0.487]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.438]
 [0.487]
 [0.514]
 [0.445]
 [0.487]]
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.85822546
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.72653539827665
maxi score, test score, baseline:  0.5061 1.0 1.0
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.22504128350152
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.937]
 [1.084]
 [0.937]
 [0.937]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.937]
 [0.937]
 [1.084]
 [0.937]
 [0.937]]
maxi score, test score, baseline:  0.5061 1.0 1.0
siam score:  -0.8617012
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.8606,     0.0004,     0.0001,     0.0736,     0.0653],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9988,     0.0002,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0000,     0.9747,     0.0008,     0.0239],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0349,     0.0003,     0.0004,     0.8899,     0.0746],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0960,     0.0007,     0.0352,     0.1101,     0.7580],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.52109741576958
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.15023465760492
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.09663963317871
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.767]
 [0.767]
 [0.824]
 [0.767]
 [0.736]] [[32.797]
 [32.797]
 [32.778]
 [32.797]
 [32.441]] [[0.767]
 [0.767]
 [0.824]
 [0.767]
 [0.736]]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.03347416336343
maxi score, test score, baseline:  0.5081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.90617475436781
maxi score, test score, baseline:  0.5101 1.0 1.0
printing an ep nov before normalisation:  28.438494546322186
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.375]
 [1.355]
 [1.327]
 [1.313]] [[17.878]
 [19.198]
 [17.761]
 [16.617]
 [18.004]] [[2.084]
 [2.418]
 [2.271]
 [2.141]
 [2.25 ]]
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.8566,     0.0005,     0.0011,     0.0813,     0.0605],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9982,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9576,     0.0005,     0.0416],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0326, 0.0062, 0.0162, 0.8694, 0.0755], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.2042, 0.0444, 0.2000, 0.0036, 0.5479], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5101 1.0 1.0
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  24.444339275360107
maxi score, test score, baseline:  0.5101 1.0 1.0
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5101 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5101 1.0 1.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.218]
 [1.218]
 [1.218]
 [1.218]
 [1.218]]
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.045]
 [1.103]
 [1.123]
 [1.012]] [[37.7  ]
 [37.7  ]
 [44.771]
 [42.46 ]
 [29.777]] [[2.03 ]
 [2.03 ]
 [2.41 ]
 [2.324]
 [1.637]]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.330336570739746
maxi score, test score, baseline:  0.5121 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.162]
 [1.245]
 [1.162]
 [1.162]
 [1.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.162]
 [1.245]
 [1.162]
 [1.162]
 [1.162]]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.015924780452025
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.77363717644598
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.26015973794314
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5161 1.0 1.0
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8529706
maxi score, test score, baseline:  0.5181 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8569128
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.7714, 0.0021, 0.0009, 0.0519, 0.1737], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9999,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9884,     0.0003,     0.0111],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0911, 0.0013, 0.0012, 0.7910, 0.1154], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1576, 0.0017, 0.0051, 0.1072, 0.7285], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.748532084048286
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.90785000253702
Printing some Q and Qe and total Qs values:  [[1.284]
 [1.284]
 [1.284]
 [1.284]
 [1.284]] [[17.095]
 [17.095]
 [17.095]
 [17.095]
 [17.095]] [[18.379]
 [18.379]
 [18.379]
 [18.379]
 [18.379]]
maxi score, test score, baseline:  0.5221 1.0 1.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.86630034
printing an ep nov before normalisation:  27.88247307643079
maxi score, test score, baseline:  0.5241 1.0 1.0
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.09794344428968
Printing some Q and Qe and total Qs values:  [[1.32]
 [1.32]
 [1.32]
 [1.32]
 [1.32]] [[22.953]
 [22.953]
 [22.953]
 [22.953]
 [22.953]] [[2.986]
 [2.986]
 [2.986]
 [2.986]
 [2.986]]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.029]
 [1.029]
 [1.052]
 [1.029]
 [1.029]] [[40.362]
 [40.362]
 [44.086]
 [40.362]
 [40.362]] [[2.473]
 [2.473]
 [2.719]
 [2.473]
 [2.473]]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5241 1.0 1.0
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.95980536417799
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.05612524570693
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.161]
 [1.161]
 [1.183]
 [1.161]
 [1.161]] [[45.103]
 [45.103]
 [46.576]
 [45.103]
 [45.103]] [[2.477]
 [2.477]
 [2.566]
 [2.477]
 [2.477]]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5261 1.0 1.0
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.85396177
siam score:  -0.85579574
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8580595
actor:  0 policy actor:  0  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  35.442403008232496
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5321 1.0 1.0
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.831778900972715
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.235]
 [1.289]
 [1.258]
 [1.235]
 [1.235]] [[34.881]
 [37.234]
 [41.06 ]
 [34.881]
 [34.881]] [[2.069]
 [2.224]
 [2.357]
 [2.069]
 [2.069]]
printing an ep nov before normalisation:  16.776450550196742
Printing some Q and Qe and total Qs values:  [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.894]
 [0.894]
 [0.894]
 [0.894]
 [0.894]]
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5321 1.0 1.0
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  40.63104054782101
maxi score, test score, baseline:  0.5321 1.0 1.0
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5361 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.504490852355957
printing an ep nov before normalisation:  29.045791625976562
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.056215317572516
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.84331179344854
printing an ep nov before normalisation:  38.349845507958065
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.096]
 [1.096]
 [1.245]
 [1.096]
 [1.205]] [[31.422]
 [31.422]
 [32.088]
 [31.422]
 [28.614]] [[1.779]
 [1.779]
 [1.956]
 [1.779]
 [1.77 ]]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  2.6376277662363634
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85460824
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5421 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  37.0187839669587
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
printing an ep nov before normalisation:  30.297169482465403
printing an ep nov before normalisation:  17.475720107385673
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5441 1.0 1.0
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.194]
 [1.151]
 [1.151]
 [1.151]] [[67.99 ]
 [58.518]
 [59.811]
 [59.811]
 [59.811]] [[2.66 ]
 [2.433]
 [2.43 ]
 [2.43 ]
 [2.43 ]]
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5441 1.0 1.0
maxi score, test score, baseline:  0.5441 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8562943
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.72307777404785
maxi score, test score, baseline:  0.5461 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.1257352935407141
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.019]
 [1.019]
 [1.019]
 [1.019]] [[44.195]
 [44.195]
 [44.195]
 [44.195]
 [44.195]] [[59.93]
 [59.93]
 [59.93]
 [59.93]
 [59.93]]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  54.222487617298775
printing an ep nov before normalisation:  24.12252055557859
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.04632996144999
maxi score, test score, baseline:  0.5461 1.0 1.0
printing an ep nov before normalisation:  15.751121917321475
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.89294586285116
maxi score, test score, baseline:  0.5461 1.0 1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.09565686626283
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.893]
 [0.902]
 [0.82 ]
 [0.893]] [[19.807]
 [19.807]
 [22.439]
 [22.732]
 [19.807]] [[0.893]
 [0.893]
 [0.902]
 [0.82 ]
 [0.893]]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.26833593163005
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
line 256 mcts: sample exp_bonus 20.46369579476142
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.06433926619442
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5461 1.0 1.0
printing an ep nov before normalisation:  0.0023681819098442247
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7420, 0.0019, 0.0013, 0.0275, 0.2273], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0013,     0.9969,     0.0002,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9990,     0.0004,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0000,     0.0018,     0.9895,     0.0080],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0055, 0.0020, 0.3052, 0.1513, 0.5360], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.788782596588135
maxi score, test score, baseline:  0.5461 1.0 1.0
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.073]
 [1.073]
 [1.114]
 [1.073]
 [1.073]] [[62.863]
 [62.863]
 [61.652]
 [62.863]
 [62.863]] [[2.406]
 [2.406]
 [2.405]
 [2.406]
 [2.406]]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.064466675683484
maxi score, test score, baseline:  0.5461 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
printing an ep nov before normalisation:  19.846903238581206
siam score:  -0.8557006
printing an ep nov before normalisation:  36.54961585998535
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.34735734774358
Printing some Q and Qe and total Qs values:  [[0.75 ]
 [0.75 ]
 [0.854]
 [0.75 ]
 [0.75 ]] [[36.099]
 [36.099]
 [40.281]
 [36.099]
 [36.099]] [[0.75 ]
 [0.75 ]
 [0.854]
 [0.75 ]
 [0.75 ]]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.921]
 [0.816]
 [0.854]
 [0.854]] [[34.846]
 [32.64 ]
 [36.63 ]
 [34.846]
 [34.846]] [[0.854]
 [0.921]
 [0.816]
 [0.854]
 [0.854]]
printing an ep nov before normalisation:  31.99384601185832
printing an ep nov before normalisation:  37.19617764328509
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5700999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.8576,     0.0007,     0.0005,     0.0341,     0.1070],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0017,     0.9975,     0.0003,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9739,     0.0043,     0.0216],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0648,     0.0004,     0.0135,     0.8745,     0.0467],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0532, 0.0030, 0.2415, 0.0734, 0.6290], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.954]
 [1.018]
 [1.151]
 [1.018]
 [1.039]] [[28.684]
 [45.628]
 [42.985]
 [45.628]
 [36.927]] [[1.336]
 [2.16 ]
 [2.174]
 [2.16 ]
 [1.79 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  18.918657732459984
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.53693100048523
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.57889642134341
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.8624,     0.0003,     0.0022,     0.0111,     0.1241],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0034,     0.9941,     0.0001,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9239,     0.0026,     0.0732],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0224,     0.0006,     0.0020,     0.9045,     0.0706],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1563, 0.0018, 0.0149, 0.1367, 0.6904], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.233]
 [1.274]
 [1.257]
 [1.233]
 [1.233]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.233]
 [1.274]
 [1.257]
 [1.233]
 [1.233]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  75.8024315133013
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5841 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  22.758243083953857
maxi score, test score, baseline:  0.5861 1.0 1.0
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5861 1.0 1.0
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.2330447903338
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.483092784881592
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5881 1.0 1.0
printing an ep nov before normalisation:  38.12282112127432
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5881 1.0 1.0
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9971,     0.0002,     0.0004,     0.0009,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9991,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0091,     0.9444,     0.0007,     0.0454],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0000,     0.0278,     0.9285,     0.0435],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0876, 0.0125, 0.1168, 0.0820, 0.7010], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.5881 1.0 1.0
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5881 1.0 1.0
printing an ep nov before normalisation:  31.405206471726178
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.284633336778075
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.309]
 [1.347]
 [1.292]
 [1.309]
 [1.309]] [[45.71 ]
 [65.05 ]
 [57.342]
 [45.71 ]
 [45.71 ]] [[1.463]
 [1.619]
 [1.517]
 [1.463]
 [1.463]]
printing an ep nov before normalisation:  50.41677066753791
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5881 1.0 1.0
printing an ep nov before normalisation:  31.34593912170148
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.16741670935643
maxi score, test score, baseline:  0.5881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.810650467623034
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.91703476205983
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5961 1.0 1.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.58979091544563
printing an ep nov before normalisation:  32.369409766738464
actions average: 
K:  2  action  0 :  tensor([    0.9090,     0.0036,     0.0005,     0.0447,     0.0422],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9997,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0006,     0.9228,     0.0029,     0.0732],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0051,     0.0001,     0.0155,     0.9401,     0.0392],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1807, 0.0254, 0.1072, 0.0618, 0.6249], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.389477497249874
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.5961 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.098]
 [1.486]
 [1.394]
 [1.184]
 [1.375]] [[18.551]
 [19.346]
 [22.1  ]
 [16.049]
 [20.212]] [[0.588]
 [2.018]
 [2.072]
 [1.542]
 [1.953]]
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.5961 1.0 1.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.128]
 [1.01 ]
 [1.342]
 [1.053]
 [1.156]] [[17.097]
 [17.415]
 [14.127]
 [16.887]
 [14.986]] [[2.239]
 [2.156]
 [2.122]
 [2.141]
 [2.032]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.064]
 [1.064]
 [1.075]
 [1.052]
 [1.064]] [[48.289]
 [48.289]
 [48.384]
 [52.56 ]
 [48.289]] [[2.386]
 [2.386]
 [2.401]
 [2.526]
 [2.386]]
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.5961 1.0 1.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.335680820358654
maxi score, test score, baseline:  0.5961 1.0 1.0
maxi score, test score, baseline:  0.5961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.50834558560275
siam score:  -0.8377069
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.5981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6001 1.0 1.0
actions average: 
K:  2  action  0 :  tensor([    0.8416,     0.0006,     0.0091,     0.0058,     0.1429],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0032,     0.9920,     0.0002,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9573,     0.0003,     0.0423],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0334,     0.0001,     0.0078,     0.8696,     0.0891],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0650, 0.0009, 0.2704, 0.1588, 0.5048], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.81156631648062
Printing some Q and Qe and total Qs values:  [[0.837]
 [0.837]
 [1.063]
 [0.837]
 [0.837]] [[54.251]
 [54.251]
 [52.59 ]
 [54.251]
 [54.251]] [[1.977]
 [1.977]
 [2.145]
 [1.977]
 [1.977]]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8367048
printing an ep nov before normalisation:  39.678845142107264
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.785428537450976
Printing some Q and Qe and total Qs values:  [[1.314]
 [1.314]
 [1.402]
 [1.278]
 [1.358]] [[28.986]
 [28.986]
 [35.413]
 [29.74 ]
 [30.398]] [[1.641]
 [1.641]
 [1.876]
 [1.622]
 [1.717]]
siam score:  -0.83611107
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.33]
 [1.33]
 [1.33]
 [1.33]
 [1.33]] [[16.136]
 [16.136]
 [16.136]
 [16.136]
 [16.136]] [[2.322]
 [2.322]
 [2.322]
 [2.322]
 [2.322]]
maxi score, test score, baseline:  0.6001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.197]
 [1.197]
 [1.194]
 [1.197]
 [0.962]] [[18.75 ]
 [18.75 ]
 [16.82 ]
 [18.75 ]
 [17.866]] [[2.864]
 [2.864]
 [2.575]
 [2.864]
 [2.497]]
siam score:  -0.8328301
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6001 1.0 1.0
maxi score, test score, baseline:  0.6001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.451914557230204
maxi score, test score, baseline:  0.6021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6021 1.0 1.0
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.83222586
actor:  1 policy actor:  1  step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.837749
maxi score, test score, baseline:  0.6041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.772]
 [0.707]
 [0.707]
 [0.707]] [[41.894]
 [36.534]
 [41.894]
 [41.894]
 [41.894]] [[0.707]
 [0.772]
 [0.707]
 [0.707]
 [0.707]]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
actions average: 
K:  2  action  0 :  tensor([0.7647, 0.0014, 0.0031, 0.1073, 0.1235], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9984,     0.0001,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0006,     0.0003,     0.9641,     0.0048,     0.0302],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0354,     0.0006,     0.0105,     0.7522,     0.2013],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1819, 0.0340, 0.1878, 0.1738, 0.4226], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.521434858125765
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.156]
 [1.156]
 [1.156]
 [1.156]] [[29.283]
 [24.656]
 [24.656]
 [24.656]
 [24.656]] [[2.845]
 [2.322]
 [2.322]
 [2.322]
 [2.322]]
siam score:  -0.8467642
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.284]
 [1.375]
 [1.303]
 [1.284]
 [1.284]] [[34.921]
 [29.531]
 [32.816]
 [34.921]
 [34.921]] [[2.866]
 [2.606]
 [2.748]
 [2.866]
 [2.866]]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  15.56481122970581
printing an ep nov before normalisation:  20.702032801026775
printing an ep nov before normalisation:  22.221403121948242
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8524357
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8488428
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.8730,     0.0015,     0.0005,     0.0001,     0.1249],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0026,     0.9961,     0.0003,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9743,     0.0002,     0.0253],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0000,     0.0357,     0.9548,     0.0089],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0657, 0.0222, 0.1668, 0.0456, 0.6997], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9246,     0.0018,     0.0001,     0.0012,     0.0724],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9963,     0.0003,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9703,     0.0046,     0.0250],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0307,     0.0002,     0.0007,     0.8782,     0.0903],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1096, 0.0057, 0.0905, 0.1178, 0.6764], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
printing an ep nov before normalisation:  0.017558102943411313
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  21.705501040688283
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.54590741532766
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.277]
 [1.304]
 [1.333]
 [1.277]
 [1.277]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.277]
 [1.304]
 [1.333]
 [1.277]
 [1.277]]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 48.150343825543445
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.86 ]
 [0.838]
 [0.86 ]
 [0.86 ]
 [0.86 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.86 ]
 [0.838]
 [0.86 ]
 [0.86 ]
 [0.86 ]]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8361241
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.14111869707257
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.976]
 [0.976]
 [0.976]
 [0.976]
 [0.976]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.119]
 [1.192]
 [1.119]
 [1.086]] [[55.584]
 [55.584]
 [46.791]
 [55.584]
 [55.816]] [[3.002]
 [3.002]
 [2.682]
 [3.002]
 [2.98 ]]
maxi score, test score, baseline:  0.6081 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.167]
 [1.167]
 [1.167]
 [1.167]] [[19.818]
 [19.818]
 [19.818]
 [19.818]
 [19.818]] [[2.329]
 [2.329]
 [2.329]
 [2.329]
 [2.329]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.805581790565544
maxi score, test score, baseline:  0.6081 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([0.8014, 0.0018, 0.0012, 0.0351, 0.1606], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9957,     0.0002,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9667,     0.0052,     0.0280],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0010, 0.0018, 0.0232, 0.8777, 0.0963], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0445, 0.0907, 0.1683, 0.0892, 0.6073], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.936]
 [1.105]
 [0.936]
 [0.936]
 [0.936]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.936]
 [1.105]
 [0.936]
 [0.936]
 [0.936]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.726833092316127
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.887508754881996
Printing some Q and Qe and total Qs values:  [[1.264]
 [1.264]
 [1.264]
 [1.264]
 [1.264]] [[19.476]
 [19.476]
 [19.476]
 [19.476]
 [19.476]] [[27.226]
 [27.226]
 [27.226]
 [27.226]
 [27.226]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.92917507407322
printing an ep nov before normalisation:  33.18758010864258
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.667791158155396
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.50342845916748
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20007
Printing some Q and Qe and total Qs values:  [[1.095]
 [1.095]
 [1.095]
 [1.064]
 [1.095]] [[47.375]
 [47.375]
 [47.375]
 [51.629]
 [47.375]] [[2.566]
 [2.566]
 [2.566]
 [2.731]
 [2.566]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.82271363500414
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9974,     0.0016,     0.0002,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0133,     0.9782,     0.0005,     0.0000,     0.0079],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0006,     0.9664,     0.0007,     0.0322],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0098,     0.0000,     0.0126,     0.9572,     0.0204],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0096, 0.0114, 0.1696, 0.0027, 0.8066], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.115]
 [1.344]
 [1.324]
 [0.358]
 [1.292]] [[24.603]
 [24.961]
 [28.799]
 [32.083]
 [29.097]] [[2.282]
 [2.544]
 [2.878]
 [2.215]
 [2.873]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.929864385533712
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.222]
 [1.374]
 [1.361]
 [1.27 ]
 [1.324]] [[25.169]
 [24.133]
 [23.47 ]
 [23.382]
 [21.019]] [[1.714]
 [1.827]
 [1.789]
 [1.695]
 [1.659]]
maxi score, test score, baseline:  0.6081 1.0 1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.98 ]
 [0.906]
 [0.906]
 [0.906]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.906]
 [0.98 ]
 [0.906]
 [0.906]
 [0.906]]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.635344283511838
Printing some Q and Qe and total Qs values:  [[1.199]
 [1.41 ]
 [1.336]
 [1.113]
 [1.162]] [[12.49 ]
 [10.474]
 [ 9.621]
 [18.556]
 [11.399]] [[1.748]
 [1.869]
 [1.758]
 [1.927]
 [1.663]]
printing an ep nov before normalisation:  0.29255540151325476
actor:  1 policy actor:  1  step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.82630620489245
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6061 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.94 ]
 [0.949]
 [0.94 ]
 [0.818]
 [0.94 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.94 ]
 [0.949]
 [0.94 ]
 [0.818]
 [0.94 ]]
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.341]
 [1.373]
 [1.341]
 [1.341]
 [1.341]] [[43.046]
 [48.772]
 [43.046]
 [43.046]
 [43.046]] [[2.103]
 [2.28 ]
 [2.103]
 [2.103]
 [2.103]]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.05653879362396
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.    0.    0.875 0.042 0.083]
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6121 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.164]
 [1.48 ]
 [1.362]
 [1.255]
 [1.229]] [[15.635]
 [10.671]
 [16.248]
 [20.163]
 [15.216]] [[0.826]
 [1.932]
 [2.051]
 [2.109]
 [1.874]]
Printing some Q and Qe and total Qs values:  [[1.18]
 [1.18]
 [1.18]
 [1.26]
 [1.18]] [[17.56 ]
 [17.56 ]
 [17.56 ]
 [26.113]
 [17.56 ]] [[1.487]
 [1.487]
 [1.487]
 [2.033]
 [1.487]]
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.233]
 [1.372]
 [1.233]
 [1.258]
 [1.285]] [[16.328]
 [15.686]
 [16.328]
 [22.022]
 [18.632]] [[1.876]
 [1.963]
 [1.876]
 [2.359]
 [2.113]]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.50784044752535
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.901931651960115
actor:  1 policy actor:  1  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8268293
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.6141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.617413335215396
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6081 1.0 1.0
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.6081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.69618205635855
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
actor:  1 policy actor:  1  step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[1.192]
 [1.192]
 [1.192]
 [1.192]
 [1.192]] [[35.157]
 [35.157]
 [35.157]
 [35.157]
 [35.157]] [[2.548]
 [2.548]
 [2.548]
 [2.548]
 [2.548]]
maxi score, test score, baseline:  0.6101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
