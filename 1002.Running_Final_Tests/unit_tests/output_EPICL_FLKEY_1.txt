append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 40}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lake_KEY.gridWorld'>
same_env_each_time:True
env_size:[7, 7]
observable_size:[7, 7]
game_modes:2
env_map:[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
max_steps:120
actions_size:5
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(2, 49)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  6.083661213071778
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  6.992495881162881
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
siam score:  0.0005224459508264607
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.005546694944402383
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.022902924182937677
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 2 threads
Frames:  1198 train batches done:  31 episodes:  93
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([0.4370, 0.2360, 0.0674, 0.1228, 0.1367], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0931, 0.6256, 0.1697, 0.0146, 0.0970], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0891, 0.1781, 0.3966, 0.1767, 0.1594], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.1636, 0.0169, 0.2358, 0.4031, 0.1805], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.2126, 0.2307, 0.1593, 0.1854, 0.2120], grad_fn=<DivBackward0>)
siam score:  -0.16625383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 1 threads
Frames:  1198 train batches done:  75 episodes:  93
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([0.3218, 0.2005, 0.0789, 0.1811, 0.2177], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1499, 0.6376, 0.1057, 0.0157, 0.0910], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0507, 0.0849, 0.4304, 0.2423, 0.1918], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1874, 0.0178, 0.1194, 0.4130, 0.2625], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1466, 0.1038, 0.2038, 0.3669, 0.1790], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([0.5965, 0.0345, 0.0572, 0.1432, 0.1685], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0682, 0.8309, 0.0517, 0.0050, 0.0442], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0230, 0.0279, 0.7269, 0.0994, 0.1228], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0921, 0.0161, 0.1646, 0.4639, 0.2633], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1225, 0.0479, 0.2255, 0.2853, 0.3188], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.43051192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.4785387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([0.5971, 0.1262, 0.0147, 0.1308, 0.1313], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1192, 0.6408, 0.1135, 0.0124, 0.1141], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0196, 0.0612, 0.6619, 0.1297, 0.1275], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.1621, 0.0158, 0.1591, 0.4124, 0.2506], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1247, 0.1791, 0.1703, 0.2042, 0.3216], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.5134403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Sims:  6 1 epoch:  1681 pick best:  False frame count:  1681
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5303922
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.007186940305466
UNIT TEST: sample policy line 217 mcts : [0. 0. 0. 1. 0.]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.512283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.291656374931335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.56335664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([0.7287, 0.0674, 0.0019, 0.0825, 0.1194], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0679, 0.8281, 0.0210, 0.0063, 0.0767], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0064, 0.0447, 0.5111, 0.1907, 0.2472], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0873, 0.0404, 0.0692, 0.5278, 0.2753], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1913, 0.1131, 0.0739, 0.1308, 0.4910], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.5664096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([0.6076, 0.0703, 0.0016, 0.1200, 0.2004], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0659,     0.8265,     0.0199,     0.0003,     0.0874],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0015, 0.2763, 0.4344, 0.0926, 0.1952], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0415, 0.0209, 0.1312, 0.5441, 0.2623], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0182, 0.2255, 0.1372, 0.2053, 0.4139], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.6213229
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.895]
 [56.895]
 [56.895]
 [56.895]
 [56.895]] [[75.842]
 [75.842]
 [75.842]
 [75.842]
 [75.842]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([0.5803, 0.1382, 0.0033, 0.0696, 0.2086], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0737, 0.7476, 0.0217, 0.0154, 0.1416], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0010, 0.0902, 0.7211, 0.0607, 0.1270], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1949, 0.0019, 0.1596, 0.3896, 0.2540], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0267, 0.2918, 0.0767, 0.2239, 0.3810], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6214626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.55712426806468
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.503]
 [22.503]
 [25.667]
 [22.503]
 [22.503]] [[1.53]
 [1.53]
 [2.  ]
 [1.53]
 [1.53]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.63883203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6464691
actions average: 
K:  2  action  0 :  tensor([    0.9082,     0.0415,     0.0000,     0.0160,     0.0343],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.1188, 0.7885, 0.0312, 0.0098, 0.0516], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.2251,     0.7139,     0.0066,     0.0542],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0374, 0.0199, 0.0521, 0.5738, 0.3168], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0159, 0.1551, 0.1024, 0.2685, 0.4580], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.67]
 [31.67]
 [31.67]
 [62.08]
 [31.67]] [[-0.048]
 [-0.048]
 [-0.048]
 [ 1.063]
 [-0.048]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6280894
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  72.51722812652588
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.67107236
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 64.55962304178654
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  100.45184923535892
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 82.84831657043442
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6775595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  100.07511971994703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.874]
 [49.323]
 [52.701]
 [49.503]
 [51.658]] [[0.74 ]
 [0.597]
 [0.639]
 [0.6  ]
 [0.626]]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  94.07881234986215
printing an ep nov before normalisation:  73.30554255621753
printing an ep nov before normalisation:  77.15822519516513
printing an ep nov before normalisation:  52.51179694937651
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  85.35719635073892
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.51072184273885
printing an ep nov before normalisation:  45.015249252319336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6836584
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.2]
 [63.2]
 [63.2]
 [63.2]
 [63.2]] [[0.573]
 [0.573]
 [0.573]
 [0.573]
 [0.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([0.9533, 0.0026, 0.0032, 0.0201, 0.0208], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0191, 0.8916, 0.0301, 0.0220, 0.0371], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0015,     0.9620,     0.0045,     0.0319],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0006,     0.0240,     0.6585,     0.3163],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0090, 0.0219, 0.0351, 0.3185, 0.6155], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  93.7584578325821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  83.88629890061866
printing an ep nov before normalisation:  39.63102919402008
printing an ep nov before normalisation:  55.58103663803593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.28833411543883
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.334]
 [34.949]
 [38.332]
 [33.609]
 [31.338]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.745]
 [15.697]
 [15.697]
 [15.697]
 [15.697]] [[1.258]
 [0.532]
 [0.532]
 [0.532]
 [0.532]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.13429880142212
printing an ep nov before normalisation:  0.8039682546581162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.88544178755203
printing an ep nov before normalisation:  46.70231056015211
printing an ep nov before normalisation:  57.35182583664939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.299033641815186
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.6698704
printing an ep nov before normalisation:  53.94647516359402
printing an ep nov before normalisation:  48.42509949636552
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  74.98005948842236
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.9631462097168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.8520,     0.0632,     0.0000,     0.0389,     0.0460],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0014, 0.9018, 0.0293, 0.0114, 0.0561], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0368, 0.0899, 0.5681, 0.1130, 0.1922], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2216, 0.0446, 0.0621, 0.4302, 0.2415], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1631, 0.0435, 0.0695, 0.2003, 0.5237], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 61.910484976684344
printing an ep nov before normalisation:  58.59100658615477
printing an ep nov before normalisation:  53.54127176277693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  20.4440723279756
printing an ep nov before normalisation:  24.375811960297824
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.901964371129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.558]
 [80.558]
 [80.558]
 [80.558]
 [80.558]] [[1.573]
 [1.573]
 [1.573]
 [1.573]
 [1.573]]
printing an ep nov before normalisation:  102.35852796996461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.79046724774596
printing an ep nov before normalisation:  51.946970896257234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.99746654646133
printing an ep nov before normalisation:  23.27734781580988
line 256 mcts: sample exp_bonus 78.76987929303495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.68835646
printing an ep nov before normalisation:  43.18217966880585
printing an ep nov before normalisation:  54.25175666809082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.8296947002786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  73.26966034902505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.701]
 [52.058]
 [54.955]
 [66.336]
 [52.058]] [[1.049]
 [0.599]
 [0.665]
 [0.926]
 [0.599]]
actions average: 
K:  1  action  0 :  tensor([    0.9251,     0.0004,     0.0000,     0.0373,     0.0373],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0050,     0.9082,     0.0003,     0.0005,     0.0861],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0036,     0.8922,     0.0320,     0.0720],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0514,     0.0005,     0.0550,     0.6817,     0.2114],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.1595, 0.0170, 0.0608, 0.2421, 0.5206], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.591]
 [69.591]
 [69.591]
 [69.591]
 [69.591]] [[0.891]
 [0.891]
 [0.891]
 [0.891]
 [0.891]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.64784526824951
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.68934
using explorer policy with actor:  1
printing an ep nov before normalisation:  88.23695533844213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.597]
 [60.13 ]
 [69.565]
 [56.04 ]
 [39.278]] [[0.224]
 [0.14 ]
 [0.191]
 [0.118]
 [0.028]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.76013593889756
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  113.46670343523857
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7038277
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.321]
 [45.386]
 [43.951]
 [43.43 ]
 [42.718]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  22.37937021497449
printing an ep nov before normalisation:  65.63746333122253
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  104.84568782623742
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.177864457898686
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.288268089294434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.67 ]
 [28.192]
 [35.113]
 [33.955]
 [32.329]] [[0.997]
 [0.817]
 [1.044]
 [1.006]
 [0.953]]
siam score:  -0.7032863
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
main train batch thing paused
add a thread
Adding thread: now have 4 threads
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.26483990287787
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.301937103271484
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  74.50877536088558
printing an ep nov before normalisation:  63.403157713273906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.317]
 [67.317]
 [67.317]
 [67.317]
 [67.317]] [[0.347]
 [0.347]
 [0.347]
 [0.347]
 [0.347]]
printing an ep nov before normalisation:  65.63924491009436
printing an ep nov before normalisation:  40.132527038901266
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.28126263629332
printing an ep nov before normalisation:  77.66484513551127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.75693754749882
printing an ep nov before normalisation:  58.53069540456214
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.552]
 [83.758]
 [83.758]
 [83.758]
 [83.758]] [[1.154]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  56.862132638953454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.848702821150994
printing an ep nov before normalisation:  63.90105474636143
printing an ep nov before normalisation:  52.95620260978722
using explorer policy with actor:  1
printing an ep nov before normalisation:  104.80496483656171
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.38776221397589
printing an ep nov before normalisation:  0.18168481946882292
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.796]
 [68.796]
 [56.6  ]
 [68.796]
 [55.48 ]] [[2.821]
 [2.821]
 [2.   ]
 [2.821]
 [1.925]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.291]
 [46.291]
 [46.291]
 [46.291]
 [46.291]] [[1.047]
 [1.047]
 [1.047]
 [1.047]
 [1.047]]
printing an ep nov before normalisation:  90.22354291816828
printing an ep nov before normalisation:  63.95953491947915
printing an ep nov before normalisation:  44.205923080444336
line 256 mcts: sample exp_bonus 39.18765011504599
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.03516581908518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.240500344170464
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.643]
 [45.643]
 [45.643]
 [45.643]
 [45.643]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.54423554157972
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.343]
 [25.064]
 [19.153]
 [21.342]
 [19.948]] [[0.191]
 [0.094]
 [0.059]
 [0.072]
 [0.064]]
line 256 mcts: sample exp_bonus 46.47011626002772
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.981081987745426
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.713]
 [30.713]
 [30.713]
 [30.713]
 [30.713]] [[0.128]
 [0.128]
 [0.128]
 [0.128]
 [0.128]]
printing an ep nov before normalisation:  38.703484862133145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  80.40314351728696
printing an ep nov before normalisation:  94.62508130864019
printing an ep nov before normalisation:  63.50909270755113
printing an ep nov before normalisation:  66.13438010552223
siam score:  -0.7228197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.04601425376174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 110.4729151725769
siam score:  -0.7303553
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.008]
 [65.675]
 [44.008]
 [44.008]
 [44.008]] [[0.731]
 [1.375]
 [0.731]
 [0.731]
 [0.731]]
printing an ep nov before normalisation:  56.45837324815361
printing an ep nov before normalisation:  44.57844826072905
printing an ep nov before normalisation:  67.94875310703188
actions average: 
K:  4  action  0 :  tensor([    0.9391,     0.0184,     0.0000,     0.0119,     0.0306],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.1136,     0.8533,     0.0000,     0.0016,     0.0315],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0014,     0.8505,     0.0733,     0.0746],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0692, 0.0010, 0.0019, 0.6185, 0.3094], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0660, 0.0652, 0.0095, 0.3041, 0.5553], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.06046740347772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.749016699741205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  5.1152117755441395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  15.09032377015842
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.241]
 [69.595]
 [67.506]
 [80.32 ]
 [77.316]] [[1.48 ]
 [1.06 ]
 [0.99 ]
 [1.416]
 [1.316]]
line 256 mcts: sample exp_bonus 65.10418193089208
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.687]
 [54.369]
 [65.102]
 [62.131]
 [54.369]] [[1.384]
 [1.05 ]
 [1.434]
 [1.328]
 [1.05 ]]
printing an ep nov before normalisation:  86.97527912756475
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.7213162194013
printing an ep nov before normalisation:  87.07552260743296
printing an ep nov before normalisation:  60.72085472788917
printing an ep nov before normalisation:  40.014779344583346
printing an ep nov before normalisation:  17.29987141457699
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  88.68586222330728
printing an ep nov before normalisation:  62.00125694274902
printing an ep nov before normalisation:  110.56673731598686
siam score:  -0.71215457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
deleting a thread, now have 3 threads
Frames:  11324 train batches done:  1317 episodes:  833
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.581]
 [55.581]
 [55.581]
 [55.581]
 [55.581]] [[0.938]
 [0.938]
 [0.938]
 [0.938]
 [0.938]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.656]
 [44.095]
 [44.095]
 [44.095]
 [44.095]] [[1.324]
 [1.056]
 [1.056]
 [1.056]
 [1.056]]
printing an ep nov before normalisation:  60.869867422459166
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7318828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[85.944]
 [58.254]
 [64.505]
 [61.461]
 [49.374]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  73.25725274438321
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.900917053222656
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.212]
 [48.269]
 [50.372]
 [47.955]
 [47.205]] [[1.185]
 [1.414]
 [1.533]
 [1.396]
 [1.354]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.53986928877507
printing an ep nov before normalisation:  65.71428292983532
printing an ep nov before normalisation:  53.397531509399414
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.21853493140849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.3007697192194
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9474,     0.0154,     0.0000,     0.0093,     0.0278],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9625,     0.0060,     0.0007,     0.0306],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0091,     0.9793,     0.0025,     0.0090],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0122, 0.0058, 0.0422, 0.6432, 0.2966], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0094, 0.0189, 0.0248, 0.3591, 0.5877], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.8886,     0.0522,     0.0003,     0.0165,     0.0425],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0045,     0.9646,     0.0007,     0.0018,     0.0283],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0022, 0.1045, 0.7047, 0.0643, 0.1242], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0032, 0.0019, 0.0154, 0.7372, 0.2423], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0838, 0.0960, 0.0472, 0.3557, 0.4174], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.247619005596725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  2.5092952380958877
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.149284723904522
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.469]
 [46.059]
 [64.027]
 [50.981]
 [41.161]] [[0.39 ]
 [0.433]
 [0.925]
 [0.568]
 [0.299]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.432]
 [52.522]
 [48.901]
 [58.581]
 [61.036]] [[1.422]
 [1.162]
 [1.043]
 [1.361]
 [1.442]]
actions average: 
K:  0  action  0 :  tensor([    0.9657,     0.0013,     0.0000,     0.0178,     0.0152],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0280,     0.9563,     0.0019,     0.0002,     0.0137],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0081, 0.0139, 0.9628, 0.0062, 0.0089], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0027, 0.0027, 0.0405, 0.5739, 0.3803], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0259, 0.0523, 0.0051, 0.2180, 0.6987], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 38.56648158994269
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7280202
actions average: 
K:  1  action  0 :  tensor([    0.9817,     0.0134,     0.0001,     0.0029,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9799,     0.0005,     0.0003,     0.0179],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0123,     0.9035,     0.0328,     0.0515],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0278,     0.0003,     0.0006,     0.7856,     0.1857],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0415, 0.0073, 0.0008, 0.2554, 0.6950], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  82.93393888879424
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.401163221699804
printing an ep nov before normalisation:  78.05634999528787
printing an ep nov before normalisation:  75.67476017553136
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.81700672683596
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.644]
 [52.026]
 [52.026]
 [52.026]
 [52.026]] [[0.946]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.671364294457035
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.06403446710332
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.517]
 [35.987]
 [42.095]
 [55.29 ]
 [35.922]] [[0.729]
 [0.597]
 [0.825]
 [1.317]
 [0.594]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.026]
 [45.791]
 [67.064]
 [67.509]
 [61.64 ]] [[1.313]
 [0.815]
 [1.468]
 [1.482]
 [1.301]]
deleting a thread, now have 2 threads
Frames:  12737 train batches done:  1491 episodes:  881
printing an ep nov before normalisation:  76.76420211791992
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.06845345330825
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.18324137862352
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  73.08699514440995
printing an ep nov before normalisation:  44.00486946105957
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.48095638644243
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.665661741422554
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.04455605833055
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.74735993
printing an ep nov before normalisation:  37.4131555408664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7520164
printing an ep nov before normalisation:  69.02583263751589
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 86.92643301376803
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([0.8776, 0.0446, 0.0427, 0.0131, 0.0220], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9776,     0.0129,     0.0002,     0.0092],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0099, 0.0110, 0.9433, 0.0193, 0.0165], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0011,     0.0001,     0.0011,     0.6127,     0.3850],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0050, 0.0110, 0.0335, 0.1971, 0.7535], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.78093481063843
printing an ep nov before normalisation:  58.878179467956215
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.48895971380885
printing an ep nov before normalisation:  0.029223654061638626
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9956,     0.0001,     0.0000,     0.0013,     0.0030],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.8873,     0.0057,     0.0003,     0.1054],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9990,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0229,     0.7039,     0.2729],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0031,     0.0001,     0.0484,     0.3261,     0.6223],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7350183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.32907783062198
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.115]
 [59.115]
 [54.871]
 [68.74 ]
 [59.115]] [[1.367]
 [1.367]
 [1.234]
 [1.667]
 [1.367]]
printing an ep nov before normalisation:  40.629118064172495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.47 ]
 [47.479]
 [51.249]
 [49.866]
 [47.545]] [[0.963]
 [1.103]
 [1.191]
 [1.159]
 [1.104]]
printing an ep nov before normalisation:  64.32071208953857
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.46328399408887
printing an ep nov before normalisation:  68.97725520615492
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.83334790103716
printing an ep nov before normalisation:  109.43000379425567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.50901040833237
siam score:  -0.7428486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.087595016667734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.688414949619116
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.35488510131836
printing an ep nov before normalisation:  88.42377774704664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9573,     0.0070,     0.0312,     0.0002,     0.0043],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9935,     0.0006,     0.0006,     0.0052],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9977,     0.0008,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0098, 0.0009, 0.0101, 0.7085, 0.2707], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0061, 0.0007, 0.0024, 0.4063, 0.5845], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.66469937067845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.910825209696306
printing an ep nov before normalisation:  68.13068859560417
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.965]
 [38.965]
 [38.965]
 [38.965]
 [38.965]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
actions average: 
K:  3  action  0 :  tensor([    0.8978,     0.0216,     0.0002,     0.0357,     0.0448],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0314,     0.9564,     0.0003,     0.0020,     0.0099],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9790,     0.0120,     0.0088],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0008,     0.0328,     0.8052,     0.1604],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0070, 0.0045, 0.0786, 0.4369, 0.4729], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.75914732095667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7572212
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.75061136
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.605]
 [48.924]
 [36.605]
 [36.605]
 [36.605]] [[0.6  ]
 [0.985]
 [0.6  ]
 [0.6  ]
 [0.6  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.289174981146274
UNIT TEST: sample policy line 217 mcts : [0.103 0.154 0.154 0.231 0.359]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.75144136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.452171325683594
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
printing an ep nov before normalisation:  83.23252004052732
siam score:  -0.76211846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.1289441712374355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.25877629582131
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  4  action  0 :  tensor([    0.9357,     0.0008,     0.0004,     0.0284,     0.0348],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9909,     0.0053,     0.0000,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0001,     0.9439,     0.0247,     0.0311],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0003,     0.0091,     0.7045,     0.2855],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0686, 0.0280, 0.0404, 0.1509, 0.7121], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.94 ]
 [35.785]
 [40.961]
 [35.199]
 [29.274]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.11123602858704
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7666565
printing an ep nov before normalisation:  53.30162048339844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.205 0.128 0.077 0.462 0.128]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.761]
 [49.297]
 [55.705]
 [46.486]
 [53.186]] [[0.701]
 [0.611]
 [0.778]
 [0.537]
 [0.713]]
printing an ep nov before normalisation:  33.84680817810079
siam score:  -0.7720204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.306933042206474
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.86888737469185
printing an ep nov before normalisation:  58.28616171486239
printing an ep nov before normalisation:  39.34468131232542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  12.746714319454568
printing an ep nov before normalisation:  4.550710901721686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.86315386528656
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.592]
 [59.223]
 [60.198]
 [53.128]
 [55.88 ]] [[0.23 ]
 [0.262]
 [0.27 ]
 [0.208]
 [0.232]]
printing an ep nov before normalisation:  68.53354028335053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.875114078306602
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.6540207862854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.32760248578323
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.412]
 [35.412]
 [35.412]
 [35.412]
 [35.412]] [[0.425]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  57.804897819029016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9719,     0.0025,     0.0000,     0.0112,     0.0144],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9882,     0.0037,     0.0000,     0.0080],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0046, 0.0511, 0.8624, 0.0283, 0.0537], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0082, 0.0026, 0.0022, 0.7286, 0.2584], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0534, 0.0610, 0.0396, 0.3275, 0.5185], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
printing an ep nov before normalisation:  55.001348383805244
siam score:  -0.7543063
printing an ep nov before normalisation:  63.137139358081534
printing an ep nov before normalisation:  47.03470044163839
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.41822814941406
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.015999851744936677
printing an ep nov before normalisation:  56.85634905408125
printing an ep nov before normalisation:  56.10548320322303
printing an ep nov before normalisation:  58.682768021862955
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.392]
 [42.392]
 [42.392]
 [36.142]
 [42.392]] [[2.216]
 [2.216]
 [2.216]
 [1.667]
 [2.216]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.921]
 [33.948]
 [35.42 ]
 [29.696]
 [28.655]] [[1.34 ]
 [1.42 ]
 [1.534]
 [1.09 ]
 [1.009]]
printing an ep nov before normalisation:  51.82485464984918
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.34440707200087
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.55672995430683
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
printing an ep nov before normalisation:  23.109295382850828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9981,     0.0002,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0006,     0.9507,     0.0156,     0.0330],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0001,     0.0134,     0.7752,     0.2106],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0064, 0.0205, 0.0256, 0.3319, 0.6156], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.49501670783933
printing an ep nov before normalisation:  30.792818069458008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.32456398010254
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9926,     0.0001,     0.0000,     0.0011,     0.0061],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9983,     0.0009,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0156, 0.0082, 0.8942, 0.0273, 0.0546], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0034, 0.0185, 0.0089, 0.6052, 0.3639], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0046,     0.0004,     0.0423,     0.2892,     0.6635],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.7644997582759
printing an ep nov before normalisation:  60.604396823421986
siam score:  -0.77383864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.695]
 [49.527]
 [59.254]
 [44.446]
 [47.541]] [[0.745]
 [0.897]
 [1.205]
 [0.737]
 [0.835]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.093]
 [22.98 ]
 [23.996]
 [24.371]
 [23.186]] [[1.163]
 [1.152]
 [1.25 ]
 [1.286]
 [1.172]]
printing an ep nov before normalisation:  74.89540067399798
printing an ep nov before normalisation:  87.74891984150018
printing an ep nov before normalisation:  73.16720312954085
line 256 mcts: sample exp_bonus 47.61070392624716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7703849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.246]
 [34.39 ]
 [39.208]
 [43.51 ]
 [39.988]] [[0.755]
 [0.504]
 [0.658]
 [0.795]
 [0.683]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 56.87183836928714
printing an ep nov before normalisation:  37.88332132985736
printing an ep nov before normalisation:  41.76669424371457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.436667530849235
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.836230773002555
printing an ep nov before normalisation:  63.826794656443624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.7588832227302
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
printing an ep nov before normalisation:  3.1989512139804788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.162615329276548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7671016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.577]
 [44.828]
 [40.036]
 [43.577]
 [43.577]] [[1.615]
 [1.683]
 [1.423]
 [1.615]
 [1.615]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7739126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.05836200714111
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.507]
 [31.019]
 [36.289]
 [33.452]
 [30.764]] [[1.235]
 [0.635]
 [0.853]
 [0.736]
 [0.624]]
printing an ep nov before normalisation:  60.88700168716151
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7841884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.519515754386966
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7850311
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.513]
 [28.046]
 [43.16 ]
 [36.531]
 [29.944]] [[0.538]
 [0.37 ]
 [0.763]
 [0.591]
 [0.42 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.556]
 [48.158]
 [46.791]
 [46.22 ]
 [42.388]] [[1.403]
 [1.175]
 [1.117]
 [1.093]
 [0.931]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.803]
 [56.803]
 [56.803]
 [56.803]
 [56.803]] [[1.15]
 [1.15]
 [1.15]
 [1.15]
 [1.15]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.89204025268555
siam score:  -0.77429724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.07771672254773
printing an ep nov before normalisation:  50.67180633544922
printing an ep nov before normalisation:  70.77815037353987
UNIT TEST: sample policy line 217 mcts : [0.103 0.179 0.436 0.179 0.103]
actions average: 
K:  4  action  0 :  tensor([    0.7627,     0.0164,     0.0000,     0.1142,     0.1066],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9887,     0.0044,     0.0001,     0.0066],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0130, 0.0333, 0.8706, 0.0492, 0.0339], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0026,     0.0004,     0.0987,     0.6824,     0.2159],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0015, 0.1678, 0.0612, 0.2292, 0.5403], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.968]
 [44.588]
 [44.588]
 [44.588]
 [44.588]] [[1.333]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
printing an ep nov before normalisation:  84.1474639639211
printing an ep nov before normalisation:  73.61937824130482
printing an ep nov before normalisation:  66.02445015154805
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.008]
 [44.563]
 [44.563]
 [44.563]
 [44.563]] [[0.952]
 [0.758]
 [0.758]
 [0.758]
 [0.758]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9741,     0.0005,     0.0000,     0.0130,     0.0123],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9973,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9763,     0.0129,     0.0106],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0913,     0.0005,     0.0388,     0.6361,     0.2332],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0198, 0.0547, 0.1258, 0.2654, 0.5342], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.67460108041724
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  81.33157736317288
printing an ep nov before normalisation:  69.03974700466861
printing an ep nov before normalisation:  35.05033016204834
using explorer policy with actor:  1
siam score:  -0.76576823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9899,     0.0006,     0.0000,     0.0025,     0.0070],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0296, 0.9123, 0.0104, 0.0094, 0.0384], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.9903,     0.0010,     0.0081],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0998,     0.7053,     0.1945],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0037, 0.0526, 0.0747, 0.2723, 0.5967], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.358]
 [51.242]
 [62.683]
 [60.928]
 [51.242]] [[0.249]
 [0.557]
 [0.794]
 [0.758]
 [0.557]]
printing an ep nov before normalisation:  48.56980632697262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([0.9710, 0.0058, 0.0109, 0.0053, 0.0070], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0218,     0.9710,     0.0017,     0.0001,     0.0054],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9307,     0.0183,     0.0509],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0002,     0.0135,     0.7310,     0.2547],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0036, 0.0022, 0.0371, 0.3355, 0.6217], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.89628754946939
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  76.83493393024997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.12884068245402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.52883173172119
printing an ep nov before normalisation:  71.12471984502758
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.27487988529715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7917278
siam score:  -0.79220885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.416595186506
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.66218480289
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.028]
 [70.207]
 [67.003]
 [70.207]
 [79.97 ]] [[0.894]
 [0.772]
 [0.723]
 [0.772]
 [0.924]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  75.5772055897745
printing an ep nov before normalisation:  72.38974804147611
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  75.71025390214712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.689878462015514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.05661320521667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  1.8016858917246736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.08628211817337
actions average: 
K:  3  action  0 :  tensor([    0.9961,     0.0005,     0.0000,     0.0015,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0028,     0.9687,     0.0006,     0.0004,     0.0275],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0019, 0.0042, 0.9475, 0.0070, 0.0395], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0584, 0.0010, 0.0141, 0.5955, 0.3310], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0196,     0.0085,     0.0002,     0.2790,     0.6927],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.782517
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.065757825115796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[116.789]
 [116.789]
 [119.761]
 [116.789]
 [116.789]] [[1.273]
 [1.273]
 [1.318]
 [1.273]
 [1.273]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.095]
 [56.122]
 [37.768]
 [62.366]
 [58.172]] [[0.943]
 [1.04 ]
 [0.599]
 [1.19 ]
 [1.089]]
printing an ep nov before normalisation:  28.211874368178993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  81.54716018743787
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
STARTED EXPV TRAINING ON FRAME NO.  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.396]
 [36.396]
 [59.205]
 [36.396]
 [36.396]] [[0.468]
 [0.468]
 [1.016]
 [0.468]
 [0.468]]
Starting evaluation
siam score:  -0.7847521
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.03697377730175
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  20.83556749091934
actions average: 
K:  3  action  0 :  tensor([    0.9264,     0.0118,     0.0005,     0.0262,     0.0350],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0119,     0.9351,     0.0258,     0.0007,     0.0264],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0004,     0.9941,     0.0003,     0.0053],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0002,     0.0092,     0.7653,     0.2249],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0361, 0.0034, 0.0992, 0.2206, 0.6407], grad_fn=<DivBackward0>)
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9445,     0.0052,     0.0006,     0.0251,     0.0246],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0024, 0.9102, 0.0294, 0.0035, 0.0545], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0133,     0.8639,     0.0609,     0.0619],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0140, 0.0696, 0.0178, 0.6712, 0.2274], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0070, 0.0030, 0.0280, 0.3113, 0.6508], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.744043220636684
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20036
printing an ep nov before normalisation:  49.37410903976912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.43097257328545
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.036]
 [46.139]
 [47.576]
 [47.944]
 [46.139]] [[1.732]
 [1.805]
 [1.9  ]
 [1.925]
 [1.805]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.18017853983676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.71353816903452
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.47315959062632
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.69309619363692
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.11425869597937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.85 ]
 [37.688]
 [40.214]
 [35.282]
 [34.807]] [[0.924]
 [1.012]
 [1.133]
 [0.897]
 [0.874]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.28918541650574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.24648813521911
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.459]
 [66.459]
 [69.275]
 [72.268]
 [66.459]] [[0.868]
 [0.868]
 [0.917]
 [0.969]
 [0.868]]
printing an ep nov before normalisation:  14.164838342464066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.20683479309082
printing an ep nov before normalisation:  92.72019175062098
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  40.97189914157011
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.868]
 [53.214]
 [47.701]
 [60.385]
 [54.495]] [[0.637]
 [0.492]
 [0.399]
 [0.612]
 [0.513]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.10305565569821
printing an ep nov before normalisation:  55.5693372854741
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9871,     0.0007,     0.0000,     0.0066,     0.0056],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0018, 0.9519, 0.0010, 0.0123, 0.0330], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0055, 0.0037, 0.9756, 0.0086, 0.0065], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0028, 0.0016, 0.1499, 0.6584, 0.1873], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0106, 0.0008, 0.1005, 0.2997, 0.5883], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 2.1320288158198944e-11
0.0 9.751977642416253e-12
0.0 0.0
0.0 2.6207088481292498e-11
0.0 2.5186482386738615e-11
0.0 3.3584859592352925e-11
0.0 2.006615355042299e-11
0.0 1.974613300318776e-11
0.0 2.4909707870807944e-11
0.0 1.14385724502636e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.566864013671875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.78627336
printing an ep nov before normalisation:  38.758201599121094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9702,     0.0050,     0.0014,     0.0001,     0.0233],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0065,     0.9776,     0.0108,     0.0003,     0.0047],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9995,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0064, 0.0110, 0.0028, 0.7985, 0.1813], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0293, 0.0318, 0.0538, 0.2761, 0.6090], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.385]
 [53.385]
 [53.385]
 [58.841]
 [53.385]] [[1.346]
 [1.346]
 [1.346]
 [1.633]
 [1.346]]
printing an ep nov before normalisation:  61.84214796366607
siam score:  -0.7932512
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.47519661965646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.825]
 [56.825]
 [56.825]
 [56.825]
 [40.937]] [[1.16 ]
 [1.16 ]
 [1.16 ]
 [1.16 ]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.952]
 [46.701]
 [46.701]
 [46.701]
 [46.701]] [[0.815]
 [0.586]
 [0.586]
 [0.586]
 [0.586]]
printing an ep nov before normalisation:  52.69758633266063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.04556167782501
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.878451347351074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.715]
 [56.854]
 [66.246]
 [71.204]
 [70.758]] [[1.734]
 [1.302]
 [1.618]
 [1.785]
 [1.77 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.82756284222939
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  90.56423830987588
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.588044833410684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.950928316948286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.68245583845979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.50575080603928
printing an ep nov before normalisation:  44.39831733703613
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0082, 0.9316, 0.0013, 0.0145, 0.0444], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9734,     0.0177,     0.0089],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0003,     0.0109,     0.7087,     0.2792],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0068, 0.0063, 0.0615, 0.3822, 0.5433], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  92.95138037317908
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[91.724]
 [94.937]
 [95.106]
 [98.828]
 [99.068]] [[1.713]
 [1.787]
 [1.79 ]
 [1.876]
 [1.881]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.14152907509913
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.598659613618146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.78015524
printing an ep nov before normalisation:  58.43180063582814
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.335]
 [41.905]
 [54.122]
 [34.525]
 [48.242]] [[0.463]
 [0.811]
 [1.373]
 [0.472]
 [1.103]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.98993650843185
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.194]
 [52.194]
 [52.194]
 [52.194]
 [52.194]] [[1.212]
 [1.212]
 [1.212]
 [1.212]
 [1.212]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.72454440246933
using explorer policy with actor:  1
siam score:  -0.7841421
printing an ep nov before normalisation:  33.79971597815547
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9603,     0.0046,     0.0009,     0.0164,     0.0179],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9197,     0.0763,     0.0006,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0047,     0.9223,     0.0271,     0.0459],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0008, 0.0011, 0.0013, 0.7266, 0.2703], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0100, 0.0026, 0.0694, 0.2984, 0.6196], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.88]
 [57.88]
 [57.88]
 [57.88]
 [57.88]] [[0.978]
 [0.978]
 [0.978]
 [0.978]
 [0.978]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.74732908439612
printing an ep nov before normalisation:  35.49748659133911
printing an ep nov before normalisation:  26.358844489779266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  9.333961318822048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.008694051357086405
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.96726213015315
line 256 mcts: sample exp_bonus 57.42052551692435
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.136]
 [51.02 ]
 [53.941]
 [59.385]
 [57.247]] [[1.608]
 [1.293]
 [1.406]
 [1.618]
 [1.535]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.801848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [54.966]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.182]
 [ 0.241]
 [-0.182]
 [-0.182]
 [-0.182]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.620139982332734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  23.803597236196627
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.7998987
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9919,     0.0001,     0.0001,     0.0078],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0080,     0.8922,     0.0396,     0.0600],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0042,     0.0003,     0.0004,     0.6658,     0.3293],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0655,     0.0014,     0.0002,     0.2588,     0.6742],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.93422031402588
printing an ep nov before normalisation:  51.16664035981324
printing an ep nov before normalisation:  67.1288521419597
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.092036523618034
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.897791520132394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.771228
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.54919293521097
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.597]
 [69.597]
 [69.597]
 [69.597]
 [69.597]] [[0.218]
 [0.218]
 [0.218]
 [0.218]
 [0.218]]
printing an ep nov before normalisation:  76.46410270892082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.4966176786574
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.116]
 [44.116]
 [44.116]
 [44.116]
 [44.116]] [[0.137]
 [0.137]
 [0.137]
 [0.137]
 [0.137]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  76.70688521191602
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.703]
 [51.192]
 [43.969]
 [43.969]
 [39.755]] [[1.151]
 [1.211]
 [0.921]
 [0.921]
 [0.752]]
actions average: 
K:  2  action  0 :  tensor([    0.9626,     0.0201,     0.0005,     0.0011,     0.0157],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0009,     0.9808,     0.0023,     0.0001,     0.0160],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9636,     0.0156,     0.0207],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0026,     0.0006,     0.0082,     0.8181,     0.1705],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0507, 0.0824, 0.0278, 0.1812, 0.6580], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.906]
 [41.906]
 [65.934]
 [41.906]
 [41.906]] [[0.797]
 [0.797]
 [1.667]
 [0.797]
 [0.797]]
printing an ep nov before normalisation:  68.58462472926152
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.258474349975586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  89.4086708425335
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.238]
 [47.303]
 [45.875]
 [47.131]
 [46.553]] [[0.553]
 [0.329]
 [0.31 ]
 [0.327]
 [0.319]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.746]
 [43.874]
 [67.959]
 [81.686]
 [79.815]] [[0.718]
 [0.377]
 [0.589]
 [0.709]
 [0.693]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.53584247895816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  77.63985114430112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.42 ]
 [30.043]
 [24.281]
 [26.034]
 [25.868]] [[0.212]
 [0.102]
 [0.071]
 [0.08 ]
 [0.08 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.665]
 [31.434]
 [41.39 ]
 [51.254]
 [42.31 ]] [[0.759]
 [0.462]
 [0.608]
 [0.753]
 [0.621]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.965]
 [51.767]
 [55.664]
 [71.383]
 [69.237]] [[1.247]
 [0.838]
 [0.936]
 [1.333]
 [1.279]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.252]
 [65.744]
 [60.898]
 [65.744]
 [65.744]] [[1.252]
 [1.206]
 [1.117]
 [1.206]
 [1.206]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7767453
printing an ep nov before normalisation:  48.5976619530269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.12724456255634
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.863]
 [46.863]
 [61.802]
 [46.863]
 [46.863]] [[0.656]
 [0.656]
 [1.031]
 [0.656]
 [0.656]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.163]
 [29.271]
 [43.081]
 [54.302]
 [42.111]] [[0.735]
 [0.437]
 [0.644]
 [0.812]
 [0.63 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.099086195455556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.680506271503425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.198]
 [43.198]
 [43.198]
 [43.198]
 [43.198]] [[1.621]
 [1.621]
 [1.621]
 [1.621]
 [1.621]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
printing an ep nov before normalisation:  44.077150819094435
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.11185042416249
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.619]
 [38.332]
 [33.558]
 [37.244]
 [33.937]] [[0.546]
 [0.297]
 [0.228]
 [0.281]
 [0.233]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.036]
 [41.036]
 [41.036]
 [41.036]
 [41.036]] [[0.878]
 [0.878]
 [0.878]
 [0.878]
 [0.878]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.251]
 [53.826]
 [52.611]
 [62.959]
 [57.559]] [[0.238]
 [0.202]
 [0.192]
 [0.277]
 [0.233]]
printing an ep nov before normalisation:  52.77946503200903
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.025]
 [49.033]
 [49.355]
 [36.525]
 [43.56 ]] [[0.842]
 [1.047]
 [1.058]
 [0.619]
 [0.86 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.70592235244986
printing an ep nov before normalisation:  63.920481803324584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.90758347089429
printing an ep nov before normalisation:  67.6787166349106
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.596]
 [49.596]
 [49.596]
 [49.596]
 [49.596]] [[1.024]
 [1.024]
 [1.024]
 [1.024]
 [1.024]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.553212504748515
printing an ep nov before normalisation:  61.378758804823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.370155232768525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  81.95723612708387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.9172658833344371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.63077163696289
printing an ep nov before normalisation:  66.71055175436285
printing an ep nov before normalisation:  37.78424418357409
printing an ep nov before normalisation:  74.72761338971482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.504568099975586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.04 ]
 [55.138]
 [55.449]
 [55.232]
 [55.449]] [[0.968]
 [0.772]
 [0.778]
 [0.774]
 [0.778]]
siam score:  -0.7895979
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.782036781311035
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.033]
 [39.919]
 [40.473]
 [36.509]
 [35.89 ]] [[0.153]
 [0.191]
 [0.194]
 [0.175]
 [0.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.21160280603163
printing an ep nov before normalisation:  21.58743207590404
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.393]
 [73.393]
 [90.961]
 [73.393]
 [73.393]] [[0.256]
 [0.256]
 [0.333]
 [0.256]
 [0.256]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.96252699124962
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.76651372453483
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  84.17531047283562
printing an ep nov before normalisation:  74.56952095031738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.525603241487595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.41116523742676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  51.3149164833723
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9878,     0.0004,     0.0000,     0.0057,     0.0061],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9936,     0.0003,     0.0001,     0.0059],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9570,     0.0157,     0.0271],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0099,     0.0493,     0.7848,     0.1554],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0039,     0.0002,     0.0356,     0.3425,     0.6179],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.853]
 [34.853]
 [33.387]
 [35.224]
 [34.853]] [[1.866]
 [1.866]
 [1.732]
 [1.899]
 [1.866]]
printing an ep nov before normalisation:  97.46729789201697
printing an ep nov before normalisation:  86.9755488688128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.839]
 [59.839]
 [63.368]
 [59.839]
 [59.839]] [[1.743]
 [1.743]
 [1.846]
 [1.743]
 [1.743]]
actions average: 
K:  1  action  0 :  tensor([    0.9884,     0.0018,     0.0000,     0.0018,     0.0081],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0312, 0.9510, 0.0011, 0.0057, 0.0111], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0235,     0.0007,     0.9390,     0.0265,     0.0103],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0017,     0.0001,     0.0028,     0.6689,     0.3265],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0048, 0.0194, 0.0402, 0.1616, 0.7740], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  89.16500170405986
printing an ep nov before normalisation:  81.23549247842519
siam score:  -0.8009355
printing an ep nov before normalisation:  43.86693180093822
printing an ep nov before normalisation:  59.12521300047228
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.932944024181396
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 26.659246571881127
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.58840613433277
printing an ep nov before normalisation:  48.065066928975874
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.43366977483846
printing an ep nov before normalisation:  59.09831080464288
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.7801570892334
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.457888086971344
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.502]
 [59.792]
 [59.792]
 [59.792]
 [59.792]] [[1.35 ]
 [1.114]
 [1.114]
 [1.114]
 [1.114]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.50497531890869
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.01778933494388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.005]
 [63.486]
 [65.329]
 [65.457]
 [61.795]] [[0.441]
 [0.479]
 [0.508]
 [0.51 ]
 [0.453]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  49.88478476472072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.678]
 [42.678]
 [42.678]
 [42.678]
 [42.678]] [[0.985]
 [0.985]
 [0.985]
 [0.985]
 [0.985]]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9931,     0.0000,     0.0000,     0.0025,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9925,     0.0012,     0.0000,     0.0059],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0015,     0.9704,     0.0088,     0.0193],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0207,     0.0001,     0.0006,     0.7570,     0.2216],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0451, 0.0092, 0.0332, 0.2749, 0.6376], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.286231713668315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.67513302227821
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.769 0.051 0.128 0.026 0.026]
siam score:  -0.79692686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.154]
 [57.154]
 [57.154]
 [57.154]
 [57.154]] [[1.315]
 [1.315]
 [1.315]
 [1.315]
 [1.315]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  2.5239989913657723
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.26397346923939
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  75.50116829055783
siam score:  -0.79732543
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.434]
 [55.754]
 [46.765]
 [46.765]
 [48.743]] [[0.682]
 [0.542]
 [0.397]
 [0.397]
 [0.429]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.986]
 [63.044]
 [52.465]
 [64.415]
 [66.313]] [[0.899]
 [0.872]
 [0.726]
 [0.891]
 [0.917]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.35085821701891
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.92333173612327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 38.16495878419013
siam score:  -0.7851134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.53957153646678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.749]
 [49.749]
 [55.574]
 [49.749]
 [49.749]] [[0.834]
 [0.834]
 [1.   ]
 [0.834]
 [0.834]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9987,     0.0001,     0.0000,     0.0009,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9971,     0.0004,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0082, 0.0036, 0.9448, 0.0196, 0.0239], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0131,     0.0006,     0.0441,     0.6717,     0.2705],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0012,     0.0003,     0.0030,     0.2786,     0.7168],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.4821699856156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.588792837541554
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.41 ]
 [49.004]
 [33.447]
 [49.006]
 [45.674]] [[0.619]
 [0.908]
 [0.62 ]
 [0.908]
 [0.847]]
printing an ep nov before normalisation:  44.670780935957474
siam score:  -0.7723862
siam score:  -0.77386016
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  89.29899024154052
printing an ep nov before normalisation:  44.673396154524426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.96609382164253
siam score:  -0.7830189
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 34.52257081866264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7952831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9936,     0.0002,     0.0000,     0.0026,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0036, 0.9670, 0.0076, 0.0020, 0.0198], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0043, 0.0316, 0.8821, 0.0157, 0.0664], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0031,     0.0003,     0.0744,     0.6190,     0.3032],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0276, 0.0007, 0.0550, 0.2143, 0.7023], grad_fn=<DivBackward0>)
siam score:  -0.8142453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.58282178806447
printing an ep nov before normalisation:  44.3956713776261
siam score:  -0.816925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.57532262802124
printing an ep nov before normalisation:  42.792566548475975
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.104]
 [31.475]
 [40.24 ]
 [29.918]
 [27.719]] [[1.143]
 [0.611]
 [0.981]
 [0.545]
 [0.453]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.657]
 [46.378]
 [46.378]
 [46.378]
 [46.378]] [[0.718]
 [0.488]
 [0.488]
 [0.488]
 [0.488]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.65440098444621
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([0.9722, 0.0012, 0.0011, 0.0085, 0.0171], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0048,     0.9779,     0.0064,     0.0005,     0.0104],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9988,     0.0004,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0075,     0.0006,     0.0765,     0.6385,     0.2769],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0095,     0.0005,     0.0754,     0.2422,     0.6724],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.230080594259434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.24308669567108
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.289]
 [41.304]
 [55.321]
 [41.511]
 [38.965]] [[0.961]
 [0.716]
 [1.208]
 [0.723]
 [0.634]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.15169141643728
printing an ep nov before normalisation:  51.849354049859855
printing an ep nov before normalisation:  61.889200519680124
printing an ep nov before normalisation:  68.28936182745039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.57485180506455
printing an ep nov before normalisation:  92.21396279401692
printing an ep nov before normalisation:  74.59184449914402
printing an ep nov before normalisation:  26.566050730807547
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.801]
 [45.801]
 [58.877]
 [45.801]
 [45.801]] [[0.226]
 [0.226]
 [0.333]
 [0.226]
 [0.226]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.60506378655737
printing an ep nov before normalisation:  47.852919172270596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.9991688257375131
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.49360972665657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.8457787098121
printing an ep nov before normalisation:  52.48758516645925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  105.92068223452443
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.79778075
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.562163639649086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.145]
 [40.009]
 [61.74 ]
 [40.009]
 [40.009]] [[0.66 ]
 [0.686]
 [1.346]
 [0.686]
 [0.686]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.67947864532471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.05642161940168
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.35 ]
 [44.335]
 [44.335]
 [44.335]
 [44.335]] [[1.229]
 [0.872]
 [0.872]
 [0.872]
 [0.872]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 65.92848535195803
UNIT TEST: sample policy line 217 mcts : [0.128 0.256 0.103 0.128 0.385]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  72.90714556072653
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.40828353467352
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.113]
 [69.682]
 [60.442]
 [60.442]
 [60.442]] [[1.488]
 [1.806]
 [1.499]
 [1.499]
 [1.499]]
using explorer policy with actor:  1
siam score:  -0.7787874
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.008704207856169432
printing an ep nov before normalisation:  51.661773663644816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.244]
 [49.956]
 [39.525]
 [48.447]
 [48.1  ]] [[0.261]
 [0.231]
 [0.135]
 [0.217]
 [0.214]]
siam score:  -0.7920394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.48750972747803
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  1.0532534464431365
printing an ep nov before normalisation:  64.40467460610894
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.17658997388956
printing an ep nov before normalisation:  38.560295231952466
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.180899000427765
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  72.84751113709807
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81001145
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.459763526916504
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.807]
 [37.157]
 [44.636]
 [33.66 ]
 [34.258]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  49.43039552918153
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  14.770900727773132
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.90959914846863
siam score:  -0.8055181
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.715526768869886
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.130008697509766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.588693551058235
printing an ep nov before normalisation:  96.87264914719529
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.178]
 [63.178]
 [63.178]
 [63.178]
 [63.178]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
line 256 mcts: sample exp_bonus 58.07393637229224
printing an ep nov before normalisation:  82.98985200764268
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.10447691064168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.140593882455036
printing an ep nov before normalisation:  39.91236985739005
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.98497779907748
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.5201530456543
printing an ep nov before normalisation:  63.220335018163965
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.166]
 [59.707]
 [65.213]
 [65.3  ]
 [59.778]] [[0.88 ]
 [1.016]
 [1.151]
 [1.154]
 [1.018]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.69890773673797
using explorer policy with actor:  1
printing an ep nov before normalisation:  89.49342708770888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.80725473
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.314]
 [38.644]
 [49.339]
 [36.318]
 [37.852]] [[0.819]
 [0.561]
 [0.846]
 [0.5  ]
 [0.54 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.227]
 [47.244]
 [47.244]
 [47.244]
 [47.244]] [[1.148]
 [0.766]
 [0.766]
 [0.766]
 [0.766]]
printing an ep nov before normalisation:  70.03905468403778
printing an ep nov before normalisation:  48.991237973376926
printing an ep nov before normalisation:  58.91352623339544
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0495,     0.9483,     0.0000,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9687,     0.0166,     0.0142],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0156,     0.0005,     0.0206,     0.8043,     0.1589],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0234, 0.0079, 0.0728, 0.3583, 0.5376], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.38108615204688
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.77 ]
 [17.486]
 [26.376]
 [27.191]
 [50.232]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  51.87907059212665
printing an ep nov before normalisation:  68.01127187045114
line 256 mcts: sample exp_bonus 71.32100762998284
printing an ep nov before normalisation:  77.33136207746203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.87402721796238
printing an ep nov before normalisation:  64.38069495725712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.20086228191169
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.415]
 [58.335]
 [71.513]
 [58.335]
 [58.335]] [[1.4  ]
 [1.509]
 [2.   ]
 [1.509]
 [1.509]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.694]
 [46.223]
 [39.839]
 [45.128]
 [44.341]] [[1.067]
 [0.686]
 [0.505]
 [0.655]
 [0.633]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.96 ]
 [64.644]
 [58.035]
 [67.944]
 [68.96 ]] [[1.667]
 [1.468]
 [1.164]
 [1.62 ]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8231018
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.372]
 [40.699]
 [56.355]
 [38.478]
 [38.478]] [[0.539]
 [0.572]
 [0.972]
 [0.516]
 [0.516]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.044]
 [56.044]
 [50.034]
 [56.044]
 [56.044]] [[1.537]
 [1.537]
 [1.304]
 [1.537]
 [1.537]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.13752968907926
printing an ep nov before normalisation:  52.17643838919652
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.00522436205389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.37288983540045
printing an ep nov before normalisation:  76.81751783784864
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.067]
 [46.447]
 [46.447]
 [46.447]
 [46.447]] [[1.749]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.434368832073645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.61034935492282
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.376257162662505
line 256 mcts: sample exp_bonus 96.34215037288232
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  79.3754257507537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.59471512933493
printing an ep nov before normalisation:  35.720395675350154
siam score:  -0.8080041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.61906306139819
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.9663368503136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.083001416895016
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.631542996377576
printing an ep nov before normalisation:  25.531327959041732
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.222388449815426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.003]
 [73.003]
 [62.676]
 [73.003]
 [73.003]] [[1.628]
 [1.628]
 [1.301]
 [1.628]
 [1.628]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.4419750317984
printing an ep nov before normalisation:  55.99112856085141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.019801664639089722
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.91104784059073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.60530364201429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8200917
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.43668142534683
siam score:  -0.8210049
printing an ep nov before normalisation:  48.49248050376627
printing an ep nov before normalisation:  45.89763376601643
printing an ep nov before normalisation:  63.73232097257096
deleting a thread, now have 2 threads
Frames:  32534 train batches done:  3810 episodes:  1690
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.335]
 [26.335]
 [56.831]
 [26.335]
 [26.335]] [[0.244]
 [0.244]
 [0.782]
 [0.244]
 [0.244]]
printing an ep nov before normalisation:  58.03004539591764
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9983,     0.0001,     0.0000,     0.0005,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0319, 0.9279, 0.0016, 0.0139, 0.0247], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9748,     0.0025,     0.0225],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0070,     0.0001,     0.0205,     0.7315,     0.2409],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1373, 0.0430, 0.0013, 0.3351, 0.4832], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.483533655073124
actions average: 
K:  1  action  0 :  tensor([    0.9984,     0.0000,     0.0000,     0.0006,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9984,     0.0001,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0027,     0.9689,     0.0107,     0.0174],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0007,     0.0002,     0.0076,     0.7272,     0.2643],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0006,     0.0008,     0.0451,     0.2431,     0.7104],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.38801253720632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 1 threads
Frames:  32757 train batches done:  3844 episodes:  1697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.312456202908606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.233562318669364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.11039656772185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.41219139099121
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.16555164700891
printing an ep nov before normalisation:  86.72699620472861
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8166085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.281535208463566
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.45152521133423
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.43597168414862
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.00941624319215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.96451055127997
printing an ep nov before normalisation:  33.985975661306036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81522965
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.34055174172702
printing an ep nov before normalisation:  32.20518685558391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.44196793368406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  76.40646989018079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.473]
 [56.473]
 [56.473]
 [56.473]
 [56.473]] [[0.722]
 [0.722]
 [0.722]
 [0.722]
 [0.722]]
printing an ep nov before normalisation:  1.114311190244166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.08946466412947
printing an ep nov before normalisation:  31.33571001066983
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.05 ]
 [47.625]
 [47.625]
 [72.565]
 [47.625]] [[0.743]
 [0.4  ]
 [0.4  ]
 [0.799]
 [0.4  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.247]
 [52.247]
 [52.247]
 [52.247]
 [52.247]] [[52.247]
 [52.247]
 [52.247]
 [52.247]
 [52.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81782144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 79.84003314369942
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8131837
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.40555286407471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.240262031555176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.769]
 [60.298]
 [59.758]
 [60.298]
 [60.298]] [[1.481]
 [1.184]
 [1.169]
 [1.184]
 [1.184]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  69.69067707276668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.32935333074117
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.40401186528682
using explorer policy with actor:  1
siam score:  -0.8145636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8159251
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.17447790776622
siam score:  -0.8171027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.02806880249864463
printing an ep nov before normalisation:  39.3965739846341
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.548]
 [77.548]
 [77.548]
 [77.548]
 [77.548]] [[1.002]
 [1.002]
 [1.002]
 [1.002]
 [1.002]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.136]
 [56.238]
 [31.625]
 [43.033]
 [56.845]] [[0.916]
 [0.77 ]
 [0.25 ]
 [0.491]
 [0.783]]
printing an ep nov before normalisation:  77.5556238206292
printing an ep nov before normalisation:  56.063428735681335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.60660729767523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8222926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.796528816223145
siam score:  -0.82018477
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.303]
 [74.049]
 [76.084]
 [76.084]
 [76.084]] [[1.836]
 [1.719]
 [1.792]
 [1.792]
 [1.792]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.72734020067135
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8168113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81881166
siam score:  -0.8178243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  85.24067198402034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  89.09320719689278
printing an ep nov before normalisation:  26.60435199737549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0635,     0.8848,     0.0001,     0.0004,     0.0512],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0007,     0.9443,     0.0209,     0.0340],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0014, 0.0012, 0.0271, 0.7159, 0.2544], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0784, 0.0013, 0.0554, 0.2547, 0.6101], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.05910930000397
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.0534237806543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  76.40330015034455
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  73.17224227878678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  73.34038794345086
printing an ep nov before normalisation:  69.98231940251694
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8148583
printing an ep nov before normalisation:  40.42048430670247
siam score:  -0.81671447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.67 ]
 [51.342]
 [51.342]
 [51.342]
 [51.342]] [[1.131]
 [0.757]
 [0.757]
 [0.757]
 [0.757]]
actions average: 
K:  0  action  0 :  tensor([    0.9946,     0.0004,     0.0014,     0.0021,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9986,     0.0001,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0056,     0.9078,     0.0381,     0.0483],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0016, 0.0020, 0.0146, 0.7200, 0.2618], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0026, 0.0312, 0.0460, 0.3094, 0.6108], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.842]
 [50.507]
 [50.507]
 [50.507]
 [50.507]] [[1.402]
 [0.962]
 [0.962]
 [0.962]
 [0.962]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.657999983198756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.7839298248291
actions average: 
K:  3  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9897,     0.0002,     0.0001,     0.0095],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9644,     0.0199,     0.0156],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0010,     0.0004,     0.0242,     0.6889,     0.2855],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0019,     0.0004,     0.0012,     0.3330,     0.6635],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.9751193424649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.003555947711220142
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.48898864134745
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.99546010426557
siam score:  -0.8063829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([0.9359, 0.0091, 0.0045, 0.0270, 0.0236], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0046,     0.9891,     0.0019,     0.0000,     0.0044],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0069, 0.0777, 0.8903, 0.0175, 0.0075], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0087,     0.0004,     0.0436,     0.7257,     0.2217],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1216, 0.1995, 0.0008, 0.1825, 0.4956], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.48539626776941
using explorer policy with actor:  1
printing an ep nov before normalisation:  80.9563108075524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.89983452749635
siam score:  -0.8118458
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  84.27361386526245
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: False
Self play flag: True
add more workers flag:  True
expV_train_flag:  True
expV_train_start_flag:  20036
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  50.68795378598
printing an ep nov before normalisation:  49.29589839486189
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.709]
 [45.182]
 [49.709]
 [63.469]
 [49.709]] [[0.803]
 [0.643]
 [0.803]
 [1.288]
 [0.803]]
printing an ep nov before normalisation:  75.02760023820386
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9978,     0.0000,     0.0000,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9292,     0.0059,     0.0129,     0.0514],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0019,     0.9733,     0.0099,     0.0149],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0017,     0.0005,     0.0222,     0.7616,     0.2140],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0068, 0.0052, 0.0698, 0.3282, 0.5900], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.22198837291027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.74230689348431
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.309]
 [36.025]
 [50.532]
 [41.309]
 [39.343]] [[0.651]
 [0.483]
 [0.944]
 [0.651]
 [0.589]]
printing an ep nov before normalisation:  51.69214133760697
printing an ep nov before normalisation:  87.02677777564412
printing an ep nov before normalisation:  77.26888236937509
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  87.46487863281477
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.667 0.026 0.256 0.026 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.313859939575195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8341541
printing an ep nov before normalisation:  73.5426747136761
printing an ep nov before normalisation:  82.03412284661555
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[103.975]
 [ 99.796]
 [101.888]
 [ 99.7  ]
 [ 99.7  ]] [[0.993]
 [0.941]
 [0.967]
 [0.94 ]
 [0.94 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  95.4762895776083
printing an ep nov before normalisation:  48.66943309433315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.939]
 [58.24 ]
 [68.787]
 [43.887]
 [43.887]] [[0.371]
 [0.616]
 [0.765]
 [0.413]
 [0.413]]
line 256 mcts: sample exp_bonus 50.73213759856637
printing an ep nov before normalisation:  74.38251587071906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.69973119226094
siam score:  -0.82267195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  74.42609856424188
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.436]
 [26.987]
 [40.603]
 [37.59 ]
 [37.845]] [[0.76 ]
 [0.377]
 [0.567]
 [0.525]
 [0.528]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  77.27744904448144
printing an ep nov before normalisation:  77.28101045008258
siam score:  -0.82728404
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  80.69763944680378
printing an ep nov before normalisation:  0.0058977609194244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.71475707979997
printing an ep nov before normalisation:  55.505475997924805
printing an ep nov before normalisation:  63.56683895687138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  81.80051503447392
printing an ep nov before normalisation:  83.09551895599391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  77.46878347987885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8225914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.97309707762207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.00031893725633835857
printing an ep nov before normalisation:  64.72945205708844
printing an ep nov before normalisation:  88.41007536710818
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.63076686859131
actions average: 
K:  0  action  0 :  tensor([    0.9985,     0.0007,     0.0000,     0.0005,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9946,     0.0000,     0.0000,     0.0048],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0119, 0.0102, 0.9464, 0.0174, 0.0141], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0096,     0.0002,     0.0007,     0.6697,     0.3198],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0333,     0.0151,     0.0004,     0.2683,     0.6830],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.418]
 [27.676]
 [49.32 ]
 [29.892]
 [30.504]] [[0.67 ]
 [0.478]
 [1.355]
 [0.568]
 [0.592]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.86340306859465
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.825322
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.71508354367479
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.285747082469385
printing an ep nov before normalisation:  57.114798949647785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.16479484914012
printing an ep nov before normalisation:  49.25394535064697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.32278722883025
printing an ep nov before normalisation:  54.682996555119736
printing an ep nov before normalisation:  60.49187750607332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.98283152320126
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.986]
 [58.987]
 [41.715]
 [64.508]
 [62.927]] [[1.395]
 [1.583]
 [1.119]
 [1.731]
 [1.689]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.758]
 [71.678]
 [70.4  ]
 [72.089]
 [70.2  ]] [[1.601]
 [1.804]
 [1.76 ]
 [1.818]
 [1.753]]
printing an ep nov before normalisation:  48.37150573730469
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  86.21892227880096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.243]
 [44.317]
 [44.317]
 [55.875]
 [44.317]] [[1.572]
 [1.193]
 [1.193]
 [1.508]
 [1.193]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.701149779083039
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.136]
 [58.136]
 [58.136]
 [58.136]
 [58.136]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.34727357686636
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.768]
 [48.768]
 [48.768]
 [50.926]
 [48.768]] [[1.189]
 [1.189]
 [1.189]
 [1.269]
 [1.189]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.596]
 [41.587]
 [53.133]
 [41.587]
 [41.28 ]] [[0.627]
 [0.597]
 [0.943]
 [0.597]
 [0.588]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.235361099243164
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9934,     0.0000,     0.0000,     0.0043,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9910,     0.0001,     0.0000,     0.0085],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9693,     0.0173,     0.0133],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0012,     0.0004,     0.0280,     0.8347,     0.1357],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0078, 0.0062, 0.0389, 0.2133, 0.7338], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.775560781988474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  79.0006155672529
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.055]
 [71.18 ]
 [60.733]
 [76.953]
 [75.3  ]] [[1.301]
 [1.202]
 [1.025]
 [1.299]
 [1.271]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.06856190580368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.83094496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0082,     0.9915,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0037,     0.0007,     0.0004,     0.7210,     0.2742],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0008,     0.0009,     0.0006,     0.2939,     0.7037],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.07636816709615
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  69.37923559762072
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.30777931213379
printing an ep nov before normalisation:  69.30927806431433
printing an ep nov before normalisation:  28.475055537581806
actions average: 
K:  0  action  0 :  tensor([    0.9953,     0.0001,     0.0000,     0.0009,     0.0037],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9978,     0.0007,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9781,     0.0129,     0.0089],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0004,     0.0012,     0.7572,     0.2402],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0007,     0.0005,     0.0054,     0.2781,     0.7153],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.750451526765616
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [60.427]
 [56.177]] [[-0.47 ]
 [-0.47 ]
 [-0.47 ]
 [ 0.634]
 [ 0.556]]
printing an ep nov before normalisation:  0.1328944877406002
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.11 ]
 [48.608]
 [40.218]
 [45.527]
 [43.341]] [[0.777]
 [0.701]
 [0.518]
 [0.633]
 [0.586]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.294]
 [44.949]
 [44.949]
 [44.949]
 [44.949]] [[0.732]
 [0.302]
 [0.302]
 [0.302]
 [0.302]]
printing an ep nov before normalisation:  43.471598625183105
printing an ep nov before normalisation:  54.06659973992242
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.44599890485197
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 53.170338177693544
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.213]
 [62.264]
 [52.028]
 [64.328]
 [64.328]] [[1.667]
 [1.564]
 [1.207]
 [1.636]
 [1.636]]
printing an ep nov before normalisation:  40.8716851165039
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.096]
 [45.725]
 [45.725]
 [36.411]
 [45.725]] [[1.667]
 [2.113]
 [2.113]
 [1.375]
 [2.113]]
printing an ep nov before normalisation:  64.95298994226344
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.802]
 [30.802]
 [32.244]
 [30.802]
 [30.802]] [[0.265]
 [0.265]
 [0.287]
 [0.265]
 [0.265]]
printing an ep nov before normalisation:  57.58544996447567
printing an ep nov before normalisation:  47.73457732985218
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.777163948917185
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[90.56]
 [90.56]
 [90.56]
 [90.56]
 [90.56]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
printing an ep nov before normalisation:  64.64870161769917
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0025949417744186576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.82996213
printing an ep nov before normalisation:  65.6555253339035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9998,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0010,     0.9727,     0.0113,     0.0149],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0012, 0.0012, 0.0206, 0.7749, 0.2021], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0019, 0.0655, 0.0998, 0.2608, 0.5720], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.9724,     0.0001,     0.0000,     0.0181,     0.0095],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9202,     0.0003,     0.0001,     0.0793],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9353,     0.0371,     0.0274],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0005,     0.0227,     0.7123,     0.2639],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0001,     0.0016,     0.0428,     0.2876,     0.6679],
       grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.9979,     0.0018,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9804,     0.0001,     0.0000,     0.0194],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0016,     0.9781,     0.0090,     0.0112],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0013, 0.0173, 0.0309, 0.8376, 0.1129], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0131, 0.2140, 0.0016, 0.2618, 0.5095], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.70255678535543
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.375]
 [47.375]
 [58.056]
 [47.375]
 [47.375]] [[0.677]
 [0.677]
 [0.951]
 [0.677]
 [0.677]]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9764,     0.0109,     0.0008,     0.0010,     0.0109],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9928,     0.0002,     0.0000,     0.0070],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9295,     0.0493,     0.0210],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0133, 0.0018, 0.0007, 0.6521, 0.3321], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0219, 0.0136, 0.0514, 0.3080, 0.6050], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.93092546076986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.89233440982787
printing an ep nov before normalisation:  40.910665398278375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8325406
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.96110919739064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.94859067785963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [63.307]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.678]
 [ 0.817]
 [-0.678]
 [-0.678]
 [-0.678]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.492277304331466
printing an ep nov before normalisation:  56.52208680125913
printing an ep nov before normalisation:  41.544710395107266
using explorer policy with actor:  1
siam score:  -0.8167766
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  19.54172089891557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.51947810530505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.81271
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.962]
 [56.416]
 [55.598]
 [54.832]
 [56.366]] [[0.285]
 [0.298]
 [0.294]
 [0.29 ]
 [0.298]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  16.32171252467333
printing an ep nov before normalisation:  21.575634563844435
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.373917072391585
printing an ep nov before normalisation:  58.749495022649434
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.16117600102403
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.36949348449707
siam score:  -0.82438093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.73]
 [80.73]
 [80.73]
 [80.73]
 [80.73]] [[1.189]
 [1.189]
 [1.189]
 [1.189]
 [1.189]]
printing an ep nov before normalisation:  56.52238143069256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  99.86674466785847
printing an ep nov before normalisation:  81.33904336819772
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9695,     0.0005,     0.0002,     0.0189,     0.0109],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0194,     0.9633,     0.0004,     0.0000,     0.0169],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0003,     0.9296,     0.0457,     0.0242],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0085, 0.0244, 0.0465, 0.6560, 0.2646], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0687,     0.0037,     0.0004,     0.2093,     0.7179],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.462 0.103 0.205 0.103 0.128]
printing an ep nov before normalisation:  62.51843539250811
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0008971007275704325
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.602]
 [69.682]
 [98.559]
 [76.21 ]
 [76.21 ]] [[0.272]
 [0.302]
 [0.587]
 [0.367]
 [0.367]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  79.66435498283349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8323917
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  42.28771292774628
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.560228792585722
printing an ep nov before normalisation:  56.225629684833166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.01499843597412
printing an ep nov before normalisation:  68.14716400639512
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.44080965198579
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.40240812255632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.983]
 [67.678]
 [67.678]
 [67.678]
 [67.678]] [[1.752]
 [1.74 ]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.8  ]
 [52.222]
 [63.671]
 [46.045]
 [44.604]] [[0.222]
 [0.552]
 [0.713]
 [0.465]
 [0.444]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.25710868835449
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.862]
 [53.862]
 [70.314]
 [75.772]
 [53.862]] [[0.513]
 [0.513]
 [0.75 ]
 [0.828]
 [0.513]]
printing an ep nov before normalisation:  39.10884187556528
printing an ep nov before normalisation:  44.006026314497134
printing an ep nov before normalisation:  40.75244426727295
printing an ep nov before normalisation:  35.66411256790161
printing an ep nov before normalisation:  70.4031999930725
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.665267434492954
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.826]
 [40.126]
 [56.993]
 [38.138]
 [40.126]] [[0.654]
 [0.516]
 [1.011]
 [0.457]
 [0.516]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.90132058308151
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.71186837597779
printing an ep nov before normalisation:  68.73636081868686
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.81630893206236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.90105472971621
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.25394211428431
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.08634488593117
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  57.77329867375168
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.92127503602312
printing an ep nov before normalisation:  64.8494762805819
siam score:  -0.8277463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.84552957310231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.448]
 [48.494]
 [48.883]
 [59.993]
 [56.408]] [[1.474]
 [1.044]
 [1.059]
 [1.495]
 [1.355]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.56977653503418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.10041339020517626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.84975242614746
siam score:  -0.83486825
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.355]
 [39.39 ]
 [63.729]
 [56.491]
 [43.695]] [[0.475]
 [0.411]
 [0.932]
 [0.777]
 [0.503]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.023]
 [56.023]
 [56.023]
 [60.672]
 [56.023]] [[1.195]
 [1.195]
 [1.195]
 [1.356]
 [1.195]]
siam score:  -0.8366143
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.053]
 [62.62 ]
 [68.045]
 [72.223]
 [62.62 ]] [[1.118]
 [1.187]
 [1.332]
 [1.444]
 [1.187]]
printing an ep nov before normalisation:  56.224248200359526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.524413878538944
printing an ep nov before normalisation:  52.48250922603416
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.029]
 [40.029]
 [40.029]
 [40.029]
 [40.029]] [[0.204]
 [0.204]
 [0.204]
 [0.204]
 [0.204]]
siam score:  -0.83315694
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.515]
 [20.027]
 [17.515]
 [17.515]
 [17.515]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.094]
 [48.556]
 [65.163]
 [60.471]
 [57.116]] [[1.058]
 [0.638]
 [1.087]
 [0.96 ]
 [0.87 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.708]
 [46.425]
 [52.495]
 [45.708]
 [45.708]] [[1.155]
 [1.189]
 [1.474]
 [1.155]
 [1.155]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.76105711894795
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.625]
 [54.909]
 [53.828]
 [55.074]
 [53.625]] [[0.774]
 [0.807]
 [0.779]
 [0.811]
 [0.774]]
actions average: 
K:  0  action  0 :  tensor([    0.9621,     0.0001,     0.0000,     0.0189,     0.0189],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9995,     0.0001,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9772,     0.0154,     0.0074],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0107, 0.0031, 0.0388, 0.7815, 0.1659], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0178, 0.0134, 0.1026, 0.3419, 0.5244], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.664980449157945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.085185050964355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  69.50493917454268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.78756523132324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8291137
siam score:  -0.828609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.05553678932883
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.244]
 [36.339]
 [36.339]
 [36.339]
 [36.339]] [[1.48 ]
 [0.892]
 [0.892]
 [0.892]
 [0.892]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9873,     0.0101,     0.0000,     0.0001,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9992,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9471,     0.0087,     0.0438],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0076, 0.0019, 0.0041, 0.6867, 0.2997], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0024,     0.0007,     0.0246,     0.1779,     0.7945],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.6791574330049
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.4221483042977
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9672,     0.0065,     0.0000,     0.0116,     0.0147],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0129,     0.9550,     0.0001,     0.0000,     0.0319],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0006,     0.9641,     0.0215,     0.0134],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0085,     0.0003,     0.0005,     0.7396,     0.2512],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0138,     0.0006,     0.0438,     0.3191,     0.6226],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.235922619352124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  75.75781890401191
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.0575089457821
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.44]
 [65.32]
 [65.32]
 [65.32]
 [65.32]] [[0.985]
 [0.657]
 [0.657]
 [0.657]
 [0.657]]
printing an ep nov before normalisation:  74.85804572286112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.027]
 [52.41 ]
 [59.957]
 [53.068]
 [58.089]] [[0.845]
 [0.944]
 [1.164]
 [0.963]
 [1.109]]
printing an ep nov before normalisation:  40.2940559387207
actions average: 
K:  1  action  0 :  tensor([    0.9953,     0.0002,     0.0000,     0.0005,     0.0039],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9988,     0.0010,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0003,     0.9588,     0.0143,     0.0266],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0005,     0.0498,     0.7201,     0.2286],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0333, 0.0080, 0.0441, 0.2973, 0.6174], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.851]
 [71.851]
 [71.851]
 [71.851]
 [71.851]] [[1.384]
 [1.384]
 [1.384]
 [1.384]
 [1.384]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.399]
 [49.399]
 [44.49 ]
 [50.12 ]
 [49.399]] [[1.18 ]
 [1.18 ]
 [0.964]
 [1.212]
 [1.18 ]]
siam score:  -0.8320335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.871]
 [48.871]
 [60.518]
 [48.871]
 [48.871]] [[1.15 ]
 [1.15 ]
 [1.667]
 [1.15 ]
 [1.15 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.744]
 [67.717]
 [61.076]
 [63.526]
 [63.526]] [[1.848]
 [1.886]
 [1.624]
 [1.721]
 [1.721]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.31546227352417
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  86.06403286722146
printing an ep nov before normalisation:  84.28269468013697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.697235716351294
printing an ep nov before normalisation:  42.74940485312991
printing an ep nov before normalisation:  31.353760379773647
printing an ep nov before normalisation:  6.140427331047249e-06
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 42.58610097356292
printing an ep nov before normalisation:  41.123858756649156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  91.81822286697123
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.93000080832927
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.83470327
printing an ep nov before normalisation:  11.753277560119813
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.23842408147753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.69479744389595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 2 threads
Frames:  45394 train batches done:  5317 episodes:  2174
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  67.85174804109099
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.06778469432757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.25268520846042
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.96976635682801
siam score:  -0.82573456
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.266]
 [82.266]
 [82.266]
 [82.266]
 [82.266]] [[1.96]
 [1.96]
 [1.96]
 [1.96]
 [1.96]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.489]
 [38.489]
 [38.131]
 [38.489]
 [38.489]] [[1.89 ]
 [1.89 ]
 [1.855]
 [1.89 ]
 [1.89 ]]
line 256 mcts: sample exp_bonus 45.185274936347994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.11949399962342
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.46185312482154
siam score:  -0.8329924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.0060413225888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.66786041554685
siam score:  -0.8333426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.77055396619613
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.033788941993861954
siam score:  -0.8433796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  73.71102355242962
siam score:  -0.8447072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.1550407409668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.93123902376051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.241]
 [78.241]
 [78.965]
 [78.241]
 [78.241]] [[1.896]
 [1.896]
 [1.921]
 [1.896]
 [1.896]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.71952262366655
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.922]
 [57.922]
 [57.922]
 [57.922]
 [57.922]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
printing an ep nov before normalisation:  56.23448920936735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.192718616476235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.13883006839413
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.85 ]
 [58.85 ]
 [58.899]
 [62.721]
 [61.317]] [[1.383]
 [1.383]
 [1.386]
 [1.566]
 [1.5  ]]
printing an ep nov before normalisation:  56.314157231943724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.006]
 [61.176]
 [65.936]
 [75.156]
 [64.944]] [[0.522]
 [0.463]
 [0.562]
 [0.753]
 [0.541]]
actions average: 
K:  2  action  0 :  tensor([0.9592, 0.0025, 0.0081, 0.0214, 0.0088], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0014, 0.9813, 0.0047, 0.0027, 0.0099], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9728,     0.0169,     0.0103],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0001,     0.0249,     0.7808,     0.1934],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0082, 0.0610, 0.0867, 0.2871, 0.5570], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9909,     0.0079,     0.0000,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9918,     0.0029,     0.0000,     0.0048],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9740,     0.0141,     0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0151,     0.0002,     0.0157,     0.7178,     0.2511],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0161, 0.0029, 0.0242, 0.2418, 0.7150], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.37110528113766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.5161657333374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9847,     0.0100,     0.0000,     0.0002,     0.0051],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0016,     0.9883,     0.0030,     0.0000,     0.0071],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9743,     0.0009,     0.0245],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0005,     0.0003,     0.7675,     0.2312],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0014, 0.0863, 0.2412, 0.6705], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.15871815749349
printing an ep nov before normalisation:  25.93251943588257
printing an ep nov before normalisation:  49.59780171294537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.05 ]
 [38.815]
 [42.182]
 [34.826]
 [33.825]] [[0.406]
 [0.574]
 [0.657]
 [0.474]
 [0.45 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.04648043295173
printing an ep nov before normalisation:  33.40141359132212
actions average: 
K:  1  action  0 :  tensor([    0.9605,     0.0162,     0.0000,     0.0006,     0.0227],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0419,     0.9214,     0.0006,     0.0017,     0.0344],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9719,     0.0179,     0.0100],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0348,     0.0007,     0.0004,     0.7526,     0.2114],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0857, 0.0013, 0.0362, 0.4081, 0.4686], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.91334819793701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
deleting a thread, now have 1 threads
Frames:  46844 train batches done:  5487 episodes:  2220
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8441133
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -1.2281869845830005e-11
0.0 0.0
0.0 -1.370898851861157e-11
0.0 0.0
0.0 -1.1667776354877681e-11
0.0 0.0
0.0 -1.991911708974103e-11
0.0 -2.362962564239656e-11
0.0 -1.9884520269208294e-11
0.0 -1.4279835997390436e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.640217290398155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.839267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.024]
 [59.303]
 [55.866]
 [57.27 ]
 [57.27 ]] [[1.416]
 [1.293]
 [1.18 ]
 [1.226]
 [1.226]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 62.9641859418875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  1.4074406635700143e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.02 ]
 [52.517]
 [35.833]
 [50.431]
 [53.638]] [[0.3  ]
 [0.206]
 [0.111]
 [0.194]
 [0.213]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8498859
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.31699819546279
actions average: 
K:  3  action  0 :  tensor([    0.9196,     0.0281,     0.0003,     0.0089,     0.0431],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9899,     0.0089,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9850,     0.0085,     0.0065],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0080, 0.0074, 0.0696, 0.7945, 0.1205], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0043, 0.0013, 0.0521, 0.3017, 0.6406], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.327]
 [57.327]
 [57.327]
 [57.327]
 [57.327]] [[0.719]
 [0.719]
 [0.719]
 [0.719]
 [0.719]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.76291347332902
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.2375132355969
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.70699396958609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.70526021208421
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.82555035355855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 44.90925312042236
siam score:  -0.836899
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.216803984741084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.166]
 [47.166]
 [47.166]
 [47.166]
 [47.166]] [[0.455]
 [0.455]
 [0.455]
 [0.455]
 [0.455]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.715769470053864
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.535]
 [50.862]
 [44.106]
 [30.148]
 [38.675]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.158]
 [26.064]
 [27.894]
 [27.351]
 [26.971]] [[0.411]
 [0.248]
 [0.266]
 [0.26 ]
 [0.257]]
siam score:  -0.83714044
printing an ep nov before normalisation:  50.90800781724754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.109]
 [47.153]
 [46.796]
 [46.646]
 [45.74 ]] [[0.232]
 [0.232]
 [0.229]
 [0.228]
 [0.219]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9952,     0.0001,     0.0000,     0.0026,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0435, 0.8288, 0.0512, 0.0106, 0.0659], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0028,     0.0380,     0.9037,     0.0004,     0.0551],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0091, 0.0044, 0.0307, 0.7256, 0.2303], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0082, 0.0201, 0.0368, 0.2142, 0.7207], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  59.042374164203366
printing an ep nov before normalisation:  54.18863791553711
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.031]
 [41.459]
 [41.459]
 [41.459]
 [41.459]] [[0.87]
 [0.65]
 [0.65]
 [0.65]
 [0.65]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.115]
 [39.045]
 [49.15 ]
 [35.007]
 [34.489]] [[0.392]
 [0.409]
 [0.6  ]
 [0.333]
 [0.324]]
printing an ep nov before normalisation:  61.08146269040253
printing an ep nov before normalisation:  40.56730533776694
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.50904942057363
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.352]
 [35.965]
 [30.509]
 [32.852]
 [32.892]] [[0.771]
 [0.551]
 [0.408]
 [0.469]
 [0.47 ]]
siam score:  -0.8416836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8423488
siam score:  -0.8441197
printing an ep nov before normalisation:  39.686994647992364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.796]
 [51.484]
 [51.484]
 [55.708]
 [53.206]] [[0.95 ]
 [0.718]
 [0.718]
 [0.813]
 [0.757]]
printing an ep nov before normalisation:  36.69328700151903
printing an ep nov before normalisation:  33.75666406419542
printing an ep nov before normalisation:  41.896759192434466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  75.00251395773928
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.25301121863993
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.6149051387204
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.96386241912842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  83.51845059041298
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9717,     0.0162,     0.0001,     0.0118],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0009,     0.0001,     0.9630,     0.0260,     0.0100],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0024,     0.0029,     0.7146,     0.2799],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0002,     0.0042,     0.0577,     0.3651,     0.5728],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8196013
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.374391525957456
printing an ep nov before normalisation:  60.062727977938025
printing an ep nov before normalisation:  57.64267685804363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.551520712405456
printing an ep nov before normalisation:  24.716979559141482
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0088,     0.9877,     0.0003,     0.0001,     0.0031],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9991,     0.0006,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0023,     0.0004,     0.0176,     0.8078,     0.1719],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0013,     0.0004,     0.0042,     0.1750,     0.8191],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0010864890873563127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9948,     0.0010,     0.0011,     0.0006,     0.0025],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9988,     0.0000,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9412,     0.0217,     0.0368],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0008, 0.0011, 0.0233, 0.6631, 0.3117], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0014, 0.0024, 0.0392, 0.2732, 0.6838], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.290072441101074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.058]
 [30.347]
 [45.94 ]
 [45.635]
 [45.27 ]] [[1.675]
 [1.104]
 [1.671]
 [1.66 ]
 [1.646]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.83918655
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.18190685587414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 37.61148111719538
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.251893221209414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([0.9623, 0.0080, 0.0077, 0.0100, 0.0120], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9914,     0.0026,     0.0002,     0.0050],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9989,     0.0007,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0009,     0.0002,     0.0579,     0.7447,     0.1963],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0455, 0.0697, 0.0483, 0.1892, 0.6473], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.473]
 [44.858]
 [60.052]
 [33.605]
 [31.571]] [[0.426]
 [0.879]
 [1.327]
 [0.548]
 [0.488]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.83277523687969
printing an ep nov before normalisation:  42.024903297424316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.38708021463742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9989,     0.0010,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9992,     0.0001,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0008,     0.9180,     0.0404,     0.0403],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0016,     0.0005,     0.0029,     0.8946,     0.1003],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0074, 0.0049, 0.0242, 0.2472, 0.7164], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.118374824523926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  24.41549003124237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.0272269078533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  7.425970068197785e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.7929253578186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.319274932378256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  3  action  0 :  tensor([    0.9574,     0.0199,     0.0001,     0.0160,     0.0066],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0104,     0.9638,     0.0011,     0.0003,     0.0245],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9218,     0.0490,     0.0289],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0691,     0.7739,     0.1566],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0011,     0.0003,     0.0429,     0.3188,     0.6369],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.46838426589966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.23169005557999
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.011]
 [38.784]
 [39.803]
 [35.134]
 [35.134]] [[0.899]
 [1.217]
 [1.273]
 [1.016]
 [1.016]]
printing an ep nov before normalisation:  38.55754594485378
printing an ep nov before normalisation:  45.460246903259886
printing an ep nov before normalisation:  41.45271773174365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.599]
 [49.936]
 [49.936]
 [42.433]
 [51.338]] [[1.227]
 [1.019]
 [1.019]
 [0.786]
 [1.063]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.638]
 [41.56 ]
 [32.982]
 [45.669]
 [36.133]] [[0.651]
 [0.711]
 [0.446]
 [0.837]
 [0.543]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([0.9303, 0.0263, 0.0016, 0.0023, 0.0395], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0022,     0.9789,     0.0106,     0.0001,     0.0082],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0006,     0.0184,     0.8500,     0.1306],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0003,     0.0015,     0.0191,     0.4100,     0.5691],
       grad_fn=<DivBackward0>)
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.85423243
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9409,     0.0029,     0.0000,     0.0544],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9999,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0014,     0.0004,     0.0016,     0.7064,     0.2902],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0169, 0.0225, 0.0294, 0.3152, 0.6160], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0020361774573984803
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.168]
 [47.474]
 [47.474]
 [47.474]
 [47.474]] [[1.686]
 [1.584]
 [1.584]
 [1.584]
 [1.584]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8502056
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.02889882954455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  114.09693474776341
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.575]
 [35.653]
 [30.219]
 [28.5  ]
 [28.452]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  35.32779980292351
printing an ep nov before normalisation:  61.688750201045266
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.30332290648267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.84740466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.690499623050485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8517093
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.85448813
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.91766168033804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 8.683]
 [11.349]
 [12.596]
 [10.677]
 [13.354]] [[0.271]
 [0.354]
 [0.393]
 [0.333]
 [0.417]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[85.27 ]
 [78.806]
 [86.246]
 [87.374]
 [88.925]] [[0.772]
 [0.667]
 [0.788]
 [0.806]
 [0.831]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.29767230748641
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9747,     0.0002,     0.0245,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9980,     0.0002,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0009,     0.9679,     0.0091,     0.0218],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0019, 0.0010, 0.0260, 0.6609, 0.3103], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.0041, 0.0765, 0.2856, 0.6316], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.83539295196533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.144]
 [32.836]
 [44.399]
 [42.852]
 [51.091]] [[0.925]
 [0.571]
 [0.772]
 [0.746]
 [0.889]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9976,     0.0002,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0154,     0.9651,     0.0106,     0.0087],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0068,     0.0004,     0.0585,     0.7452,     0.1891],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0046,     0.0005,     0.0285,     0.2862,     0.6803],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.961]
 [44.883]
 [54.211]
 [42.428]
 [40.961]] [[0.551]
 [0.652]
 [0.891]
 [0.589]
 [0.551]]
actions average: 
K:  4  action  0 :  tensor([    0.9394,     0.0332,     0.0001,     0.0102,     0.0172],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9828,     0.0026,     0.0000,     0.0142],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0000,     0.9522,     0.0261,     0.0214],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0010,     0.0379,     0.7540,     0.2064],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0004,     0.0021,     0.0078,     0.2947,     0.6950],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.784254312411214
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9883,     0.0001,     0.0000,     0.0021,     0.0095],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0016,     0.9472,     0.0046,     0.0003,     0.0463],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9983,     0.0008,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0002,     0.9533,     0.0463],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0919, 0.0036, 0.0008, 0.1981, 0.7056], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.44825924618324
printing an ep nov before normalisation:  26.51456594467163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.29961194405598
siam score:  -0.8647464
actions average: 
K:  3  action  0 :  tensor([    0.9981,     0.0001,     0.0000,     0.0006,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9954,     0.0011,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0039, 0.0029, 0.8685, 0.0765, 0.0481], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0073, 0.0036, 0.0075, 0.8875, 0.0941], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0198, 0.0790, 0.0285, 0.3095, 0.5633], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 43.782541346747564
actions average: 
K:  3  action  0 :  tensor([    0.9966,     0.0004,     0.0000,     0.0016,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0033, 0.9361, 0.0178, 0.0045, 0.0384], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0004,     0.9201,     0.0347,     0.0446],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0002,     0.0062,     0.6290,     0.3643],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0017, 0.0076, 0.0083, 0.1461, 0.8363], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86190397
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  90.05721699325714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.333 0.026 0.051 0.564 0.026]
printing an ep nov before normalisation:  64.71295050283847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.564 0.026 0.333 0.051 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.52466011047363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.48134625316423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8564775
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.00281946557945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.89751380563803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.23129916589051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9981,     0.0000,     0.0000,     0.0009,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0009,     0.9607,     0.0020,     0.0001,     0.0363],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9887,     0.0050,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0002,     0.0002,     0.7967,     0.2021],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0050, 0.0330, 0.0363, 0.2943, 0.6313], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.68788153858021
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.567]
 [44.788]
 [48.21 ]
 [44.788]
 [44.788]] [[1.188]
 [1.201]
 [1.408]
 [1.201]
 [1.201]]
printing an ep nov before normalisation:  57.405772366377704
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.32660328381072
printing an ep nov before normalisation:  24.025954470879384
printing an ep nov before normalisation:  38.39347824408258
printing an ep nov before normalisation:  0.0009676544868852943
printing an ep nov before normalisation:  64.76653297758632
printing an ep nov before normalisation:  51.856502237763614
actions average: 
K:  2  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0006,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9990,     0.0002,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0159,     0.9832,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0012, 0.0037, 0.0293, 0.6724, 0.2934], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0008, 0.0049, 0.0610, 0.2684, 0.6650], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.34079438525703
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.392]
 [31.794]
 [28.392]
 [29.483]
 [28.392]] [[0.379]
 [0.485]
 [0.379]
 [0.413]
 [0.379]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.241]
 [58.241]
 [62.277]
 [58.241]
 [58.241]] [[0.692]
 [0.692]
 [0.759]
 [0.692]
 [0.692]]
printing an ep nov before normalisation:  42.727187324791345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.401737213134766
printing an ep nov before normalisation:  0.007451483547527005
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.78321037313708
printing an ep nov before normalisation:  55.848179427213445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.513]
 [33.951]
 [33.951]
 [33.951]
 [33.951]] [[1.676]
 [0.904]
 [0.904]
 [0.904]
 [0.904]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.371123075377696
printing an ep nov before normalisation:  47.88582274157359
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.97600765396257
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9783,     0.0011,     0.0000,     0.0112,     0.0095],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0120,     0.9824,     0.0017,     0.0004,     0.0034],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0070,     0.0000,     0.0014,     0.8529,     0.1386],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0060, 0.1049, 0.0877, 0.2346, 0.5668], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9950,     0.0031,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9015,     0.0798,     0.0186],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0003,     0.0007,     0.7944,     0.2045],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0008, 0.0012, 0.0381, 0.3555, 0.6044], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.85418
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9828,     0.0027,     0.0003,     0.0023,     0.0120],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9959,     0.0013,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9535,     0.0303,     0.0161],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0006,     0.0519,     0.7252,     0.2221],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0282, 0.0052, 0.0548, 0.2734, 0.6385], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.304]
 [33.761]
 [33.761]
 [33.761]
 [33.761]] [[0.98]
 [0.65]
 [0.65]
 [0.65]
 [0.65]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.76729100929235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.108]
 [57.818]
 [60.217]
 [57.818]
 [57.818]] [[1.188]
 [1.421]
 [1.504]
 [1.421]
 [1.421]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.408912995529406
printing an ep nov before normalisation:  49.224339075909576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.27457904815674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.853802157457594
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  0  action  0 :  tensor([    0.9813,     0.0110,     0.0007,     0.0016,     0.0054],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0044,     0.9840,     0.0019,     0.0000,     0.0097],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9985,     0.0006,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0005,     0.0003,     0.8044,     0.1945],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0371, 0.0089, 0.0183, 0.3077, 0.6280], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.329]
 [36.878]
 [39.982]
 [45.7  ]
 [35.117]] [[0.665]
 [0.8  ]
 [0.917]
 [1.135]
 [0.733]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.423]
 [44.524]
 [49.765]
 [40.747]
 [45.21 ]] [[0.701]
 [0.963]
 [1.076]
 [0.881]
 [0.978]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.637591469312426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  81.00676930736928
printing an ep nov before normalisation:  44.9563906185215
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.234]
 [42.876]
 [31.855]
 [39.348]
 [42.25 ]] [[1.1  ]
 [1.302]
 [0.967]
 [1.195]
 [1.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.11915665047451
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.128 0.231 0.205 0.308 0.128]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9421,     0.0133,     0.0001,     0.0049,     0.0396],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9534,     0.0177,     0.0002,     0.0273],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0222,     0.9166,     0.0309,     0.0296],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0001,     0.0267,     0.8112,     0.1619],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0132, 0.0014, 0.0304, 0.2950, 0.6599], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.150957510977314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8615115
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.66052100579606
printing an ep nov before normalisation:  21.474269385460673
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.633]
 [ 0.715]
 [25.633]
 [25.633]
 [25.633]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.518]
 [47.618]
 [47.618]
 [40.405]
 [47.618]] [[0.998]
 [0.826]
 [0.826]
 [0.7  ]
 [0.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.70258496432361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.04763166972594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9990,     0.0001,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0006,     0.9493,     0.0252,     0.0248],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0009,     0.0007,     0.0154,     0.7631,     0.2199],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0134, 0.0419, 0.0754, 0.2964, 0.5729], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.47497875692946
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.74696731567383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.507]
 [44.643]
 [44.631]
 [42.455]
 [42.455]] [[1.005]
 [0.934]
 [0.933]
 [0.85 ]
 [0.85 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.013982656586045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  20.64338207244873
printing an ep nov before normalisation:  18.770368099212646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.27161387649945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.42107640730633
actions average: 
K:  2  action  0 :  tensor([    0.9967,     0.0003,     0.0000,     0.0002,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9946,     0.0003,     0.0000,     0.0050],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9781,     0.0136,     0.0082],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0015,     0.0001,     0.0008,     0.7404,     0.2572],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0011, 0.0028, 0.0020, 0.2668, 0.7273], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.665]
 [61.417]
 [57.195]
 [50.167]
 [55.544]] [[1.401]
 [1.419]
 [1.321]
 [1.159]
 [1.283]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  35.53871813604039
siam score:  -0.86436766
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8674701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.704802236689574
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86439914
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.272]
 [65.834]
 [67.542]
 [63.174]
 [63.174]] [[1.052]
 [1.342]
 [1.394]
 [1.261]
 [1.261]]
printing an ep nov before normalisation:  69.34352848325177
printing an ep nov before normalisation:  44.13584560991598
printing an ep nov before normalisation:  52.33590909262856
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.641]
 [61.722]
 [61.722]
 [65.003]
 [61.722]] [[1.333]
 [1.152]
 [1.152]
 [1.252]
 [1.152]]
printing an ep nov before normalisation:  65.43040289384862
siam score:  -0.8626703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.393]
 [50.008]
 [59.278]
 [52.032]
 [49.509]] [[0.546]
 [0.846]
 [1.067]
 [0.895]
 [0.835]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0027, 0.9376, 0.0141, 0.0155, 0.0301], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9633,     0.0229,     0.0137],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0026,     0.0002,     0.0001,     0.7788,     0.2183],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0038, 0.0891, 0.2806, 0.6257], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.01677762211827
printing an ep nov before normalisation:  50.2169832249169
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.787219677722454
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.452]
 [53.82 ]
 [47.422]
 [42.011]
 [42.011]] [[0.933]
 [1.667]
 [1.361]
 [1.103]
 [1.103]]
siam score:  -0.86344534
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9916,     0.0006,     0.0000,     0.0032,     0.0047],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0036,     0.9949,     0.0000,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0292,     0.0003,     0.9550,     0.0029,     0.0126],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0030, 0.0008, 0.0254, 0.6082, 0.3626], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0329,     0.0036,     0.0007,     0.1781,     0.7847],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.06041925411433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.104634131699314
siam score:  -0.8620148
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.75370454788208
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.517868108781244
printing an ep nov before normalisation:  57.428048582148406
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.563103175010205
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.468]
 [68.468]
 [68.468]
 [68.468]
 [68.468]] [[1.754]
 [1.754]
 [1.754]
 [1.754]
 [1.754]]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.8693166
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.648651924574054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9788,     0.0007,     0.0004,     0.0195],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9680,     0.0093,     0.0224],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0012,     0.0002,     0.0001,     0.7589,     0.2395],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0282, 0.0176, 0.0450, 0.3233, 0.5859], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.4788179397583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.527512073516846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9985,     0.0003,     0.0000,     0.0005,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9817,     0.0001,     0.0014,     0.0160],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9763,     0.0193,     0.0044],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0001,     0.0289,     0.8322,     0.1380],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0071, 0.0012, 0.0736, 0.2377, 0.6804], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.188]
 [47.213]
 [45.963]
 [48.669]
 [52.724]] [[1.577]
 [1.405]
 [1.351]
 [1.468]
 [1.644]]
printing an ep nov before normalisation:  39.421665166072934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9893,     0.0001,     0.0000,     0.0051,     0.0056],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9776,     0.0023,     0.0015,     0.0176],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0059,     0.9387,     0.0229,     0.0318],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0002,     0.0014,     0.7520,     0.2463],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0227, 0.0297, 0.0358, 0.2237, 0.6882], grad_fn=<DivBackward0>)
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.491]
 [52.491]
 [49.545]
 [52.491]
 [52.491]] [[0.518]
 [0.518]
 [0.467]
 [0.518]
 [0.518]]
printing an ep nov before normalisation:  0.0008724589713438036
printing an ep nov before normalisation:  79.16401421539155
printing an ep nov before normalisation:  62.4279348407176
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 66.11707449282272
printing an ep nov before normalisation:  52.56131055531267
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.58219829707331
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.756]
 [40.806]
 [30.082]
 [38.018]
 [34.353]] [[1.158]
 [0.786]
 [0.34 ]
 [0.67 ]
 [0.518]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.44247295319775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.88982368337274
printing an ep nov before normalisation:  57.32487894477413
printing an ep nov before normalisation:  58.842719907788556
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.301]
 [43.067]
 [50.858]
 [42.494]
 [42.428]] [[0.408]
 [0.442]
 [0.592]
 [0.431]
 [0.429]]
printing an ep nov before normalisation:  35.75597275911182
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.365]
 [47.632]
 [46.445]
 [39.738]
 [38.275]] [[0.57 ]
 [0.695]
 [0.667]
 [0.508]
 [0.473]]
printing an ep nov before normalisation:  26.457544328897445
printing an ep nov before normalisation:  28.765935031071884
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9915,     0.0026,     0.0000,     0.0035,     0.0024],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9996,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9989,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0019,     0.0005,     0.0447,     0.7781,     0.1748],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0082, 0.0136, 0.0583, 0.3793, 0.5407], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  74.95580766808168
printing an ep nov before normalisation:  0.20140311207796913
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.788]
 [74.788]
 [74.788]
 [74.788]
 [74.788]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.748]
 [26.283]
 [45.114]
 [39.659]
 [42.191]] [[0.329]
 [0.264]
 [0.454]
 [0.399]
 [0.424]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.952]
 [60.952]
 [65.166]
 [60.952]
 [60.952]] [[0.907]
 [0.907]
 [1.   ]
 [0.907]
 [0.907]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.204972974350305
printing an ep nov before normalisation:  76.42059803009033
siam score:  -0.8590544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.875]
 [44.712]
 [39.838]
 [46.618]
 [46.618]] [[1.529]
 [1.519]
 [1.22 ]
 [1.635]
 [1.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.1150655622773
printing an ep nov before normalisation:  51.986247178443264
printing an ep nov before normalisation:  44.67375935084359
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.157]
 [55.409]
 [51.124]
 [52.423]
 [46.571]] [[0.48 ]
 [0.761]
 [0.654]
 [0.687]
 [0.541]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.567]
 [37.125]
 [37.125]
 [37.125]
 [37.125]] [[0.529]
 [0.325]
 [0.325]
 [0.325]
 [0.325]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.495724030286446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.9936313316779
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.002946373832912741
printing an ep nov before normalisation:  46.46486421357467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.86730695
printing an ep nov before normalisation:  58.034253295728284
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.96517184670205
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.069589993085053
printing an ep nov before normalisation:  38.90324874422503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.42]
 [36.09]
 [36.09]
 [36.09]
 [36.09]] [[1.234]
 [0.616]
 [0.616]
 [0.616]
 [0.616]]
siam score:  -0.86679894
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.76018024767082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.925]
 [38.925]
 [38.925]
 [38.925]
 [38.925]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  40.203633308410645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  97.00004343044272
printing an ep nov before normalisation:  0.00016680073940733564
printing an ep nov before normalisation:  57.9162098597692
printing an ep nov before normalisation:  35.89478510737896
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.168]
 [31.168]
 [31.168]
 [31.168]
 [31.168]] [[0.843]
 [0.843]
 [0.843]
 [0.843]
 [0.843]]
UNIT TEST: sample policy line 217 mcts : [0.026 0.538 0.359 0.077 0.   ]
actions average: 
K:  1  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0494,     0.9306,     0.0001,     0.0001,     0.0199],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9898,     0.0003,     0.0099],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0007,     0.0004,     0.8354,     0.1625],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0008,     0.1270,     0.1146,     0.7571],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.66127872467041
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  44.422000375383774
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.672]
 [39.415]
 [37.346]
 [31.821]
 [32.962]] [[0.533]
 [0.372]
 [0.336]
 [0.24 ]
 [0.26 ]]
printing an ep nov before normalisation:  45.91791810386531
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.121]
 [49.121]
 [46.065]
 [69.445]
 [52.733]] [[0.297]
 [0.297]
 [0.262]
 [0.532]
 [0.339]]
printing an ep nov before normalisation:  55.864401808677485
printing an ep nov before normalisation:  49.54002380371094
printing an ep nov before normalisation:  40.788822174072266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.96752583042353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.665749959894065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.84732649579259
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.58042219803304
printing an ep nov before normalisation:  0.33237233521475673
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9598,     0.0170,     0.0004,     0.0087,     0.0142],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9959,     0.0007,     0.0000,     0.0033],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9736,     0.0135,     0.0128],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0240,     0.8218,     0.1540],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0062, 0.0144, 0.0475, 0.2448, 0.6870], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0006,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0198, 0.9485, 0.0038, 0.0064, 0.0214], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9827,     0.0089,     0.0082],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0261,     0.8873,     0.0859],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0007,     0.0008,     0.0662,     0.1550,     0.7773],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.801]
 [41.333]
 [68.042]
 [34.575]
 [ 0.   ]] [[ 0.126]
 [ 0.143]
 [ 0.315]
 [ 0.099]
 [-0.124]]
printing an ep nov before normalisation:  64.98373159519923
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.363]
 [34.672]
 [34.672]
 [34.672]
 [34.672]] [[1.514]
 [0.889]
 [0.889]
 [0.889]
 [0.889]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.069]
 [55.759]
 [53.637]
 [43.619]
 [44.203]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.73 ]
 [37.397]
 [38.491]
 [31.037]
 [30.943]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 51.16805229887204
printing an ep nov before normalisation:  0.0006604641419016843
printing an ep nov before normalisation:  43.544375498316484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.810322844384864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 40.70073808682551
printing an ep nov before normalisation:  7.543360425188439e-06
using explorer policy with actor:  1
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.65103081191875
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.18 ]
 [21.902]
 [23.15 ]
 [21.375]
 [22.237]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.126]
 [40.126]
 [46.501]
 [40.126]
 [40.126]] [[0.606]
 [0.606]
 [0.785]
 [0.606]
 [0.606]]
printing an ep nov before normalisation:  30.258898735046387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.41733360290527
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.3558988571167
siam score:  -0.87057054
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.07368926974378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.79150534753647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.90751950093555
printing an ep nov before normalisation:  36.66369629751384
using explorer policy with actor:  1
printing an ep nov before normalisation:  96.29984094754252
printing an ep nov before normalisation:  50.64153047681137
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.295]
 [49.834]
 [43.295]
 [43.295]
 [43.295]] [[1.25 ]
 [1.667]
 [1.25 ]
 [1.25 ]
 [1.25 ]]
printing an ep nov before normalisation:  40.40691375732422
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  43.47134295611812
printing an ep nov before normalisation:  41.14660648459245
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([0.9680, 0.0102, 0.0034, 0.0055, 0.0130], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9948,     0.0026,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9391,     0.0299,     0.0309],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0168, 0.0016, 0.0151, 0.7156, 0.2509], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0022, 0.0407, 0.0339, 0.2013, 0.7219], grad_fn=<DivBackward0>)
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 1.1559142351873191e-09
0.0 1.2467308784319376e-09
0.0 1.246315716588767e-09
0.0 8.472760354422583e-10
0.0 0.0
0.0 5.3971033702899456e-11
0.0 1.309766278047905e-09
0.0 2.6218851395633833e-09
0.0 0.0
0.0 1.093259400595601e-10
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.19999836656633
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.89085254171275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.532329038941718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.42377596254034
printing an ep nov before normalisation:  47.67333131121653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9984,     0.0001,     0.0000,     0.0010,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0321, 0.9347, 0.0056, 0.0016, 0.0260], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0114,     0.8965,     0.0636,     0.0283],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0010, 0.0017, 0.0231, 0.7921, 0.1821], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0017, 0.0052, 0.0478, 0.3340, 0.6113], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.487 0.051 0.103 0.026 0.333]
printing an ep nov before normalisation:  41.00970772720732
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.95486876108662
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.267]
 [24.267]
 [24.267]
 [24.267]
 [24.267]] [[0.13]
 [0.13]
 [0.13]
 [0.13]
 [0.13]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.86328125
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 86.34578717389255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.27060616489375
printing an ep nov before normalisation:  47.20784983571374
siam score:  -0.87303436
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.432]
 [40.208]
 [42.084]
 [43.151]
 [36.677]] [[1.279]
 [1.097]
 [1.203]
 [1.263]
 [0.897]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.329]
 [48.651]
 [71.802]
 [48.098]
 [48.651]] [[0.668]
 [0.852]
 [1.433]
 [0.838]
 [0.852]]
printing an ep nov before normalisation:  30.63882827758789
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.12349459114584
printing an ep nov before normalisation:  56.38510711163519
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.11397695541382
printing an ep nov before normalisation:  33.748154640197754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  46.58461224347321
siam score:  -0.87237334
siam score:  -0.87326545
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.770991894861915
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.66009833068281
printing an ep nov before normalisation:  36.184051474479716
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.70499921922885
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.291170845248782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88133883
UNIT TEST: sample policy line 217 mcts : [0.205 0.103 0.179 0.051 0.462]
printing an ep nov before normalisation:  47.59102938055866
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.761]
 [26.442]
 [22.12 ]
 [24.214]
 [24.232]] [[1.134]
 [0.541]
 [0.333]
 [0.434]
 [0.435]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  67.890234285138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.05606870778141
printing an ep nov before normalisation:  33.39023848501845
printing an ep nov before normalisation:  38.80701070822698
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.5856297824549
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9951,     0.0039,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0013, 0.0219, 0.8851, 0.0483, 0.0434], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0005,     0.0481,     0.7610,     0.1896],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0662, 0.0038, 0.0039, 0.2426, 0.6834], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.311550578547944
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9968,     0.0000,     0.0000,     0.0017,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9976,     0.0001,     0.0002,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0001,     0.9128,     0.0587,     0.0281],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0001,     0.0056,     0.8054,     0.1884],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0309, 0.0038, 0.0259, 0.2261, 0.7133], grad_fn=<DivBackward0>)
siam score:  -0.8737431
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.70334813070723
printing an ep nov before normalisation:  41.61452770233154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.62048437348694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.823872566223145
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.14630145897948
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.26811261842562
printing an ep nov before normalisation:  43.57881983178736
siam score:  -0.8741736
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.554458440117884
printing an ep nov before normalisation:  46.53650760650635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.298]
 [51.298]
 [51.298]
 [51.298]
 [51.298]] [[0.903]
 [0.903]
 [0.903]
 [0.903]
 [0.903]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.484852696534205
printing an ep nov before normalisation:  77.23647697928341
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.97456542417732
printing an ep nov before normalisation:  58.611247815408035
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
siam score:  -0.8712921
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.29068099399792
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.597]
 [41.254]
 [31.306]
 [36.649]
 [31.242]] [[0.288]
 [0.296]
 [0.165]
 [0.235]
 [0.164]]
printing an ep nov before normalisation:  51.23671553643343
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  44.59777374876249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.795]
 [38.718]
 [37.694]
 [38.338]
 [38.128]] [[0.572]
 [0.6  ]
 [0.569]
 [0.589]
 [0.582]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.058129815223005
line 256 mcts: sample exp_bonus 59.84064559049574
using explorer policy with actor:  1
siam score:  -0.87149125
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.95411128733895
printing an ep nov before normalisation:  65.21468441384376
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.62082537380583
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 5.842]
 [ 7.702]
 [ 4.459]
 [11.17 ]
 [12.547]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.704]
 [43.313]
 [43.313]
 [43.313]
 [43.313]] [[1.172]
 [0.986]
 [0.986]
 [0.986]
 [0.986]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.46312692382905
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 99.8936183406504
printing an ep nov before normalisation:  47.902863799035565
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.224]
 [66.224]
 [70.935]
 [66.224]
 [66.224]] [[1.112]
 [1.112]
 [1.221]
 [1.112]
 [1.112]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.36506284176562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.78 ]
 [51.78 ]
 [49.636]
 [51.78 ]
 [51.78 ]] [[1.5  ]
 [1.5  ]
 [1.387]
 [1.5  ]
 [1.5  ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.196]
 [48.196]
 [47.385]
 [48.196]
 [48.196]] [[1.574]
 [1.574]
 [1.524]
 [1.574]
 [1.574]]
actions average: 
K:  3  action  0 :  tensor([    0.9968,     0.0002,     0.0000,     0.0016,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9828,     0.0026,     0.0001,     0.0129],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9848,     0.0005,     0.0147],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0007,     0.0004,     0.0410,     0.7388,     0.2192],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0229, 0.0198, 0.0462, 0.2346, 0.6765], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.60647765315586
printing an ep nov before normalisation:  35.6129789352417
printing an ep nov before normalisation:  0.0006266368927754229
printing an ep nov before normalisation:  49.420036480554906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  80.82710273619699
printing an ep nov before normalisation:  39.5174718238218
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.86 ]
 [50.394]
 [40.217]
 [57.008]
 [40.393]] [[0.545]
 [0.389]
 [0.237]
 [0.488]
 [0.239]]
printing an ep nov before normalisation:  43.85694631609601
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8753119
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.691812403060425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.272312626444865
printing an ep nov before normalisation:  42.459168434143066
printing an ep nov before normalisation:  50.63446668539106
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.809]
 [40.809]
 [49.892]
 [40.809]
 [40.809]] [[0.614]
 [0.614]
 [0.842]
 [0.614]
 [0.614]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  74.90003871118097
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.521]
 [36.521]
 [36.521]
 [36.521]
 [36.521]] [[0.818]
 [0.818]
 [0.818]
 [0.818]
 [0.818]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.522373276566434
printing an ep nov before normalisation:  46.259098052978516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.651]
 [50.787]
 [47.651]
 [47.515]
 [49.37 ]] [[0.296]
 [0.333]
 [0.296]
 [0.294]
 [0.316]]
printing an ep nov before normalisation:  93.87593093815606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.599308013916016
printing an ep nov before normalisation:  28.63192684576959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.83 ]
 [57.83 ]
 [58.953]
 [57.83 ]
 [57.83 ]] [[1.635]
 [1.635]
 [1.679]
 [1.635]
 [1.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.017476801519145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.32685869718151
printing an ep nov before normalisation:  44.85659103257649
printing an ep nov before normalisation:  49.04403494748268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.176998465903544
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.28678062264393
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.627]
 [60.627]
 [60.627]
 [60.627]
 [60.627]] [[1.235]
 [1.235]
 [1.235]
 [1.235]
 [1.235]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.051 0.59  0.103 0.077 0.179]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.69712487435575
printing an ep nov before normalisation:  44.94788052028576
using explorer policy with actor:  1
siam score:  -0.8728112
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.642]
 [57.642]
 [57.642]
 [59.269]
 [57.642]] [[1.071]
 [1.071]
 [1.071]
 [1.124]
 [1.071]]
siam score:  -0.8714951
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.153]
 [50.748]
 [54.333]
 [48.011]
 [48.011]] [[1.468]
 [1.577]
 [1.727]
 [1.462]
 [1.462]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.028]
 [61.028]
 [61.028]
 [61.028]
 [61.028]] [[1.949]
 [1.949]
 [1.949]
 [1.949]
 [1.949]]
printing an ep nov before normalisation:  59.497989728580414
printing an ep nov before normalisation:  73.1292751982191
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.30341071196997
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  87.55346253486012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.22669509789084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.441494156726144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9905,     0.0002,     0.0000,     0.0015,     0.0078],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9903,     0.0041,     0.0000,     0.0052],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9511,     0.0289,     0.0197],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0003,     0.0298,     0.8048,     0.1647],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0018, 0.0139, 0.0377, 0.3247, 0.6219], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  58.842797327345565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.92499281411485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.87629222869873
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.364721831611575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  102.76593142387773
printing an ep nov before normalisation:  99.32263536884166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.08418419767908
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.993273176449776
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.458]
 [42.458]
 [42.458]
 [42.458]
 [42.458]] [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.02 ]
 [44.02 ]
 [50.729]
 [44.02 ]
 [44.02 ]] [[1.022]
 [1.022]
 [1.333]
 [1.022]
 [1.022]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.065]
 [63.676]
 [64.521]
 [62.824]
 [65.45 ]] [[1.083]
 [1.177]
 [1.199]
 [1.155]
 [1.223]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.64449618960153
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.46356866701421
printing an ep nov before normalisation:  48.776301714297354
UNIT TEST: sample policy line 217 mcts : [0.897 0.103 0.    0.    0.   ]
siam score:  -0.8806544
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.876]
 [46.876]
 [47.075]
 [46.296]
 [44.764]] [[1.774]
 [1.774]
 [1.785]
 [1.741]
 [1.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.30958480568903
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.533]
 [37.519]
 [37.519]
 [47.678]
 [37.519]] [[0.967]
 [0.568]
 [0.568]
 [0.905]
 [0.568]]
printing an ep nov before normalisation:  39.687677449780956
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.304]
 [49.104]
 [45.304]
 [45.304]
 [45.304]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.76313083827171
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9840,     0.0002,     0.0000,     0.0091,     0.0067],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0166,     0.9737,     0.0008,     0.0000,     0.0089],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9651,     0.0183,     0.0165],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0082, 0.0035, 0.0505, 0.7530, 0.1847], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0027, 0.0292, 0.0211, 0.3545, 0.5926], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.39748120098763
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.775]
 [50.104]
 [50.382]
 [47.775]
 [47.775]] [[0.448]
 [0.487]
 [0.492]
 [0.448]
 [0.448]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.656]
 [56.791]
 [56.791]
 [56.791]
 [56.791]] [[0.667]
 [0.619]
 [0.619]
 [0.619]
 [0.619]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.79869694943581
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.053]
 [32.053]
 [32.053]
 [32.053]
 [32.053]] [[0.13]
 [0.13]
 [0.13]
 [0.13]
 [0.13]]
siam score:  -0.86488295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.53156175001906
printing an ep nov before normalisation:  91.34372951819678
printing an ep nov before normalisation:  63.616189798091156
printing an ep nov before normalisation:  55.36437679084154
printing an ep nov before normalisation:  46.90923753534571
printing an ep nov before normalisation:  43.04385117867813
printing an ep nov before normalisation:  58.392520474550594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.447159954514134
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.62499835631419
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.04898507586459
printing an ep nov before normalisation:  44.14698281447361
printing an ep nov before normalisation:  42.58768081665039
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.299554615092944
printing an ep nov before normalisation:  89.23242146402025
printing an ep nov before normalisation:  51.61611959993811
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 32.138552754937486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.65686902455089
actions average: 
K:  0  action  0 :  tensor([    0.9829,     0.0102,     0.0000,     0.0010,     0.0058],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0181,     0.9587,     0.0011,     0.0002,     0.0220],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0036,     0.9633,     0.0158,     0.0173],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0008,     0.1139,     0.7020,     0.1828],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0009, 0.0391, 0.0289, 0.2148, 0.7163], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.45126710817486
siam score:  -0.87433904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.92098623379965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9950,     0.0023,     0.0000,     0.0014,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9919,     0.0009,     0.0000,     0.0070],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9803,     0.0125,     0.0071],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0009, 0.0011, 0.0266, 0.7301, 0.2413], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0114, 0.0022, 0.0181, 0.3097, 0.6585], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.138]
 [49.918]
 [49.918]
 [48.761]
 [49.918]] [[1.626]
 [1.27 ]
 [1.27 ]
 [1.22 ]
 [1.27 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.16205995124409
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.339]
 [42.34 ]
 [34.799]
 [33.827]
 [33.157]] [[1.259]
 [1.167]
 [0.822]
 [0.778]
 [0.747]]
printing an ep nov before normalisation:  24.109088556343632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.04586335184888
printing an ep nov before normalisation:  49.16969276962706
printing an ep nov before normalisation:  40.95217227935791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.10191752326371
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.975]
 [30.308]
 [50.616]
 [48.912]
 [38.121]] [[1.327]
 [0.875]
 [1.461]
 [1.412]
 [1.101]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.71 ]
 [41.099]
 [47.09 ]
 [35.607]
 [36.514]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.844]
 [40.514]
 [40.514]
 [40.514]
 [40.514]] [[1.282]
 [1.063]
 [1.063]
 [1.063]
 [1.063]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.93237970928373
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  30.023175143674745
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.344]
 [55.344]
 [59.34 ]
 [55.344]
 [55.344]] [[0.853]
 [0.853]
 [0.945]
 [0.853]
 [0.853]]
siam score:  -0.8738262
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.652]
 [37.892]
 [36.591]
 [35.397]
 [34.377]] [[1.034]
 [1.158]
 [1.086]
 [1.02 ]
 [0.963]]
printing an ep nov before normalisation:  44.158477393217275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.014873025123336
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.202]
 [45.202]
 [45.732]
 [45.202]
 [45.202]] [[1.453]
 [1.453]
 [1.481]
 [1.453]
 [1.453]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8731529
printing an ep nov before normalisation:  7.306172327454469e-05
printing an ep nov before normalisation:  66.58749166505548
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  68.15947805132186
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.065]
 [48.247]
 [46.432]
 [46.108]
 [44.809]] [[0.663]
 [0.721]
 [0.673]
 [0.664]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.557]
 [56.557]
 [58.918]
 [56.557]
 [56.557]] [[1.07]
 [1.07]
 [1.15]
 [1.07]
 [1.07]]
printing an ep nov before normalisation:  41.40623656614592
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.586]
 [44.586]
 [54.558]
 [44.586]
 [44.586]] [[0.819]
 [0.819]
 [1.182]
 [0.819]
 [0.819]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.87223643790708
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 62.8361403428916
printing an ep nov before normalisation:  44.5168368855407
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.06013236683591
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.893]
 [47.893]
 [47.893]
 [47.62 ]
 [52.101]] [[0.959]
 [0.959]
 [0.959]
 [0.949]
 [1.105]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.474]
 [ 9.793]
 [13.319]
 [ 9.587]
 [11.044]] [[0.328]
 [0.222]
 [0.302]
 [0.217]
 [0.251]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.109792709350586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.938]
 [46.651]
 [43.67 ]
 [45.59 ]
 [46.107]] [[0.228]
 [0.175]
 [0.155]
 [0.168]
 [0.171]]
printing an ep nov before normalisation:  66.98816248501907
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.89848029613495
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.28057829241745
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.807291984558105
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.14476013183594
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.3511313616629
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.901955477009466
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.031]
 [58.676]
 [58.676]
 [58.676]
 [58.676]] [[1.257]
 [1.204]
 [1.204]
 [1.204]
 [1.204]]
siam score:  -0.87920606
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0002,     0.0000,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9972,     0.0003,     0.0000,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9419,     0.0299,     0.0280],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0004,     0.0704,     0.7159,     0.2130],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0461, 0.0082, 0.0636, 0.3217, 0.5604], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  1.1498087815198232e-05
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.459]
 [46.459]
 [46.459]
 [46.459]
 [46.459]] [[0.797]
 [0.797]
 [0.797]
 [0.797]
 [0.797]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.15950984274802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.821]
 [55.821]
 [55.821]
 [55.821]
 [55.821]] [[1.291]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0015077019995146657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
printing an ep nov before normalisation:  45.839611652489126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.634970423143415
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.92 ]
 [42.516]
 [41.616]
 [38.26 ]
 [38.124]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  4  action  0 :  tensor([    0.9988,     0.0001,     0.0000,     0.0003,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9946,     0.0032,     0.0000,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0009,     0.0021,     0.9640,     0.0194,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0003,     0.0004,     0.8385,     0.1606],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0027, 0.0115, 0.0374, 0.2876, 0.6608], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.68665634126604
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  55.14780194832659
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.663]
 [42.627]
 [55.236]
 [37.663]
 [46.524]] [[0.111]
 [0.158]
 [0.276]
 [0.111]
 [0.194]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9988,     0.0001,     0.0000,     0.0005,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0126,     0.9658,     0.0003,     0.0001,     0.0212],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0182,     0.8931,     0.0450,     0.0436],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0028, 0.0018, 0.0740, 0.7868, 0.1346], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0001,     0.0004,     0.0725,     0.0989,     0.8281],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  78.17528638933578
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.556]
 [62.046]
 [67.26 ]
 [57.1  ]
 [60.584]] [[0.415]
 [0.757]
 [0.911]
 [0.61 ]
 [0.713]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8722275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.523937392529255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.304802894592285
printing an ep nov before normalisation:  84.62325508124381
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.52194320366727
printing an ep nov before normalisation:  46.433314407791684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.52023353691544
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9988,     0.0000,     0.0000,     0.0007,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9732,     0.0051,     0.0041,     0.0175],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0016,     0.9570,     0.0159,     0.0254],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0002,     0.0390,     0.7432,     0.2173],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0007,     0.0006,     0.0004,     0.1422,     0.8560],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.254]
 [32.196]
 [32.196]
 [32.196]
 [32.196]] [[0.786]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  66.36683782287076
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 41.00322586157538
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.205]
 [49.971]
 [43.376]
 [48.495]
 [47.968]] [[0.689]
 [0.48 ]
 [0.346]
 [0.45 ]
 [0.439]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  18.029072579382973
printing an ep nov before normalisation:  54.45340324962769
actions average: 
K:  2  action  0 :  tensor([    0.9978,     0.0001,     0.0000,     0.0011,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9981,     0.0001,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0012,     0.9318,     0.0350,     0.0319],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0011,     0.0171,     0.7839,     0.1977],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0002,     0.0025,     0.0478,     0.2784,     0.6710],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  103.16475357430649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.981]
 [67.6  ]
 [61.981]
 [61.981]
 [61.981]] [[0.969]
 [1.119]
 [0.969]
 [0.969]
 [0.969]]
printing an ep nov before normalisation:  48.79581175022307
printing an ep nov before normalisation:  52.52347839441853
printing an ep nov before normalisation:  33.54769014896826
printing an ep nov before normalisation:  58.58074254316694
printing an ep nov before normalisation:  73.45123240345202
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9887,     0.0030,     0.0000,     0.0079],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9319,     0.0382,     0.0297],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0007,     0.0002,     0.0447,     0.7402,     0.2142],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0120, 0.0448, 0.4104, 0.5315], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  91.90532913075101
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.825]
 [67.467]
 [63.825]
 [63.825]
 [63.825]] [[1.504]
 [1.647]
 [1.504]
 [1.504]
 [1.504]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.513]
 [42.513]
 [69.03 ]
 [42.513]
 [42.513]] [[0.49 ]
 [0.49 ]
 [1.071]
 [0.49 ]
 [0.49 ]]
printing an ep nov before normalisation:  42.40169108387421
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.880100692947465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.068]
 [45.157]
 [44.932]
 [45.306]
 [45.306]] [[1.1  ]
 [1.642]
 [1.625]
 [1.654]
 [1.654]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.271]
 [32.732]
 [38.284]
 [34.244]
 [26.323]] [[0.07 ]
 [0.096]
 [0.123]
 [0.104]
 [0.065]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.853]
 [63.713]
 [76.454]
 [63.713]
 [63.713]] [[0.152]
 [0.191]
 [0.263]
 [0.191]
 [0.191]]
printing an ep nov before normalisation:  64.39500473312387
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.718]
 [38.907]
 [38.907]
 [38.907]
 [38.907]] [[1.128]
 [0.851]
 [0.851]
 [0.851]
 [0.851]]
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9435,     0.0272,     0.0003,     0.0272],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9631,     0.0189,     0.0180],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0011, 0.0050, 0.0011, 0.8016, 0.1912], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0089, 0.0319, 0.0829, 0.2400, 0.6363], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.08911491290153
printing an ep nov before normalisation:  34.25613343715668
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.24766584357233
printing an ep nov before normalisation:  0.004766020140323235
printing an ep nov before normalisation:  52.236505885978744
printing an ep nov before normalisation:  40.160067668335515
printing an ep nov before normalisation:  36.32423809650163
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9995,     0.0001,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0016, 0.9533, 0.0127, 0.0038, 0.0287], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9574,     0.0198,     0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0257,     0.8508,     0.1232],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0010, 0.0185, 0.0647, 0.2955, 0.6204], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.45022949029143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.47214632337964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8662082
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  61.926997645611074
printing an ep nov before normalisation:  80.03358245731407
siam score:  -0.86783224
printing an ep nov before normalisation:  57.934429383072995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.041]
 [50.439]
 [50.439]
 [50.439]
 [50.439]] [[0.603]
 [0.516]
 [0.516]
 [0.516]
 [0.516]]
printing an ep nov before normalisation:  43.04722785949707
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.09083521798378
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.9869250670703877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.19738145272955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.35951956590556
printing an ep nov before normalisation:  56.249447734594085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.375999508890814
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  60.23480414649124
actions average: 
K:  4  action  0 :  tensor([    0.9832,     0.0006,     0.0001,     0.0060,     0.0102],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0044,     0.9647,     0.0008,     0.0009,     0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0001,     0.9758,     0.0148,     0.0091],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0133,     0.0007,     0.0813,     0.7270,     0.1778],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0028, 0.0133, 0.1005, 0.1900, 0.6935], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9979,     0.0020,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9814,     0.0012,     0.0005,     0.0160],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9980,     0.0017,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0019,     0.0002,     0.0267,     0.8133,     0.1579],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0126, 0.0505, 0.0799, 0.2342, 0.6229], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.86511072387957
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.62914346197399
printing an ep nov before normalisation:  52.023747481993226
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.372264367750205
printing an ep nov before normalisation:  56.96664563222181
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.717251609497602
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.61740350344315
printing an ep nov before normalisation:  53.13030183890482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.34796577383501
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.97735471973281
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.765]
 [42.554]
 [42.554]
 [44.738]
 [42.554]] [[1.268]
 [1.822]
 [1.822]
 [2.   ]
 [1.822]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.69544219970703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.47848855565564
printing an ep nov before normalisation:  48.8436349572689
printing an ep nov before normalisation:  0.007679917192859875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.272]
 [47.272]
 [47.945]
 [47.272]
 [47.272]] [[1.414]
 [1.414]
 [1.447]
 [1.414]
 [1.414]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9764,     0.0190,     0.0000,     0.0000,     0.0045],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0082,     0.9454,     0.0003,     0.0030,     0.0432],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9895,     0.0041,     0.0063],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0086,     0.0005,     0.0002,     0.8524,     0.1382],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0029, 0.0322, 0.0581, 0.1696, 0.7372], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  70.69933963840823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.82043981552124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.08436621843181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.901543740169295
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.89390287130829
printing an ep nov before normalisation:  29.334074761387363
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.8739292
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.323]
 [46.323]
 [48.009]
 [45.322]
 [46.323]] [[1.637]
 [1.637]
 [1.724]
 [1.585]
 [1.637]]
printing an ep nov before normalisation:  58.313810345908024
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.269]
 [43.269]
 [43.269]
 [43.269]
 [43.269]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.1420653126405
printing an ep nov before normalisation:  0.008634146201416115
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  88.24023010375954
siam score:  -0.87322557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.0474801460172
printing an ep nov before normalisation:  66.21184990214768
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.737042102425484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.58555636820348
printing an ep nov before normalisation:  48.822953620116486
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.005]
 [38.609]
 [33.657]
 [44.892]
 [38.609]] [[1.109]
 [0.778]
 [0.522]
 [1.103]
 [0.778]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.48 ]
 [36.543]
 [28.797]
 [36.543]
 [36.543]] [[0.819]
 [0.55 ]
 [0.199]
 [0.55 ]
 [0.55 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  83.2090441832269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.647064685821533
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.40776720317626
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  62.947908422108654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  65.28388176990092
printing an ep nov before normalisation:  49.78103499052175
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255964026836566, 0.06255964026836566, 0.06255964026836566, 0.6872017986581719, 0.06255964026836566, 0.06255964026836566]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255966836678094, 0.06255966836678094, 0.06255966836678094, 0.6872016581660952, 0.06255966836678094, 0.06255966836678094]
from probs:  [0.06255966836678094, 0.06255966836678094, 0.06255966836678094, 0.6872016581660952, 0.06255966836678094, 0.06255966836678094]
actions average: 
K:  2  action  0 :  tensor([    0.9173,     0.0004,     0.0005,     0.0076,     0.0742],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0013, 0.9267, 0.0025, 0.0043, 0.0652], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0021,     0.0002,     0.9637,     0.0178,     0.0162],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0006,     0.0221,     0.8156,     0.1613],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0119, 0.0020, 0.0811, 0.2734, 0.6317], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255966836678094, 0.06255966836678094, 0.06255966836678094, 0.6872016581660952, 0.06255966836678094, 0.06255966836678094]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255972456356602, 0.06255972456356602, 0.06255972456356602, 0.6872013771821698, 0.06255972456356602, 0.06255972456356602]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255972456356602, 0.06255972456356602, 0.06255972456356602, 0.6872013771821698, 0.06255972456356602, 0.06255972456356602]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255972456356602, 0.06255972456356602, 0.06255972456356602, 0.6872013771821698, 0.06255972456356602, 0.06255972456356602]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255972456356602, 0.06255972456356602, 0.06255972456356602, 0.6872013771821698, 0.06255972456356602, 0.06255972456356602]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.23230152633897
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255972456356602, 0.06255972456356602, 0.06255972456356602, 0.6872013771821698, 0.06255972456356602, 0.06255972456356602]
line 256 mcts: sample exp_bonus 30.10631141193957
printing an ep nov before normalisation:  44.180026489684195
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255975266193582, 0.06255975266193582, 0.06255975266193582, 0.6872012366903208, 0.06255975266193582, 0.06255975266193582]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255975266193582, 0.06255975266193582, 0.06255975266193582, 0.6872012366903208, 0.06255975266193582, 0.06255975266193582]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255975266193582, 0.06255975266193582, 0.06255975266193582, 0.6872012366903208, 0.06255975266193582, 0.06255975266193582]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255978076029044, 0.06255978076029044, 0.06255978076029044, 0.6872010961985477, 0.06255978076029044, 0.06255978076029044]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255978076029044, 0.06255978076029044, 0.06255978076029044, 0.6872010961985477, 0.06255978076029044, 0.06255978076029044]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255978076029044, 0.06255978076029044, 0.06255978076029044, 0.6872010961985477, 0.06255978076029044, 0.06255978076029044]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.541]
 [25.928]
 [25.928]
 [25.928]
 [25.928]] [[0.976]
 [0.425]
 [0.425]
 [0.425]
 [0.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255978076029044, 0.06255978076029044, 0.06255978076029044, 0.6872010961985477, 0.06255978076029044, 0.06255978076029044]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255978076029044, 0.06255978076029044, 0.06255978076029044, 0.6872010961985477, 0.06255978076029044, 0.06255978076029044]
printing an ep nov before normalisation:  54.85386895014156
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[81.376]
 [81.376]
 [81.376]
 [81.376]
 [81.376]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
printing an ep nov before normalisation:  46.238510671959446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255983695695419, 0.06255983695695419, 0.06255983695695419, 0.687200815215229, 0.06255983695695419, 0.06255983695695419]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625598650552633, 0.0625598650552633, 0.0625598650552633, 0.6872006747236834, 0.0625598650552633, 0.0625598650552633]
printing an ep nov before normalisation:  38.25303929804497
printing an ep nov before normalisation:  28.988046646118164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625598650552633, 0.0625598650552633, 0.0625598650552633, 0.6872006747236834, 0.0625598650552633, 0.0625598650552633]
printing an ep nov before normalisation:  54.93614580211162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625598650552633, 0.0625598650552633, 0.0625598650552633, 0.6872006747236834, 0.0625598650552633, 0.0625598650552633]
from probs:  [0.06255992125183604, 0.06255992125183604, 0.06255992125183604, 0.6872003937408196, 0.06255992125183604, 0.06255992125183604]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255994935009969, 0.06255994935009969, 0.06255994935009969, 0.6872002532495016, 0.06255994935009969, 0.06255994935009969]
printing an ep nov before normalisation:  46.62673660226614
printing an ep nov before normalisation:  55.188459026242796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255997744834813, 0.06255997744834813, 0.06255997744834813, 0.6872001127582593, 0.06255997744834813, 0.06255997744834813]
printing an ep nov before normalisation:  39.32074517674273
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255997744834813, 0.06255997744834813, 0.06255997744834813, 0.6872001127582593, 0.06255997744834813, 0.06255997744834813]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255997744834813, 0.06255997744834813, 0.06255997744834813, 0.6872001127582593, 0.06255997744834813, 0.06255997744834813]
printing an ep nov before normalisation:  48.144533470498146
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06255997744834813, 0.06255997744834813, 0.06255997744834813, 0.6872001127582593, 0.06255997744834813, 0.06255997744834813]
printing an ep nov before normalisation:  58.11170521154635
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256000554658141, 0.06256000554658141, 0.06256000554658141, 0.6871999722670928, 0.06256000554658141, 0.06256000554658141]
printing an ep nov before normalisation:  36.763581737659564
printing an ep nov before normalisation:  48.70477815962463
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256003364479953, 0.06256003364479953, 0.06256003364479953, 0.6871998317760023, 0.06256003364479953, 0.06256003364479953]
actions average: 
K:  2  action  0 :  tensor([    0.9948,     0.0002,     0.0000,     0.0024,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9582,     0.0045,     0.0000,     0.0372],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9992,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0042,     0.0001,     0.0006,     0.8377,     0.1574],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0006, 0.0061, 0.0655, 0.4629, 0.4648], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256008984119026, 0.06256008984119026, 0.06256008984119026, 0.6871995507940486, 0.06256008984119026, 0.06256008984119026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256008984119026, 0.06256008984119026, 0.06256008984119026, 0.6871995507940486, 0.06256008984119026, 0.06256008984119026]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.776]
 [53.433]
 [54.304]
 [53.419]
 [53.419]] [[0.888]
 [1.08 ]
 [1.109]
 [1.079]
 [1.079]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.661]
 [47.661]
 [47.661]
 [48.784]
 [47.661]] [[1.148]
 [1.148]
 [1.148]
 [1.19 ]
 [1.148]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256011793936289, 0.06256011793936289, 0.06256011793936289, 0.6871994103031855, 0.06256011793936289, 0.06256011793936289]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256011793936289, 0.06256011793936289, 0.06256011793936289, 0.6871994103031855, 0.06256011793936289, 0.06256011793936289]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256014603752033, 0.06256014603752033, 0.06256014603752033, 0.6871992698123983, 0.06256014603752033, 0.06256014603752033]
printing an ep nov before normalisation:  64.15890719818121
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625601741356626, 0.0625601741356626, 0.0625601741356626, 0.6871991293216868, 0.0625601741356626, 0.0625601741356626]
from probs:  [0.06256020223378972, 0.06256020223378972, 0.06256020223378972, 0.6871989888310512, 0.06256020223378972, 0.06256020223378972]
printing an ep nov before normalisation:  44.868057276720016
printing an ep nov before normalisation:  49.848014987955324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256023033190167, 0.06256023033190167, 0.06256023033190167, 0.6871988483404916, 0.06256023033190167, 0.06256023033190167]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.351]
 [30.508]
 [38.382]
 [28.007]
 [32.069]] [[0.468]
 [0.553]
 [0.862]
 [0.455]
 [0.614]]
printing an ep nov before normalisation:  35.68507931736743
printing an ep nov before normalisation:  32.171454248515914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256023033190167, 0.06256023033190167, 0.06256023033190167, 0.6871988483404916, 0.06256023033190167, 0.06256023033190167]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256023033190167, 0.06256023033190167, 0.06256023033190167, 0.6871988483404916, 0.06256023033190167, 0.06256023033190167]
printing an ep nov before normalisation:  39.3224773680392
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.83140811934878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256023033190167, 0.06256023033190167, 0.06256023033190167, 0.6871988483404916, 0.06256023033190167, 0.06256023033190167]
printing an ep nov before normalisation:  59.59637178868882
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256025842999845, 0.06256025842999845, 0.06256025842999845, 0.6871987078500077, 0.06256025842999845, 0.06256025842999845]
siam score:  -0.83015597
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256025842999845, 0.06256025842999845, 0.06256025842999845, 0.6871987078500077, 0.06256025842999845, 0.06256025842999845]
actions average: 
K:  3  action  0 :  tensor([    0.9945,     0.0005,     0.0001,     0.0002,     0.0048],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9989,     0.0007,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0046,     0.9633,     0.0127,     0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0190, 0.0049, 0.0028, 0.7491, 0.2242], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0034, 0.0191, 0.1445, 0.2948, 0.5382], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256031462614653, 0.06256031462614653, 0.06256031462614653, 0.6871984268692675, 0.06256031462614653, 0.06256031462614653]
printing an ep nov before normalisation:  40.69177062288077
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.48835482641929
printing an ep nov before normalisation:  43.591521671580146
printing an ep nov before normalisation:  43.81594067247743
printing an ep nov before normalisation:  41.14897482978796
printing an ep nov before normalisation:  0.3716453073258208
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256031462614653, 0.06256031462614653, 0.06256031462614653, 0.6871984268692675, 0.06256031462614653, 0.06256031462614653]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.582]
 [65.113]
 [77.828]
 [65.113]
 [65.113]] [[0.682]
 [0.908]
 [1.182]
 [0.908]
 [0.908]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.54295791399478
printing an ep nov before normalisation:  33.9673289730701
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256031462614653, 0.06256031462614653, 0.06256031462614653, 0.6871984268692675, 0.06256031462614653, 0.06256031462614653]
printing an ep nov before normalisation:  58.573235368464815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625603708222339, 0.0625603708222339, 0.0625603708222339, 0.6871981458888304, 0.0625603708222339, 0.0625603708222339]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.154 0.154 0.282 0.333 0.077]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625603708222339, 0.0625603708222339, 0.0625603708222339, 0.6871981458888304, 0.0625603708222339, 0.0625603708222339]
actions average: 
K:  4  action  0 :  tensor([    0.9681,     0.0257,     0.0000,     0.0055,     0.0007],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0027,     0.9634,     0.0153,     0.0000,     0.0186],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0002,     0.9415,     0.0246,     0.0336],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0014, 0.0092, 0.0489, 0.7812, 0.1593], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0101, 0.0025, 0.0007, 0.3401, 0.6465], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.069777755099466
printing an ep nov before normalisation:  0.022996029852606625
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.34003856521317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256039892025485, 0.06256039892025485, 0.06256039892025485, 0.6871980053987258, 0.06256039892025485, 0.06256039892025485]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[21.465]
 [20.682]
 [21.61 ]
 [33.305]
 [23.324]] [[0.351]
 [0.326]
 [0.355]
 [0.721]
 [0.409]]
printing an ep nov before normalisation:  0.01945044071760549
printing an ep nov before normalisation:  42.56813959254885
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256045511625123, 0.06256045511625123, 0.06256045511625123, 0.6871977244187438, 0.06256045511625123, 0.06256045511625123]
printing an ep nov before normalisation:  29.593265056610107
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256051131218696, 0.06256051131218696, 0.06256051131218696, 0.6871974434390653, 0.06256051131218696, 0.06256051131218696]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256053941013207, 0.06256053941013207, 0.06256053941013207, 0.6871973029493397, 0.06256053941013207, 0.06256053941013207]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256053941013207, 0.06256053941013207, 0.06256053941013207, 0.6871973029493397, 0.06256053941013207, 0.06256053941013207]
printing an ep nov before normalisation:  52.01459762276122
printing an ep nov before normalisation:  37.035659052377554
printing an ep nov before normalisation:  35.7765007019043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062560567508062, 0.062560567508062, 0.062560567508062, 0.6871971624596901, 0.062560567508062, 0.062560567508062]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062560567508062, 0.062560567508062, 0.062560567508062, 0.6871971624596901, 0.062560567508062, 0.062560567508062]
printing an ep nov before normalisation:  65.44798618894802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.062560567508062, 0.062560567508062, 0.062560567508062, 0.6871971624596901, 0.062560567508062, 0.062560567508062]
siam score:  -0.8552733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256059560597677, 0.06256059560597677, 0.06256059560597677, 0.687197021970116, 0.06256059560597677, 0.06256059560597677]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625606237038764, 0.0625606237038764, 0.0625606237038764, 0.6871968814806182, 0.0625606237038764, 0.0625606237038764]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625606237038764, 0.0625606237038764, 0.0625606237038764, 0.6871968814806182, 0.0625606237038764, 0.0625606237038764]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.41991948862404
printing an ep nov before normalisation:  30.95432758331299
printing an ep nov before normalisation:  45.63941348592365
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256067989963009, 0.06256067989963009, 0.06256067989963009, 0.6871966005018495, 0.06256067989963009, 0.06256067989963009]
printing an ep nov before normalisation:  60.17587063609804
printing an ep nov before normalisation:  51.26353556428191
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8525603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625607641931469, 0.0625607641931469, 0.0625607641931469, 0.6871961790342653, 0.0625607641931469, 0.0625607641931469]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625607641931469, 0.0625607641931469, 0.0625607641931469, 0.6871961790342653, 0.0625607641931469, 0.0625607641931469]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.24]
 [32.24]
 [32.24]
 [32.24]
 [32.24]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  39.75494623184204
printing an ep nov before normalisation:  49.416818358391986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625607641931469, 0.0625607641931469, 0.0625607641931469, 0.6871961790342653, 0.0625607641931469, 0.0625607641931469]
printing an ep nov before normalisation:  102.49528765848923
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625607641931469, 0.0625607641931469, 0.0625607641931469, 0.6871961790342653, 0.0625607641931469, 0.0625607641931469]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625607641931469, 0.0625607641931469, 0.0625607641931469, 0.6871961790342653, 0.0625607641931469, 0.0625607641931469]
printing an ep nov before normalisation:  56.19397277184177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256079229095551, 0.06256079229095551, 0.06256079229095551, 0.6871960385452224, 0.06256079229095551, 0.06256079229095551]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256079229095551, 0.06256079229095551, 0.06256079229095551, 0.6871960385452224, 0.06256079229095551, 0.06256079229095551]
printing an ep nov before normalisation:  80.91137353531641
printing an ep nov before normalisation:  32.387261390686035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256079229095551, 0.06256079229095551, 0.06256079229095551, 0.6871960385452224, 0.06256079229095551, 0.06256079229095551]
from probs:  [0.06256079229095551, 0.06256079229095551, 0.06256079229095551, 0.6871960385452224, 0.06256079229095551, 0.06256079229095551]
printing an ep nov before normalisation:  61.5165114402771
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.227]
 [35.685]
 [36.946]
 [27.204]
 [27.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256082038874895, 0.06256082038874895, 0.06256082038874895, 0.6871958980562551, 0.06256082038874895, 0.06256082038874895]
printing an ep nov before normalisation:  42.32610790200217
printing an ep nov before normalisation:  39.49859258001092
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256082038874895, 0.06256082038874895, 0.06256082038874895, 0.6871958980562551, 0.06256082038874895, 0.06256082038874895]
printing an ep nov before normalisation:  28.678429699641047
printing an ep nov before normalisation:  30.603930226559577
siam score:  -0.8560582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256082038874895, 0.06256082038874895, 0.06256082038874895, 0.6871958980562551, 0.06256082038874895, 0.06256082038874895]
actions average: 
K:  0  action  0 :  tensor([    0.9991,     0.0005,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9961,     0.0012,     0.0001,     0.0025],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9796,     0.0116,     0.0087],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0048,     0.0148,     0.0007,     0.7979,     0.1818],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0012, 0.0022, 0.0476, 0.3014, 0.6477], grad_fn=<DivBackward0>)
from probs:  [0.06256084848652722, 0.06256084848652722, 0.06256084848652722, 0.6871957575673638, 0.06256084848652722, 0.06256084848652722]
printing an ep nov before normalisation:  65.79252580315504
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  67.9220306730116
printing an ep nov before normalisation:  67.32397936703283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  37.23474233327673
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  54.69151020050049
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  45.083630265733554
printing an ep nov before normalisation:  59.44384272578545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  43.60211291238797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  62.64475243574794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
siam score:  -0.8523271
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.348]
 [36.594]
 [50.269]
 [51.982]
 [36.357]] [[0.566]
 [0.322]
 [0.633]
 [0.672]
 [0.316]]
printing an ep nov before normalisation:  41.802873611450195
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.01564886845942
printing an ep nov before normalisation:  52.649919289141906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.90375367967628
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.76]
 [52.76]
 [52.76]
 [52.76]
 [52.76]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
actions average: 
K:  0  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9747,     0.0005,     0.0011,     0.0228],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0002,     0.9993,     0.0003,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0046,     0.0005,     0.0126,     0.7991,     0.1832],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0438, 0.0059, 0.0293, 0.1988, 0.7222], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.883]
 [36.883]
 [49.409]
 [36.883]
 [36.883]] [[0.945]
 [0.945]
 [1.513]
 [0.945]
 [0.945]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.574]
 [82.574]
 [82.574]
 [97.884]
 [82.574]] [[1.042]
 [1.042]
 [1.042]
 [1.295]
 [1.042]]
printing an ep nov before normalisation:  37.962919082036954
actions average: 
K:  1  action  0 :  tensor([    0.9842,     0.0001,     0.0000,     0.0088,     0.0070],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9914,     0.0022,     0.0000,     0.0064],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0013,     0.9508,     0.0288,     0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0001,     0.0010,     0.8349,     0.1638],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0037, 0.0018, 0.2037, 0.7899], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.971508644886086
printing an ep nov before normalisation:  40.36204462026105
printing an ep nov before normalisation:  54.5075220145276
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.346]
 [47.909]
 [48.55 ]
 [48.721]
 [46.325]] [[1.136]
 [1.187]
 [1.209]
 [1.214]
 [1.135]]
actions average: 
K:  3  action  0 :  tensor([    0.9897,     0.0007,     0.0000,     0.0039,     0.0057],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9973,     0.0001,     0.0001,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.9672,     0.0125,     0.0200],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0136,     0.7611,     0.2250],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0077, 0.0728, 0.0947, 0.2104, 0.6144], grad_fn=<DivBackward0>)
siam score:  -0.8516037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  17.296040058135986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  79.4253668756748
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  46.308087702865
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  44.83431226123218
printing an ep nov before normalisation:  48.605155996468895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
from probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.57197454028714
siam score:  -0.842381
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.021]
 [51.021]
 [58.029]
 [51.021]
 [51.021]] [[0.997]
 [0.997]
 [1.196]
 [0.997]
 [0.997]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.614]
 [42.255]
 [46.62 ]
 [36.772]
 [43.103]] [[0.847]
 [0.76 ]
 [0.922]
 [0.556]
 [0.791]]
printing an ep nov before normalisation:  55.68492365761578
printing an ep nov before normalisation:  65.05067538667362
actions average: 
K:  3  action  0 :  tensor([    0.9958,     0.0017,     0.0000,     0.0015,     0.0010],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9655,     0.0011,     0.0000,     0.0324],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0009,     0.0002,     0.9255,     0.0576,     0.0158],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0045, 0.0140, 0.0391, 0.7361, 0.2063], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0058, 0.1045, 0.1277, 0.0954, 0.6666], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 34.35341593456076
printing an ep nov before normalisation:  37.126408627216044
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  33.180444777262416
printing an ep nov before normalisation:  49.316646290223
siam score:  -0.849327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  39.916693064073925
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.7377577581064543
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9894,     0.0019,     0.0000,     0.0014,     0.0073],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9666,     0.0008,     0.0000,     0.0320],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9970,     0.0014,     0.0016],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0002,     0.0178,     0.7307,     0.2505],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0014, 0.0187, 0.0283, 0.2224, 0.7292], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  96.14191664467688
line 256 mcts: sample exp_bonus 50.06515084594943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.345]
 [72.297]
 [67.498]
 [71.584]
 [71.584]] [[0.643]
 [0.899]
 [0.817]
 [0.886]
 [0.886]]
printing an ep nov before normalisation:  30.424074569455748
printing an ep nov before normalisation:  46.67302273489689
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
actions average: 
K:  1  action  0 :  tensor([    0.9892,     0.0005,     0.0000,     0.0069,     0.0034],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9969,     0.0003,     0.0000,     0.0026],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9666,     0.0303,     0.0027],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0009, 0.0013, 0.1116, 0.7268, 0.1595], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0002,     0.0042,     0.0433,     0.2053,     0.7471],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[66.178]
 [53.011]
 [41.233]
 [44.224]
 [52.12 ]] [[0.781]
 [0.477]
 [0.207]
 [0.276]
 [0.458]]
printing an ep nov before normalisation:  50.115637892343265
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.001]
 [0.001]
 [0.002]] [[55.209]
 [55.209]
 [50.59 ]
 [59.479]
 [55.209]] [[0.93 ]
 [0.93 ]
 [0.8  ]
 [1.047]
 [0.93 ]]
actions average: 
K:  2  action  0 :  tensor([    0.9980,     0.0001,     0.0000,     0.0008,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0219, 0.9339, 0.0015, 0.0037, 0.0389], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0020,     0.9490,     0.0204,     0.0285],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0016,     0.0037,     0.0004,     0.7645,     0.2297],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0215, 0.0098, 0.0255, 0.1821, 0.7612], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  81.67127950963496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  61.00350645876895
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8677618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  31.048028012442245
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.692226958844145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.47841872864946
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.8804994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  64.87950970356995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[46.103]
 [46.103]
 [46.103]
 [46.103]
 [46.103]] [[1.001]
 [1.001]
 [1.001]
 [1.001]
 [1.001]]
printing an ep nov before normalisation:  43.85300353099611
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  64.49369396685199
using explorer policy with actor:  1
siam score:  -0.8830703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[106.232]
 [101.615]
 [111.01 ]
 [106.693]
 [106.232]] [[1.27 ]
 [1.209]
 [1.334]
 [1.277]
 [1.27 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  39.697948884360244
printing an ep nov before normalisation:  45.08146740045689
printing an ep nov before normalisation:  59.62421825565093
printing an ep nov before normalisation:  0.17683444933425108
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.40359474026463
printing an ep nov before normalisation:  41.36794797712704
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[45.462]
 [37.382]
 [37.382]
 [34.856]
 [41.488]] [[0.874]
 [0.587]
 [0.587]
 [0.497]
 [0.732]]
printing an ep nov before normalisation:  45.62909414738739
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
line 256 mcts: sample exp_bonus 33.34511322942672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Sims:  40 1 epoch:  79885 pick best:  False frame count:  79885
printing an ep nov before normalisation:  44.20242321646611
printing an ep nov before normalisation:  62.93469975436484
printing an ep nov before normalisation:  59.099130233001176
from probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.005]
 [0.022]
 [0.002]] [[31.013]
 [39.025]
 [43.538]
 [37.398]
 [26.273]] [[0.594]
 [0.837]
 [0.976]
 [0.807]
 [0.453]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  39.668173741426855
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
line 256 mcts: sample exp_bonus 30.389013721977765
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[43.038]
 [43.88 ]
 [73.049]
 [58.844]
 [60.536]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  41.84196746842382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  27.321842054680168
using explorer policy with actor:  0
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  53.91763453816059
siam score:  -0.8779237
printing an ep nov before normalisation:  40.80462342309421
printing an ep nov before normalisation:  35.601715752749755
printing an ep nov before normalisation:  31.52066478055007
printing an ep nov before normalisation:  23.854868412017822
printing an ep nov before normalisation:  71.4928605812887
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[39.591]
 [41.274]
 [37.882]
 [30.746]
 [34.584]] [[0.001]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
line 256 mcts: sample exp_bonus 64.7504461665482
printing an ep nov before normalisation:  37.39250147419487
siam score:  -0.8780706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
line 256 mcts: sample exp_bonus 28.512017626417624
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[26.028]
 [23.063]
 [23.063]
 [23.063]
 [23.063]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  40.31491148056468
printing an ep nov before normalisation:  37.08666072196044
printing an ep nov before normalisation:  26.563294177181394
printing an ep nov before normalisation:  33.90872724222199
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.552]
 [51.939]
 [62.839]
 [61.782]
 [63.207]] [[0.758]
 [0.604]
 [0.967]
 [0.932]
 [0.979]]
printing an ep nov before normalisation:  26.585724338034687
printing an ep nov before normalisation:  40.13295650482178
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  27.740862198213367
printing an ep nov before normalisation:  30.832930631916252
using another actor
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.604]
 [23.869]
 [32.008]
 [21.604]
 [21.604]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.923561827424084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  0
printing an ep nov before normalisation:  27.87572145462036
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[54.429]
 [58.676]
 [75.679]
 [60.123]
 [57.46 ]] [[0.717]
 [0.84 ]
 [1.334]
 [0.882]
 [0.805]]
printing an ep nov before normalisation:  27.495645676609396
printing an ep nov before normalisation:  41.69980880123448
printing an ep nov before normalisation:  48.6010026971245
printing an ep nov before normalisation:  39.31027889251709
printing an ep nov before normalisation:  41.277070037941364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.72 ]
 [36.858]
 [29.643]
 [33.292]
 [33.956]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.786]
 [26.786]
 [26.786]
 [26.786]
 [26.786]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  38.344110159605144
printing an ep nov before normalisation:  47.51434192788111
printing an ep nov before normalisation:  49.451100968774035
rdn probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.646]
 [72.243]
 [97.855]
 [91.235]
 [92.162]] [[0.727]
 [0.737]
 [1.136]
 [1.033]
 [1.048]]
siam score:  -0.88454926
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
UNIT TEST: sample policy line 217 mcts : [0.256 0.462 0.103 0.077 0.103]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  29.837543964385986
printing an ep nov before normalisation:  72.67843481075639
siam score:  -0.88257325
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[60.068]
 [60.068]
 [60.068]
 [60.068]
 [60.068]] [[1.084]
 [1.084]
 [1.084]
 [1.084]
 [1.084]]
printing an ep nov before normalisation:  28.969212091897653
printing an ep nov before normalisation:  31.789567288834704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using another actor
printing an ep nov before normalisation:  79.35775438568855
printing an ep nov before normalisation:  47.937136111626565
printing an ep nov before normalisation:  57.718733274244066
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[27.285]
 [28.747]
 [39.683]
 [29.362]
 [33.673]] [[0.382]
 [0.46 ]
 [1.042]
 [0.492]
 [0.722]]
printing an ep nov before normalisation:  60.7266297528704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
siam score:  -0.88228256
printing an ep nov before normalisation:  32.4940110413331
printing an ep nov before normalisation:  60.9019065638807
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  48.52255433241217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using another actor
printing an ep nov before normalisation:  53.3160024788524
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 49.69207377958869
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.015]
 [46.015]
 [46.015]
 [46.015]
 [46.015]] [[1.449]
 [1.449]
 [1.449]
 [1.449]
 [1.449]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
siam score:  -0.8902031
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.791920137519575
printing an ep nov before normalisation:  69.43188801162782
printing an ep nov before normalisation:  42.49608631958959
Printing some Q and Qe and total Qs values:  [[0.127]
 [0.126]
 [0.118]
 [0.124]
 [0.127]] [[32.539]
 [36.109]
 [40.231]
 [41.205]
 [31.812]] [[0.768]
 [0.932]
 [1.115]
 [1.166]
 [0.735]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
line 256 mcts: sample exp_bonus 46.22906077891625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
deleting a thread, now have 1 threads
Frames:  81502 train batches done:  9545 episodes:  3469
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8793337
siam score:  -0.8800899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.   ]
 [0.   ]] [[41.156]
 [41.156]
 [43.577]
 [41.156]
 [41.156]] [[1.278]
 [1.278]
 [1.354]
 [1.278]
 [1.278]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
siam score:  -0.87790686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  46.75793320845784
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[33.679]
 [33.679]
 [34.195]
 [33.679]
 [33.679]] [[1.296]
 [1.296]
 [1.334]
 [1.296]
 [1.296]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.157]
 [33.157]
 [33.157]
 [29.367]
 [33.157]] [[1.661]
 [1.661]
 [1.661]
 [1.333]
 [1.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  44.35982328111008
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[42.944]
 [42.944]
 [42.944]
 [42.944]
 [42.944]] [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
printing an ep nov before normalisation:  56.355322082501324
printing an ep nov before normalisation:  54.305593394743966
printing an ep nov before normalisation:  55.857696323561775
printing an ep nov before normalisation:  42.62608334626657
printing an ep nov before normalisation:  53.46657993178258
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  53.65115443443202
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06256090468203827, 0.06256090468203827, 0.06256090468203827, 0.6871954765898086, 0.06256090468203827, 0.06256090468203827]
printing an ep nov before normalisation:  44.16322562206386
actor:  1 policy actor:  1  step number:  120 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  36.5198498326454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  44.701223202514825
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.91199731826782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  44.00684804905221
printing an ep nov before normalisation:  46.48980506076657
printing an ep nov before normalisation:  39.08423256548444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.003]
 [0.   ]
 [0.002]] [[27.979]
 [26.627]
 [33.991]
 [26.575]
 [28.142]] [[0.869]
 [0.824]
 [1.069]
 [0.817]
 [0.872]]
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  3  action  0 :  tensor([    0.9914,     0.0014,     0.0000,     0.0011,     0.0060],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9502,     0.0235,     0.0001,     0.0260],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0001,     0.9517,     0.0238,     0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0324,     0.8723,     0.0951],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0073, 0.0775, 0.1869, 0.7275], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  45.053672428775485
printing an ep nov before normalisation:  33.73945884218176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  28.336935573154026
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.   ]
 [0.   ]
 [0.   ]] [[40.377]
 [50.177]
 [49.243]
 [49.243]
 [49.243]] [[0.635]
 [1.001]
 [0.966]
 [0.966]
 [0.966]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  53.49853740192249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  43.8253739663182
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0064,     0.9734,     0.0002,     0.0003,     0.0197],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0468,     0.8812,     0.0330,     0.0388],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0076,     0.0006,     0.0397,     0.8623,     0.0898],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1112, 0.0342, 0.0245, 0.3153, 0.5147], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  94.91982470906049
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.726]
 [53.253]
 [53.253]
 [53.253]
 [53.253]] [[1.234]
 [1.328]
 [1.328]
 [1.328]
 [1.328]]
actions average: 
K:  4  action  0 :  tensor([    0.9981,     0.0000,     0.0000,     0.0002,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0235,     0.8744,     0.0008,     0.0004,     0.1009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0059,     0.9245,     0.0435,     0.0261],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0027,     0.0002,     0.0548,     0.7997,     0.1427],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0451, 0.0162, 0.0011, 0.1967, 0.7410], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.03 ]
 [0.003]
 [0.001]
 [0.004]] [[38.123]
 [41.491]
 [43.251]
 [34.909]
 [41.29 ]] [[0.927]
 [1.128]
 [1.194]
 [0.754]
 [1.092]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.17957586219208
printing an ep nov before normalisation:  29.965500205958687
printing an ep nov before normalisation:  29.163764777677503
printing an ep nov before normalisation:  43.75430935250037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  38.94227150292204
printing an ep nov before normalisation:  21.934606156962918
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  23.038092415608737
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8531862
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  36.13338327529394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  29.40178952491107
printing an ep nov before normalisation:  47.55095502752386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  22.772656447532565
printing an ep nov before normalisation:  38.98682079193905
printing an ep nov before normalisation:  42.73345703384631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  50.42596741018706
printing an ep nov before normalisation:  51.097380226921096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9985,     0.0000,     0.0000,     0.0001,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0015,     0.9642,     0.0124,     0.0001,     0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0246, 0.0085, 0.9468, 0.0083, 0.0118], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0394, 0.0017, 0.0346, 0.6723, 0.2519], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0006,     0.0943,     0.0140,     0.2436,     0.6475],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.85867983
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  62.79748311553261
printing an ep nov before normalisation:  45.322984905429884
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  4.770497383788098e-05
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8578564
siam score:  -0.85786927
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.402662406513635
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 46.94129283078364
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.80756488447583
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[61.03]
 [61.03]
 [61.03]
 [61.03]
 [61.03]] [[0.994]
 [0.994]
 [0.994]
 [0.994]
 [0.994]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[73.15]
 [73.15]
 [73.15]
 [73.15]
 [73.15]] [[1.239]
 [1.239]
 [1.239]
 [1.239]
 [1.239]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.8591143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  30.535183198064686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.862323
printing an ep nov before normalisation:  30.98729372024536
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.195]
 [62.195]
 [62.195]
 [62.195]
 [62.195]] [[1.273]
 [1.273]
 [1.273]
 [1.273]
 [1.273]]
using another actor
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  58.56409849438295
printing an ep nov before normalisation:  34.267919063568115
line 256 mcts: sample exp_bonus 31.055815969163856
printing an ep nov before normalisation:  26.63151026376607
siam score:  -0.86974615
printing an ep nov before normalisation:  41.22410962499045
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  36.42247854823064
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  4  action  0 :  tensor([    0.9875,     0.0005,     0.0020,     0.0035,     0.0065],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0168,     0.9596,     0.0001,     0.0002,     0.0233],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0030,     0.9755,     0.0054,     0.0156],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0008,     0.0003,     0.1876,     0.6601,     0.1512],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0353, 0.0022, 0.1711, 0.2444, 0.5471], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.087204031136295
printing an ep nov before normalisation:  36.95409628290004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  41.38464088841401
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.87511355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  1  action  0 :  tensor([    0.9981,     0.0001,     0.0006,     0.0003,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9969,     0.0001,     0.0000,     0.0029],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0010,     0.9815,     0.0004,     0.0171],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0004,     0.0027,     0.8710,     0.1255],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0523, 0.0365, 0.2253, 0.6844], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  29.268932342529297
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  33.31748008728027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.886094
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.71100819477915
line 256 mcts: sample exp_bonus 97.62623173085407
printing an ep nov before normalisation:  27.348067339417774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  1  action  0 :  tensor([    0.9773,     0.0002,     0.0000,     0.0088,     0.0137],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0027,     0.9514,     0.0125,     0.0000,     0.0334],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0007,     0.0033,     0.9757,     0.0012,     0.0191],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0280,     0.8580,     0.1138],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0017, 0.0342, 0.0307, 0.0831, 0.8503], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
printing an ep nov before normalisation:  75.68661390208482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  46.04680325794921
printing an ep nov before normalisation:  43.852988772158504
printing an ep nov before normalisation:  41.569271087646484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.88826644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.11492154885036
printing an ep nov before normalisation:  62.8429685320173
printing an ep nov before normalisation:  48.409700407834855
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]] [[24.456]
 [23.24 ]
 [25.803]
 [23.626]
 [23.385]] [[0.   ]
 [0.   ]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  45.490852602744894
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[40.924]
 [40.924]
 [44.891]
 [40.924]
 [40.924]] [[1.086]
 [1.086]
 [1.242]
 [1.086]
 [1.086]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.   ]
 [0.002]
 [0.   ]] [[51.294]
 [51.294]
 [47.975]
 [51.294]
 [49.223]] [[1.253]
 [1.253]
 [1.132]
 [1.253]
 [1.176]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[51.09 ]
 [51.09 ]
 [56.192]
 [50.514]
 [51.09 ]] [[1.024]
 [1.024]
 [1.155]
 [1.009]
 [1.024]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  59.470592314239006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
line 256 mcts: sample exp_bonus 56.5061859661684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  87.34013317121757
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9907,     0.0032,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0094,     0.9687,     0.0012,     0.0207],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0020,     0.0002,     0.0163,     0.8033,     0.1782],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.0024, 0.1885, 0.2303, 0.5775], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[25.306]
 [31.44 ]
 [45.072]
 [40.334]
 [41.606]] [[0.34 ]
 [0.529]
 [0.95 ]
 [0.804]
 [0.843]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  38.335710864062435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.9071495
printing an ep nov before normalisation:  39.29618532466065
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.003]
 [0.004]
 [0.004]
 [0.003]]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.004]
 [0.003]
 [0.003]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.004]
 [0.003]
 [0.003]
 [0.004]]
siam score:  -0.9047096
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.014054873530178
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[60.731]
 [50.323]
 [50.323]
 [50.323]
 [50.323]] [[0.67 ]
 [0.483]
 [0.483]
 [0.483]
 [0.483]]
actions average: 
K:  4  action  0 :  tensor([    0.9773,     0.0001,     0.0000,     0.0112,     0.0114],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9469,     0.0126,     0.0000,     0.0399],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0048,     0.9764,     0.0014,     0.0174],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0034, 0.0009, 0.0321, 0.8020, 0.1616], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0125, 0.0436, 0.0377, 0.2082, 0.6980], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[23.438]
 [36.333]
 [48.332]
 [52.792]
 [52.792]] [[0.445]
 [0.907]
 [1.337]
 [1.496]
 [1.496]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.003]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.001]
 [0.002]
 [0.003]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  32.64259497324626
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.41798159803432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  38.94807872614162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
from probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.024]
 [0.036]
 [0.036]
 [0.036]] [[63.4  ]
 [67.126]
 [60.548]
 [60.548]
 [60.548]] [[1.244]
 [1.357]
 [1.18 ]
 [1.18 ]
 [1.18 ]]
printing an ep nov before normalisation:  38.528334136123455
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.018]
 [0.019]
 [0.018]
 [0.014]] [[24.49 ]
 [35.278]
 [36.645]
 [31.093]
 [32.009]] [[0.5  ]
 [1.033]
 [1.102]
 [0.828]
 [0.868]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  59.16591070566481
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.015]
 [0.022]
 [0.019]
 [0.019]] [[59.184]
 [57.453]
 [48.395]
 [53.393]
 [53.393]] [[1.245]
 [1.196]
 [0.946]
 [1.085]
 [1.085]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  5.5250909241863155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
siam score:  -0.9132624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  62.89891383029362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  3  action  0 :  tensor([    0.9982,     0.0001,     0.0000,     0.0003,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9937,     0.0007,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0009,     0.9491,     0.0274,     0.0225],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0112,     0.0084,     0.8634,     0.1167],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0018, 0.1089, 0.1725, 0.1505, 0.5664], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[32.732]
 [32.732]
 [32.732]
 [32.732]
 [32.732]] [[1.206]
 [1.206]
 [1.206]
 [1.206]
 [1.206]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.005]
 [0.005]
 [0.004]] [[60.409]
 [56.133]
 [67.178]
 [65.667]
 [65.605]] [[0.941]
 [0.825]
 [1.123]
 [1.082]
 [1.08 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
actions average: 
K:  4  action  0 :  tensor([    0.9828,     0.0048,     0.0001,     0.0049,     0.0074],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0044,     0.9822,     0.0001,     0.0000,     0.0133],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0017,     0.9653,     0.0006,     0.0324],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0004,     0.0177,     0.8440,     0.1378],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0019, 0.1743, 0.0704, 0.2268, 0.5266], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  32.05265998840332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  41.07672680437573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.004]
 [0.005]
 [0.004]
 [0.005]] [[33.683]
 [46.333]
 [45.994]
 [40.526]
 [33.39 ]] [[0.492]
 [0.832]
 [0.824]
 [0.676]
 [0.485]]
printing an ep nov before normalisation:  52.453455820207466
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  58.50396355513072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.755070347057035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[34.09]
 [34.09]
 [34.09]
 [34.09]
 [34.09]] [[0.764]
 [0.764]
 [0.764]
 [0.764]
 [0.764]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.005]
 [0.005]
 [0.005]] [[35.068]
 [50.003]
 [58.066]
 [41.044]
 [45.405]] [[0.163]
 [0.601]
 [0.835]
 [0.337]
 [0.465]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.013]
 [0.018]
 [0.015]
 [0.013]] [[62.879]
 [66.446]
 [59.628]
 [57.735]
 [57.779]] [[0.973]
 [1.054]
 [0.901]
 [0.855]
 [0.854]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  51.66387424690849
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.55535408997216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255387977295099, 0.06255387977295099, 0.06255387977295099, 0.6872306011352451, 0.06255387977295099, 0.06255387977295099]
printing an ep nov before normalisation:  42.06153874770472
printing an ep nov before normalisation:  39.59058762100865
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.07]
 [0.07]
 [0.07]
 [0.11]
 [0.07]] [[29.182]
 [29.182]
 [29.182]
 [43.973]
 [29.182]] [[0.559]
 [0.559]
 [0.559]
 [1.071]
 [0.559]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.004]
 [0.004]
 [0.004]] [[67.676]
 [57.773]
 [69.139]
 [67.676]
 [67.676]] [[1.296]
 [1.017]
 [1.337]
 [1.296]
 [1.296]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.9363337
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.004]] [[28.466]
 [34.537]
 [33.61 ]
 [27.395]
 [31.769]] [[1.078]
 [1.307]
 [1.273]
 [1.037]
 [1.203]]
printing an ep nov before normalisation:  28.682031155450503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  46.76284307161991
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using another actor
printing an ep nov before normalisation:  53.61039284826037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
UNIT TEST: sample policy line 217 mcts : [0.205 0.154 0.231 0.282 0.128]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.03862190246582
printing an ep nov before normalisation:  39.82414797958076
printing an ep nov before normalisation:  50.89559061568739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using explorer policy with actor:  1
printing an ep nov before normalisation:  68.79005171035851
printing an ep nov before normalisation:  57.37626964884038
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.92360777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  30.636868476867676
printing an ep nov before normalisation:  42.511851594499404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  32.68801434528475
printing an ep nov before normalisation:  35.97976788209796
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9339,     0.0127,     0.0001,     0.0532],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0128,     0.9504,     0.0117,     0.0251],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0012,     0.0005,     0.0290,     0.7857,     0.1836],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0041, 0.0525, 0.1122, 0.1973, 0.6339], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[47.953]
 [42.419]
 [37.605]
 [42.419]
 [42.419]] [[1.157]
 [0.938]
 [0.745]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.001]
 [0.004]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.001]
 [0.004]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
from probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.9308685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  29.354825019836426
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.005]
 [0.008]
 [0.007]
 [0.005]] [[38.688]
 [39.959]
 [43.32 ]
 [41.479]
 [39.959]] [[0.758]
 [0.808]
 [0.94 ]
 [0.868]
 [0.808]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.94251366103964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[66.124]
 [66.124]
 [66.124]
 [66.124]
 [66.124]] [[0.707]
 [0.707]
 [0.707]
 [0.707]
 [0.707]]
printing an ep nov before normalisation:  60.26414583754681
printing an ep nov before normalisation:  45.15744476790925
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[54.925]
 [52.766]
 [58.456]
 [54.925]
 [54.925]] [[1.195]
 [1.107]
 [1.338]
 [1.195]
 [1.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.22641086578369
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  41.22520694357428
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  47.967259742542645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.92698306
printing an ep nov before normalisation:  32.81204893617257
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.90310958541859
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[31.124]
 [38.232]
 [35.65 ]
 [32.822]
 [31.114]] [[0.573]
 [0.838]
 [0.742]
 [0.637]
 [0.573]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  34.41217555527657
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
actions average: 
K:  1  action  0 :  tensor([    0.9964,     0.0004,     0.0000,     0.0020,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9842,     0.0020,     0.0006,     0.0132],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9248,     0.0468,     0.0282],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0010,     0.0066,     0.8671,     0.1253],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0579,     0.0979,     0.2097,     0.6341],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.91958785
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.005]] [[44.501]
 [46.098]
 [44.149]
 [43.674]
 [45.042]] [[0.636]
 [0.678]
 [0.627]
 [0.615]
 [0.65 ]]
printing an ep nov before normalisation:  0.006154718937523285
printing an ep nov before normalisation:  28.33103785961947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  32.827097797603685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  41.882827645631785
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.4827331628592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
actions average: 
K:  2  action  0 :  tensor([    0.9993,     0.0002,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0095,     0.9615,     0.0001,     0.0000,     0.0289],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0186,     0.9351,     0.0190,     0.0271],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0002,     0.0526,     0.7746,     0.1725],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0033, 0.0796, 0.0943, 0.1942, 0.6287], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.52442705662449
printing an ep nov before normalisation:  46.31921924514088
from probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  44.207023434481975
printing an ep nov before normalisation:  61.267999069258686
printing an ep nov before normalisation:  44.93606768478455
printing an ep nov before normalisation:  23.956737518310547
printing an ep nov before normalisation:  30.469100412399023
printing an ep nov before normalisation:  48.841145422549104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  0.012315472626767132
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  71.26880023300009
printing an ep nov before normalisation:  62.56092669551483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.92372495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  28.895978722355963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.004]] [[32.053]
 [32.805]
 [34.474]
 [29.086]
 [29.564]] [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.9238609
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  38.11628818511963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[39.632]
 [28.806]
 [28.806]
 [28.806]
 [28.806]] [[0.952]
 [0.558]
 [0.558]
 [0.558]
 [0.558]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using another actor
printing an ep nov before normalisation:  43.319762974954585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  37.348566237072816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  40.78094692256706
siam score:  -0.9310979
printing an ep nov before normalisation:  0.013824078101671538
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.93070036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  34.739181995391846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.012]
 [0.01 ]
 [0.013]
 [0.013]] [[39.238]
 [45.104]
 [59.583]
 [39.238]
 [39.238]] [[0.646]
 [0.765]
 [1.057]
 [0.646]
 [0.646]]
printing an ep nov before normalisation:  45.74078504417607
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.571236102080558
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  55.22451950611321
from probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
line 256 mcts: sample exp_bonus 48.87827868986354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  70.84464903786055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.9413477
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.013]
 [0.013]
 [0.012]] [[36.984]
 [40.569]
 [28.321]
 [25.718]
 [26.289]] [[0.604]
 [0.691]
 [0.397]
 [0.334]
 [0.347]]
printing an ep nov before normalisation:  46.24700786766004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  44.483232707910744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  40.174300368048364
Starting evaluation
printing an ep nov before normalisation:  39.04795381757948
printing an ep nov before normalisation:  47.038565487374456
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.006]
 [0.009]
 [0.009]
 [0.008]] [[53.856]
 [56.382]
 [38.281]
 [40.671]
 [46.043]] [[0.008]
 [0.006]
 [0.009]
 [0.009]
 [0.008]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[12.812]
 [ 5.551]
 [ 9.708]
 [ 7.558]
 [10.056]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  42.78274024111228
printing an ep nov before normalisation:  49.21802401340909
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.001]
 [0.008]
 [0.008]
 [0.008]] [[41.218]
 [34.939]
 [51.178]
 [49.02 ]
 [48.369]] [[0.007]
 [0.001]
 [0.008]
 [0.008]
 [0.008]]
printing an ep nov before normalisation:  37.54746239140163
printing an ep nov before normalisation:  43.66137900069536
line 256 mcts: sample exp_bonus 35.510971368712504
printing an ep nov before normalisation:  40.257704897012054
printing an ep nov before normalisation:  43.553512115813085
printing an ep nov before normalisation:  102.53284506036711
printing an ep nov before normalisation:  41.21713706945427
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.006]
 [0.008]
 [0.008]
 [0.009]] [[44.144]
 [38.811]
 [38.39 ]
 [40.657]
 [42.581]] [[0.008]
 [0.006]
 [0.008]
 [0.008]
 [0.009]]
using explorer policy with actor:  0
actions average: 
K:  4  action  0 :  tensor([    0.9950,     0.0010,     0.0008,     0.0011,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9968,     0.0004,     0.0001,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0401,     0.8924,     0.0254,     0.0421],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0002,     0.0372,     0.8617,     0.1008],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0007, 0.0906, 0.1438, 0.1183, 0.6466], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.01 ]] [[46.583]
 [46.27 ]
 [58.405]
 [52.016]
 [50.95 ]] [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.01 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  42.78896094820038
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.2685755183907
using explorer policy with actor:  0
siam score:  -0.94468856
printing an ep nov before normalisation:  39.73078752235307
using explorer policy with actor:  0
printing an ep nov before normalisation:  41.36612990208086
printing an ep nov before normalisation:  41.06516417093128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  45.83692608541769
printing an ep nov before normalisation:  35.12396964146283
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
line 256 mcts: sample exp_bonus 29.324747477115324
using explorer policy with actor:  0
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.554283618927
printing an ep nov before normalisation:  26.42387628555298
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.001]
 [0.008]
 [0.009]
 [0.007]] [[40.45 ]
 [34.246]
 [31.526]
 [39.481]
 [35.032]] [[0.008]
 [0.001]
 [0.008]
 [0.009]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.54367439468299
printing an ep nov before normalisation:  31.936336831375087
rdn probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.01 ]
 [0.01 ]
 [0.012]
 [0.011]] [[46.444]
 [40.234]
 [40.234]
 [39.029]
 [38.424]] [[0.942]
 [0.729]
 [0.729]
 [0.691]
 [0.668]]
printing an ep nov before normalisation:  56.24132423641039
printing an ep nov before normalisation:  39.17904921367169
printing an ep nov before normalisation:  42.43622564310314
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0098,     0.9685,     0.0078,     0.0000,     0.0138],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0078, 0.0013, 0.9030, 0.0357, 0.0522], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0010,     0.0003,     0.0204,     0.9018,     0.0765],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0093, 0.0018, 0.0587, 0.2096, 0.7207], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
siam score:  -0.93756914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
printing an ep nov before normalisation:  67.50780842806833
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[40.832]
 [39.29 ]
 [40.199]
 [39.29 ]
 [39.29 ]] [[1.332]
 [1.25 ]
 [1.298]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.01 ]
 [0.011]
 [0.01 ]
 [0.01 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.01 ]
 [0.011]
 [0.01 ]
 [0.01 ]]
printing an ep nov before normalisation:  57.263430043270105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
line 256 mcts: sample exp_bonus 54.00581887549134
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255153792590341, 0.06255153792590341, 0.06255153792590341, 0.687242310370483, 0.06255153792590341, 0.06255153792590341]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[41.98]
 [48.61]
 [41.98]
 [41.98]
 [41.98]] [[1.036]
 [1.336]
 [1.036]
 [1.036]
 [1.036]]
siam score:  -0.9363284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.01 ]
 [0.009]
 [0.007]] [[40.391]
 [30.758]
 [30.5  ]
 [31.715]
 [33.364]] [[0.819]
 [0.52 ]
 [0.514]
 [0.551]
 [0.601]]
printing an ep nov before normalisation:  41.95846295765285
printing an ep nov before normalisation:  1.0180223171118996e-05
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.   ]
 [0.008]
 [0.009]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.   ]
 [0.008]
 [0.009]
 [0.007]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.01 ]
 [0.008]
 [0.008]] [[36.805]
 [42.347]
 [45.049]
 [34.837]
 [36.328]] [[0.611]
 [0.782]
 [0.866]
 [0.55 ]
 [0.596]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  55.2451928548462
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[56.01]
 [56.01]
 [56.01]
 [56.01]
 [56.01]] [[1.322]
 [1.322]
 [1.322]
 [1.322]
 [1.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.93850374
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.4897462844653
actions average: 
K:  2  action  0 :  tensor([    0.9985,     0.0010,     0.0004,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9529,     0.0137,     0.0001,     0.0331],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0002,     0.9371,     0.0298,     0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0301,     0.0002,     0.0269,     0.8608,     0.0819],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0334, 0.0261, 0.0791, 0.2021, 0.6593], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  81.49157463588763
actions average: 
K:  3  action  0 :  tensor([    0.9972,     0.0001,     0.0000,     0.0011,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0325,     0.9351,     0.0141,     0.0001,     0.0182],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0571,     0.8747,     0.0128,     0.0551],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0050,     0.0003,     0.0673,     0.7118,     0.2156],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0135, 0.0566, 0.1979, 0.7310], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
line 256 mcts: sample exp_bonus 51.02992556975661
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
line 256 mcts: sample exp_bonus 34.01596854646173
siam score:  -0.9381511
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9392424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  49.29153599031115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.011]
 [0.012]
 [0.013]
 [0.012]] [[27.017]
 [29.135]
 [38.398]
 [31.357]
 [30.783]] [[0.411]
 [0.499]
 [0.889]
 [0.594]
 [0.569]]
printing an ep nov before normalisation:  48.06954697631093
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  70.22822189028516
actions average: 
K:  0  action  0 :  tensor([    0.9985,     0.0001,     0.0000,     0.0000,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9943,     0.0000,     0.0000,     0.0055],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0040,     0.9618,     0.0037,     0.0299],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0011,     0.0006,     0.0307,     0.8137,     0.1539],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0016, 0.0277, 0.1156, 0.1903, 0.6648], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  32.895683131470946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.6817583449551
printing an ep nov before normalisation:  59.24705843109222
printing an ep nov before normalisation:  38.30744072369749
siam score:  -0.9426357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.50388326117893
siam score:  -0.9393039
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9381655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  50.938235432176064
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.018]
 [0.016]
 [0.015]
 [0.018]] [[46.664]
 [46.664]
 [54.292]
 [52.207]
 [46.664]] [[0.885]
 [0.885]
 [1.109]
 [1.046]
 [0.885]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  71.7718728813331
siam score:  -0.9456433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  41.84754454759541
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.014]
 [0.014]
 [0.014]
 [0.014]] [[48.774]
 [45.823]
 [45.823]
 [45.823]
 [45.823]] [[1.19 ]
 [1.064]
 [1.064]
 [1.064]
 [1.064]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
printing an ep nov before normalisation:  40.229640045833804
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.718]
 [0.895]
 [0.943]
 [0.818]
 [0.858]] [[54.026]
 [58.52 ]
 [50.17 ]
 [54.649]
 [53.378]] [[1.884]
 [2.228]
 [1.967]
 [2.008]
 [2.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06255036696287218, 0.06255036696287218, 0.06255036696287218, 0.6872481651856394, 0.06255036696287218, 0.06255036696287218]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.60313599525361
using another actor
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0001,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9860,     0.0002,     0.0000,     0.0136],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0107,     0.9287,     0.0271,     0.0334],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0031,     0.0009,     0.8882,     0.1076],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0012, 0.0048, 0.1456, 0.2233, 0.6251], grad_fn=<DivBackward0>)
from probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  47.50737767334433
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.011]
 [0.011]
 [0.011]] [[55.802]
 [29.31 ]
 [27.234]
 [26.751]
 [26.991]] [[0.503]
 [0.201]
 [0.178]
 [0.173]
 [0.175]]
printing an ep nov before normalisation:  44.22877734043663
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.13623574458671
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.011]
 [0.011]
 [0.011]
 [0.011]] [[39.888]
 [39.888]
 [39.888]
 [39.888]
 [39.888]] [[0.608]
 [0.608]
 [0.608]
 [0.608]
 [0.608]]
printing an ep nov before normalisation:  42.132568359375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.8928,     0.0266,     0.0002,     0.0797],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0129,     0.8776,     0.0482,     0.0610],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0001,     0.8431,     0.1566],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0090, 0.0020, 0.0989, 0.1949, 0.6951], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.293]
 [0.307]
 [0.285]
 [0.307]
 [0.294]] [[52.582]
 [43.325]
 [38.142]
 [43.325]
 [43.6  ]] [[1.158]
 [0.857]
 [0.66 ]
 [0.857]
 [0.854]]
printing an ep nov before normalisation:  41.8516705433434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  47.16221205654371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  39.57311331381384
siam score:  -0.93900084
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  39.59547762792383
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  61.641964346374984
printing an ep nov before normalisation:  46.55158274860166
printing an ep nov before normalisation:  58.3280265060179
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.015]
 [0.013]
 [0.014]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.015]
 [0.013]
 [0.014]
 [0.013]]
actions average: 
K:  3  action  0 :  tensor([0.9761, 0.0010, 0.0011, 0.0017, 0.0200], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0359, 0.8813, 0.0046, 0.0078, 0.0703], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0016,     0.8864,     0.0302,     0.0817],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0157, 0.0029, 0.0491, 0.7131, 0.2192], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0278, 0.0708, 0.0463, 0.3065, 0.5485], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
using another actor
from probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[44.59 ]
 [51.376]
 [44.59 ]
 [44.59 ]
 [44.59 ]] [[1.055]
 [1.313]
 [1.055]
 [1.055]
 [1.055]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.013]
 [0.009]] [[44.382]
 [44.382]
 [44.382]
 [49.783]
 [44.382]] [[1.272]
 [1.272]
 [1.272]
 [1.508]
 [1.272]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  41.70192920393113
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
actions average: 
K:  4  action  0 :  tensor([    0.9953,     0.0013,     0.0000,     0.0013,     0.0020],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9092,     0.0209,     0.0004,     0.0689],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0136,     0.9059,     0.0158,     0.0647],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0400,     0.0004,     0.0018,     0.8009,     0.1569],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0102, 0.0026, 0.0026, 0.1856, 0.7990], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.755966821687014
printing an ep nov before normalisation:  45.8840938298039
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.6373074679551
from probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.27877379192193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.977041957483564
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.024]
 [0.019]
 [0.019]
 [0.032]] [[36.64 ]
 [35.267]
 [31.09 ]
 [31.09 ]
 [34.94 ]] [[1.421]
 [1.328]
 [1.037]
 [1.037]
 [1.314]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
siam score:  -0.9321237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  27.461257772165826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  45.31996182561562
printing an ep nov before normalisation:  24.28713321685791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  62.757474416899065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.69854003316756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.009]
 [0.011]
 [0.01 ]
 [0.01 ]] [[37.741]
 [32.474]
 [32.961]
 [32.701]
 [38.472]] [[0.908]
 [0.684]
 [0.706]
 [0.694]
 [0.939]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.01 ]
 [0.011]
 [0.01 ]
 [0.01 ]] [[45.586]
 [42.317]
 [47.078]
 [40.999]
 [40.999]] [[1.192]
 [1.015]
 [1.275]
 [0.943]
 [0.943]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  18.52953440643016
printing an ep nov before normalisation:  21.560134189403193
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9325464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  33.552608489990234
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
UNIT TEST: sample policy line 217 mcts : [0.154 0.154 0.205 0.256 0.231]
printing an ep nov before normalisation:  50.99651089772322
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.008]] [[57.275]
 [74.133]
 [69.376]
 [77.735]
 [74.133]] [[0.636]
 [1.061]
 [0.941]
 [1.153]
 [1.061]]
printing an ep nov before normalisation:  76.93004867027358
printing an ep nov before normalisation:  79.71874571991434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  32.995117859782795
actions average: 
K:  0  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9944,     0.0001,     0.0000,     0.0054],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0011, 0.0075, 0.9587, 0.0016, 0.0312], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0022,     0.0003,     0.0268,     0.7676,     0.2030],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0289, 0.0806, 0.0838, 0.2023, 0.6044], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.01 ]
 [0.01 ]] [[33.518]
 [34.812]
 [31.446]
 [34.   ]
 [33.353]] [[1.167]
 [1.255]
 [1.026]
 [1.201]
 [1.156]]
siam score:  -0.9368068
siam score:  -0.936302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  37.94507707864293
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  56.115731943429154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  73.05313982334265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
actions average: 
K:  4  action  0 :  tensor([    0.9686,     0.0165,     0.0000,     0.0011,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9432,     0.0006,     0.0010,     0.0550],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9566,     0.0049,     0.0384],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0007,     0.0411,     0.7868,     0.1708],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0006, 0.0637, 0.0896, 0.2810, 0.5651], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
siam score:  -0.9371771
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  37.58110113074289
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.67717286256195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.001]
 [0.012]
 [0.012]
 [0.012]] [[50.681]
 [45.077]
 [42.762]
 [43.351]
 [41.509]] [[0.513]
 [0.407]
 [0.38 ]
 [0.389]
 [0.358]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  55.602133151449166
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  50.40714928573125
printing an ep nov before normalisation:  42.954580091391584
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.82550359822001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.41809272766113
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
line 256 mcts: sample exp_bonus 35.98005771636963
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.271133485201005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.1  ]
 [0.139]
 [0.141]
 [0.099]] [[41.872]
 [38.651]
 [31.846]
 [31.863]
 [37.498]] [[1.178]
 [1.137]
 [0.911]
 [0.914]
 [1.091]]
printing an ep nov before normalisation:  24.31044090602482
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.01025334784500842
printing an ep nov before normalisation:  41.034605610763315
printing an ep nov before normalisation:  41.35843760758425
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
line 256 mcts: sample exp_bonus 18.98273284384318
printing an ep nov before normalisation:  46.288082832913766
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  48.69622833376538
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  18.539889885117795
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.9245659
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[31.728]
 [40.137]
 [51.693]
 [44.213]
 [45.367]] [[0.261]
 [0.455]
 [0.723]
 [0.55 ]
 [0.577]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[37.63 ]
 [36.801]
 [48.008]
 [42.089]
 [40.016]] [[0.424]
 [0.401]
 [0.693]
 [0.54 ]
 [0.486]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.307]
 [46.307]
 [46.307]
 [46.307]
 [46.307]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
from probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
siam score:  -0.9257628
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  51.56149483135707
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[47.562]
 [47.562]
 [47.562]
 [47.562]
 [47.562]] [[0.733]
 [0.733]
 [0.733]
 [0.733]
 [0.733]]
from probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[63.508]
 [54.133]
 [62.983]
 [60.745]
 [63.508]] [[1.342]
 [0.987]
 [1.322]
 [1.238]
 [1.342]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  59.807569319266086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  45.026702754323736
printing an ep nov before normalisation:  40.26140082178375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  22.36589243173614
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[31.227]
 [31.227]
 [31.227]
 [31.227]
 [31.227]] [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[43.851]
 [43.851]
 [43.851]
 [43.851]
 [43.851]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
printing an ep nov before normalisation:  39.74482269172426
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  35.54055694689045
siam score:  -0.9204878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  67.09838608448437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  44.82060099452885
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.545075197264964
printing an ep nov before normalisation:  61.5489518733217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.008]
 [0.007]] [[ 0.   ]
 [ 0.   ]
 [71.768]
 [78.398]
 [ 0.   ]] [[-0.279]
 [-0.279]
 [ 0.72 ]
 [ 0.813]
 [-0.279]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  37.19754776388355
printing an ep nov before normalisation:  53.268252781514796
printing an ep nov before normalisation:  48.85239383308807
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
printing an ep nov before normalisation:  36.565141056340245
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254966437241052, 0.06254966437241052, 0.06254966437241052, 0.6872516781379474, 0.06254966437241052, 0.06254966437241052]
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  47.48903521083016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.009]
 [0.008]
 [0.008]
 [0.009]] [[47.357]
 [36.206]
 [40.258]
 [44.171]
 [36.206]] [[1.041]
 [0.616]
 [0.771]
 [0.92 ]
 [0.616]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.56699904186578
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  0.00288406889410453
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  76.35308336323041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  61.85573453839993
printing an ep nov before normalisation:  44.05239572373683
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.13646435443863
line 256 mcts: sample exp_bonus 54.13551134538464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[56.531]
 [56.531]
 [56.531]
 [56.531]
 [56.531]] [[1.341]
 [1.341]
 [1.341]
 [1.341]
 [1.341]]
printing an ep nov before normalisation:  46.72142708519474
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  37.368292722790095
printing an ep nov before normalisation:  29.34705759783536
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.005]] [[61.649]
 [75.076]
 [94.419]
 [81.116]
 [81.116]] [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.005]]
printing an ep nov before normalisation:  37.706640031602646
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.   ]
 [0.006]
 [0.006]
 [0.005]] [[37.707]
 [48.993]
 [76.003]
 [43.503]
 [48.589]] [[0.228]
 [0.412]
 [0.867]
 [0.326]
 [0.409]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
line 256 mcts: sample exp_bonus 29.47911082266473
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  51.030794846400816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  47.26292651743105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0797732440105392
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  28.639179806102003
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
siam score:  -0.906116
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.01 ]
 [0.009]
 [0.014]
 [0.009]] [[40.252]
 [39.897]
 [36.734]
 [34.015]
 [36.734]] [[1.456]
 [1.433]
 [1.199]
 [1.003]
 [1.199]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  0.03207077045601636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  38.15661658110294
printing an ep nov before normalisation:  71.8987892675675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  41.97060330550869
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.007]
 [0.007]
 [0.007]] [[23.374]
 [27.501]
 [36.65 ]
 [33.826]
 [26.394]] [[0.43 ]
 [0.596]
 [0.965]
 [0.851]
 [0.553]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  31.308790324199077
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  38.89797161455253
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
UNIT TEST: sample policy line 217 mcts : [0.179 0.154 0.154 0.154 0.359]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  36.546520140675504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.001]
 [0.009]
 [0.007]
 [0.008]] [[34.462]
 [41.543]
 [44.142]
 [47.009]
 [43.656]] [[0.454]
 [0.717]
 [0.824]
 [0.931]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
actions average: 
K:  3  action  0 :  tensor([    0.9963,     0.0006,     0.0000,     0.0009,     0.0021],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9184,     0.0078,     0.0001,     0.0733],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0013,     0.9707,     0.0004,     0.0276],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0006,     0.0402,     0.7950,     0.1640],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0772, 0.0791, 0.1970, 0.6458], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.007]] [[36.449]
 [43.287]
 [43.287]
 [43.287]
 [51.632]] [[0.738]
 [1.027]
 [1.027]
 [1.027]
 [1.38 ]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.006]
 [0.005]
 [0.005]] [[80.969]
 [66.463]
 [66.463]
 [62.627]
 [72.037]] [[1.311]
 [0.963]
 [0.963]
 [0.87 ]
 [1.096]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
siam score:  -0.91085875
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]] [[45.306]
 [45.306]
 [45.306]
 [55.965]
 [45.306]] [[1.229]
 [1.229]
 [1.229]
 [1.672]
 [1.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[45.669]
 [45.669]
 [39.339]
 [45.669]
 [45.669]] [[1.131]
 [1.131]
 [0.932]
 [1.131]
 [1.131]]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.008]
 [0.006]
 [0.006]] [[32.681]
 [34.451]
 [52.495]
 [34.451]
 [34.451]] [[0.413]
 [0.458]
 [0.91 ]
 [0.458]
 [0.458]]
printing an ep nov before normalisation:  47.02438388780635
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.89084859180149
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.76643425648259
printing an ep nov before normalisation:  82.12683191800686
printing an ep nov before normalisation:  39.646124839782715
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.18711404571774892
line 256 mcts: sample exp_bonus 38.91712813060824
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.6716014600658582
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  51.583749150789934
printing an ep nov before normalisation:  52.702926517457605
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0086],
        [0.0000],
        [0.0041],
        [0.0002],
        [0.1231],
        [0.0031],
        [0.0000],
        [0.0036],
        [0.1306],
        [0.0045]], dtype=torch.float64)
0.0 0.008554340871173774
0.970299 0.970299
0.0 0.004077948285665361
0.0 0.00018121448737476567
0.0 0.12310190055069607
0.0 0.0031482418805632646
0.0 0.0
0.0 0.003573784965110022
0.0 0.13063997113853412
0.0 0.004481733570771464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[63.319]
 [63.319]
 [75.903]
 [61.42 ]
 [63.319]] [[0.872]
 [0.872]
 [1.103]
 [0.837]
 [0.872]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  62.66844760699927
printing an ep nov before normalisation:  31.27709169947604
printing an ep nov before normalisation:  17.528171479143726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.007]
 [0.006]
 [0.006]] [[32.29 ]
 [36.481]
 [40.497]
 [32.29 ]
 [32.29 ]] [[0.604]
 [0.751]
 [0.894]
 [0.604]
 [0.604]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8980076
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.007]
 [0.007]
 [0.006]] [[31.574]
 [44.193]
 [48.37 ]
 [41.9  ]
 [42.413]] [[0.53 ]
 [0.929]
 [1.063]
 [0.857]
 [0.873]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  38.445850056966776
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
using another actor
actions average: 
K:  1  action  0 :  tensor([    0.9869,     0.0002,     0.0024,     0.0053,     0.0053],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9596,     0.0002,     0.0001,     0.0401],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0014, 0.0051, 0.8915, 0.0331, 0.0689], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0357,     0.8921,     0.0717],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0027, 0.1503, 0.0917, 0.1115, 0.6438], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  46.546775170708806
printing an ep nov before normalisation:  31.91887855529785
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  41.43450692696154
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  35.6297419241886
printing an ep nov before normalisation:  44.176149651083975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  60.39037211948712
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.004]
 [0.004]
 [0.003]] [[32.109]
 [28.376]
 [31.059]
 [36.094]
 [27.945]] [[0.125]
 [0.097]
 [0.119]
 [0.152]
 [0.097]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.674763691438386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]] [[29.314]
 [29.314]
 [29.314]
 [29.314]
 [37.911]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0186,     0.9489,     0.0017,     0.0001,     0.0306],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0096,     0.9374,     0.0181,     0.0349],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0007,     0.0007,     0.0236,     0.8150,     0.1599],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0018, 0.0432, 0.0877, 0.1738, 0.6935], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  42.5176603534398
siam score:  -0.89935184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
printing an ep nov before normalisation:  72.77881962362139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
from probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
actions average: 
K:  4  action  0 :  tensor([    0.9595,     0.0001,     0.0015,     0.0321,     0.0068],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9390,     0.0080,     0.0003,     0.0526],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0007,     0.9194,     0.0257,     0.0541],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0002,     0.0209,     0.8488,     0.1297],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0795, 0.0354, 0.1533, 0.7307], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  1.2728159203764022
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[38.851]
 [34.242]
 [52.963]
 [36.805]
 [34.406]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.997261166260525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.899274131458256
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.005]
 [0.004]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  45.91375221350886
printing an ep nov before normalisation:  45.24607932568183
printing an ep nov before normalisation:  38.89676570892334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.36498849131275
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  53.45684477902879
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  65.68843770904273
from probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
siam score:  -0.89374787
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  33.89629602432251
printing an ep nov before normalisation:  50.84313257727852
printing an ep nov before normalisation:  0.000346478004189521
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
from probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
printing an ep nov before normalisation:  51.21409667490033
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.149]
 [0.152]
 [0.173]
 [0.141]] [[46.411]
 [43.584]
 [41.441]
 [37.853]
 [44.94 ]] [[1.233]
 [1.255]
 [1.203]
 [1.133]
 [1.281]]
printing an ep nov before normalisation:  39.31830352467401
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254919597350143, 0.06254919597350143, 0.06254919597350143, 0.6872540201324929, 0.06254919597350143, 0.06254919597350143]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 29.018361668085863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  74.00932765697107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.005]
 [0.005]
 [0.006]] [[59.085]
 [53.701]
 [63.859]
 [62.534]
 [63.303]] [[1.036]
 [0.911]
 [1.149]
 [1.118]
 [1.136]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  45.20171729807518
siam score:  -0.89691544
printing an ep nov before normalisation:  41.76236788431803
printing an ep nov before normalisation:  53.6758932995907
printing an ep nov before normalisation:  49.25554703132499
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.003]] [[38.523]
 [34.666]
 [39.833]
 [34.549]
 [32.567]] [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.003]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[55.778]
 [52.288]
 [53.796]
 [53.238]
 [54.672]] [[1.338]
 [1.186]
 [1.252]
 [1.227]
 [1.29 ]]
siam score:  -0.88951766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.76128990738199
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([0.9602, 0.0085, 0.0013, 0.0075, 0.0226], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0248,     0.9378,     0.0006,     0.0001,     0.0367],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0006,     0.9815,     0.0047,     0.0128],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0001,     0.0232,     0.9156,     0.0609],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0035, 0.0488, 0.0215, 0.1708, 0.7554], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.679634825263545
printing an ep nov before normalisation:  33.957273960113525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.001]
 [0.004]
 [0.005]
 [0.004]] [[28.785]
 [36.767]
 [31.039]
 [36.21 ]
 [31.139]] [[0.856]
 [1.334]
 [0.991]
 [1.304]
 [0.997]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.58224172702102
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[36.562]
 [36.883]
 [55.323]
 [36.883]
 [36.883]] [[0.484]
 [0.492]
 [0.956]
 [0.492]
 [0.492]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  45.00186858770857
actions average: 
K:  3  action  0 :  tensor([    0.9666,     0.0006,     0.0114,     0.0004,     0.0210],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9209,     0.0096,     0.0007,     0.0681],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0019, 0.0017, 0.9606, 0.0022, 0.0336], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0323,     0.1375,     0.7387,     0.0912],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0739, 0.1457, 0.0787, 0.7006], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[65.09 ]
 [69.183]
 [75.433]
 [63.678]
 [65.09 ]] [[1.195]
 [1.286]
 [1.425]
 [1.163]
 [1.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  61.11354763350038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  45.928095929667734
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.076]
 [0.099]
 [0.092]
 [0.065]] [[34.594]
 [27.507]
 [31.204]
 [44.122]
 [33.853]] [[0.849]
 [0.539]
 [0.714]
 [1.237]
 [0.789]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.057]
 [0.057]] [[48.438]
 [48.438]
 [48.438]
 [48.438]
 [48.438]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
printing an ep nov before normalisation:  49.31038410078036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.73506337148831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  74.60642263534271
siam score:  -0.8787892
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.007]
 [0.004]
 [0.004]] [[53.715]
 [53.715]
 [39.153]
 [53.715]
 [53.715]] [[1.337]
 [1.337]
 [0.759]
 [1.337]
 [1.337]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.026 0.436 0.256 0.179 0.103]
printing an ep nov before normalisation:  56.33588092886168
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  43.53249718075497
from probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  25.606263777889225
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.06 ]
 [0.008]
 [0.118]
 [0.077]] [[30.114]
 [30.168]
 [28.675]
 [37.783]
 [29.875]] [[0.01 ]
 [0.06 ]
 [0.008]
 [0.118]
 [0.077]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
from probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  34.03589768826916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  27.54446167457096
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.   ]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.   ]
 [0.003]
 [0.003]
 [0.003]]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  40.937654899211786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  36.78724360509149
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.39248609542847
using explorer policy with actor:  0
using explorer policy with actor:  0
using explorer policy with actor:  0
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0212,     0.9003,     0.0018,     0.0002,     0.0765],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0014,     0.9676,     0.0136,     0.0174],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0000,     0.0003,     0.9943,     0.0054],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0042, 0.0562, 0.0937, 0.1404, 0.7055], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 39.4058961378989
printing an ep nov before normalisation:  31.230975168186106
printing an ep nov before normalisation:  35.37340677280723
printing an ep nov before normalisation:  40.769500732421875
printing an ep nov before normalisation:  52.63367070498537
printing an ep nov before normalisation:  45.22665310358826
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[36.526]
 [36.526]
 [36.526]
 [36.526]
 [36.526]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
printing an ep nov before normalisation:  43.21342822882326
line 256 mcts: sample exp_bonus 42.827529282157066
printing an ep nov before normalisation:  41.87708377838135
printing an ep nov before normalisation:  43.69983346943537
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  47.31250868111865
printing an ep nov before normalisation:  22.13212013244629
printing an ep nov before normalisation:  36.30445459425291
using explorer policy with actor:  0
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.003]] [[20.361]
 [21.709]
 [38.276]
 [24.724]
 [27.494]] [[0.003]
 [0.003]
 [0.004]
 [0.003]
 [0.003]]
printing an ep nov before normalisation:  32.90080785751343
printing an ep nov before normalisation:  41.545317680190806
printing an ep nov before normalisation:  57.758878892161306
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.013]
 [0.009]
 [0.01 ]
 [0.009]] [[59.621]
 [53.181]
 [65.86 ]
 [65.731]
 [66.381]] [[0.972]
 [0.811]
 [1.135]
 [1.132]
 [1.149]]
using explorer policy with actor:  0
printing an ep nov before normalisation:  49.17008797887948
actions average: 
K:  2  action  0 :  tensor([    0.9951,     0.0010,     0.0000,     0.0016,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0009, 0.8955, 0.0358, 0.0012, 0.0666], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0120, 0.0298, 0.8895, 0.0066, 0.0621], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0002,     0.0171,     0.7180,     0.2640],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0246,     0.1203,     0.2289,     0.6258],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
rdn probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.93424236182159
printing an ep nov before normalisation:  24.072885513305664
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.49564838409424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
printing an ep nov before normalisation:  53.203169930869365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.0625488614002718, 0.0625488614002718, 0.0625488614002718, 0.687255692998641, 0.0625488614002718, 0.0625488614002718]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  25.79468127615796
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.007]] [[30.404]
 [59.275]
 [65.347]
 [51.536]
 [55.651]] [[0.386]
 [0.981]
 [1.109]
 [0.823]
 [0.906]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  30.05659580230713
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.732]
 [21.732]
 [21.732]
 [21.732]
 [21.732]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  29.454589686749383
printing an ep nov before normalisation:  28.707463442034648
printing an ep nov before normalisation:  27.986426119485305
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.005]
 [0.004]] [[46.775]
 [46.775]
 [46.775]
 [52.585]
 [46.775]] [[0.76 ]
 [0.76 ]
 [0.76 ]
 [0.896]
 [0.76 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  23.951537515877956
printing an ep nov before normalisation:  34.325260151566376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.003]
 [0.003]
 [0.003]]
printing an ep nov before normalisation:  34.64691374037001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  44.429242494613376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  26.71898450484099
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
line 256 mcts: sample exp_bonus 34.74950730978081
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  56.439116721392296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.004]
 [0.004]
 [0.003]] [[33.707]
 [42.278]
 [36.224]
 [38.359]
 [33.707]] [[0.702]
 [1.041]
 [0.803]
 [0.887]
 [0.702]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[58.359]
 [54.038]
 [61.17 ]
 [58.359]
 [59.856]] [[1.424]
 [1.268]
 [1.525]
 [1.424]
 [1.478]]
printing an ep nov before normalisation:  22.507400512695312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[48.583]
 [48.583]
 [48.583]
 [48.583]
 [48.583]] [[0.929]
 [0.929]
 [0.929]
 [0.929]
 [0.929]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.282]
 [0.338]
 [0.262]
 [0.323]
 [0.285]] [[55.46 ]
 [59.347]
 [53.772]
 [55.656]
 [55.447]] [[1.753]
 [1.993]
 [1.653]
 [1.803]
 [1.756]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
printing an ep nov before normalisation:  37.94528082554646
printing an ep nov before normalisation:  36.35978020305217
Printing some Q and Qe and total Qs values:  [[1.458]
 [0.702]
 [0.536]
 [0.702]
 [0.427]] [[30.185]
 [44.352]
 [44.021]
 [44.352]
 [45.156]] [[2.255]
 [1.873]
 [1.699]
 [1.873]
 [1.619]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.06254861046893848, 0.06254861046893848, 0.06254861046893848, 0.6872569476553076, 0.06254861046893848, 0.06254861046893848]
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.8806853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
printing an ep nov before normalisation:  30.860394877083788
siam score:  -0.8814476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.59778070449829
printing an ep nov before normalisation:  46.119827971269444
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
printing an ep nov before normalisation:  37.880300616417294
printing an ep nov before normalisation:  36.4365294089908
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.01 ]
 [0.006]
 [0.006]] [[40.715]
 [40.715]
 [47.084]
 [40.715]
 [40.715]] [[0.909]
 [0.909]
 [1.161]
 [0.909]
 [0.909]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]] [[29.861]
 [28.881]
 [23.016]
 [26.542]
 [24.111]] [[0.642]
 [0.61 ]
 [0.413]
 [0.531]
 [0.448]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
printing an ep nov before normalisation:  44.53517862056718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057946903179285886, 0.057946903179285886, 0.057946903179285886, 0.6366071077571065, 0.13160527952574982, 0.057946903179285886]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.09933662414551
printing an ep nov before normalisation:  35.67560733260144
printing an ep nov before normalisation:  32.83317269282841
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795556811355361, 0.05795556811355361, 0.05795556811355361, 0.6367024805189454, 0.13147524702684016, 0.05795556811355361]
printing an ep nov before normalisation:  84.81364104181017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05795556811355361, 0.05795556811355361, 0.05795556811355361, 0.6367024805189454, 0.13147524702684016, 0.05795556811355361]
using explorer policy with actor:  1
siam score:  -0.8750945
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.083]
 [0.057]
 [0.03 ]
 [0.078]] [[41.878]
 [41.371]
 [35.294]
 [35.024]
 [38.429]] [[0.976]
 [0.955]
 [0.701]
 [0.664]
 [0.84 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
printing an ep nov before normalisation:  51.05760097503662
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  24.95441432551739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.013]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[48.875]
 [48.828]
 [48.875]
 [48.875]
 [48.875]] [[1.343]
 [1.344]
 [1.343]
 [1.343]
 [1.343]]
printing an ep nov before normalisation:  75.99591460747645
printing an ep nov before normalisation:  41.83801633755956
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.007]
 [0.006]
 [0.006]] [[39.931]
 [39.931]
 [34.858]
 [39.158]
 [40.021]] [[1.334]
 [1.334]
 [1.056]
 [1.292]
 [1.339]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.057964200477311954, 0.057964200477311954, 0.057964200477311954, 0.6367974947853431, 0.1313457033054092, 0.057964200477311954]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.01 ]
 [0.011]
 [0.009]] [[ 96.972]
 [ 96.972]
 [ 92.654]
 [102.038]
 [ 96.972]] [[1.483]
 [1.483]
 [1.409]
 [1.573]
 [1.483]]
printing an ep nov before normalisation:  50.66517017908225
siam score:  -0.8810986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05738313526768442, 0.05738313526768442, 0.05738313526768442, 0.6303981712789936, 0.14006928765026871, 0.05738313526768442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.05738313526768442, 0.05738313526768442, 0.05738313526768442, 0.6303981712789936, 0.14006928765026871, 0.05738313526768442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05738313526768442, 0.05738313526768442, 0.05738313526768442, 0.6303981712789936, 0.14006928765026871, 0.05738313526768442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05738313526768442, 0.05738313526768442, 0.05738313526768442, 0.6303981712789936, 0.14006928765026871, 0.05738313526768442]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05739268985473843, 0.05739268985473843, 0.05739268985473843, 0.6305033356115584, 0.13992590496948784, 0.05739268985473843]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05739268985473843, 0.05739268985473843, 0.05739268985473843, 0.6305033356115584, 0.13992590496948784, 0.05739268985473843]
printing an ep nov before normalisation:  38.75495669719564
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05739268985473843, 0.05739268985473843, 0.05739268985473843, 0.6305033356115584, 0.13992590496948784, 0.05739268985473843]
printing an ep nov before normalisation:  26.457014083862305
printing an ep nov before normalisation:  39.42784264355431
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[49.125]
 [49.125]
 [49.125]
 [49.125]
 [49.125]] [[0.876]
 [0.876]
 [0.876]
 [0.876]
 [0.876]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05739268985473843, 0.05739268985473843, 0.05739268985473843, 0.6305033356115584, 0.13992590496948784, 0.05739268985473843]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.011]
 [0.009]
 [0.009]] [[26.636]
 [30.506]
 [28.343]
 [24.632]
 [24.699]] [[0.692]
 [0.882]
 [0.778]
 [0.595]
 [0.598]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05739268985473843, 0.05739268985473843, 0.05739268985473843, 0.6305033356115584, 0.13992590496948784, 0.05739268985473843]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05740220916249507, 0.05740220916249507, 0.05740220916249507, 0.6306081116360004, 0.13978305171401925, 0.05740220916249507]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05740220916249507, 0.05740220916249507, 0.05740220916249507, 0.6306081116360004, 0.13978305171401925, 0.05740220916249507]
siam score:  -0.8794408
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05740220916249507, 0.05740220916249507, 0.05740220916249507, 0.6306081116360004, 0.13978305171401925, 0.05740220916249507]
printing an ep nov before normalisation:  34.30123985619907
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05741169338599183, 0.05741169338599183, 0.05741169338599183, 0.6307125014990361, 0.13964072495699645, 0.05741169338599183]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.58539779815183
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05307183719095577, 0.05307183719095577, 0.05307183719095577, 0.5829450702539313, 0.20476758098224565, 0.05307183719095577]
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.093]
 [0.04 ]
 [0.04 ]
 [0.04 ]] [[69.297]
 [69.883]
 [69.297]
 [69.297]
 [69.297]] [[1.213]
 [1.275]
 [1.213]
 [1.213]
 [1.213]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05307183719095577, 0.05307183719095577, 0.05307183719095577, 0.5829450702539313, 0.20476758098224565, 0.05307183719095577]
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9435,     0.0002,     0.0000,     0.0560],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0198,     0.9247,     0.0229,     0.0321],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0019, 0.0009, 0.0034, 0.8554, 0.1384], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0078, 0.0304, 0.0766, 0.1148, 0.7705], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.77140809521895
printing an ep nov before normalisation:  47.805365291264934
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.178912280111504
printing an ep nov before normalisation:  45.519492874584785
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
printing an ep nov before normalisation:  40.196955700463135
printing an ep nov before normalisation:  71.28699139946626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
printing an ep nov before normalisation:  29.493946760943558
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.1271],
        [    0.0000],
        [    0.0039],
        [    0.0000],
        [    0.0064],
        [    0.0711],
        [    0.0049],
        [    0.0000],
        [    0.0046]], dtype=torch.float64)
0.0 0.0
0.0 0.12709599082345963
0.0 8.679831388755276e-06
0.0 0.0038713761309758193
0.0 0.0
0.0 0.006407279243725531
0.0 0.0710641935209007
0.0 0.004940161206703803
0.0 0.0
0.0 0.004583700399979259
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  33.85089874267578
printing an ep nov before normalisation:  53.125888426387824
printing an ep nov before normalisation:  33.89391899108887
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9516,     0.0219,     0.0003,     0.0259],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0007,     0.9446,     0.0100,     0.0446],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0004,     0.0005,     0.8549,     0.1433],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0030, 0.0700, 0.0564, 0.2035, 0.6670], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
printing an ep nov before normalisation:  40.4884211323149
printing an ep nov before normalisation:  32.34081290752221
siam score:  -0.8823123
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.009]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.009]
 [0.008]
 [0.008]]
printing an ep nov before normalisation:  34.942098677345086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.005]
 [0.005]
 [0.005]
 [0.004]] [[31.662]
 [33.627]
 [28.853]
 [31.349]
 [32.547]] [[0.98 ]
 [1.094]
 [0.831]
 [0.967]
 [1.033]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
printing an ep nov before normalisation:  30.972894032796226
actions average: 
K:  4  action  0 :  tensor([    0.9996,     0.0003,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9703,     0.0001,     0.0000,     0.0291],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0128,     0.9534,     0.0009,     0.0327],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0364,     0.0006,     0.0399,     0.8658,     0.0573],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0680, 0.0824, 0.2609, 0.5876], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.004]
 [0.004]
 [0.006]] [[35.753]
 [39.142]
 [39.694]
 [39.694]
 [39.721]] [[1.682]
 [1.84 ]
 [1.864]
 [1.864]
 [1.867]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.008]
 [0.007]
 [0.007]] [[30.367]
 [35.139]
 [25.228]
 [26.81 ]
 [29.206]] [[0.371]
 [0.489]
 [0.245]
 [0.283]
 [0.343]]
printing an ep nov before normalisation:  9.340952544789616e-06
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
siam score:  -0.8763025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05308791880325994, 0.05308791880325994, 0.05308791880325994, 0.583122075499158, 0.20452624928780225, 0.05308791880325994]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  39.842774152457
from probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  37.6273274201052
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9970,     0.0000,     0.0001,     0.0003,     0.0026],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9339,     0.0215,     0.0016,     0.0430],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0113,     0.9667,     0.0018,     0.0200],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0001,     0.0038,     0.7701,     0.2257],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0017, 0.0476, 0.1046, 0.0898, 0.7563], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.36896583432416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  63.26638556480769
printing an ep nov before normalisation:  50.07368090928959
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[45.731]
 [45.731]
 [45.037]
 [45.731]
 [45.731]] [[1.208]
 [1.208]
 [1.176]
 [1.208]
 [1.208]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
actions average: 
K:  2  action  0 :  tensor([    0.9945,     0.0004,     0.0000,     0.0018,     0.0033],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9728,     0.0018,     0.0001,     0.0250],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0004,     0.9624,     0.0101,     0.0270],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0048,     0.9261,     0.0688],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0008, 0.1423, 0.0897, 0.0736, 0.6936], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  39.58399188092546
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.46848890612559
printing an ep nov before normalisation:  43.18829655535649
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[41.201]
 [41.201]
 [41.201]
 [41.201]
 [41.201]] [[0.716]
 [0.716]
 [0.716]
 [0.716]
 [0.716]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  43.86275478446089
printing an ep nov before normalisation:  64.46869267595582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
printing an ep nov before normalisation:  37.929132835874164
printing an ep nov before normalisation:  45.82462041232993
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.440489096759855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.067]
 [0.056]
 [0.056]
 [0.04 ]] [[41.883]
 [37.042]
 [41.883]
 [41.883]
 [45.229]] [[1.187]
 [0.986]
 [1.187]
 [1.187]
 [1.317]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.734772361944685
printing an ep nov before normalisation:  42.979206610661066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05411045417128538, 0.05411045417128538, 0.05411045417128538, 0.5943802741491155, 0.18917790916574292, 0.05411045417128538]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
printing an ep nov before normalisation:  42.263842176321226
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
printing an ep nov before normalisation:  28.99605213301838
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.009]
 [0.008]
 [0.008]] [[57.463]
 [57.463]
 [54.557]
 [56.774]
 [57.986]] [[1.324]
 [1.324]
 [1.228]
 [1.301]
 [1.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.054933479636419265, 0.054933479636419265, 0.054933479636419265, 0.6034418523303611, 0.1768242291239619, 0.054933479636419265]
printing an ep nov before normalisation:  49.28650148227378
printing an ep nov before normalisation:  34.77534294128418
actions average: 
K:  4  action  0 :  tensor([    0.9647,     0.0213,     0.0000,     0.0003,     0.0137],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9652,     0.0015,     0.0006,     0.0326],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0124,     0.9028,     0.0259,     0.0586],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0349,     0.0007,     0.0199,     0.8685,     0.0761],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0042, 0.1415, 0.0836, 0.1420, 0.6287], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
printing an ep nov before normalisation:  52.78229687666441
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[44.152]
 [44.152]
 [43.137]
 [44.152]
 [44.152]] [[1.408]
 [1.408]
 [1.362]
 [1.408]
 [1.408]]
printing an ep nov before normalisation:  26.758413314819336
printing an ep nov before normalisation:  34.22617435455322
siam score:  -0.8888081
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
printing an ep nov before normalisation:  28.432302474975586
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.007]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.007]
 [0.008]
 [0.008]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
printing an ep nov before normalisation:  45.21830801130967
printing an ep nov before normalisation:  50.33474962804725
printing an ep nov before normalisation:  39.334715714869844
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05561019560589297, 0.05561019560589297, 0.05561019560589297, 0.6108925509097615, 0.16666666666666666, 0.05561019560589297]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.225]
 [0.225]
 [0.219]
 [0.225]] [[32.67 ]
 [32.67 ]
 [32.67 ]
 [42.905]
 [32.67 ]] [[0.804]
 [0.804]
 [0.804]
 [1.133]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.18502889360526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
printing an ep nov before normalisation:  80.38214938344443
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
printing an ep nov before normalisation:  49.356920439166146
line 256 mcts: sample exp_bonus 0.00031990912475521327
printing an ep nov before normalisation:  76.85845484928119
siam score:  -0.9006216
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.008]
 [0.01 ]
 [0.01 ]] [[73.851]
 [73.851]
 [92.66 ]
 [73.851]
 [73.851]] [[0.975]
 [0.975]
 [1.257]
 [0.975]
 [0.975]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
printing an ep nov before normalisation:  46.801745111181944
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
Printing some Q and Qe and total Qs values:  [[0.464]
 [0.464]
 [0.464]
 [0.464]
 [0.464]] [[37.12]
 [37.12]
 [37.12]
 [37.12]
 [37.12]] [[49.946]
 [49.946]
 [49.946]
 [49.946]
 [49.946]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05268902393849147, 0.05268902393849147, 0.05268902393849147, 0.5787396826839155, 0.2105042215621187, 0.05268902393849147]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  112 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05345464251903482, 0.05345464251903482, 0.05345464251903482, 0.5871684706435849, 0.19901295928027582, 0.05345464251903482]
printing an ep nov before normalisation:  66.67907835382483
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05345464251903482, 0.05345464251903482, 0.05345464251903482, 0.5871684706435849, 0.19901295928027582, 0.05345464251903482]
printing an ep nov before normalisation:  46.518264740178864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05345464251903482, 0.05345464251903482, 0.05345464251903482, 0.5871684706435849, 0.19901295928027582, 0.05345464251903482]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05345464251903482, 0.05345464251903482, 0.05345464251903482, 0.5871684706435849, 0.19901295928027582, 0.05345464251903482]
printing an ep nov before normalisation:  31.835976028423364
printing an ep nov before normalisation:  33.1351637840271
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.019]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[47.122]
 [53.464]
 [51.375]
 [51.375]
 [51.375]] [[1.049]
 [1.352]
 [1.247]
 [1.247]
 [1.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.075]
 [0.016]
 [0.016]
 [0.016]] [[68.144]
 [60.741]
 [46.615]
 [46.615]
 [46.615]] [[0.807]
 [0.749]
 [0.449]
 [0.449]
 [0.449]]
printing an ep nov before normalisation:  0.08534987083763212
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  88.7174028046192
siam score:  -0.90519166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
printing an ep nov before normalisation:  0.6393609451883719
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8983715
printing an ep nov before normalisation:  39.62639648675777
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.63905189893411
siam score:  -0.8955971
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.06943617673451
printing an ep nov before normalisation:  35.849018034026486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
siam score:  -0.8984864
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.015]
 [0.015]
 [0.015]
 [0.015]] [[63.155]
 [52.49 ]
 [52.49 ]
 [52.49 ]
 [52.49 ]] [[0.768]
 [0.574]
 [0.574]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.013]
 [0.015]
 [0.015]
 [0.014]] [[37.273]
 [38.722]
 [47.455]
 [37.573]
 [39.314]] [[0.721]
 [0.767]
 [1.051]
 [0.731]
 [0.787]]
siam score:  -0.9002754
siam score:  -0.9008206
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.014]
 [0.012]
 [0.012]] [[47.146]
 [47.146]
 [43.152]
 [47.146]
 [47.146]] [[0.895]
 [0.895]
 [0.753]
 [0.895]
 [0.895]]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.016]
 [0.015]
 [0.015]] [[41.732]
 [41.732]
 [77.934]
 [41.732]
 [41.732]] [[0.599]
 [0.599]
 [1.305]
 [0.599]
 [0.599]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
printing an ep nov before normalisation:  40.28449807014489
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
printing an ep nov before normalisation:  56.81025562439875
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[34.525]
 [34.525]
 [34.525]
 [34.525]
 [34.525]] [[1.629]
 [1.629]
 [1.629]
 [1.629]
 [1.629]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.014]
 [0.014]
 [0.014]] [[44.698]
 [42.375]
 [36.398]
 [39.109]
 [41.256]] [[1.089]
 [0.997]
 [0.756]
 [0.866]
 [0.953]]
printing an ep nov before normalisation:  25.336441424311683
printing an ep nov before normalisation:  54.91180198867687
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.507780349371
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
siam score:  -0.8969493
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05410990676262543, 0.05410990676262543, 0.05410990676262543, 0.5943823543020235, 0.1891780186474749, 0.05410990676262543]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  34.798695738397434
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  31.91615610661254
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.019]
 [0.014]
 [0.01 ]
 [0.014]] [[35.871]
 [33.69 ]
 [35.871]
 [39.757]
 [35.871]] [[0.014]
 [0.019]
 [0.014]
 [0.01 ]
 [0.014]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
using explorer policy with actor:  1
siam score:  -0.88321626
printing an ep nov before normalisation:  39.00882907411461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
from probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  30.23228161966697
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  43.10959680146858
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
siam score:  -0.87776643
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.47434048196762
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.007]
 [0.008]
 [0.008]
 [0.007]] [[34.002]
 [34.292]
 [37.245]
 [32.936]
 [33.749]] [[0.   ]
 [0.007]
 [0.008]
 [0.008]
 [0.007]]
from probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  87.65453997648325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  30.125867555224087
printing an ep nov before normalisation:  53.38462643066794
printing an ep nov before normalisation:  28.17901611328125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.552868715082276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  32.390245178904244
siam score:  -0.8810052
printing an ep nov before normalisation:  27.298518805426262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  35.850896838279525
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[33.003]
 [39.223]
 [32.707]
 [33.003]
 [33.003]] [[0.701]
 [1.058]
 [0.685]
 [0.701]
 [0.701]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  49.837436417239665
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8788136
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  40.89907543597057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.007]
 [0.009]
 [0.009]
 [0.007]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.007]
 [0.009]
 [0.009]
 [0.007]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.01 ]] [[33.558]
 [42.882]
 [58.475]
 [41.799]
 [53.837]] [[0.331]
 [0.596]
 [1.04 ]
 [0.565]
 [0.908]]
actions average: 
K:  3  action  0 :  tensor([    0.9986,     0.0000,     0.0004,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9815,     0.0007,     0.0001,     0.0175],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0027,     0.9652,     0.0015,     0.0306],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0013,     0.8881,     0.1104],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0122, 0.1498, 0.0348, 0.0519, 0.7513], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
printing an ep nov before normalisation:  50.00429931238348
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.05467707204710636, 0.05467707204710636, 0.05467707204710636, 0.6006263458174629, 0.18066536599411173, 0.05467707204710636]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.16245619276924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
printing an ep nov before normalisation:  44.38601493835449
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.003]
 [0.   ]
 [0.006]
 [0.004]] [[33.26 ]
 [40.22 ]
 [36.99 ]
 [35.518]
 [39.278]] [[0.668]
 [1.003]
 [0.843]
 [0.777]
 [0.958]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.011]
 [0.009]
 [0.008]] [[43.026]
 [37.247]
 [41.148]
 [38.639]
 [37.92 ]] [[0.969]
 [0.754]
 [0.903]
 [0.807]
 [0.78 ]]
printing an ep nov before normalisation:  35.88718495903737
printing an ep nov before normalisation:  50.59949349206892
actions average: 
K:  2  action  0 :  tensor([    0.9964,     0.0010,     0.0002,     0.0013,     0.0011],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9272,     0.0028,     0.0003,     0.0692],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9468,     0.0221,     0.0309],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0025,     0.0461,     0.8526,     0.0986],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0021, 0.0816, 0.0685, 0.1005, 0.7473], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.32149485396751
printing an ep nov before normalisation:  35.10194329558301
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
using another actor
from probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
printing an ep nov before normalisation:  31.168128842486546
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
printing an ep nov before normalisation:  26.947034332663048
printing an ep nov before normalisation:  42.37126504596253
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.01]
 [0.01]
 [0.01]
 [0.01]
 [0.01]] [[52.35 ]
 [52.35 ]
 [54.315]
 [54.328]
 [54.677]] [[1.241]
 [1.241]
 [1.308]
 [1.309]
 [1.321]]
printing an ep nov before normalisation:  33.42678149541219
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[40.124]
 [39.988]
 [39.988]
 [39.988]
 [39.988]] [[1.342]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
printing an ep nov before normalisation:  24.14447546005249
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
printing an ep nov before normalisation:  50.98564021405486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.079]
 [0.075]
 [0.175]
 [0.166]] [[34.691]
 [26.396]
 [27.653]
 [41.12 ]
 [38.362]] [[0.783]
 [0.477]
 [0.514]
 [1.045]
 [0.948]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.055172786322473445, 0.055172786322473445, 0.055172786322473445, 0.6060837244937811, 0.1732251302163251, 0.055172786322473445]
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05560975150787077, 0.05560975150787077, 0.05560975150787077, 0.6108943273018502, 0.16666666666666666, 0.05560975150787077]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05560975150787077, 0.05560975150787077, 0.05560975150787077, 0.6108943273018502, 0.16666666666666666, 0.05560975150787077]
printing an ep nov before normalisation:  40.617053799133494
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.008]
 [0.009]
 [0.009]
 [0.009]] [[37.924]
 [42.881]
 [42.307]
 [42.498]
 [43.707]] [[0.796]
 [1.03 ]
 [1.004]
 [1.012]
 [1.07 ]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.009]
 [0.007]
 [0.007]] [[55.908]
 [47.753]
 [40.248]
 [45.895]
 [45.895]] [[1.103]
 [0.892]
 [0.699]
 [0.844]
 [0.844]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
siam score:  -0.8634786
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  38.12825907072144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  58.61484534355446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  29.465173438628522
using another actor
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  26.42393993113932
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  25.681402886363415
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
siam score:  -0.858394
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[28.705]
 [28.705]
 [39.473]
 [28.705]
 [28.705]] [[0.623]
 [0.623]
 [1.095]
 [0.623]
 [0.623]]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.004]
 [0.008]
 [0.006]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.006]
 [0.004]
 [0.008]
 [0.006]
 [0.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
from probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
Starting evaluation
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
printing an ep nov before normalisation:  58.22188561178583
printing an ep nov before normalisation:  42.56729302874608
printing an ep nov before normalisation:  68.42770518471966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.05362752845530744, 0.05362752845530744, 0.05362752845530744, 0.5890760778775354, 0.1964138083012349, 0.05362752845530744]
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  32.67570304958122
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[26.67]
 [26.67]
 [26.67]
 [26.67]
 [26.67]] [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[40.017]
 [40.017]
 [38.127]
 [40.017]
 [43.318]] [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]]
printing an ep nov before normalisation:  42.20259360482767
printing an ep nov before normalisation:  43.51003566258111
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.382958607328625
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.75560627188385
printing an ep nov before normalisation:  26.333321836127894
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[4.467]
 [5.667]
 [6.092]
 [7.231]
 [4.8  ]] [[0.32 ]
 [0.407]
 [0.437]
 [0.52 ]
 [0.344]]
actor:  0 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05362753195317922, 0.05362753195317922, 0.05362753195317922, 0.5890760648065408, 0.1964138073807423, 0.05362753195317922]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05362753195317922, 0.05362753195317922, 0.05362753195317922, 0.5890760648065408, 0.1964138073807423, 0.05362753195317922]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.05362753195317922, 0.05362753195317922, 0.05362753195317922, 0.5890760648065408, 0.1964138073807423, 0.05362753195317922]
printing an ep nov before normalisation:  38.89050880706953
actor:  0 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
printing an ep nov before normalisation:  24.3195737592201
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.05362753901237644, 0.05362753901237644, 0.05362753901237644, 0.5890760384274354, 0.19641380552305884, 0.05362753901237644]
maxi score, test score, baseline:  0.0061 0.0 0.0061
probs:  [0.05362753901237644, 0.05362753901237644, 0.05362753901237644, 0.5890760384274354, 0.19641380552305884, 0.05362753901237644]
printing an ep nov before normalisation:  43.90079135857634
rdn probs:  [0.05362753901237644, 0.05362753901237644, 0.05362753901237644, 0.5890760384274354, 0.19641380552305884, 0.05362753901237644]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
printing an ep nov before normalisation:  26.2056827545166
printing an ep nov before normalisation:  38.78831289886893
printing an ep nov before normalisation:  27.675292098888995
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
printing an ep nov before normalisation:  23.92717388588402
printing an ep nov before normalisation:  34.13661827641479
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.43477198681421
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[27.209]
 [27.209]
 [27.209]
 [27.209]
 [27.209]] [[0.063]
 [0.063]
 [0.063]
 [0.063]
 [0.063]]
printing an ep nov before normalisation:  45.75547599742421
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
printing an ep nov before normalisation:  26.917754717743424
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05308734635927527, 0.05308734635927527, 0.05308734635927527, 0.583124174460435, 0.20452644010246376, 0.05308734635927527]
printing an ep nov before normalisation:  31.183400149973302
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  68.30465106729037
siam score:  -0.88091415
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.011]
 [0.018]
 [0.014]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.011]
 [0.018]
 [0.014]
 [0.009]]
printing an ep nov before normalisation:  50.97109305671096
printing an ep nov before normalisation:  66.44696314096808
from probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
printing an ep nov before normalisation:  66.35295732547334
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
from probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
maxi score, test score, baseline:  0.0061 0.15 0.15
printing an ep nov before normalisation:  30.890194535780882
printing an ep nov before normalisation:  36.530807891443395
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  36.142765212548596
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05362786304047769, 0.05362786304047769, 0.05362786304047769, 0.5890748275855833, 0.19641372025250584, 0.05362786304047769]
printing an ep nov before normalisation:  35.88059858357039
printing an ep nov before normalisation:  52.15532547441963
printing an ep nov before normalisation:  45.27432930341824
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.27738948047144
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.118]
 [0.12 ]
 [0.057]
 [0.15 ]] [[30.358]
 [31.863]
 [30.358]
 [31.348]
 [31.315]] [[1.312]
 [1.423]
 [1.312]
 [1.323]
 [1.414]]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
using explorer policy with actor:  1
printing an ep nov before normalisation:  66.3474421235482
maxi score, test score, baseline:  0.0061 0.15 0.15
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[62.062]
 [62.062]
 [62.062]
 [62.062]
 [62.062]] [[1.661]
 [1.661]
 [1.661]
 [1.661]
 [1.661]]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.208]
 [0.208]
 [0.208]
 [0.208]] [[36.68]
 [36.68]
 [36.68]
 [36.68]
 [36.68]] [[1.147]
 [1.147]
 [1.147]
 [1.147]
 [1.147]]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.32877039212949
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
printing an ep nov before normalisation:  39.87437356163048
printing an ep nov before normalisation:  34.34628666152809
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.05178236827342495, 0.05178236827342495, 0.05178236827342495, 0.5687617110430127, 0.2241088158632875, 0.05178236827342495]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  33.663731005995245
printing an ep nov before normalisation:  33.41943504833529
printing an ep nov before normalisation:  30.577407679526267
siam score:  -0.899677
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.24995719155223395, 0.050059931826872545]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.009]
 [0.015]
 [0.015]
 [0.013]] [[28.833]
 [29.979]
 [25.6  ]
 [27.071]
 [28.237]] [[1.154]
 [1.249]
 [0.884]
 [1.008]
 [1.106]]
printing an ep nov before normalisation:  44.35261861319813
printing an ep nov before normalisation:  52.27865942224648
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.249957191552234, 0.050059931826872545]
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.249957191552234, 0.050059931826872545]
printing an ep nov before normalisation:  49.45529805882008
printing an ep nov before normalisation:  37.885528796820715
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.02 ]
 [0.017]
 [0.017]
 [0.017]] [[48.356]
 [46.391]
 [48.356]
 [48.356]
 [48.356]] [[0.953]
 [0.887]
 [0.953]
 [0.953]
 [0.953]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.063]
 [0.032]
 [0.018]
 [0.046]] [[31.217]
 [25.162]
 [33.664]
 [39.347]
 [35.034]] [[0.557]
 [0.329]
 [0.705]
 [0.964]
 [0.785]]
siam score:  -0.898226
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.249957191552234, 0.050059931826872545]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[38.955]
 [38.955]
 [41.588]
 [38.955]
 [38.955]] [[1.307]
 [1.307]
 [1.48 ]
 [1.307]
 [1.307]]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.249957191552234, 0.050059931826872545]
printing an ep nov before normalisation:  32.63312816619873
printing an ep nov before normalisation:  39.23793408759359
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.050059931826872545, 0.050059931826872545, 0.050059931826872545, 0.549803081140276, 0.249957191552234, 0.050059931826872545]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.0120267277215
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04844864245784059, 0.04844864245784059, 0.04844864245784059, 0.5320678324030382, 0.27413759776559954, 0.04844864245784059]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04844864245784059, 0.04844864245784059, 0.04844864245784059, 0.5320678324030382, 0.2741375977655995, 0.04844864245784059]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  17.78691291809082
line 256 mcts: sample exp_bonus 24.778974666002195
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  23.41530643684119
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0061 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.897]
 [0.681]
 [0.47 ]
 [0.654]] [[36.16 ]
 [23.858]
 [33.788]
 [31.856]
 [31.61 ]] [[0.775]
 [0.897]
 [0.681]
 [0.47 ]
 [0.654]]
actor:  0 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.24667769228874
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.068]
 [0.101]
 [0.121]
 [0.084]] [[22.501]
 [24.211]
 [40.192]
 [33.869]
 [22.501]] [[0.526]
 [0.573]
 [1.196]
 [0.982]
 [0.526]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  36.953812378307
actions average: 
K:  2  action  0 :  tensor([    0.9977,     0.0003,     0.0000,     0.0015,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9247,     0.0265,     0.0001,     0.0486],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0015,     0.0002,     0.9663,     0.0153,     0.0167],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0013,     0.0005,     0.0008,     0.9361,     0.0614],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0047, 0.1467, 0.0746, 0.7726], grad_fn=<DivBackward0>)
using another actor
printing an ep nov before normalisation:  46.50496320056221
using explorer policy with actor:  0
printing an ep nov before normalisation:  38.58736515045166
printing an ep nov before normalisation:  0.016153054677943146
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  47.741960615668475
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  36.01672275759325
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  43.63304093115332
printing an ep nov before normalisation:  49.62855192672091
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.016]
 [0.018]
 [0.018]
 [0.02 ]] [[26.777]
 [26.799]
 [37.879]
 [29.054]
 [29.466]] [[0.645]
 [0.643]
 [1.225]
 [0.763]
 [0.787]]
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.016]
 [0.017]
 [0.018]
 [0.017]] [[54.187]
 [51.858]
 [55.051]
 [54.943]
 [54.943]] [[1.221]
 [1.138]
 [1.251]
 [1.247]
 [1.247]]
siam score:  -0.907296
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.017]
 [0.016]
 [0.016]] [[33.722]
 [37.992]
 [42.217]
 [33.722]
 [33.722]] [[0.678]
 [0.87 ]
 [1.062]
 [0.678]
 [0.678]]
printing an ep nov before normalisation:  56.97815750092177
printing an ep nov before normalisation:  59.41692451191531
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  37.6368522644043
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.016]
 [0.022]
 [0.02 ]
 [0.021]] [[40.65 ]
 [29.409]
 [40.077]
 [40.576]
 [42.588]] [[0.925]
 [0.459]
 [0.907]
 [0.926]
 [1.01 ]]
printing an ep nov before normalisation:  42.69115086387331
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  29.187743423161862
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  32.21572356802271
printing an ep nov before normalisation:  25.512834797735806
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.034]
 [0.034]
 [0.034]
 [0.018]] [[37.485]
 [28.917]
 [28.917]
 [28.917]
 [31.839]] [[0.778]
 [0.508]
 [0.508]
 [0.508]
 [0.59 ]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  41.40201388206129
siam score:  -0.907134
maxi score, test score, baseline:  0.0081 0.15 0.15
from probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  67.6920448813187
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  33.10473680496216
using another actor
printing an ep nov before normalisation:  30.642464282708747
printing an ep nov before normalisation:  37.94387574176959
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.67288389728404
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
actions average: 
K:  0  action  0 :  tensor([    0.9720,     0.0007,     0.0012,     0.0019,     0.0241],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9004,     0.0094,     0.0005,     0.0893],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0029, 0.0110, 0.8811, 0.0379, 0.0671], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0042,     0.8818,     0.1138],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0101, 0.0035, 0.1036, 0.1189, 0.7639], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  19.49335126779623
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  59.455166960090175
printing an ep nov before normalisation:  34.59488289784078
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.747626730326544
printing an ep nov before normalisation:  19.523448711747236
printing an ep nov before normalisation:  32.39619527994238
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  0.08239557965197264
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.02 ]
 [0.022]
 [0.022]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.02 ]
 [0.022]
 [0.022]
 [0.021]]
printing an ep nov before normalisation:  44.75755405252682
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  33.14369868449062
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  39.25307715355323
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
printing an ep nov before normalisation:  42.03340485784957
printing an ep nov before normalisation:  43.03733131858042
printing an ep nov before normalisation:  45.276664003413934
printing an ep nov before normalisation:  29.989166259765625
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[38.683]
 [38.683]
 [38.683]
 [38.683]
 [38.683]] [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]]
siam score:  -0.91225946
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  39.90122708366947
printing an ep nov before normalisation:  45.70554523016943
printing an ep nov before normalisation:  42.7469233274356
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04693807811616343, 0.04693807811616343, 0.07817162295542515, 0.5154412507050893, 0.2655728919909955, 0.04693807811616343]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  47.49041007476406
printing an ep nov before normalisation:  40.58003175969937
siam score:  -0.91111755
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
siam score:  -0.9100698
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
printing an ep nov before normalisation:  87.60072456454135
printing an ep nov before normalisation:  27.546196206989467
printing an ep nov before normalisation:  53.242554458875404
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681298787294546, 0.047681298787294546, 0.0774276407571376, 0.5236227703047831, 0.2559056925761958, 0.047681298787294546]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.024]
 [0.021]
 [0.021]] [[35.692]
 [35.692]
 [41.183]
 [35.692]
 [35.692]] [[0.913]
 [0.913]
 [1.179]
 [0.913]
 [0.913]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.57853206988753
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04835695866033154, 0.04835695866033154, 0.07675128858185197, 0.5310605673261788, 0.24711726811097456, 0.04835695866033154]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04835695866033154, 0.04835695866033154, 0.07675128858185197, 0.5310605673261788, 0.24711726811097456, 0.04835695866033154]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04897386942475781, 0.04897386942475781, 0.07613374571135216, 0.5378516425834561, 0.23909300343091824, 0.04897386942475781]
printing an ep nov before normalisation:  57.790266825710376
printing an ep nov before normalisation:  32.27164805073941
maxi score, test score, baseline:  0.0081 0.15 0.15
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.85917440750975
printing an ep nov before normalisation:  40.09585645847137
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04897386942475781, 0.04897386942475781, 0.07613374571135216, 0.5378516425834561, 0.23909300343091824, 0.04897386942475781]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[1.041]
 [1.041]
 [1.041]
 [1.041]
 [1.041]] [[0.035]
 [0.035]
 [0.035]
 [0.035]
 [0.035]]
printing an ep nov before normalisation:  53.16882101329156
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04897386942475781, 0.04897386942475781, 0.07613374571135216, 0.5378516425834561, 0.23909300343091824, 0.04897386942475781]
printing an ep nov before normalisation:  43.180644296844235
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04897386942475781, 0.04897386942475781, 0.07613374571135216, 0.5378516425834561, 0.23909300343091824, 0.04897386942475781]
printing an ep nov before normalisation:  29.61585521697998
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.151351050259272
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.017]
 [0.019]] [[34.605]
 [34.605]
 [34.605]
 [36.43 ]
 [34.605]] [[1.09 ]
 [1.09 ]
 [1.09 ]
 [1.189]
 [1.09 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
printing an ep nov before normalisation:  22.640509226074588
line 256 mcts: sample exp_bonus 39.29497455766488
printing an ep nov before normalisation:  48.43451186413505
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
printing an ep nov before normalisation:  40.51247628583676
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
maxi score, test score, baseline:  0.0081 0.15 0.15
siam score:  -0.913831
printing an ep nov before normalisation:  31.03078107122777
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
printing an ep nov before normalisation:  37.08727052156836
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.019]
 [0.022]
 [0.018]] [[71.194]
 [71.194]
 [57.477]
 [64.327]
 [74.031]] [[1.253]
 [1.253]
 [0.962]
 [1.112]
 [1.316]]
printing an ep nov before normalisation:  45.36846599639501
Printing some Q and Qe and total Qs values:  [[0.104]
 [0.115]
 [0.104]
 [0.104]
 [0.087]] [[32.056]
 [44.304]
 [32.056]
 [32.056]
 [41.759]] [[0.604]
 [1.151]
 [0.604]
 [0.604]
 [1.012]]
printing an ep nov before normalisation:  39.24549136733872
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
printing an ep nov before normalisation:  27.732652011810874
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.049539374249137944, 0.049539374249137944, 0.07556766145303322, 0.544076831123148, 0.23173738467640484, 0.049539374249137944]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.84978684001155
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 43.284994414996156
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
printing an ep nov before normalisation:  33.718661214078665
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  36.4135185356999
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.032]
 [0.031]
 [0.031]
 [0.032]] [[38.206]
 [48.404]
 [51.563]
 [52.958]
 [52.805]] [[0.692]
 [1.049]
 [1.158]
 [1.207]
 [1.202]]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
Printing some Q and Qe and total Qs values:  [[0.364]
 [0.364]
 [0.364]
 [0.412]
 [0.364]] [[26.869]
 [26.869]
 [26.869]
 [39.116]
 [26.869]] [[0.753]
 [0.753]
 [0.753]
 [1.301]
 [0.753]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.050059641467255625, 0.050059641467255625, 0.0750468611528437, 0.5498040351790172, 0.22497017926637217, 0.050059641467255625]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  4  action  0 :  tensor([    0.9229,     0.0256,     0.0002,     0.0004,     0.0510],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9212,     0.0258,     0.0006,     0.0520],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0317,     0.8884,     0.0275,     0.0521],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0023,     0.0175,     0.9098,     0.0698],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0063, 0.1149, 0.0760, 0.1018, 0.7011], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.05053989049453123, 0.05053989049453123, 0.07456612004738684, 0.5550907111044989, 0.21872349736452049, 0.05053989049453123]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.032]
 [0.031]
 [0.032]] [[71.19 ]
 [73.091]
 [72.817]
 [70.996]
 [71.19 ]] [[1.599]
 [1.652]
 [1.644]
 [1.592]
 [1.599]]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.05053989049453123, 0.05053989049453123, 0.07456612004738684, 0.5550907111044989, 0.21872349736452049, 0.05053989049453123]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.05053989049453123, 0.05053989049453123, 0.07456612004738684, 0.5550907111044989, 0.21872349736452049, 0.05053989049453123]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.057576286810715
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.026]
 [0.026]
 [0.026]] [[38.403]
 [38.403]
 [38.403]
 [38.403]
 [38.403]] [[1.448]
 [1.448]
 [1.448]
 [1.448]
 [1.448]]
printing an ep nov before normalisation:  32.92623043060303
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0081 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.094]
 [0.096]
 [0.096]
 [0.092]] [[47.514]
 [42.912]
 [43.088]
 [43.088]
 [47.317]] [[0.055]
 [0.094]
 [0.096]
 [0.096]
 [0.092]]
from probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.415]
 [22.263]
 [22.263]
 [22.263]
 [22.263]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  52.62845014524817
printing an ep nov before normalisation:  52.62902998970288
printing an ep nov before normalisation:  54.293989288212465
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.   ]
 [0.033]
 [0.025]
 [0.024]] [[43.547]
 [32.819]
 [36.076]
 [27.601]
 [31.135]] [[0.773]
 [0.565]
 [0.654]
 [0.5  ]
 [0.56 ]]
printing an ep nov before normalisation:  48.10032072764059
printing an ep nov before normalisation:  38.82439871742419
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
siam score:  -0.915444
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.   ]
 [0.026]
 [0.026]
 [0.026]] [[28.664]
 [22.354]
 [33.329]
 [28.664]
 [28.664]] [[1.324]
 [1.012]
 [1.535]
 [1.324]
 [1.324]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.91030310503775
siam score:  -0.9181931
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.036]
 [0.049]
 [0.045]
 [0.035]] [[30.837]
 [22.604]
 [25.682]
 [41.934]
 [23.367]] [[0.754]
 [0.455]
 [0.581]
 [1.175]
 [0.482]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
printing an ep nov before normalisation:  42.05290316308994
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
printing an ep nov before normalisation:  28.968071937561035
printing an ep nov before normalisation:  63.05970191955242
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
printing an ep nov before normalisation:  27.860221605250917
actions average: 
K:  4  action  0 :  tensor([    0.9418,     0.0003,     0.0001,     0.0123,     0.0456],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9155,     0.0007,     0.0001,     0.0836],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0185,     0.8939,     0.0451,     0.0422],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0066, 0.0014, 0.0295, 0.8986, 0.0640], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0015, 0.1699, 0.0791, 0.1163, 0.6331], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
maxi score, test score, baseline:  0.0081 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.002]
 [0.038]
 [0.025]
 [0.021]] [[32.189]
 [32.167]
 [37.107]
 [27.673]
 [34.778]] [[0.33 ]
 [0.311]
 [0.432]
 [0.257]
 [0.376]]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  31.968900520925253
printing an ep nov before normalisation:  29.777055039639873
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04935607934475714, 0.04935607934475714, 0.07281819680913905, 0.5420605460967772, 0.23705301905981246, 0.04935607934475714]
printing an ep nov before normalisation:  38.98170623995178
Printing some Q and Qe and total Qs values:  [[0.092]
 [0.092]
 [0.092]
 [0.044]
 [0.092]] [[27.177]
 [27.177]
 [27.177]
 [38.428]
 [27.177]] [[0.754]
 [0.754]
 [0.754]
 [1.509]
 [0.754]]
printing an ep nov before normalisation:  32.0341682434082
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  46.785896165030344
printing an ep nov before normalisation:  46.87789026949258
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  42.55865728914458
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  20.736941960115892
line 256 mcts: sample exp_bonus 44.00955643794478
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.747151374816895
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.0
printing an ep nov before normalisation:  43.45400333404541
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  39.83790533830219
printing an ep nov before normalisation:  45.11188409359352
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  24.898927211761475
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.046]] [[67.961]
 [67.961]
 [67.961]
 [67.961]
 [67.961]] [[0.713]
 [0.713]
 [0.713]
 [0.713]
 [0.713]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  29.999708910137333
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.049]
 [0.041]
 [0.041]] [[38.201]
 [38.201]
 [60.518]
 [38.201]
 [38.201]] [[0.431]
 [0.431]
 [0.904]
 [0.431]
 [0.431]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  41.52897402768798
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  37.795553459009554
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04884135165516898, 0.04884135165516898, 0.07321900303685815, 0.5363943792889525, 0.2438625627086824, 0.04884135165516898]
printing an ep nov before normalisation:  39.494925184273015
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
Printing some Q and Qe and total Qs values:  [[0.918]
 [0.924]
 [0.93 ]
 [0.931]
 [0.934]] [[25.741]
 [28.482]
 [20.513]
 [29.973]
 [14.148]] [[2.007]
 [2.129]
 [1.798]
 [2.2  ]
 [1.533]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
Printing some Q and Qe and total Qs values:  [[0.256]
 [0.256]
 [0.256]
 [0.232]
 [0.256]] [[42.381]
 [42.381]
 [42.381]
 [52.867]
 [42.381]] [[0.875]
 [0.875]
 [0.875]
 [1.197]
 [0.875]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  44.53335390490019
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
printing an ep nov before normalisation:  68.35687388184034
printing an ep nov before normalisation:  52.47815672670565
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
siam score:  -0.93203056
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
printing an ep nov before normalisation:  45.49180652334692
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.047681087158970854, 0.047681087158970854, 0.07147820306051002, 0.523623405189754, 0.2618551302728233, 0.047681087158970854]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.83324432373047
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.048226582379779476, 0.048226582379779476, 0.07115046966111248, 0.5296282152877726, 0.25454156791177657, 0.048226582379779476]
using another actor
from probs:  [0.048226582379779476, 0.048226582379779476, 0.07115046966111248, 0.5296282152877726, 0.25454156791177657, 0.048226582379779476]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  34.858083101047775
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.026]] [[29.757]
 [34.853]
 [33.034]
 [33.034]
 [32.968]] [[0.847]
 [1.145]
 [1.038]
 [1.038]
 [1.037]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.048226582379779476, 0.048226582379779476, 0.07115046966111248, 0.5296282152877726, 0.25454156791177657, 0.048226582379779476]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.048226582379779476, 0.048226582379779476, 0.07115046966111248, 0.5296282152877726, 0.25454156791177657, 0.048226582379779476]
printing an ep nov before normalisation:  48.71443357109449
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04873346103091871, 0.04873346103091871, 0.07084593708762145, 0.535207934278379, 0.24774574554124337, 0.04873346103091871]
printing an ep nov before normalisation:  36.83094898309634
printing an ep nov before normalisation:  38.95407221180411
printing an ep nov before normalisation:  32.31647317389197
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.135]
 [0.159]
 [0.158]
 [0.138]] [[31.858]
 [30.838]
 [29.624]
 [33.432]
 [29.863]] [[0.938]
 [0.857]
 [0.826]
 [0.998]
 [0.816]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  36.688495915996896
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
Printing some Q and Qe and total Qs values:  [[0.179]
 [0.142]
 [0.164]
 [0.167]
 [0.182]] [[44.997]
 [48.114]
 [39.12 ]
 [30.984]
 [38.283]] [[0.359]
 [0.343]
 [0.302]
 [0.248]
 [0.314]]
Printing some Q and Qe and total Qs values:  [[1.017]
 [1.105]
 [1.017]
 [1.017]
 [1.017]] [[38.501]
 [40.265]
 [38.501]
 [38.501]
 [38.501]] [[1.833]
 [1.991]
 [1.833]
 [1.833]
 [1.833]]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.16523294407753
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
printing an ep nov before normalisation:  41.30348744177113
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
maxi score, test score, baseline:  0.0081 0.15 0.15
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.349]
 [0.539]
 [0.592]
 [0.429]] [[33.37 ]
 [28.974]
 [35.364]
 [37.148]
 [34.988]] [[0.89 ]
 [0.99 ]
 [1.484]
 [1.621]
 [1.356]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04768101020303015, 0.04768101020303015, 0.06931476592369133, 0.5236236360575762, 0.26401856740964197, 0.04768101020303015]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.773]
 [0.773]
 [0.773]
 [0.773]] [[36.427]
 [36.427]
 [36.427]
 [36.427]
 [36.427]] [[2.357]
 [2.357]
 [2.357]
 [2.357]
 [2.357]]
using explorer policy with actor:  1
siam score:  -0.9270905
printing an ep nov before normalisation:  22.452002730495057
printing an ep nov before normalisation:  51.875162804511845
maxi score, test score, baseline:  0.0081 0.15 0.15
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04667316344172001, 0.04667316344172001, 0.06784848754024, 0.51253029360916, 0.27960172852543996, 0.04667316344172001]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.96981794266862
printing an ep nov before normalisation:  40.06944190793635
line 256 mcts: sample exp_bonus 29.336399197578434
printing an ep nov before normalisation:  36.668663372160275
printing an ep nov before normalisation:  28.948750495910645
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04667316344172001, 0.04667316344172001, 0.06784848754024, 0.51253029360916, 0.27960172852543996, 0.04667316344172001]
UNIT TEST: sample policy line 217 mcts : [0.051 0.154 0.59  0.051 0.154]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.583]
 [0.583]
 [0.636]
 [0.557]] [[38.17 ]
 [31.156]
 [31.156]
 [37.124]
 [32.911]] [[0.775]
 [0.758]
 [0.758]
 [0.875]
 [0.751]]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.04719357458662971, 0.04719357458662971, 0.06767467608606462, 0.5182589090736325, 0.27248569108041365, 0.04719357458662971]
printing an ep nov before normalisation:  45.83094564420256
printing an ep nov before normalisation:  0.016652698890311513
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0081 0.15 0.15
printing an ep nov before normalisation:  41.674729315438476
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.0663178923834022, 0.046248137526749306, 0.0663178923834022, 0.5078524992297658, 0.2670154409499311, 0.046248137526749306]
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.0663178923834022, 0.046248137526749306, 0.0663178923834022, 0.5078524992297658, 0.2670154409499311, 0.046248137526749306]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.0663178923834022, 0.046248137526749306, 0.0663178923834022, 0.5078524992297658, 0.2670154409499311, 0.046248137526749306]
printing an ep nov before normalisation:  36.708627656447
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.0663178923834022, 0.046248137526749306, 0.0663178923834022, 0.5078524992297658, 0.2670154409499311, 0.046248137526749306]
printing an ep nov before normalisation:  21.716878414154053
printing an ep nov before normalisation:  27.509337226441712
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.0663178923834022, 0.046248137526749306, 0.0663178923834022, 0.5078524992297658, 0.2670154409499311, 0.046248137526749306]
actor:  1 policy actor:  1  step number:  102 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.07733392715454
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
printing an ep nov before normalisation:  32.95401334762573
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.001]
 [0.001]
 [0.   ]
 [0.   ]] [[15.121]
 [12.461]
 [19.624]
 [17.322]
 [18.831]] [[0.604]
 [0.426]
 [0.67 ]
 [0.591]
 [0.643]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
printing an ep nov before normalisation:  37.23716735839844
using another actor
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
printing an ep nov before normalisation:  46.724335655288996
maxi score, test score, baseline:  0.0081 0.15 0.15
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
printing an ep nov before normalisation:  67.14177831573706
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
printing an ep nov before normalisation:  39.56289698426812
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06620051466624063, 0.04675545298873882, 0.06620051466624063, 0.5134369332487824, 0.26065113144125873, 0.04675545298873882]
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[49.256]
 [49.256]
 [49.256]
 [49.256]
 [49.256]] [[1.004]
 [1.004]
 [1.004]
 [1.004]
 [1.004]]
printing an ep nov before normalisation:  41.70585254658364
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.024]
 [0.024]
 [0.024]
 [0.023]] [[46.829]
 [46.829]
 [46.829]
 [43.698]
 [48.872]] [[1.566]
 [1.566]
 [1.566]
 [1.403]
 [1.67 ]]
printing an ep nov before normalisation:  46.51542528293196
printing an ep nov before normalisation:  37.51587244371771
maxi score, test score, baseline:  0.0081 0.15 0.15
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.123512243419896
printing an ep nov before normalisation:  85.78815522733989
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06609022339653281, 0.047232140283382726, 0.06609022339653281, 0.518684218112135, 0.2546710545280338, 0.047232140283382726]
printing an ep nov before normalisation:  25.201468607408728
printing an ep nov before normalisation:  37.90750831245318
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.032]
 [0.031]
 [0.031]] [[36.14 ]
 [28.725]
 [37.343]
 [28.613]
 [22.271]] [[0.029]
 [0.029]
 [0.032]
 [0.031]
 [0.031]]
printing an ep nov before normalisation:  55.98135909522864
printing an ep nov before normalisation:  32.70746151056308
printing an ep nov before normalisation:  26.22473620202662
printing an ep nov before normalisation:  41.26294358984411
printing an ep nov before normalisation:  30.45642375946045
printing an ep nov before normalisation:  34.7057959041884
maxi score, test score, baseline:  0.0081 0.15 0.15
probs:  [0.06609022339653281, 0.047232140283382726, 0.06609022339653281, 0.518684218112135, 0.2546710545280338, 0.047232140283382726]
printing an ep nov before normalisation:  59.45621585808331
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  69.70811337591422
printing an ep nov before normalisation:  28.507866859436035
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.394129155388484
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.25 ]
 [0.262]
 [0.25 ]
 [0.25 ]] [[32.055]
 [32.055]
 [39.414]
 [32.055]
 [32.055]] [[0.25 ]
 [0.25 ]
 [0.262]
 [0.25 ]
 [0.25 ]]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.304174423217773
Printing some Q and Qe and total Qs values:  [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]] [[36.186]
 [36.186]
 [36.186]
 [36.186]
 [36.186]] [[0.224]
 [0.224]
 [0.224]
 [0.224]
 [0.224]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.183]
 [0.183]
 [0.183]
 [0.183]
 [0.183]] [[41.855]
 [41.855]
 [41.855]
 [41.855]
 [41.855]] [[1.586]
 [1.586]
 [1.586]
 [1.586]
 [1.586]]
siam score:  -0.928132
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.25322771042076
rdn probs:  [0.06609022339653281, 0.047232140283382726, 0.06609022339653281, 0.518684218112135, 0.2546710545280338, 0.047232140283382726]
siam score:  -0.93129957
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
printing an ep nov before normalisation:  40.58115035951426
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
using another actor
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836677, 0.04723307518056053, 0.06609101067836677, 0.5186814626257163, 0.25467036565642914, 0.04723307518056053]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.019]
 [0.035]
 [0.033]
 [0.027]] [[26.744]
 [36.74 ]
 [30.986]
 [24.162]
 [23.743]] [[0.021]
 [0.019]
 [0.035]
 [0.033]
 [0.027]]
printing an ep nov before normalisation:  30.681467261550978
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.035]
 [0.037]
 [0.037]
 [0.037]] [[37.221]
 [35.411]
 [37.221]
 [37.221]
 [37.617]] [[1.677]
 [1.553]
 [1.677]
 [1.677]
 [1.704]]
printing an ep nov before normalisation:  35.29136605578499
printing an ep nov before normalisation:  45.67447696060986
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836677, 0.04723307518056053, 0.06609101067836677, 0.5186814626257163, 0.25467036565642914, 0.04723307518056053]
from probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.25467036565642914, 0.04723307518056052]
printing an ep nov before normalisation:  52.31590558065574
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
from probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
printing an ep nov before normalisation:  40.37580336933708
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
printing an ep nov before normalisation:  30.22099528239298
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
using explorer policy with actor:  1
siam score:  -0.92919654
Printing some Q and Qe and total Qs values:  [[0.066]
 [0.072]
 [0.066]
 [0.066]
 [0.064]] [[48.24 ]
 [47.852]
 [49.589]
 [48.678]
 [48.754]] [[1.455]
 [1.446]
 [1.506]
 [1.472]
 [1.472]]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.9853,     0.0091,     0.0000,     0.0004,     0.0051],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9473,     0.0104,     0.0001,     0.0419],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0046, 0.0187, 0.8912, 0.0115, 0.0740], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0002,     0.0235,     0.9047,     0.0716],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0006,     0.1284,     0.0950,     0.1096,     0.6663],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.63317298414742
printing an ep nov before normalisation:  46.15424583640878
printing an ep nov before normalisation:  68.67078629607553
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
printing an ep nov before normalisation:  63.71745201208654
printing an ep nov before normalisation:  33.69345159168089
printing an ep nov before normalisation:  26.768315818225553
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
printing an ep nov before normalisation:  48.5085370529596
printing an ep nov before normalisation:  49.80495785537021
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
printing an ep nov before normalisation:  36.71244379033989
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
Printing some Q and Qe and total Qs values:  [[0.359]
 [0.634]
 [0.552]
 [0.634]
 [0.634]] [[51.999]
 [52.933]
 [50.767]
 [52.933]
 [52.933]] [[1.486]
 [1.789]
 [1.643]
 [1.789]
 [1.789]]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06598716063533037, 0.04768179590236013, 0.06598716063533037, 0.5236212789595862, 0.24904080796503272, 0.04768179590236013]
siam score:  -0.9299275
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836676, 0.04723307518056052, 0.06609101067836676, 0.5186814626257162, 0.2546703656564291, 0.04723307518056052]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836677, 0.04723307518056053, 0.06609101067836677, 0.5186814626257163, 0.25467036565642914, 0.04723307518056053]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06609101067836677, 0.04723307518056053, 0.06609101067836677, 0.5186814626257163, 0.25467036565642914, 0.04723307518056053]
line 256 mcts: sample exp_bonus 79.36223864431867
printing an ep nov before normalisation:  39.97242928830349
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.168]
 [1.202]
 [1.241]
 [0.393]
 [1.029]] [[32.12 ]
 [32.766]
 [23.668]
 [30.04 ]
 [27.682]] [[2.129]
 [2.183]
 [1.95 ]
 [1.292]
 [1.858]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0648022739341409, 0.046826204628401064, 0.08277834323988073, 0.5142040065776369, 0.2445629669915393, 0.046826204628401064]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0648022739341409, 0.046826204628401064, 0.08277834323988073, 0.5142040065776369, 0.2445629669915393, 0.046826204628401064]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0648022739341409, 0.046826204628401064, 0.08277834323988073, 0.5142040065776369, 0.2445629669915393, 0.046826204628401064]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.69695567304109
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06473919021638554, 0.04726590853919448, 0.08221247189357658, 0.5190445138233528, 0.239472006988296, 0.04726590853919448]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.67924655260677
siam score:  -0.9339077
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06467953936387319, 0.04768168481340762, 0.08167739391433877, 0.523621612226444, 0.234658084868529, 0.04768168481340762]
printing an ep nov before normalisation:  50.793785607335074
printing an ep nov before normalisation:  38.153711394679235
printing an ep nov before normalisation:  32.90389849019887
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06467953936387319, 0.04768168481340762, 0.08167739391433877, 0.523621612226444, 0.234658084868529, 0.04768168481340762]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06467953936387319, 0.04768168481340762, 0.08167739391433877, 0.523621612226444, 0.234658084868529, 0.04768168481340762]
printing an ep nov before normalisation:  38.4766752873822
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.06467953936387319, 0.04768168481340762, 0.08167739391433877, 0.523621612226444, 0.234658084868529, 0.04768168481340762]
UNIT TEST: sample policy line 217 mcts : [0.256 0.333 0.051 0.128 0.231]
printing an ep nov before normalisation:  44.276969076333394
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 30.386744364192502
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
printing an ep nov before normalisation:  39.28059384137834
printing an ep nov before normalisation:  40.63474655151367
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.037]
 [0.04 ]
 [0.047]
 [0.034]
 [0.022]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
printing an ep nov before normalisation:  40.32670481462432
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
printing an ep nov before normalisation:  23.511328411410112
Printing some Q and Qe and total Qs values:  [[1.172]
 [1.172]
 [1.172]
 [1.172]
 [1.172]] [[25.812]
 [25.812]
 [25.812]
 [25.812]
 [25.812]] [[2.149]
 [2.149]
 [2.149]
 [2.149]
 [2.149]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07899408010060234, 0.04611686013832822, 0.07899408010060234, 0.506397939610166, 0.243380179911973, 0.04611686013832822]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  52.02335151401782
printing an ep nov before normalisation:  38.498553507155144
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.07771756415843295, 0.0453724359736207, 0.09389012825083909, 0.4982042305609923, 0.2394432050824942, 0.0453724359736207]
printing an ep nov before normalisation:  34.783360958099365
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0773347391303777, 0.045805823529334516, 0.09309919693089928, 0.5029750997444605, 0.23497931713559356, 0.045805823529334516]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0773347391303777, 0.045805823529334516, 0.09309919693089928, 0.5029750997444605, 0.23497931713559356, 0.045805823529334516]
maxi score, test score, baseline:  0.026099999999999998 0.45 0.45
probs:  [0.0773347391303777, 0.045805823529334516, 0.09309919693089928, 0.5029750997444605, 0.23497931713559356, 0.045805823529334516]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  44.99149072682365
actor:  0 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07697075933584062, 0.04621787682241455, 0.09234720059255364, 0.5075111145238056, 0.23073517190297096, 0.04621787682241455]
printing an ep nov before normalisation:  46.42597302743061
printing an ep nov before normalisation:  45.31396079990134
Printing some Q and Qe and total Qs values:  [[0.042]
 [0.043]
 [0.042]
 [0.042]
 [0.042]] [[40.745]
 [41.687]
 [40.745]
 [40.745]
 [40.745]] [[1.544]
 [1.601]
 [1.544]
 [1.544]
 [1.544]]
printing an ep nov before normalisation:  31.142472834786997
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.71933543579136
Printing some Q and Qe and total Qs values:  [[0.229]
 [0.255]
 [0.229]
 [0.135]
 [0.229]] [[46.38 ]
 [49.681]
 [46.38 ]
 [45.823]
 [46.38 ]] [[1.315]
 [1.486]
 [1.315]
 [1.196]
 [1.315]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.033]
 [0.033]
 [0.033]]
printing an ep nov before normalisation:  59.17525693560684
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07662426666790492, 0.04661013333498436, 0.09163133333436523, 0.5118291999952532, 0.22669493333250784, 0.04661013333498436]
printing an ep nov before normalisation:  56.45685428280203
Printing some Q and Qe and total Qs values:  [[0.341]
 [0.462]
 [0.511]
 [0.459]
 [0.432]] [[53.324]
 [51.46 ]
 [46.928]
 [49.189]
 [52.12 ]] [[1.969]
 [2.005]
 [1.849]
 [1.899]
 [2.005]]
using another actor
from probs:  [0.07662426666790492, 0.04661013333498436, 0.09163133333436523, 0.5118291999952532, 0.22669493333250784, 0.04661013333498436]
using explorer policy with actor:  0
printing an ep nov before normalisation:  43.285584410463976
printing an ep nov before normalisation:  56.61617787551876
actor:  1 policy actor:  1  step number:  114 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.028]
 [0.037]
 [0.031]
 [0.031]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.029]
 [0.028]
 [0.037]
 [0.031]
 [0.031]]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07549233333542849, 0.04592227928205394, 0.09027736036211577, 0.5042581171093593, 0.2381276306289884, 0.04592227928205394]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07549233333542849, 0.04592227928205394, 0.09027736036211577, 0.5042581171093593, 0.2381276306289884, 0.04592227928205394]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07549233333542849, 0.04592227928205394, 0.09027736036211577, 0.5042581171093593, 0.2381276306289884, 0.04592227928205394]
printing an ep nov before normalisation:  61.12847973108251
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.073387457209623
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07549233333542849, 0.04592227928205394, 0.09027736036211577, 0.5042581171093593, 0.2381276306289884, 0.04592227928205394]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07519303998938891, 0.0463066315649854, 0.08963624420159068, 0.5084891663554415, 0.2340682863236082, 0.0463066315649854]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07519303998938891, 0.0463066315649854, 0.08963624420159068, 0.5084891663554415, 0.2340682863236082, 0.0463066315649854]
printing an ep nov before normalisation:  50.124922080252105
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07439340707038411, 0.045254482987347536, 0.08896286911190243, 0.4969078062744146, 0.24922695156860364, 0.045254482987347536]
actions average: 
K:  3  action  0 :  tensor([    0.9441,     0.0092,     0.0000,     0.0059,     0.0408],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9385,     0.0160,     0.0001,     0.0454],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0222,     0.8500,     0.0255,     0.1021],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0008,     0.0918,     0.8650,     0.0424],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0006,     0.0523,     0.1320,     0.0874,     0.7277],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.44486209063573
Printing some Q and Qe and total Qs values:  [[0.025]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[67.676]
 [54.471]
 [54.471]
 [54.471]
 [54.471]] [[1.148]
 [0.835]
 [0.835]
 [0.835]
 [0.835]]
maxi score, test score, baseline:  0.0281 0.45 0.45
printing an ep nov before normalisation:  38.868684323089354
printing an ep nov before normalisation:  22.013835906982422
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.031]
 [0.043]
 [0.042]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.031]
 [0.043]
 [0.042]
 [0.043]]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
maxi score, test score, baseline:  0.0281 0.45 0.45
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
maxi score, test score, baseline:  0.0281 0.45 0.45
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
printing an ep nov before normalisation:  42.01974138767219
printing an ep nov before normalisation:  36.26096963882446
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
Printing some Q and Qe and total Qs values:  [[0.363]
 [0.363]
 [0.229]
 [0.202]
 [0.363]] [[40.739]
 [40.739]
 [41.561]
 [40.763]
 [40.739]] [[1.969]
 [1.969]
 [1.896]
 [1.809]
 [1.969]]
printing an ep nov before normalisation:  32.78885927487347
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07412342262658572, 0.045648578306560814, 0.08836084478659817, 0.5012460874269593, 0.24497248854673512, 0.045648578306560814]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.00773803630566
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
printing an ep nov before normalisation:  39.12405234661328
siam score:  -0.93530023
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
printing an ep nov before normalisation:  37.9166421341859
printing an ep nov before normalisation:  55.244750079665096
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.062]
 [0.063]
 [0.062]
 [0.064]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.062]
 [0.063]
 [0.062]
 [0.064]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.043]
 [0.057]
 [0.052]
 [0.052]] [[41.215]
 [30.458]
 [43.111]
 [40.833]
 [38.594]] [[0.665]
 [0.37 ]
 [0.724]
 [0.658]
 [0.597]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.65865981629108
printing an ep nov before normalisation:  43.123124114749636
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
printing an ep nov before normalisation:  37.085074877168815
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
maxi score, test score, baseline:  0.0281 0.45 0.45
from probs:  [0.07386546993679308, 0.04602511091783102, 0.08778564944627414, 0.5053910347307052, 0.2409076240505655, 0.04602511091783102]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.45 0.45
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07361876224591823, 0.046385229244723575, 0.08723552874651556, 0.509355290265033, 0.2370199602530862, 0.046385229244723575]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.265529706705145
siam score:  -0.9300278
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.367431828399404
siam score:  -0.9292566
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07338257993007875, 0.04672998371962505, 0.0867088780353056, 0.5131504174025647, 0.2332981571928009, 0.04672998371962505]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0281 0.45 0.45
maxi score, test score, baseline:  0.0281 0.45 0.45
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
printing an ep nov before normalisation:  51.2713315482468
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
Printing some Q and Qe and total Qs values:  [[1.115]
 [1.179]
 [1.115]
 [1.115]
 [1.139]] [[39.607]
 [38.614]
 [39.607]
 [39.607]
 [41.092]] [[2.008]
 [2.03 ]
 [2.008]
 [2.008]
 [2.097]]
printing an ep nov before normalisation:  57.07651725768251
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
siam score:  -0.9347702
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
Printing some Q and Qe and total Qs values:  [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]] [[37.606]
 [37.606]
 [37.606]
 [37.606]
 [37.606]] [[0.932]
 [0.932]
 [0.932]
 [0.932]
 [0.932]]
printing an ep nov before normalisation:  32.56981134414673
maxi score, test score, baseline:  0.0281 0.45 0.45
probs:  [0.07293920738122714, 0.047377173030652704, 0.08572022455651433, 0.5202748085162795, 0.22631141348467365, 0.047377173030652704]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0301 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
printing an ep nov before normalisation:  47.70628850042916
printing an ep nov before normalisation:  52.732577323913574
from probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
printing an ep nov before normalisation:  19.692466207817336
maxi score, test score, baseline:  0.0301 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
line 256 mcts: sample exp_bonus 48.1690888551582
printing an ep nov before normalisation:  45.16369957804804
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0301 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
maxi score, test score, baseline:  0.0301 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
printing an ep nov before normalisation:  39.27333562592598
actor:  0 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
printing an ep nov before normalisation:  29.018492698669434
from probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
printing an ep nov before normalisation:  50.13010271183827
actions average: 
K:  4  action  0 :  tensor([    0.9924,     0.0011,     0.0003,     0.0023,     0.0039],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9395,     0.0107,     0.0001,     0.0493],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0148,     0.8778,     0.0299,     0.0773],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0225,     0.9530,     0.0244],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0047, 0.1053, 0.0465, 0.1348, 0.7087], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994275, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07273085464352366, 0.04768130477068553, 0.08525562957994272, 0.5236227523546101, 0.22302815388055244, 0.04768130477068553]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07183202983993964, 0.047092559363402155, 0.08420176507820838, 0.5171424984176144, 0.23263858793743328, 0.047092559363402155]
printing an ep nov before normalisation:  32.35927611274385
from probs:  [0.07183202983993964, 0.047092559363402155, 0.08420176507820838, 0.5171424984176144, 0.23263858793743328, 0.047092559363402155]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07183202983993964, 0.047092559363402155, 0.08420176507820838, 0.5171424984176144, 0.23263858793743328, 0.047092559363402155]
line 256 mcts: sample exp_bonus 48.42755196794753
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07183202983993964, 0.047092559363402155, 0.08420176507820838, 0.5171424984176144, 0.23263858793743328, 0.047092559363402155]
siam score:  -0.93603957
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07183202983993964, 0.047092559363402155, 0.08420176507820838, 0.5171424984176144, 0.23263858793743328, 0.047092559363402155]
siam score:  -0.9360359
printing an ep nov before normalisation:  58.08405689030195
printing an ep nov before normalisation:  46.65751363365547
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.519]
 [0.519]
 [0.519]
 [0.519]
 [0.519]] [[31.069]
 [31.069]
 [31.069]
 [31.069]
 [31.069]] [[1.298]
 [1.298]
 [1.298]
 [1.298]
 [1.298]]
Printing some Q and Qe and total Qs values:  [[0.223]
 [0.223]
 [0.223]
 [0.223]
 [0.223]] [[47.621]
 [47.621]
 [47.621]
 [47.621]
 [47.621]] [[1.511]
 [1.511]
 [1.511]
 [1.511]
 [1.511]]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9394018
printing an ep nov before normalisation:  40.55768471474538
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.48275711565446
printing an ep nov before normalisation:  31.743481601520195
printing an ep nov before normalisation:  29.24931526184082
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0716517485325894, 0.04739262049835691, 0.08378131254970567, 0.5204456171658906, 0.22933608075510067, 0.04739262049835691]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0716517485325894, 0.0473926204983569, 0.08378131254970565, 0.5204456171658904, 0.22933608075510067, 0.0473926204983569]
printing an ep nov before normalisation:  38.55554359103192
line 256 mcts: sample exp_bonus 39.404597366805646
UNIT TEST: sample policy line 217 mcts : [0.385 0.026 0.179 0.282 0.128]
printing an ep nov before normalisation:  41.167022201329715
printing an ep nov before normalisation:  38.97960662863433
printing an ep nov before normalisation:  19.814835786819458
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0716517485325894, 0.0473926204983569, 0.08378131254970565, 0.5204456171658904, 0.22933608075510067, 0.0473926204983569]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07147833458494189, 0.0476812515645107, 0.0833768760951575, 0.5236229119731345, 0.22615937421774465, 0.0476812515645107]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.28566242995127
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07131140293889703, 0.04795909345454528, 0.08298755768107291, 0.5266814378837561, 0.22310141458718344, 0.04795909345454528]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07131140293889703, 0.04795909345454528, 0.08298755768107291, 0.5266814378837561, 0.22310141458718344, 0.04795909345454528]
line 256 mcts: sample exp_bonus 50.247083663514466
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07131140293889703, 0.04795909345454528, 0.08298755768107291, 0.5266814378837561, 0.22310141458718344, 0.04795909345454528]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07131140293889703, 0.04795909345454528, 0.08298755768107291, 0.5266814378837561, 0.22310141458718344, 0.04795909345454528]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07131140293889703, 0.04795909345454528, 0.08298755768107291, 0.5266814378837561, 0.22310141458718344, 0.04795909345454528]
printing an ep nov before normalisation:  49.49378735396523
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.048]
 [0.081]
 [0.056]
 [0.051]] [[44.419]
 [48.584]
 [38.828]
 [43.469]
 [45.48 ]] [[0.28 ]
 [0.309]
 [0.263]
 [0.276]
 [0.287]]
printing an ep nov before normalisation:  34.41133225512016
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.29006802892336
printing an ep nov before normalisation:  66.74854041735458
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.58185377379464
printing an ep nov before normalisation:  39.848071969212455
Printing some Q and Qe and total Qs values:  [[0.893]
 [0.893]
 [0.893]
 [0.893]
 [0.903]] [[49.971]
 [49.971]
 [49.971]
 [49.971]
 [49.003]] [[2.441]
 [2.441]
 [2.441]
 [2.441]
 [2.401]]
printing an ep nov before normalisation:  47.06778419581678
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0703451011857624, 0.04768120342554962, 0.08167705006586877, 0.5236230563900177, 0.2289923855072518, 0.04768120342554962]
printing an ep nov before normalisation:  54.72329039626649
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0703451011857624, 0.04768120342554962, 0.08167705006586877, 0.5236230563900177, 0.2289923855072518, 0.04768120342554962]
printing an ep nov before normalisation:  39.85743999481201
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0703451011857624, 0.04768120342554962, 0.08167705006586877, 0.5236230563900177, 0.2289923855072518, 0.04768120342554962]
Printing some Q and Qe and total Qs values:  [[1.262]
 [1.262]
 [1.262]
 [1.262]
 [1.262]] [[42.964]
 [42.964]
 [42.964]
 [42.964]
 [42.964]] [[2.462]
 [2.462]
 [2.462]
 [2.462]
 [2.462]]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.0703451011857624, 0.04768120342554962, 0.08167705006586877, 0.5236230563900177, 0.2289923855072518, 0.04768120342554962]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([    0.9994,     0.0004,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0251,     0.8920,     0.0098,     0.0002,     0.0729],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0121,     0.8543,     0.0335,     0.0993],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0101,     0.0361,     0.7863,     0.1673],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0028, 0.0547, 0.0920, 0.1332, 0.7173], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
Printing some Q and Qe and total Qs values:  [[0.053]
 [0.223]
 [0.091]
 [0.091]
 [0.091]] [[34.766]
 [48.082]
 [38.994]
 [38.994]
 [38.994]] [[0.053]
 [0.223]
 [0.091]
 [0.091]
 [0.091]]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
printing an ep nov before normalisation:  60.41788826234294
printing an ep nov before normalisation:  52.1056002612767
printing an ep nov before normalisation:  53.15270232044898
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
printing an ep nov before normalisation:  43.44895222864939
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
from probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
maxi score, test score, baseline:  0.032100000000000004 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
printing an ep nov before normalisation:  45.67113855992207
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07020616678635373, 0.047946051429358434, 0.08133622446485136, 0.5265385316047573, 0.2260269742853208, 0.047946051429358434]
printing an ep nov before normalisation:  25.80384638184126
printing an ep nov before normalisation:  28.80039773829669
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07007209627141767, 0.048201627502682025, 0.08100733065578548, 0.5293519404148658, 0.22316537765256708, 0.048201627502682025]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
printing an ep nov before normalisation:  38.01378868624888
printing an ep nov before normalisation:  31.403729849238033
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.07007209627141765, 0.048201627502682025, 0.08100733065578548, 0.5293519404148658, 0.22316537765256703, 0.048201627502682025]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
printing an ep nov before normalisation:  50.506317425308424
printing an ep nov before normalisation:  52.01851632301223
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
printing an ep nov before normalisation:  31.910499363235093
printing an ep nov before normalisation:  37.520066897074386
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
printing an ep nov before normalisation:  40.3259552445322
siam score:  -0.93877435
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.396]
 [0.35 ]
 [0.119]
 [0.35 ]] [[35.553]
 [45.762]
 [36.479]
 [21.428]
 [36.479]] [[0.437]
 [1.127]
 [0.802]
 [0.119]
 [0.802]]
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
from probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
printing an ep nov before normalisation:  21.97123504943885
from probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
printing an ep nov before normalisation:  38.64272412060789
maxi score, test score, baseline:  0.034100000000000005 0.45 0.45
probs:  [0.08013175248205379, 0.047681159662823935, 0.08013175248205379, 0.5236231876781949, 0.2207509880320497, 0.047681159662823935]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.685119855272546
actor:  0 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07983245623663, 0.04793417485416756, 0.07983245623663, 0.5264083955911042, 0.21805834222730058, 0.04793417485416756]
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9595,     0.0015,     0.0001,     0.0371],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0032,     0.8638,     0.0157,     0.1170],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0001,     0.0361,     0.8985,     0.0650],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0054, 0.0527, 0.0400, 0.0938, 0.8081], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 35.934458419159505
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.797]
 [0.797]
 [0.805]
 [0.682]] [[31.675]
 [31.675]
 [31.675]
 [38.326]
 [33.049]] [[1.489]
 [1.489]
 [1.489]
 [1.774]
 [1.43 ]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.0789932115321873, 0.04743076768377473, 0.08951402614832482, 0.5208674254099632, 0.2157638015419751, 0.04743076768377473]
UNIT TEST: sample policy line 217 mcts : [0.282 0.256 0.051 0.333 0.077]
printing an ep nov before normalisation:  48.13073367928211
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.0789932115321873, 0.04743076768377473, 0.08951402614832482, 0.5208674254099632, 0.2157638015419751, 0.04743076768377473]
maxi score, test score, baseline:  0.0361 0.45 0.45
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.0789932115321873, 0.04743076768377473, 0.08951402614832482, 0.5208674254099632, 0.2157638015419751, 0.04743076768377473]
Printing some Q and Qe and total Qs values:  [[0.35 ]
 [0.341]
 [0.366]
 [0.367]
 [0.337]] [[21.951]
 [21.09 ]
 [28.766]
 [29.186]
 [20.648]] [[0.847]
 [0.797]
 [1.189]
 [1.21 ]
 [0.771]]
printing an ep nov before normalisation:  39.439302954276236
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.0789932115321873, 0.04743076768377473, 0.08951402614832482, 0.5208674254099632, 0.2157638015419751, 0.04743076768377473]
printing an ep nov before normalisation:  32.314707022121745
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.0789932115321873, 0.04743076768377473, 0.08951402614832482, 0.5208674254099632, 0.2157638015419751, 0.04743076768377473]
printing an ep nov before normalisation:  27.6146233327483
printing an ep nov before normalisation:  0.0013043684617741746
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.479426574411605
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.305]
 [0.305]
 [0.305]
 [0.305]] [[64.218]
 [64.218]
 [64.218]
 [64.218]
 [64.218]] [[1.903]
 [1.903]
 [1.903]
 [1.903]
 [1.903]]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07872082760843117, 0.04768111970552452, 0.08906739690940005, 0.523623307550093, 0.2132262285210266, 0.04768111970552452]
maxi score, test score, baseline:  0.0361 0.45 0.45
printing an ep nov before normalisation:  47.03240769677512
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.414]
 [0.464]
 [0.48 ]
 [0.444]] [[26.892]
 [24.229]
 [26.697]
 [32.028]
 [26.721]] [[0.433]
 [0.414]
 [0.464]
 [0.48 ]
 [0.444]]
printing an ep nov before normalisation:  44.670650960825874
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.138]
 [1.212]
 [1.141]
 [1.096]
 [1.218]] [[32.947]
 [36.636]
 [37.603]
 [31.718]
 [36.747]] [[1.96 ]
 [2.196]
 [2.167]
 [1.865]
 [2.206]]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07845731911369491, 0.04792331419151238, 0.08863532075442242, 0.5262893913057053, 0.21077134044315252, 0.04792331419151238]
printing an ep nov before normalisation:  25.571912795632755
maxi score, test score, baseline:  0.0361 0.45 0.45
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.33698843815114
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07820225920323771, 0.048157743460941094, 0.08821709778400326, 0.5288699953376871, 0.20839516075318973, 0.048157743460941094]
siam score:  -0.93681914
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07820225920323771, 0.048157743460941094, 0.08821709778400326, 0.5288699953376871, 0.20839516075318973, 0.048157743460941094]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07845731911369491, 0.04792331419151238, 0.08863532075442242, 0.5262893913057053, 0.21077134044315252, 0.04792331419151238]
printing an ep nov before normalisation:  41.798856610716484
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07845731911369491, 0.04792331419151238, 0.08863532075442242, 0.5262893913057053, 0.21077134044315252, 0.04792331419151238]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  61.26348729122005
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
from probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500499, 0.04744132995982724]
maxi score, test score, baseline:  0.0361 0.45 0.45
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500499, 0.04744132995982724]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
printing an ep nov before normalisation:  32.6339652765025
maxi score, test score, baseline:  0.0361 0.45 0.45
maxi score, test score, baseline:  0.0361 0.45 0.45
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
siam score:  -0.94222945
printing an ep nov before normalisation:  0.02411855052798728
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.003]
 [0.003]
 [0.002]
 [0.002]] [[ 0.076]
 [ 0.074]
 [ 0.078]
 [24.349]
 [24.349]] [[0.005]
 [0.008]
 [0.008]
 [1.457]
 [1.457]]
maxi score, test score, baseline:  0.0361 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
printing an ep nov before normalisation:  30.396804809570312
actor:  0 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  24.432605006273228
from probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
printing an ep nov before normalisation:  48.21751848403201
printing an ep nov before normalisation:  48.43230147297762
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07766747166015274, 0.04744132995982724, 0.08774285222692789, 0.5209842165982599, 0.21872279959500504, 0.04744132995982724]
printing an ep nov before normalisation:  42.5318593507964
Printing some Q and Qe and total Qs values:  [[1.47 ]
 [1.295]
 [1.295]
 [1.295]
 [1.436]] [[21.169]
 [27.014]
 [27.014]
 [27.014]
 [30.683]] [[2.272]
 [2.318]
 [2.318]
 [2.318]
 [2.599]]
printing an ep nov before normalisation:  0.0002069929240633428
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  26.861823621967407
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.07339362503226
maxi score, test score, baseline:  0.0381 0.45 0.45
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
maxi score, test score, baseline:  0.0381 0.45 0.45
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.970955848693848
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0766679233547539, 0.047213789179946085, 0.08648596807968983, 0.5184799359768713, 0.223938594228793, 0.047213789179946085]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07514821235428565, 0.056494465787712396, 0.08477133900582565, 0.5081889116735852, 0.22911823877892548, 0.046278832399665695]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07514821235428565, 0.056494465787712396, 0.08477133900582565, 0.5081889116735852, 0.22911823877892548, 0.046278832399665695]
printing an ep nov before normalisation:  20.591408778631106
printing an ep nov before normalisation:  37.404292893332524
siam score:  -0.93448484
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07514821235428565, 0.056494465787712396, 0.08477133900582565, 0.5081889116735852, 0.22911823877892548, 0.046278832399665695]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07495704659885395, 0.05658628202754893, 0.08443418801515697, 0.510905551748793, 0.2265913092597023, 0.04652562234994488]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07495704659885395, 0.05658628202754893, 0.08443418801515697, 0.510905551748793, 0.2265913092597023, 0.04652562234994488]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07495704659885395, 0.05658628202754893, 0.08443418801515697, 0.510905551748793, 0.2265913092597023, 0.04652562234994488]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9801,     0.0014,     0.0001,     0.0184],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0086,     0.8625,     0.0536,     0.0746],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0005,     0.0324,     0.8524,     0.1138],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0166, 0.1654, 0.0632, 0.1110, 0.6439], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.251]
 [1.251]
 [1.251]
 [1.251]
 [1.251]] [[43.924]
 [43.924]
 [43.924]
 [43.924]
 [43.924]] [[2.918]
 [2.918]
 [2.918]
 [2.918]
 [2.918]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.0742539618833207, 0.05605578815396671, 0.0836420668411802, 0.5061067899448579, 0.23385174616693233, 0.046089647009742184]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
Printing some Q and Qe and total Qs values:  [[0.071]
 [0.085]
 [0.085]
 [0.085]
 [0.085]] [[85.081]
 [66.297]
 [66.297]
 [66.297]
 [66.297]] [[1.303]
 [0.964]
 [0.964]
 [0.964]
 [0.964]]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
printing an ep nov before normalisation:  55.64742951476635
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
printing an ep nov before normalisation:  42.56465435028076
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07408063536024884, 0.05615188992928426, 0.08332974718717578, 0.5087888912258162, 0.23131553641800717, 0.046333299879467944]
printing an ep nov before normalisation:  42.593293905216626
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07391388000495934, 0.05622583991152619, 0.08302824127581136, 0.5114032210058562, 0.22885802160944368, 0.046570796192403285]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07391388000495934, 0.05622583991152619, 0.08302824127581136, 0.5114032210058562, 0.22885802160944368, 0.046570796192403285]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07391388000495934, 0.05622583991152619, 0.08302824127581136, 0.5114032210058562, 0.22885802160944368, 0.046570796192403285]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  48.615237428772744
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07375042505154636, 0.05631673449041356, 0.08273372411798156, 0.5139320793068727, 0.2264665091809453, 0.04680052785224064]
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07375190750772105, 0.056297746706357454, 0.08273538744139351, 0.5139424242576714, 0.22647106638015282, 0.04680146770670369]
printing an ep nov before normalisation:  88.55840123402969
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07375190750772105, 0.056297746706357454, 0.08273538744139351, 0.5139424242576714, 0.22647106638015282, 0.04680146770670369]
from probs:  [0.07375190750772105, 0.056297746706357454, 0.08273538744139351, 0.5139424242576714, 0.22647106638015282, 0.04680146770670369]
printing an ep nov before normalisation:  56.460960191735296
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.   ]
 [0.053]
 [0.057]
 [0.055]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.087]
 [0.   ]
 [0.053]
 [0.057]
 [0.055]]
printing an ep nov before normalisation:  33.76368694849271
maxi score, test score, baseline:  0.0381 0.45 0.45
probs:  [0.07375190750772105, 0.056297746706357454, 0.08273538744139351, 0.5139424242576716, 0.22647106638015282, 0.0468014677067037]
Printing some Q and Qe and total Qs values:  [[0.08]
 [0.08]
 [0.08]
 [0.08]
 [0.08]] [[45.29]
 [45.29]
 [45.29]
 [45.29]
 [45.29]] [[0.948]
 [0.948]
 [0.948]
 [0.948]
 [0.948]]
actor:  0 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07375190750772105, 0.056297746706357454, 0.08273538744139351, 0.5139424242576716, 0.22647106638015282, 0.0468014677067037]
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07306165840711176, 0.06513868318102388, 0.08196092441935265, 0.5091256930069157, 0.22434918061520695, 0.04636386037038909]
printing an ep nov before normalisation:  47.44164610412294
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.36179065704346
printing an ep nov before normalisation:  34.56353964159904
printing an ep nov before normalisation:  41.428736321982285
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
printing an ep nov before normalisation:  52.79378861225089
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
printing an ep nov before normalisation:  27.555757297101348
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.3160021046776
Printing some Q and Qe and total Qs values:  [[0.305]
 [0.299]
 [0.573]
 [0.573]
 [0.674]] [[46.339]
 [40.042]
 [39.308]
 [39.308]
 [40.712]] [[1.789]
 [1.455]
 [1.69 ]
 [1.69 ]
 [1.865]]
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
printing an ep nov before normalisation:  49.60267682942959
line 256 mcts: sample exp_bonus 41.02823758737689
printing an ep nov before normalisation:  28.928306102752686
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
printing an ep nov before normalisation:  42.20402435695735
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
actions average: 
K:  1  action  0 :  tensor([    0.9985,     0.0006,     0.0000,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9423,     0.0308,     0.0004,     0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0210,     0.9380,     0.0007,     0.0401],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0000,     0.0192,     0.8639,     0.1168],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0057, 0.0719, 0.0255, 0.0981, 0.7989], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
Printing some Q and Qe and total Qs values:  [[0.708]
 [0.744]
 [0.769]
 [0.708]
 [0.708]] [[41.33 ]
 [45.807]
 [45.103]
 [41.33 ]
 [41.33 ]] [[1.871]
 [2.102]
 [2.096]
 [1.871]
 [1.871]]
printing an ep nov before normalisation:  48.820677827058645
printing an ep nov before normalisation:  27.585169012316758
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
line 256 mcts: sample exp_bonus 36.11223190797796
printing an ep nov before normalisation:  54.11477357677914
printing an ep nov before normalisation:  56.54242711867763
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.0727703852338307, 0.06506688628405619, 0.08142313049969115, 0.5140603937927142, 0.21986705475345852, 0.04681214943624931]
Printing some Q and Qe and total Qs values:  [[1.318]
 [1.318]
 [0.925]
 [1.318]
 [1.152]] [[26.562]
 [26.562]
 [25.564]
 [26.562]
 [28.409]] [[3.123]
 [3.123]
 [2.624]
 [3.123]
 [3.152]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.402]
 [0.546]
 [0.402]
 [0.402]
 [0.329]] [[46.844]
 [39.816]
 [46.844]
 [46.844]
 [49.858]] [[1.212]
 [1.182]
 [1.212]
 [1.212]
 [1.213]]
siam score:  -0.930848
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07214670701340907, 0.06450933983693317, 0.08072517156708102, 0.5096483992506793, 0.22655906897950445, 0.04641131335239317]
actions average: 
K:  0  action  0 :  tensor([    0.9696,     0.0179,     0.0002,     0.0002,     0.0120],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9253,     0.0165,     0.0006,     0.0575],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0019, 0.0270, 0.8516, 0.0161, 0.1034], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0003,     0.9153,     0.0841],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0010, 0.0476, 0.0833, 0.1814, 0.6868], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  34.288753625732866
siam score:  -0.9340193
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07214670701340907, 0.06450933983693317, 0.080725171567081, 0.5096483992506793, 0.2265590689795044, 0.04641131335239317]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201667015050184, 0.06448275656424186, 0.08047893327819665, 0.5120543527906334, 0.22433740644900893, 0.04662988076741732]
printing an ep nov before normalisation:  34.0780427500683
printing an ep nov before normalisation:  45.557309403474235
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201667015050184, 0.06448275656424186, 0.08047893327819665, 0.5120543527906334, 0.22433740644900893, 0.04662988076741732]
printing an ep nov before normalisation:  49.189261375355386
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201937443754147, 0.06444759105921145, 0.08048195586038089, 0.5120736084251918, 0.22434584004865116, 0.04663163016902317]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.76227624955834
printing an ep nov before normalisation:  52.553537446004654
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201937443754147, 0.06444759105921145, 0.08048195586038089, 0.5120736084251918, 0.22434584004865116, 0.04663163016902317]
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201937443754147, 0.06444759105921145, 0.08048195586038089, 0.5120736084251918, 0.22434584004865116, 0.04663163016902317]
printing an ep nov before normalisation:  76.20575122250237
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07201937443754147, 0.06444759105921145, 0.08048195586038089, 0.5120736084251918, 0.22434584004865116, 0.04663163016902317]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.05620260129698
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07189277253663055, 0.0644221873733665, 0.08024225007204332, 0.5144150819135072, 0.22218336817406031, 0.046844339930392255]
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.105]
 [0.126]
 [0.098]
 [0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.113]
 [0.105]
 [0.126]
 [0.098]
 [0.116]]
printing an ep nov before normalisation:  37.25587365619638
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
probs:  [0.07189277253663055, 0.0644221873733665, 0.08024225007204332, 0.5144150819135072, 0.22218336817406031, 0.046844339930392255]
printing an ep nov before normalisation:  18.513304189723392
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.45 0.45
Starting evaluation
printing an ep nov before normalisation:  77.17650192480066
printing an ep nov before normalisation:  51.004542623247424
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.073]
 [0.08 ]
 [0.078]
 [0.079]] [[35.036]
 [30.648]
 [40.089]
 [34.03 ]
 [34.213]] [[0.072]
 [0.073]
 [0.08 ]
 [0.078]
 [0.079]]
printing an ep nov before normalisation:  50.36924772482367
printing an ep nov before normalisation:  35.49286661077903
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.04276385593345
printing an ep nov before normalisation:  53.49621377902641
Printing some Q and Qe and total Qs values:  [[0.231]
 [0.341]
 [0.231]
 [0.231]
 [0.231]] [[56.381]
 [42.39 ]
 [56.381]
 [56.381]
 [56.381]] [[0.231]
 [0.341]
 [0.231]
 [0.231]
 [0.231]]
printing an ep nov before normalisation:  41.69956025735213
printing an ep nov before normalisation:  54.597697257995605
printing an ep nov before normalisation:  50.044588154743465
printing an ep nov before normalisation:  44.460196217849145
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.]
 [1.]
 [1.]
 [1.]
 [1.]] [[ 5.561]
 [ 9.55 ]
 [14.246]
 [14.181]
 [10.117]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.968]
 [0.891]
 [0.891]
 [0.891]] [[30.482]
 [31.859]
 [30.482]
 [30.482]
 [30.482]] [[0.891]
 [0.968]
 [0.891]
 [0.891]
 [0.891]]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  25.177860260009766
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.24160488351948
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.001]
 [1.001]
 [1.002]
 [1.   ]
 [1.   ]] [[28.238]
 [14.455]
 [28.289]
 [14.124]
 [30.058]] [[1.001]
 [1.001]
 [1.002]
 [1.   ]
 [1.   ]]
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0641 0.45 0.45
probs:  [0.07118356970512028, 0.06387180101887573, 0.07935554647209947, 0.512470315121997, 0.22645112827772498, 0.04666763940418269]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.07118356970512028, 0.06387180101887573, 0.07935554647209947, 0.512470315121997, 0.22645112827772498, 0.04666763940418269]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.07118438838607033, 0.06387268239161026, 0.07935629508576102, 0.5124673501693668, 0.22645061568019323, 0.0466686682869983]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.07118438838607033, 0.06387268239161026, 0.07935629508576102, 0.5124673501693668, 0.22645061568019323, 0.0466686682869983]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.07118438838607033, 0.06387268239161026, 0.07935629508576102, 0.5124673501693668, 0.22645061568019323, 0.0466686682869983]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.07118438838607033, 0.06387268239161026, 0.07935629508576102, 0.5124673501693668, 0.22645061568019323, 0.0466686682869983]
Printing some Q and Qe and total Qs values:  [[0.787]
 [0.787]
 [0.787]
 [0.758]
 [0.787]] [[40.304]
 [40.304]
 [40.304]
 [42.254]
 [40.304]] [[1.722]
 [1.722]
 [1.722]
 [1.758]
 [1.722]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.30652122732524
siam score:  -0.93416715
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.07118438838607033, 0.06387268239161026, 0.07935629508576102, 0.5124673501693668, 0.22645061568019323, 0.0466686682869983]
printing an ep nov before normalisation:  34.77030532649761
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using another actor
printing an ep nov before normalisation:  46.385266587291966
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06461326703079429, 0.057015341875244235, 0.08159686443731791, 0.5231703970069322, 0.22595744239276874, 0.047646687256942516]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.073]
 [0.086]
 [0.073]
 [0.073]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.073]
 [0.073]
 [0.086]
 [0.073]
 [0.073]]
printing an ep nov before normalisation:  66.32751552403887
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06461326703079429, 0.057015341875244235, 0.08159686443731791, 0.5231703970069322, 0.22595744239276874, 0.047646687256942516]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06461326703079429, 0.057015341875244235, 0.08159686443731791, 0.5231703970069322, 0.22595744239276874, 0.047646687256942516]
printing an ep nov before normalisation:  33.43835366264421
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06461326703079429, 0.057015341875244235, 0.08159686443731791, 0.5231703970069322, 0.22595744239276874, 0.047646687256942516]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06461326703079429, 0.057015341875244235, 0.08159686443731791, 0.5231703970069322, 0.22595744239276874, 0.047646687256942516]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06458555498344913, 0.0570895233263972, 0.08134139045215345, 0.5253710303728178, 0.22376599193614016, 0.04784650892904211]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06458555498344913, 0.0570895233263972, 0.08134139045215345, 0.5253710303728178, 0.22376599193614016, 0.04784650892904211]
printing an ep nov before normalisation:  41.545379544153114
siam score:  -0.9328662
printing an ep nov before normalisation:  43.80820893885769
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.957]
 [0.957]
 [0.957]
 [0.957]] [[29.537]
 [28.389]
 [28.389]
 [28.389]
 [28.389]] [[1.397]
 [1.382]
 [1.382]
 [1.382]
 [1.382]]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06458555498344913, 0.0570895233263972, 0.08134139045215345, 0.5253710303728178, 0.22376599193614016, 0.04784650892904211]
printing an ep nov before normalisation:  4.599589290798178
actor:  1 policy actor:  1  step number:  115 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.83268624455518
Printing some Q and Qe and total Qs values:  [[0.335]
 [0.385]
 [0.335]
 [0.335]
 [0.335]] [[43.399]
 [42.134]
 [43.399]
 [43.399]
 [43.399]] [[1.668]
 [1.644]
 [1.668]
 [1.668]
 [1.668]]
printing an ep nov before normalisation:  43.02141894025873
Printing some Q and Qe and total Qs values:  [[0.973]
 [0.973]
 [0.973]
 [0.973]
 [0.973]] [[28.378]
 [28.378]
 [28.378]
 [28.378]
 [28.378]] [[1.404]
 [1.404]
 [1.404]
 [1.404]
 [1.404]]
Printing some Q and Qe and total Qs values:  [[1.292]
 [1.292]
 [1.292]
 [1.292]
 [1.292]] [[47.703]
 [47.703]
 [47.703]
 [47.703]
 [47.703]] [[2.525]
 [2.525]
 [2.525]
 [2.525]
 [2.525]]
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06455857637609301, 0.05716174145661465, 0.08109267796080938, 0.5275134207481511, 0.22163254143089844, 0.04804104202743349]
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06455857637609301, 0.05716174145661465, 0.08109267796080938, 0.5275134207481511, 0.22163254143089844, 0.04804104202743349]
maxi score, test score, baseline:  0.0661 0.65 0.65
from probs:  [0.06455857637609301, 0.05716174145661465, 0.08109267796080938, 0.5275134207481511, 0.22163254143089844, 0.04804104202743349]
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0661 0.65 0.65
printing an ep nov before normalisation:  47.1607308345613
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.378756405654336
printing an ep nov before normalisation:  50.75961680291461
siam score:  -0.93168265
maxi score, test score, baseline:  0.0661 0.65 0.65
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06403216864503755, 0.0566594944235991, 0.08043120102543336, 0.52320507529612, 0.2280224924489956, 0.04764956816081448]
maxi score, test score, baseline:  0.0661 0.65 0.65
probs:  [0.06403216864503755, 0.0566594944235991, 0.08043120102543336, 0.52320507529612, 0.2280224924489956, 0.04764956816081448]
actor:  0 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  23.618255804291515
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.06403216864503755, 0.0566594944235991, 0.08043120102543336, 0.52320507529612, 0.2280224924489956, 0.04764956816081448]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.136]
 [0.184]
 [0.135]
 [0.14 ]
 [0.138]] [[50.295]
 [45.398]
 [68.516]
 [55.708]
 [56.054]] [[0.854]
 [0.81 ]
 [1.197]
 [0.96 ]
 [0.965]]
Printing some Q and Qe and total Qs values:  [[0.112]
 [0.526]
 [0.2  ]
 [0.2  ]
 [0.2  ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.112]
 [0.526]
 [0.2  ]
 [0.2  ]
 [0.2  ]]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05767436138532225, 0.04994765423088066, 0.07486086888772507, 0.5388965714526008, 0.22953943640935032, 0.04908110763412085]
deleting a thread, now have 1 threads
Frames:  131475 train batches done:  15403 episodes:  5041
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05767436138532225, 0.04994765423088066, 0.07486086888772507, 0.5388965714526008, 0.22953943640935032, 0.04908110763412085]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05767436138532225, 0.04994765423088066, 0.07486086888772507, 0.5388965714526008, 0.22953943640935032, 0.04908110763412085]
siam score:  -0.931734
printing an ep nov before normalisation:  31.944051354581493
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.421]
 [0.41 ]
 [0.389]
 [0.446]] [[44.654]
 [50.723]
 [49.643]
 [55.264]
 [48.528]] [[1.644]
 [2.019]
 [1.946]
 [2.245]
 [1.919]]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.057740475016680816, 0.050118609617303064, 0.07469378310127808, 0.5409097554277026, 0.22727355586265338, 0.0492638209743822]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.057740475016680816, 0.050118609617303064, 0.07469378310127808, 0.5409097554277026, 0.22727355586265338, 0.0492638209743822]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.057740475016680816, 0.050118609617303064, 0.07469378310127808, 0.5409097554277026, 0.22727355586265338, 0.0492638209743822]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  36.854658126831055
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
printing an ep nov before normalisation:  28.830105956326545
printing an ep nov before normalisation:  55.780510334976455
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.043]
 [0.945]
 [0.905]
 [0.848]] [[38.592]
 [31.589]
 [32.032]
 [35.031]
 [38.524]] [[1.044]
 [0.16 ]
 [1.065]
 [1.048]
 [1.016]]
printing an ep nov before normalisation:  55.47758963216525
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
actions average: 
K:  4  action  0 :  tensor([    0.9955,     0.0001,     0.0000,     0.0002,     0.0041],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9758,     0.0100,     0.0000,     0.0141],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0009, 0.0204, 0.9090, 0.0113, 0.0584], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0008,     0.0002,     0.0002,     0.9124,     0.0864],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0281, 0.0911, 0.0677, 0.0722, 0.7409], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056640300992749006, 0.04904002912243966, 0.07354557861082028, 0.5384407131077803, 0.22569307717346174, 0.056640300992749006]
Printing some Q and Qe and total Qs values:  [[1.303]
 [1.027]
 [0.279]
 [1.211]
 [1.347]] [[30.936]
 [24.159]
 [26.06 ]
 [32.016]
 [29.889]] [[2.074]
 [1.628]
 [0.928]
 [2.008]
 [2.091]]
printing an ep nov before normalisation:  26.128750210379675
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.149]
 [0.144]
 [0.171]
 [0.149]
 [0.149]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.149]
 [0.144]
 [0.171]
 [0.149]
 [0.149]]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05671915159998089, 0.04922034094739273, 0.07339874912629846, 0.5404274798631904, 0.22351512686315664, 0.05671915159998089]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05671915159998089, 0.04922034094739273, 0.07339874912629846, 0.5404274798631904, 0.22351512686315664, 0.05671915159998089]
Printing some Q and Qe and total Qs values:  [[0.998]
 [0.998]
 [0.998]
 [1.061]
 [0.998]] [[34.901]
 [34.901]
 [34.901]
 [35.417]
 [34.901]] [[2.135]
 [2.135]
 [2.135]
 [2.23 ]
 [2.135]]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05671915159998089, 0.04922034094739273, 0.07339874912629846, 0.5404274798631904, 0.22351512686315664, 0.05671915159998089]
printing an ep nov before normalisation:  37.62751471219992
siam score:  -0.9335201
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05671915159998089, 0.04922034094739273, 0.07339874912629846, 0.5404274798631904, 0.22351512686315664, 0.05671915159998089]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05671915159998089, 0.04922034094739273, 0.07339874912629846, 0.5404274798631904, 0.22351512686315664, 0.05671915159998089]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.50471059831713
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.056795924680366164, 0.04939590198258974, 0.07325578825112122, 0.5423619000176401, 0.22139456038791666, 0.056795924680366164]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.050697159093876405, 0.051937570816481045, 0.06784719943249708, 0.5566233490831861, 0.22219756248008307, 0.050697159093876405]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.05085760809335539, 0.052081224208746016, 0.06777543090614752, 0.5583922924771192, 0.22003583622127662, 0.05085760809335539]
printing an ep nov before normalisation:  34.864476454472616
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.03 ]
 [0.039]
 [0.034]
 [0.035]
 [0.033]] [[0.   ]
 [0.001]
 [0.001]
 [0.001]
 [0.   ]]
using another actor
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
using explorer policy with actor:  0
printing an ep nov before normalisation:  52.17808723449707
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.556]
 [0.547]
 [0.539]
 [0.552]] [[28.267]
 [27.494]
 [44.925]
 [35.958]
 [30.225]] [[0.528]
 [0.556]
 [0.547]
 [0.539]
 [0.552]]
maxi score, test score, baseline:  0.06810000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
actor:  0 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
siam score:  -0.9327568
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
UNIT TEST: sample policy line 217 mcts : [0.    0.692 0.026 0.256 0.026]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.050860433799227854, 0.05202846569305665, 0.06777919880680845, 0.5584233840266454, 0.22004808387503375, 0.050860433799227854]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
printing an ep nov before normalisation:  27.561613594835773
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.344]
 [0.601]
 [0.344]
 [0.344]
 [0.344]] [[41.135]
 [51.687]
 [41.135]
 [41.135]
 [41.135]] [[1.179]
 [1.816]
 [1.179]
 [1.179]
 [1.179]]
printing an ep nov before normalisation:  61.967977837403005
printing an ep nov before normalisation:  25.92374086380005
line 256 mcts: sample exp_bonus 33.08906369617788
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05101656660457752, 0.05216899336916201, 0.0677092936794679, 0.5601447423887338, 0.21794383735348127, 0.05101656660457752]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05101656660457752, 0.05216899336916201, 0.0677092936794679, 0.5601447423887338, 0.21794383735348127, 0.05101656660457752]
printing an ep nov before normalisation:  23.20460489971102
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05101656660457752, 0.05216899336916201, 0.0677092936794679, 0.5601447423887338, 0.21794383735348127, 0.05101656660457752]
Printing some Q and Qe and total Qs values:  [[0.851]
 [1.027]
 [0.936]
 [0.937]
 [0.933]] [[51.109]
 [44.637]
 [44.891]
 [47.423]
 [47.819]] [[2.328]
 [2.23 ]
 [2.15 ]
 [2.258]
 [2.271]]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05101656660457752, 0.05216899336916201, 0.0677092936794679, 0.5601447423887338, 0.21794383735348127, 0.05101656660457752]
line 256 mcts: sample exp_bonus 46.09589542970883
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05101656660457752, 0.05216899336916201, 0.0677092936794679, 0.5601447423887338, 0.21794383735348127, 0.05101656660457752]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.467701802665694
siam score:  -0.9310245
line 256 mcts: sample exp_bonus 25.553154922239294
printing an ep nov before normalisation:  31.18506542926234
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
printing an ep nov before normalisation:  36.94349711740372
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.039]
 [0.039]
 [0.039]
 [0.039]]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
Printing some Q and Qe and total Qs values:  [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]] [[56.728]
 [56.728]
 [56.728]
 [56.728]
 [56.728]] [[1.334]
 [1.334]
 [1.334]
 [1.334]
 [1.334]]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
from probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
siam score:  -0.93244934
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
maxi score, test score, baseline:  0.07010000000000001 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
printing an ep nov before normalisation:  29.46059562141384
actor:  0 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717597, 0.051737861023477084, 0.06714943809481033, 0.5555059510900239, 0.22441678973733675, 0.05059498002717597]
printing an ep nov before normalisation:  47.01544484616001
Printing some Q and Qe and total Qs values:  [[0.348]
 [0.544]
 [0.348]
 [0.348]
 [0.348]] [[47.283]
 [45.509]
 [47.283]
 [47.283]
 [47.283]] [[1.651]
 [1.775]
 [1.651]
 [1.651]
 [1.651]]
Printing some Q and Qe and total Qs values:  [[0.358]
 [0.561]
 [0.433]
 [0.433]
 [0.554]] [[45.977]
 [44.545]
 [42.427]
 [42.427]
 [46.518]] [[1.394]
 [1.542]
 [1.331]
 [1.331]
 [1.612]]
printing an ep nov before normalisation:  30.354816573581992
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
using explorer policy with actor:  1
siam score:  -0.9337943
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
printing an ep nov before normalisation:  42.31434564172834
printing an ep nov before normalisation:  35.23866782687346
maxi score, test score, baseline:  0.0721 0.65 0.65
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
printing an ep nov before normalisation:  102.61037315229372
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
printing an ep nov before normalisation:  43.40908573710949
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
printing an ep nov before normalisation:  40.6093743491591
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
maxi score, test score, baseline:  0.0721 0.65 0.65
maxi score, test score, baseline:  0.0721 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
actor:  0 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  51.15253813940291
Printing some Q and Qe and total Qs values:  [[1.167]
 [1.229]
 [1.167]
 [1.167]
 [1.181]] [[43.817]
 [40.502]
 [43.817]
 [43.817]
 [44.92 ]] [[2.291]
 [2.196]
 [2.291]
 [2.291]
 [2.358]]
printing an ep nov before normalisation:  26.69787883758545
maxi score, test score, baseline:  0.0741 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
actions average: 
K:  4  action  0 :  tensor([    0.9908,     0.0002,     0.0000,     0.0038,     0.0052],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0105,     0.9136,     0.0131,     0.0004,     0.0624],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0111,     0.9121,     0.0013,     0.0752],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0001,     0.0268,     0.8979,     0.0751],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.0903, 0.1208, 0.0779, 0.7098], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.9582,     0.0007,     0.0000,     0.0142,     0.0269],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9519,     0.0013,     0.0001,     0.0465],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0105,     0.8791,     0.0345,     0.0755],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0179,     0.9591,     0.0228],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0033, 0.0884, 0.0880, 0.0492, 0.7712], grad_fn=<DivBackward0>)
from probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
maxi score, test score, baseline:  0.0741 0.65 0.65
printing an ep nov before normalisation:  31.069586213680193
maxi score, test score, baseline:  0.0741 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
maxi score, test score, baseline:  0.0741 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
printing an ep nov before normalisation:  0.4985012929003574
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0761 0.65 0.65
probs:  [0.05059498002717598, 0.0517378610234771, 0.06714943809481033, 0.555505951090024, 0.22441678973733675, 0.05059498002717598]
Printing some Q and Qe and total Qs values:  [[0.95 ]
 [0.943]
 [0.95 ]
 [0.95 ]
 [0.95 ]] [[33.331]
 [30.739]
 [33.331]
 [33.331]
 [33.331]] [[0.95 ]
 [0.943]
 [0.95 ]
 [0.95 ]
 [0.95 ]]
printing an ep nov before normalisation:  41.839337461451755
actor:  0 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.61843237926483
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0781 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
printing an ep nov before normalisation:  31.95178066493765
maxi score, test score, baseline:  0.0781 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
printing an ep nov before normalisation:  54.17133761008967
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.052]
 [0.043]
 [0.043]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.043]
 [0.043]
 [0.052]
 [0.043]
 [0.043]]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0781 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
printing an ep nov before normalisation:  29.294185638427734
maxi score, test score, baseline:  0.0781 0.65 0.65
using explorer policy with actor:  0
printing an ep nov before normalisation:  35.49243701348824
actor:  0 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
printing an ep nov before normalisation:  40.51444534005496
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.050180320236192184, 0.051313812303876175, 0.07480801152496262, 0.5509433764411914, 0.22257415925758534, 0.050180320236192184]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.088]
 [0.126]
 [0.13 ]
 [0.099]] [[48.59 ]
 [52.494]
 [43.488]
 [61.69 ]
 [48.174]] [[0.547]
 [0.634]
 [0.505]
 [0.848]
 [0.566]]
maxi score, test score, baseline:  0.0801 0.65 0.65
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
printing an ep nov before normalisation:  26.44409418106079
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
printing an ep nov before normalisation:  53.55942310355602
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
printing an ep nov before normalisation:  0.17008621108445254
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.063480491208225
printing an ep nov before normalisation:  34.25320863723755
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.050343385128419176, 0.05140914852424189, 0.07465294068075622, 0.552740866543385, 0.22051027399477854, 0.050343385128419176]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.050343385128419176, 0.05140914852424189, 0.07465294068075622, 0.552740866543385, 0.22051027399477854, 0.050343385128419176]
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.050343385128419176, 0.05140914852424189, 0.07465294068075622, 0.552740866543385, 0.22051027399477854, 0.050343385128419176]
printing an ep nov before normalisation:  42.153310950333946
maxi score, test score, baseline:  0.0801 0.65 0.65
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
printing an ep nov before normalisation:  37.90791787025332
maxi score, test score, baseline:  0.0801 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0821 0.65 0.65
maxi score, test score, baseline:  0.0821 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
maxi score, test score, baseline:  0.0821 0.65 0.65
probs:  [0.05018301462720754, 0.05126278363184033, 0.07481203144716479, 0.5509730232996715, 0.22258613236690822, 0.05018301462720754]
Printing some Q and Qe and total Qs values:  [[1.433]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[36.466]
 [42.595]
 [42.595]
 [42.595]
 [42.595]] [[1.912]
 [2.008]
 [2.008]
 [2.008]
 [2.008]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 18.11079430875068
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.04976085315214752, 0.05925803722876223, 0.07418218363487106, 0.5463279063008595, 0.22071016653121228, 0.04976085315214752]
Printing some Q and Qe and total Qs values:  [[0.252]
 [0.399]
 [0.252]
 [0.125]
 [0.245]] [[41.703]
 [43.508]
 [41.703]
 [41.9  ]
 [44.579]] [[0.252]
 [0.399]
 [0.252]
 [0.125]
 [0.245]]
printing an ep nov before normalisation:  35.85877180099487
maxi score, test score, baseline:  0.0821 0.65 0.65
probs:  [0.04976085315214752, 0.05925803722876223, 0.07418218363487106, 0.5463279063008595, 0.22071016653121228, 0.04976085315214752]
printing an ep nov before normalisation:  33.9597408988721
actor:  0 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04976085315214752, 0.05925803722876223, 0.07418218363487106, 0.5463279063008595, 0.22071016653121228, 0.04976085315214752]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04976085315214752, 0.05925803722876223, 0.07418218363487106, 0.5463279063008595, 0.22071016653121228, 0.04976085315214752]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04976085315214752, 0.05925803722876223, 0.07418218363487106, 0.5463279063008595, 0.22071016653121228, 0.04976085315214752]
siam score:  -0.92957395
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04934746358673953, 0.06708716092909081, 0.07356542315628817, 0.5417793081675619, 0.21887318057358002, 0.04934746358673953]
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9568,     0.0003,     0.0002,     0.0423],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0023,     0.8600,     0.0263,     0.1112],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0195,     0.8987,     0.0815],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0349, 0.0672, 0.0427, 0.0874, 0.7678], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04934746358673953, 0.06708716092909081, 0.07356542315628817, 0.5417793081675619, 0.21887318057358002, 0.04934746358673953]
Printing some Q and Qe and total Qs values:  [[1.294]
 [1.481]
 [1.333]
 [0.743]
 [1.311]] [[29.727]
 [20.425]
 [20.606]
 [24.22 ]
 [27.891]] [[2.506]
 [2.312]
 [2.171]
 [1.73 ]
 [2.448]]
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0021,     0.9755,     0.0008,     0.0001,     0.0216],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0157,     0.9616,     0.0004,     0.0214],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0020,     0.0003,     0.9480,     0.0496],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0012, 0.1043, 0.0932, 0.0375, 0.7639], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04934746358673953, 0.06708716092909081, 0.07356542315628817, 0.5417793081675619, 0.21887318057358002, 0.04934746358673953]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04934746358673953, 0.06708716092909081, 0.07356542315628817, 0.5417793081675619, 0.21887318057358002, 0.04934746358673953]
printing an ep nov before normalisation:  34.19382732274475
printing an ep nov before normalisation:  29.2388916015625
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.17115374834812
using explorer policy with actor:  1
siam score:  -0.9343689
Printing some Q and Qe and total Qs values:  [[0.926]
 [1.096]
 [1.102]
 [1.102]
 [0.973]] [[59.235]
 [50.686]
 [45.207]
 [45.207]
 [58.185]] [[2.612]
 [2.491]
 [2.311]
 [2.311]
 [2.624]]
printing an ep nov before normalisation:  40.595050740426245
printing an ep nov before normalisation:  39.35131017420582
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.049515850955158876, 0.06702923928795906, 0.07342485697619072, 0.5436353087231507, 0.2168788931023819, 0.049515850955158876]
siam score:  -0.9322705
actor:  1 policy actor:  1  step number:  106 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.166]
 [0.181]
 [0.186]
 [0.191]] [[35.508]
 [40.593]
 [46.36 ]
 [45.375]
 [53.135]] [[0.777]
 [0.9  ]
 [1.049]
 [1.031]
 [1.217]]
actions average: 
K:  4  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0023,     0.8812,     0.0159,     0.0003,     0.1003],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0002,     0.9158,     0.0303,     0.0533],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0003,     0.0142,     0.9029,     0.0824],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0182, 0.1515, 0.0185, 0.1221, 0.6896], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.049125015332342654, 0.06649983243010237, 0.07284484605331452, 0.5393348502324281, 0.21516383037914574, 0.05703162557266661]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.049125015332342654, 0.06649983243010237, 0.07284484605331452, 0.5393348502324281, 0.21516383037914574, 0.05703162557266661]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.049292762678301336, 0.06645042774904499, 0.07271614082123706, 0.5411837036799518, 0.2132564096788515, 0.05710055539261325]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
printing an ep nov before normalisation:  46.143735905027796
printing an ep nov before normalisation:  38.49309067444611
from probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
maxi score, test score, baseline:  0.08410000000000001 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]] [[29.598]
 [29.598]
 [29.598]
 [29.598]
 [29.598]] [[0.827]
 [0.827]
 [0.827]
 [0.827]
 [0.827]]
actor:  0 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
siam score:  -0.93041915
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
maxi score, test score, baseline:  0.0861 0.65 0.65
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04945636873957005, 0.06640224275255611, 0.07259061301652263, 0.5429869133145587, 0.21139607867823818, 0.05716778349855425]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
maxi score, test score, baseline:  0.0861 0.65 0.65
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.452]
 [0.591]
 [0.583]
 [0.597]] [[28.034]
 [26.066]
 [37.136]
 [28.034]
 [27.878]] [[0.583]
 [0.452]
 [0.591]
 [0.583]
 [0.597]]
Printing some Q and Qe and total Qs values:  [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]] [[51.497]
 [51.497]
 [51.497]
 [51.497]
 [51.497]] [[0.115]
 [0.115]
 [0.115]
 [0.115]
 [0.115]]
maxi score, test score, baseline:  0.0861 0.65 0.65
printing an ep nov before normalisation:  46.115513191155664
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
printing an ep nov before normalisation:  39.41179675963062
maxi score, test score, baseline:  0.0861 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04961598500397751, 0.06635523282468184, 0.07246814640915854, 0.5447461487828996, 0.20958111484024466, 0.05723337213903785]
printing an ep nov before normalisation:  28.08307977447724
printing an ep nov before normalisation:  43.72740906855267
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.049615985003977514, 0.06635523282468185, 0.07246814640915854, 0.5447461487828997, 0.20958111484024466, 0.057233372139037855]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.049615985003977514, 0.06635523282468185, 0.07246814640915854, 0.5447461487828997, 0.20958111484024466, 0.057233372139037855]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.049615985003977514, 0.06635523282468185, 0.07246814640915854, 0.5447461487828997, 0.20958111484024466, 0.057233372139037855]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.049615985003977514, 0.06635523282468185, 0.07246814640915854, 0.5447461487828997, 0.20958111484024466, 0.057233372139037855]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0492415371725318, 0.06585414918403808, 0.0719208173358275, 0.5406259407106053, 0.2155562583700336, 0.056801297226963705]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.074]
 [0.204]
 [0.087]
 [0.085]] [[47.665]
 [33.484]
 [31.695]
 [41.921]
 [34.777]] [[0.067]
 [0.074]
 [0.204]
 [0.087]
 [0.085]]
printing an ep nov before normalisation:  34.35029128127464
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0492415371725318, 0.06585414918403808, 0.0719208173358275, 0.5406259407106053, 0.2155562583700336, 0.056801297226963705]
printing an ep nov before normalisation:  34.350010358252426
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0492415371725318, 0.06585414918403808, 0.0719208173358275, 0.5406259407106053, 0.2155562583700336, 0.056801297226963705]
printing an ep nov before normalisation:  46.51346433378827
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0492415371725318, 0.06585414918403808, 0.0719208173358275, 0.5406259407106053, 0.2155562583700336, 0.056801297226963705]
maxi score, test score, baseline:  0.0881 0.65 0.65
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9286376
using another actor
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
siam score:  -0.9273399
printing an ep nov before normalisation:  49.886888137504904
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
printing an ep nov before normalisation:  49.16832857904685
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04940062051998458, 0.06581460592381734, 0.0718087387202974, 0.5423792209268665, 0.2137268206556119, 0.0568699932534222]
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04955594469342794, 0.06577599707941352, 0.07169930854676493, 0.5440910707512874, 0.21194061295123254, 0.056937065977873604]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04955594469342794, 0.06577599707941352, 0.07169930854676493, 0.5440910707512874, 0.21194061295123254, 0.056937065977873604]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04955988256006629, 0.06570162847746255, 0.0717050102607826, 0.5441344012093975, 0.21195748569865266, 0.0569415917936384]
printing an ep nov before normalisation:  50.103353199035
siam score:  -0.9286154
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04955988256006629, 0.06570162847746255, 0.0717050102607826, 0.5441344012093975, 0.21195748569865266, 0.0569415917936384]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.04955988256006629, 0.06570162847746255, 0.0717050102607826, 0.5441344012093975, 0.21195748569865266, 0.0569415917936384]
actions average: 
K:  1  action  0 :  tensor([    0.9335,     0.0001,     0.0001,     0.0337,     0.0326],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0024,     0.9112,     0.0002,     0.0004,     0.0858],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0229,     0.9472,     0.0001,     0.0295],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0002,     0.0036,     0.8389,     0.1569],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0789, 0.1654, 0.0275, 0.0230, 0.7051], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 33.13726973363606
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.05092754793520902, 0.060065267178051496, 0.06632748036545404, 0.5591253181332947, 0.21262683845278174, 0.05092754793520902]
printing an ep nov before normalisation:  45.594123855946194
maxi score, test score, baseline:  0.0881 0.65 0.65
maxi score, test score, baseline:  0.0881 0.65 0.65
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0509317524138448, 0.059987534477296, 0.06633295923302411, 0.5591715774467625, 0.2126444240152277, 0.0509317524138448]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0509317524138448, 0.059987534477296, 0.06633295923302411, 0.5591715774467625, 0.2126444240152277, 0.0509317524138448]
printing an ep nov before normalisation:  32.73426532745361
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.09907567601751
printing an ep nov before normalisation:  43.44760731616176
siam score:  -0.92910415
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.05107318273512494, 0.060018697818432394, 0.066286858035257, 0.5607313052895492, 0.21081677338651159, 0.05107318273512494]
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.05107318273512494, 0.060018697818432394, 0.066286858035257, 0.5607313052895492, 0.21081677338651159, 0.05107318273512494]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.05067969848614354, 0.06727302953814379, 0.0657758868868355, 0.5564020099093245, 0.2091896766934092, 0.05067969848614354]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.015338810176785955
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0508213702976138, 0.06721660777256355, 0.06573733822595153, 0.5579642798610971, 0.20743903354516013, 0.0508213702976138]
line 256 mcts: sample exp_bonus 15.350125719807531
maxi score, test score, baseline:  0.0881 0.65 0.65
probs:  [0.0508213702976138, 0.06721660777256355, 0.06573733822595153, 0.5579642798610971, 0.20743903354516013, 0.0508213702976138]
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.871]
 [0.903]
 [0.871]
 [0.871]
 [0.871]] [[60.255]
 [50.191]
 [60.255]
 [60.255]
 [60.255]] [[0.871]
 [0.903]
 [0.871]
 [0.871]
 [0.871]]
actor:  0 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0901 0.65 0.65
probs:  [0.0508213702976138, 0.06721660777256355, 0.06573733822595153, 0.5579642798610971, 0.20743903354516013, 0.0508213702976138]
maxi score, test score, baseline:  0.0901 0.65 0.65
probs:  [0.0508213702976138, 0.06721660777256355, 0.06573733822595153, 0.5579642798610971, 0.20743903354516013, 0.0508213702976138]
printing an ep nov before normalisation:  53.69119548448908
printing an ep nov before normalisation:  31.93294048309326
maxi score, test score, baseline:  0.0901 0.65 0.65
probs:  [0.0508213702976138, 0.06721660777256355, 0.06573733822595153, 0.5579642798610971, 0.20743903354516013, 0.0508213702976138]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.82811126112635
actor:  0 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.347052959620235
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05095969941761847, 0.06716151725666751, 0.06569969910577586, 0.5594896886590486, 0.20572969614327108, 0.05095969941761847]
printing an ep nov before normalisation:  44.33195670919126
siam score:  -0.93697596
printing an ep nov before normalisation:  38.93769076952755
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05095969941761847, 0.06716151725666751, 0.06569969910577586, 0.5594896886590486, 0.20572969614327108, 0.05095969941761847]
maxi score, test score, baseline:  0.0921 0.65 0.65
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.022]
 [1.022]
 [1.022]
 [1.022]] [[54.13 ]
 [44.278]
 [44.278]
 [44.278]
 [44.278]] [[2.216]
 [1.861]
 [1.861]
 [1.861]
 [1.861]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.11412111817463
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.051094802771022865, 0.06710771142433021, 0.06566293771124986, 0.5609795256789678, 0.20406021964340634, 0.051094802771022865]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.051094802771022865, 0.06710771142433021, 0.06566293771124986, 0.5609795256789678, 0.20406021964340634, 0.051094802771022865]
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.051094802771022865, 0.06710771142433021, 0.06566293771124986, 0.5609795256789678, 0.20406021964340634, 0.051094802771022865]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.614692256727245
maxi score, test score, baseline:  0.0921 0.65 0.65
printing an ep nov before normalisation:  41.266846827476165
maxi score, test score, baseline:  0.0921 0.65 0.65
Printing some Q and Qe and total Qs values:  [[0.12 ]
 [0.001]
 [0.216]
 [0.159]
 [0.16 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.12 ]
 [0.001]
 [0.216]
 [0.159]
 [0.16 ]]
siam score:  -0.93661857
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05072591980325274, 0.06662295004328785, 0.06518863152538994, 0.5569208300780548, 0.20981574874676198, 0.05072591980325274]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05072591980325274, 0.06662295004328785, 0.06518863152538994, 0.5569208300780548, 0.20981574874676198, 0.05072591980325274]
siam score:  -0.93613976
Printing some Q and Qe and total Qs values:  [[0.126]
 [0.172]
 [0.172]
 [0.172]
 [0.172]] [[39.483]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[ 1.154]
 [-0.584]
 [-0.584]
 [-0.584]
 [-0.584]]
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.732]
 [0.706]
 [0.753]
 [0.732]] [[40.967]
 [40.967]
 [43.248]
 [37.741]
 [40.967]] [[1.946]
 [1.946]
 [2.045]
 [1.792]
 [1.946]]
printing an ep nov before normalisation:  30.393166542053223
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05072591980325274, 0.06662295004328785, 0.06518863152538994, 0.5569208300780548, 0.20981574874676198, 0.05072591980325274]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05072591980325274, 0.06662295004328785, 0.06518863152538994, 0.5569208300780548, 0.20981574874676198, 0.05072591980325274]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05072591980325274, 0.06662295004328785, 0.06518863152538994, 0.5569208300780548, 0.20981574874676198, 0.05072591980325274]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93648374
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05036233736711118, 0.06614515426721239, 0.06472114071231605, 0.5529204544492818, 0.21548857583696726, 0.05036233736711118]
printing an ep nov before normalisation:  41.60193125328457
printing an ep nov before normalisation:  43.31751531590827
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.936]
 [0.936]
 [0.936]] [[43.641]
 [43.641]
 [43.641]
 [43.641]
 [43.641]] [[2.073]
 [2.073]
 [2.073]
 [2.073]
 [2.073]]
printing an ep nov before normalisation:  42.84516813855128
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.05036233736711118, 0.06614515426721239, 0.06472114071231605, 0.5529204544492818, 0.21548857583696726, 0.05036233736711118]
actions average: 
K:  0  action  0 :  tensor([    0.9978,     0.0002,     0.0000,     0.0006,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9825,     0.0002,     0.0002,     0.0165],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0111,     0.9185,     0.0282,     0.0419],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0003,     0.0005,     0.8545,     0.1443],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0009, 0.0154, 0.0926, 0.0751, 0.8161], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.228]
 [0.199]
 [0.182]
 [0.178]] [[48.448]
 [48.734]
 [56.126]
 [47.266]
 [47.296]] [[1.236]
 [1.286]
 [1.474]
 [1.198]
 [1.195]]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050500773475437466, 0.06610427166877207, 0.0646964372453133, 0.5544468373060301, 0.2137509068290097, 0.050500773475437466]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050500773475437466, 0.06610427166877207, 0.0646964372453133, 0.5544468373060301, 0.2137509068290097, 0.050500773475437466]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.824]
 [0.824]
 [0.867]
 [0.824]] [[38.978]
 [38.978]
 [38.978]
 [35.705]
 [38.978]] [[2.39 ]
 [2.39 ]
 [2.39 ]
 [2.198]
 [2.39 ]]
printing an ep nov before normalisation:  39.149768675245376
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050500773475437466, 0.06610427166877207, 0.0646964372453133, 0.5544468373060301, 0.2137509068290097, 0.050500773475437466]
printing an ep nov before normalisation:  41.2905709417713
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050500773475437466, 0.06610427166877207, 0.0646964372453133, 0.5544468373060301, 0.2137509068290097, 0.050500773475437466]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.805]
 [0.805]
 [0.805]
 [0.901]
 [0.805]] [[47.66 ]
 [47.66 ]
 [47.66 ]
 [49.654]
 [47.66 ]] [[2.26 ]
 [2.26 ]
 [2.26 ]
 [2.447]
 [2.26 ]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.5180476088935393
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.58197275355165
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04979509359939099, 0.06518003508861665, 0.06379192006703238, 0.5466824332006606, 0.22475542444490845, 0.04979509359939099]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04979954016432399, 0.06509641039595393, 0.06379761952723065, 0.5467313575475103, 0.22477553220065716, 0.04979954016432399]
printing an ep nov before normalisation:  24.39270257949829
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9561,     0.0006,     0.0001,     0.0426],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0005,     0.9378,     0.0261,     0.0355],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0027,     0.0011,     0.0005,     0.8711,     0.1246],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0180, 0.0903, 0.0540, 0.1635, 0.6742], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.299735281202526
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04994077243349342, 0.06506816290365099, 0.06378376182599609, 0.5482883905635894, 0.22297813983977677, 0.04994077243349342]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04994077243349342, 0.06506816290365099, 0.06378376182599609, 0.5482883905635894, 0.22297813983977677, 0.04994077243349342]
printing an ep nov before normalisation:  36.07205867767334
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04994077243349342, 0.06506816290365099, 0.06378376182599609, 0.5482883905635894, 0.22297813983977677, 0.04994077243349342]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04994077243349342, 0.06506816290365099, 0.06378376182599609, 0.5482883905635894, 0.22297813983977677, 0.04994077243349342]
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04994077243349342, 0.06506816290365099, 0.06378376182599609, 0.5482883905635894, 0.22297813983977677, 0.04994077243349342]
printing an ep nov before normalisation:  23.134695492792197
actor:  1 policy actor:  1  step number:  119 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0921 0.65 0.65
printing an ep nov before normalisation:  48.71029853605147
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.22123941859104426, 0.050083265808084046]
printing an ep nov before normalisation:  20.589868748093398
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.22123941859104426, 0.050083265808084046]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.22123941859104426, 0.050083265808084046]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.22123941859104426, 0.050083265808084046]
printing an ep nov before normalisation:  50.918916442823694
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.22123941859104426, 0.050083265808084046]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.077]
 [0.154]
 [0.13 ]
 [0.113]] [[35.158]
 [43.159]
 [42.873]
 [25.255]
 [31.227]] [[0.743]
 [1.031]
 [1.098]
 [0.444]
 [0.641]]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.2212394185910442, 0.050083265808084046]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.2212394185910442, 0.050083265808084046]
Printing some Q and Qe and total Qs values:  [[1.02]
 [1.02]
 [1.02]
 [1.02]
 [1.02]] [[39.153]
 [39.153]
 [39.153]
 [39.153]
 [39.153]] [[2.353]
 [2.353]
 [2.353]
 [2.353]
 [2.353]]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.2212394185910442, 0.050083265808084046]
using explorer policy with actor:  1
siam score:  -0.92884475
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050083265808084046, 0.06495905982773885, 0.06377575803072086, 0.549859231934328, 0.2212394185910442, 0.050083265808084046]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050218372685521394, 0.0649329112094043, 0.06376243655409543, 0.5513487358227608, 0.21951917104269686, 0.050218372685521394]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050218372685521394, 0.0649329112094043, 0.06376243655409543, 0.5513487358227608, 0.21951917104269686, 0.050218372685521394]
siam score:  -0.9245549
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050218372685521394, 0.0649329112094043, 0.06376243655409543, 0.5513487358227608, 0.21951917104269686, 0.050218372685521394]
printing an ep nov before normalisation:  21.307132244110107
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.050218372685521394, 0.0649329112094043, 0.06376243655409543, 0.5513487358227608, 0.21951917104269686, 0.050218372685521394]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  70.4408658010309
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04987645014635204, 0.0713106624137828, 0.06332807562861852, 0.5475865929902117, 0.21802176867468298, 0.04987645014635204]
printing an ep nov before normalisation:  30.240629234077918
line 256 mcts: sample exp_bonus 21.61472830580612
siam score:  -0.92733306
maxi score, test score, baseline:  0.0921 0.65 0.65
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04987645014635204, 0.0713106624137828, 0.06332807562861852, 0.5475865929902117, 0.21802176867468298, 0.04987645014635204]
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.942]
 [0.942]
 [0.974]
 [0.951]] [[36.452]
 [36.452]
 [36.452]
 [44.473]
 [45.407]] [[1.866]
 [1.866]
 [1.866]
 [2.203]
 [2.215]]
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04987645014635204, 0.0713106624137828, 0.06332807562861852, 0.5475865929902117, 0.21802176867468298, 0.04987645014635204]
printing an ep nov before normalisation:  47.00000537533059
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04987645014635204, 0.0713106624137828, 0.06332807562861852, 0.5475865929902117, 0.21802176867468298, 0.04987645014635204]
printing an ep nov before normalisation:  49.633611925194565
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04987645014635204, 0.0713106624137828, 0.06332807562861852, 0.5475865929902117, 0.21802176867468298, 0.04987645014635204]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.0495405362105276, 0.07757633742303024, 0.0629013477258609, 0.5438905622778597, 0.21655068015219378, 0.0495405362105276]
maxi score, test score, baseline:  0.0921 0.65 0.65
actions average: 
K:  0  action  0 :  tensor([    0.9967,     0.0000,     0.0000,     0.0001,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9521,     0.0009,     0.0001,     0.0469],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0018,     0.9661,     0.0166,     0.0154],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0077,     0.0021,     0.0003,     0.9157,     0.0743],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0119, 0.0801, 0.0158, 0.0356, 0.8566], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0921 0.65 0.65
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  26.520020961761475
maxi score, test score, baseline:  0.0921 0.65 0.65
probs:  [0.04921234273539991, 0.07706196322913651, 0.062484427501946244, 0.5402794790976149, 0.22174944470050248, 0.04921234273539991]
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.04921780805933733, 0.07695928184617695, 0.06249137037026156, 0.5403396135635343, 0.2217741181013525, 0.04921780805933733]
from probs:  [0.04921780805933733, 0.07695928184617695, 0.06249137037026156, 0.5403396135635343, 0.2217741181013525, 0.04921780805933733]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.04921780805933733, 0.07695928184617695, 0.06249137037026156, 0.5403396135635343, 0.2217741181013525, 0.04921780805933733]
printing an ep nov before normalisation:  55.7123062867571
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.991]
 [1.302]
 [0.97 ]
 [1.207]
 [1.097]] [[35.834]
 [32.699]
 [25.775]
 [30.647]
 [27.443]] [[2.09 ]
 [2.255]
 [1.6  ]
 [2.064]
 [1.805]]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.04921780805933733, 0.07695928184617695, 0.06249137037026156, 0.5403396135635343, 0.2217741181013525, 0.04921780805933733]
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461073, 0.049357922366204836]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461073, 0.049357922366204836]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461073, 0.049357922366204836]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049495121566450834, 0.07665942835490162, 0.06249252471669784, 0.543396441275837, 0.2184613625196619, 0.049495121566450834]
printing an ep nov before normalisation:  38.543217696368906
from probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
printing an ep nov before normalisation:  22.571413285796236
from probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
siam score:  -0.92394495
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
printing an ep nov before normalisation:  37.42468293533797
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
maxi score, test score, baseline:  0.0941 0.65 0.65
probs:  [0.049357922366204836, 0.0768077790775987, 0.062491953610697586, 0.5418840940346833, 0.22010032854461067, 0.049357922366204836]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0961 0.65 0.65
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.049495121566450834, 0.07665942835490162, 0.06249252471669784, 0.543396441275837, 0.2184613625196619, 0.049495121566450834]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.049495121566450834, 0.07665942835490162, 0.06249252471669784, 0.543396441275837, 0.2184613625196619, 0.049495121566450834]
printing an ep nov before normalisation:  0.01312922886825163
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.049495121566450834, 0.07665942835490162, 0.06249252471669784, 0.543396441275837, 0.21846136251966183, 0.049495121566450834]
printing an ep nov before normalisation:  31.98535680770874
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.1984],
        [0.7153],
        [0.1511],
        [0.3053],
        [0.3106],
        [0.9375],
        [0.1573],
        [0.3238],
        [0.2502]], dtype=torch.float64)
0.970299 0.970299
0.0 0.1983664043328611
0.0 0.7152508310642465
0.0 0.15110925386348562
0.0 0.3053329024682647
0.0 0.3106289046284856
0.0 0.9375478385279309
0.0 0.1573395245426423
0.0 0.32381266331214464
0.0 0.2501758091648818
printing an ep nov before normalisation:  45.14304833488335
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.068230284553714
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04917609057522935, 0.07616487899953749, 0.06208951282521631, 0.5398861360747341, 0.22350729095005337, 0.04917609057522935]
printing an ep nov before normalisation:  81.67154279258386
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04917609057522935, 0.07616487899953749, 0.06208951282521631, 0.5398861360747341, 0.22350729095005337, 0.04917609057522935]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.07567667953802648, 0.06169167550873869, 0.5364209023415795, 0.22848843088243942, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
printing an ep nov before normalisation:  44.81418969661617
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.019]
 [1.019]
 [0.989]
 [0.967]] [[23.983]
 [23.983]
 [23.983]
 [27.638]
 [25.761]] [[1.673]
 [1.673]
 [1.673]
 [1.808]
 [1.702]]
printing an ep nov before normalisation:  37.26042813912832
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.3396],
        [0.1650],
        [0.2758],
        [0.1450],
        [0.1501],
        [0.1501],
        [0.1424],
        [0.3396],
        [0.6339]], dtype=torch.float64)
0.0 0.0
0.0 0.3396296291615376
0.0 0.16503518769122832
0.0 0.27577292765283185
0.0 0.1449613780311161
0.0 0.1500853499238111
0.0 0.1500853499238111
0.0 0.14236169136849425
0.0 0.3396296291615376
0.0 0.6338828232457636
maxi score, test score, baseline:  0.0961 0.65 0.65
Printing some Q and Qe and total Qs values:  [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.395]] [[31.941]
 [31.941]
 [31.941]
 [31.941]
 [33.042]] [[2.658]
 [2.658]
 [2.658]
 [2.658]
 [2.728]]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.194]
 [0.225]
 [0.191]
 [0.194]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.171]
 [0.194]
 [0.225]
 [0.191]
 [0.194]]
printing an ep nov before normalisation:  41.090132657401895
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
printing an ep nov before normalisation:  32.563750421494696
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04886115586460787, 0.0756766795380265, 0.06169167550873869, 0.5364209023415795, 0.22848843088243947, 0.04886115586460787]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  46.03787154856577
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.44220759933099
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04900026624351273, 0.07554321940976358, 0.06170036810094587, 0.5379541877546885, 0.22680169224757663, 0.04900026624351273]
Printing some Q and Qe and total Qs values:  [[1.335]
 [1.337]
 [0.397]
 [1.309]
 [1.342]] [[36.025]
 [43.967]
 [40.736]
 [44.289]
 [44.38 ]] [[2.302]
 [2.737]
 [1.62 ]
 [2.726]
 [2.764]]
siam score:  -0.93795377
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04900026624351273, 0.07554321940976358, 0.06170036810094587, 0.5379541877546885, 0.22680169224757663, 0.04900026624351273]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04869161141748899, 0.07506695919304526, 0.06131151852340583, 0.5345580349952876, 0.23168026445328324, 0.04869161141748899]
Printing some Q and Qe and total Qs values:  [[0.716]
 [0.835]
 [0.814]
 [0.932]
 [0.943]] [[28.327]
 [27.098]
 [31.867]
 [31.284]
 [29.321]] [[1.416]
 [1.477]
 [1.684]
 [1.774]
 [1.691]]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04869161141748899, 0.07506695919304526, 0.06131151852340583, 0.5345580349952876, 0.23168026445328324, 0.04869161141748899]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.164]
 [1.081]
 [0.986]
 [1.066]
 [1.081]] [[27.817]
 [32.569]
 [39.187]
 [31.274]
 [32.569]] [[1.665]
 [1.74 ]
 [1.866]
 [1.683]
 [1.74 ]]
printing an ep nov before normalisation:  0.0012193470354304736
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.04838683013178042, 0.07459667592207485, 0.060927548910111116, 0.5312045030975123, 0.2364976118067409, 0.04838683013178042]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.712]
 [0.712]
 [0.712]
 [0.712]
 [0.872]] [[31.462]
 [31.462]
 [31.462]
 [31.462]
 [32.604]] [[0.981]
 [0.981]
 [0.981]
 [0.981]
 [1.156]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.16054365947292
printing an ep nov before normalisation:  36.692000985711374
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
printing an ep nov before normalisation:  44.952017769756765
maxi score, test score, baseline:  0.0961 0.65 0.65
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.297]
 [0.721]
 [1.269]
 [1.269]] [[23.449]
 [27.976]
 [19.001]
 [23.449]
 [23.449]] [[2.555]
 [2.986]
 [1.611]
 [2.555]
 [2.555]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
printing an ep nov before normalisation:  37.26809606599957
printing an ep nov before normalisation:  40.188521773283675
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053980003631585734, 0.0798874046658193, 0.060172504366597664, 0.524610059492492, 0.2335625249469315, 0.04778750289657382]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0961 0.65 0.65
printing an ep nov before normalisation:  38.841324863989016
printing an ep nov before normalisation:  29.339463710784912
Printing some Q and Qe and total Qs values:  [[1.484]
 [1.369]
 [1.401]
 [1.274]
 [1.425]] [[24.324]
 [26.099]
 [26.711]
 [35.913]
 [26.721]] [[2.334]
 [2.282]
 [2.335]
 [2.53 ]
 [2.359]]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053735098679604104, 0.079231763258181, 0.05982942338375175, 0.522998100898973, 0.23656483980403353, 0.047640773975456456]
printing an ep nov before normalisation:  32.18939237254459
maxi score, test score, baseline:  0.0961 0.65 0.65
printing an ep nov before normalisation:  28.91072046718076
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 0.65 0.65
using another actor
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053735098679604104, 0.079231763258181, 0.05982942338375175, 0.522998100898973, 0.23656483980403353, 0.047640773975456456]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9463888
Printing some Q and Qe and total Qs values:  [[0.289]
 [0.289]
 [0.323]
 [0.289]
 [0.289]] [[60.031]
 [60.031]
 [70.927]
 [60.031]
 [60.031]] [[1.691]
 [1.691]
 [2.042]
 [1.691]
 [1.691]]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.139]
 [0.221]
 [0.168]
 [0.168]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.168]
 [0.139]
 [0.221]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053410097165062925, 0.07875220749560186, 0.059467479634313695, 0.5198285472973725, 0.24118895371183685, 0.04735271469581215]
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053410097165062925, 0.07875220749560186, 0.059467479634313695, 0.5198285472973725, 0.24118895371183685, 0.04735271469581215]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053410097165062925, 0.07875220749560186, 0.059467479634313695, 0.5198285472973725, 0.24118895371183685, 0.04735271469581215]
printing an ep nov before normalisation:  61.14495209146394
siam score:  -0.9423317
siam score:  -0.9407274
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.053410097165062925, 0.07875220749560186, 0.059467479634313695, 0.5198285472973725, 0.24118895371183685, 0.04735271469581215]
actor:  1 policy actor:  1  step number:  95 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05349783799135416, 0.07859658650751614, 0.05949705105131482, 0.5214364566682864, 0.2394734428501349, 0.04749862493139349]
printing an ep nov before normalisation:  45.611324653554675
siam score:  -0.938329
printing an ep nov before normalisation:  22.534291744232178
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05358390968552539, 0.07844392596556181, 0.05952605991831459, 0.5230137780758716, 0.23779056690199035, 0.0476417594527362]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05358390968552539, 0.07844392596556181, 0.05952605991831459, 0.5230137780758716, 0.23779056690199035, 0.0476417594527362]
printing an ep nov before normalisation:  48.87630177029282
printing an ep nov before normalisation:  21.97358393277538
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05358390968552539, 0.07844392596556181, 0.05952605991831459, 0.5230137780758716, 0.23779056690199035, 0.0476417594527362]
printing an ep nov before normalisation:  29.449121327523507
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05326786916391041, 0.07798093098680023, 0.059174893697186516, 0.5199228072927229, 0.24229265422874582, 0.0473608446306343]
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05326786916391041, 0.07798093098680023, 0.059174893697186516, 0.5199228072927229, 0.24229265422874582, 0.0473608446306343]
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.83921795450231
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.87272970229487
maxi score, test score, baseline:  0.0961 0.65 0.65
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05295554309166058, 0.07752337762481665, 0.05882785476056129, 0.5168681649348172, 0.24674182816538423, 0.04708323142275987]
maxi score, test score, baseline:  0.0961 0.65 0.65
probs:  [0.05295554309166058, 0.07752337762481665, 0.05882785476056129, 0.5168681649348172, 0.24674182816538423, 0.04708323142275987]
printing an ep nov before normalisation:  18.707723125868107
actor:  0 policy actor:  1  step number:  118 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.77740990043361
printing an ep nov before normalisation:  44.959615882574965
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05295554309166058, 0.07752337762481665, 0.05882785476056129, 0.5168681649348172, 0.24674182816538423, 0.04708323142275987]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.052646866367021206, 0.07707117050644369, 0.06432287517513537, 0.5138492142875307, 0.24530101170090488, 0.046808861962964125]
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.052646866367021206, 0.07707117050644369, 0.06432287517513537, 0.5138492142875307, 0.24530101170090488, 0.046808861962964125]
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.052646866367021206, 0.07707117050644369, 0.06432287517513537, 0.5138492142875307, 0.24530101170090488, 0.046808861962964125]
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.052646866367021206, 0.07707117050644369, 0.06432287517513537, 0.5138492142875307, 0.24530101170090488, 0.046808861962964125]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05243571528026125, 0.0764946314113021, 0.06393705079656371, 0.5124891359323598, 0.2479584190574031, 0.04668504752211002]
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05243571528026125, 0.0764946314113021, 0.06393705079656371, 0.5124891359323598, 0.2479584190574031, 0.04668504752211002]
actions average: 
K:  2  action  0 :  tensor([    0.9928,     0.0048,     0.0000,     0.0002,     0.0023],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0013,     0.9558,     0.0002,     0.0007,     0.0420],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0038, 0.0010, 0.9905, 0.0022, 0.0024], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0001,     0.0264,     0.7776,     0.1950],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0008, 0.1177, 0.1704, 0.0883, 0.6228], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05244134705702623, 0.07639528094200806, 0.06394392068785858, 0.5125442922903208, 0.24798509878117636, 0.04669006024161004]
printing an ep nov before normalisation:  44.65371770309654
printing an ep nov before normalisation:  29.206663271859295
printing an ep nov before normalisation:  41.839881661857376
Printing some Q and Qe and total Qs values:  [[1.474]
 [1.419]
 [1.269]
 [1.443]
 [1.425]] [[41.175]
 [38.306]
 [39.33 ]
 [39.486]
 [38.208]] [[3.014]
 [2.851]
 [2.74 ]
 [2.919]
 [2.853]]
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05244134705702623, 0.07639528094200806, 0.06394392068785858, 0.5125442922903208, 0.24798509878117636, 0.04669006024161004]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.698]
 [0.538]
 [0.638]
 [0.571]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.544]
 [0.698]
 [0.538]
 [0.638]
 [0.571]]
printing an ep nov before normalisation:  52.649764035276476
printing an ep nov before normalisation:  65.61400795443782
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05214193805464671, 0.07595879573620269, 0.06357868829586333, 0.5096119477033114, 0.25228506727593747, 0.04642356293403841]
printing an ep nov before normalisation:  24.99607630324698
printing an ep nov before normalisation:  34.256485326152536
printing an ep nov before normalisation:  47.186283993171685
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05214193805464671, 0.07595879573620269, 0.06357868829586333, 0.5096119477033114, 0.25228506727593747, 0.04642356293403841]
printing an ep nov before normalisation:  31.058675555044328
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05214193805464671, 0.07595879573620269, 0.06357868829586333, 0.5096119477033114, 0.25228506727593747, 0.04642356293403841]
printing an ep nov before normalisation:  0.11260967407181965
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05223631574542976, 0.07583714284493182, 0.06356932905188992, 0.5112233546570664, 0.2505640486084826, 0.04656980909219968]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.051942451559662144, 0.07541020274521379, 0.06321156239790747, 0.5083414405085988, 0.25478644664807837, 0.04630789614053946]
printing an ep nov before normalisation:  33.61403788133177
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.051942451559662144, 0.07541020274521379, 0.06321156239790747, 0.5083414405085988, 0.25478644664807837, 0.04630789614053946]
printing an ep nov before normalisation:  31.562956906591047
actions average: 
K:  3  action  0 :  tensor([    0.9994,     0.0001,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9316,     0.0087,     0.0000,     0.0594],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0156,     0.9259,     0.0096,     0.0489],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0001,     0.0003,     0.9652,     0.0341],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0021, 0.0240, 0.1024, 0.1164, 0.7550], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.16599826143164
printing an ep nov before normalisation:  0.061452838890545536
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
printing an ep nov before normalisation:  34.4830992037317
maxi score, test score, baseline:  0.0981 0.65 0.65
printing an ep nov before normalisation:  34.23772842318677
printing an ep nov before normalisation:  39.52313026720371
printing an ep nov before normalisation:  25.518276336557495
maxi score, test score, baseline:  0.0981 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
actor:  0 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.93173957
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.4763658990739
printing an ep nov before normalisation:  41.0646865411725
Printing some Q and Qe and total Qs values:  [[0.322]
 [1.292]
 [0.013]
 [1.001]
 [1.296]] [[21.968]
 [31.892]
 [23.895]
 [24.633]
 [32.256]] [[0.625]
 [2.081]
 [0.411]
 [1.435]
 [2.102]]
printing an ep nov before normalisation:  24.76198434829712
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
printing an ep nov before normalisation:  59.52564040813684
actions average: 
K:  2  action  0 :  tensor([    0.9975,     0.0013,     0.0000,     0.0003,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9719,     0.0004,     0.0003,     0.0268],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0187,     0.8277,     0.0455,     0.1077],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0006,     0.0347,     0.8793,     0.0851],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0087, 0.0434, 0.0311, 0.2118, 0.7050], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.05165256693205843, 0.08057881420837087, 0.06285864068004718, 0.5054985537256034, 0.2533618943958561, 0.04604953005806404]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.051749408070514404, 0.08041852509849634, 0.06285586915537038, 0.5071143125496101, 0.25166570759792223, 0.0461961775280864]
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]] [[49.232]
 [49.232]
 [49.232]
 [49.232]
 [49.232]] [[0.139]
 [0.139]
 [0.139]
 [0.139]
 [0.139]]
siam score:  -0.9309366
siam score:  -0.93005705
maxi score, test score, baseline:  0.10010000000000001 0.65 0.65
probs:  [0.051749408070514404, 0.08041852509849634, 0.06285586915537038, 0.5071143125496101, 0.25166570759792223, 0.0461961775280864]
using explorer policy with actor:  0
actor:  0 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1021 0.65 0.65
probs:  [0.051749408070514404, 0.08041852509849634, 0.06285586915537038, 0.5071143125496101, 0.25166570759792223, 0.0461961775280864]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1021 0.65 0.65
probs:  [0.051749408070514404, 0.08041852509849633, 0.06285586915537039, 0.5071143125496101, 0.2516657075979223, 0.0461961775280864]
maxi score, test score, baseline:  0.1021 0.65 0.65
maxi score, test score, baseline:  0.1021 0.65 0.65
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1021 0.65 0.65
probs:  [0.051749408070514404, 0.08041852509849633, 0.06285586915537039, 0.5071143125496101, 0.2516657075979223, 0.0461961775280864]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.51553706786141
printing an ep nov before normalisation:  64.62111522576971
printing an ep nov before normalisation:  52.830102821312344
printing an ep nov before normalisation:  28.339221477508545
printing an ep nov before normalisation:  42.40477194186113
printing an ep nov before normalisation:  51.36915553592008
Printing some Q and Qe and total Qs values:  [[0.721]
 [0.622]
 [0.622]
 [0.644]
 [0.638]] [[32.69 ]
 [40.404]
 [40.404]
 [35.565]
 [30.058]] [[0.721]
 [0.622]
 [0.622]
 [0.644]
 [0.638]]
printing an ep nov before normalisation:  38.94921102211294
printing an ep nov before normalisation:  39.41454887390137
printing an ep nov before normalisation:  52.04921047103648
printing an ep nov before normalisation:  57.25738530081445
printing an ep nov before normalisation:  55.576144051408214
printing an ep nov before normalisation:  42.491259069104686
Printing some Q and Qe and total Qs values:  [[0.971]
 [0.941]
 [0.941]
 [0.941]
 [0.941]] [[50.261]
 [51.796]
 [51.796]
 [51.796]
 [51.796]] [[0.971]
 [0.941]
 [0.941]
 [0.941]
 [0.941]]
maxi score, test score, baseline:  0.1021 0.65 0.65
probs:  [0.05146589715755549, 0.08546489253470557, 0.06251136979079108, 0.5043302751202143, 0.2502844045557959, 0.045943160840937705]
Printing some Q and Qe and total Qs values:  [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[50.35]
 [50.35]
 [50.35]
 [50.35]
 [50.35]] [[0.783]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]] [[41.273]
 [28.879]
 [28.879]
 [28.879]
 [28.879]] [[0.557]
 [0.55 ]
 [0.55 ]
 [0.55 ]
 [0.55 ]]
maxi score, test score, baseline:  0.1021 0.65 0.65
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.11410000000000001 0.65 0.65
probs:  [0.0514658971575555, 0.08546489253470559, 0.06251136979079108, 0.5043302751202143, 0.2502844045557959, 0.045943160840937705]
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  39.96200587488159
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.009787608063334119
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1341 0.65 0.65
probs:  [0.0514658971575555, 0.08546489253470559, 0.06251136979079108, 0.5043302751202143, 0.2502844045557959, 0.045943160840937705]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.9646,     0.0199,     0.0000,     0.0004,     0.0151],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9560,     0.0011,     0.0004,     0.0422],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0004,     0.9475,     0.0229,     0.0292],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0008,     0.0003,     0.8620,     0.1362],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0196, 0.0034, 0.0445, 0.0815, 0.8510], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.0514658971575555, 0.08546489253470559, 0.06251136979079108, 0.5043302751202143, 0.2502844045557959, 0.045943160840937705]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.426]
 [0.411]
 [0.443]
 [0.427]
 [0.432]] [[25.815]
 [29.681]
 [25.605]
 [28.217]
 [25.051]] [[1.084]
 [1.243]
 [1.092]
 [1.193]
 [1.056]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.591]
 [0.413]
 [0.351]
 [0.587]] [[49.184]
 [47.118]
 [41.3  ]
 [41.398]
 [45.206]] [[1.85 ]
 [1.894]
 [1.426]
 [1.369]
 [1.795]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
printing an ep nov before normalisation:  16.569758082235595
siam score:  -0.92703253
Printing some Q and Qe and total Qs values:  [[1.098]
 [1.356]
 [1.174]
 [0.453]
 [1.106]] [[35.582]
 [26.622]
 [29.498]
 [35.302]
 [37.336]] [[2.358]
 [2.298]
 [2.218]
 [1.703]
 [2.427]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.88808523332833
printing an ep nov before normalisation:  36.016415536066496
siam score:  -0.9270906
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.0998486414848
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.543]
 [0.566]
 [0.45 ]
 [0.62 ]] [[38.709]
 [35.559]
 [34.596]
 [32.49 ]
 [33.089]] [[0.858]
 [0.714]
 [0.729]
 [0.594]
 [0.769]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.455]
 [0.564]
 [0.455]
 [0.455]
 [0.429]] [[42.543]
 [52.166]
 [42.543]
 [42.543]
 [49.617]] [[0.664]
 [0.831]
 [0.664]
 [0.664]
 [0.681]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
siam score:  -0.92866594
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.849]
 [0.898]
 [0.848]
 [0.954]] [[37.384]
 [30.238]
 [34.689]
 [26.365]
 [26.033]] [[1.143]
 [0.974]
 [1.057]
 [0.944]
 [1.047]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  71.44863200935261
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.171]
 [0.279]
 [0.192]
 [0.171]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.171]
 [0.171]
 [0.279]
 [0.192]
 [0.171]]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
printing an ep nov before normalisation:  50.87788532354725
printing an ep nov before normalisation:  52.07671775315637
printing an ep nov before normalisation:  46.961422164673
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.93350530184545
printing an ep nov before normalisation:  59.92298897620206
printing an ep nov before normalisation:  48.29744585923739
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.198984157290155
printing an ep nov before normalisation:  40.382171230097406
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.097986698150635
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.6594280155978
maxi score, test score, baseline:  0.14209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 53.61259875504492
siam score:  -0.92572284
printing an ep nov before normalisation:  27.559283674896548
printing an ep nov before normalisation:  31.869435288170095
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.90482789374393
maxi score, test score, baseline:  0.14409999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.113]
 [0.275]
 [0.135]
 [0.128]
 [0.13 ]] [[38.151]
 [49.731]
 [26.293]
 [29.38 ]
 [29.004]] [[0.113]
 [0.275]
 [0.135]
 [0.128]
 [0.13 ]]
printing an ep nov before normalisation:  39.64565662801407
printing an ep nov before normalisation:  0.0005859052834011891
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9306833
siam score:  -0.93003523
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.768148621291985
Printing some Q and Qe and total Qs values:  [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.51 ]
 [0.509]] [[22.457]
 [22.457]
 [22.457]
 [24.13 ]
 [23.107]] [[0.52 ]
 [0.52 ]
 [0.52 ]
 [0.51 ]
 [0.509]]
Printing some Q and Qe and total Qs values:  [[0.84 ]
 [0.915]
 [0.84 ]
 [0.84 ]
 [0.793]] [[33.007]
 [41.531]
 [33.007]
 [33.007]
 [32.763]] [[0.84 ]
 [0.915]
 [0.84 ]
 [0.84 ]
 [0.793]]
maxi score, test score, baseline:  0.14609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
printing an ep nov before normalisation:  38.21138313445424
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.34360783197086
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.97249760055776
using explorer policy with actor:  1
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.677587676533797
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92642075
printing an ep nov before normalisation:  25.95915368890368
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9270581
maxi score, test score, baseline:  0.14809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.726997110400404
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.405]
 [1.361]
 [1.386]
 [1.386]
 [1.388]] [[27.   ]
 [31.548]
 [31.052]
 [31.052]
 [32.051]] [[2.718]
 [3.135]
 [3.109]
 [3.109]
 [3.212]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.341]
 [1.373]
 [1.385]
 [1.412]
 [1.367]] [[27.104]
 [28.19 ]
 [25.139]
 [22.92 ]
 [23.327]] [[2.232]
 [2.325]
 [2.166]
 [2.068]
 [2.046]]
Printing some Q and Qe and total Qs values:  [[1.302]
 [1.226]
 [1.226]
 [1.186]
 [1.231]] [[24.367]
 [21.891]
 [21.891]
 [20.316]
 [21.767]] [[2.189]
 [2.023]
 [2.023]
 [1.926]
 [2.023]]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.63251491172815
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9535,     0.0108,     0.0001,     0.0011,     0.0346],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9750,     0.0104,     0.0001,     0.0145],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9712,     0.0169,     0.0118],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0003,     0.0005,     0.8821,     0.1170],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0007,     0.1214,     0.0548,     0.0818,     0.7415],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.02126481044033
maxi score, test score, baseline:  0.15009999999999998 1.0 1.0
printing an ep nov before normalisation:  41.17751823023995
Printing some Q and Qe and total Qs values:  [[0.562]
 [0.562]
 [0.562]
 [0.675]
 [0.562]] [[30.576]
 [30.576]
 [30.576]
 [27.379]
 [30.576]] [[0.562]
 [0.562]
 [0.562]
 [0.675]
 [0.562]]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9305832
printing an ep nov before normalisation:  38.56837592324364
UNIT TEST: sample policy line 217 mcts : [0.026 0.051 0.026 0.872 0.026]
Printing some Q and Qe and total Qs values:  [[1.464]
 [1.352]
 [1.352]
 [1.352]
 [1.34 ]] [[36.342]
 [27.243]
 [27.243]
 [27.243]
 [29.119]] [[1.989]
 [1.679]
 [1.679]
 [1.679]
 [1.708]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.363858222961426
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9963,     0.0004,     0.0000,     0.0015,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9625,     0.0001,     0.0000,     0.0372],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0007,     0.0002,     0.9413,     0.0015,     0.0563],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0008,     0.0374,     0.8215,     0.1395],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0229, 0.1474, 0.0295, 0.0228, 0.7775], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  22.068731784820557
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9295163
printing an ep nov before normalisation:  61.47842981187105
printing an ep nov before normalisation:  40.93303774541838
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.038041055347463
printing an ep nov before normalisation:  41.02212518031156
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.71897006034851
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
siam score:  -0.93237585
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.502]
 [0.276]
 [0.265]
 [0.267]] [[35.84 ]
 [45.867]
 [55.993]
 [40.731]
 [43.492]] [[0.557]
 [1.052]
 [1.054]
 [0.699]
 [0.762]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.9943586106762
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.26328897476196
siam score:  -0.92966187
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  42.38009418383703
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.679]
 [0.566]
 [0.566]
 [0.566]] [[43.626]
 [47.181]
 [43.626]
 [43.626]
 [43.626]] [[1.855]
 [2.156]
 [1.855]
 [1.855]
 [1.855]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.947492225295512
printing an ep nov before normalisation:  36.76582674217645
printing an ep nov before normalisation:  38.50850582122803
printing an ep nov before normalisation:  38.06294173353853
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  4.101726176486409e-06
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.45808219909668
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9994,     0.0001,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9505,     0.0013,     0.0005,     0.0471],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0099,     0.9528,     0.0011,     0.0360],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0005,     0.0595,     0.8082,     0.1312],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0030, 0.0865, 0.0363, 0.0907, 0.7835], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.28355753614088
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.03357741245965
printing an ep nov before normalisation:  42.053146503595464
siam score:  -0.9185382
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  117 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.92025465
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.996]
 [0.974]
 [0.974]
 [0.974]
 [0.974]] [[34.554]
 [33.77 ]
 [33.77 ]
 [33.77 ]
 [33.77 ]] [[1.172]
 [1.143]
 [1.143]
 [1.143]
 [1.143]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.766]
 [0.848]
 [0.766]
 [0.766]
 [0.766]] [[38.627]
 [33.38 ]
 [38.627]
 [38.627]
 [38.627]] [[0.766]
 [0.848]
 [0.766]
 [0.766]
 [0.766]]
actor:  0 policy actor:  0  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.70598077774048
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.35361220787893
printing an ep nov before normalisation:  39.88657800054581
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1541 1.0 1.0
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.794432640075684
maxi score, test score, baseline:  0.1541 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.225]
 [1.326]
 [0.537]
 [1.297]
 [1.348]] [[24.302]
 [28.268]
 [27.793]
 [25.33 ]
 [27.943]] [[1.636]
 [1.9  ]
 [1.092]
 [1.751]
 [1.909]]
printing an ep nov before normalisation:  40.077829882110514
printing an ep nov before normalisation:  32.586634159088135
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.388512155475034
maxi score, test score, baseline:  0.1541 1.0 1.0
printing an ep nov before normalisation:  38.650722480125864
printing an ep nov before normalisation:  32.09868404760305
printing an ep nov before normalisation:  35.27433184068149
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9239986
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  23.431212902069092
siam score:  -0.92479014
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.80405318589208
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.94041633605957
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.17939853668213
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9316528
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.31268870917985
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.133]
 [0.14 ]
 [0.169]
 [0.172]
 [0.172]] [[28.463]
 [36.011]
 [43.531]
 [34.528]
 [34.421]] [[0.133]
 [0.14 ]
 [0.169]
 [0.172]
 [0.172]]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.563]
 [0.112]
 [0.738]
 [0.739]] [[32.437]
 [31.884]
 [35.834]
 [39.075]
 [33.882]] [[0.023]
 [0.563]
 [0.112]
 [0.738]
 [0.739]]
Printing some Q and Qe and total Qs values:  [[0.403]
 [0.566]
 [0.303]
 [0.403]
 [0.403]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.403]
 [0.566]
 [0.303]
 [0.403]
 [0.403]]
Printing some Q and Qe and total Qs values:  [[0.208]
 [0.001]
 [0.565]
 [0.246]
 [0.245]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.208]
 [0.001]
 [0.565]
 [0.246]
 [0.245]]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.42992728329846
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.010332107543945
siam score:  -0.933841
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.796]
 [0.67 ]
 [0.67 ]
 [0.67 ]] [[42.945]
 [56.111]
 [42.945]
 [42.945]
 [42.945]] [[0.966]
 [1.262]
 [0.966]
 [0.966]
 [0.966]]
maxi score, test score, baseline:  0.1561 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9904,     0.0035,     0.0000,     0.0028,     0.0033],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0025,     0.9053,     0.0115,     0.0003,     0.0805],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0011, 0.0141, 0.9394, 0.0101, 0.0354], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0000,     0.0016,     0.9958,     0.0025],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0076, 0.1288, 0.0182, 0.0250, 0.8203], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.596]
 [0.877]
 [0.877]
 [0.92 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.538]
 [0.596]
 [0.877]
 [0.877]
 [0.92 ]]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9980,     0.0001,     0.0000,     0.0003,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0011,     0.9148,     0.0099,     0.0005,     0.0737],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0153,     0.9197,     0.0233,     0.0415],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0026, 0.0021, 0.0153, 0.8761, 0.1039], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0485, 0.0658, 0.0609, 0.0823, 0.7425], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.46660234577078
actions average: 
K:  0  action  0 :  tensor([    0.9388,     0.0006,     0.0000,     0.0372,     0.0235],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9460,     0.0003,     0.0001,     0.0535],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0016,     0.8931,     0.0264,     0.0784],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0016, 0.0022, 0.0571, 0.7660, 0.1731], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.1156, 0.0325, 0.1752, 0.6744], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.698]
 [0.832]
 [0.698]
 [0.698]
 [0.564]] [[44.009]
 [61.894]
 [44.009]
 [44.009]
 [56.235]] [[1.057]
 [1.58 ]
 [1.057]
 [1.057]
 [1.189]]
siam score:  -0.93429166
printing an ep nov before normalisation:  42.8291858360813
printing an ep nov before normalisation:  23.796199721140916
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1581 1.0 1.0
siam score:  -0.9322684
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.05020641016176
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 26.45974197682866
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1581 1.0 1.0
printing an ep nov before normalisation:  40.9846199275902
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1581 1.0 1.0
siam score:  -0.9298224
printing an ep nov before normalisation:  50.3746231728586
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  2  action  0 :  tensor([    0.9687,     0.0002,     0.0005,     0.0002,     0.0305],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9485,     0.0092,     0.0003,     0.0418],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0085, 0.0129, 0.9127, 0.0132, 0.0527], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0007,     0.0505,     0.8133,     0.1347],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0411, 0.0112, 0.0836, 0.1527, 0.7114], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.71666021230083
printing an ep nov before normalisation:  35.366546463120116
Printing some Q and Qe and total Qs values:  [[1.417]
 [1.371]
 [1.359]
 [1.417]
 [1.381]] [[31.089]
 [33.931]
 [34.319]
 [31.089]
 [32.244]] [[2.05 ]
 [2.133]
 [2.139]
 [2.05 ]
 [2.067]]
maxi score, test score, baseline:  0.1581 1.0 1.0
printing an ep nov before normalisation:  17.88890996091668
Printing some Q and Qe and total Qs values:  [[0.676]
 [0.676]
 [0.676]
 [0.702]
 [0.654]] [[31.643]
 [31.643]
 [31.643]
 [31.071]
 [31.54 ]] [[0.676]
 [0.676]
 [0.676]
 [0.702]
 [0.654]]
printing an ep nov before normalisation:  40.69245469983202
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.080631732940674
siam score:  -0.9310916
printing an ep nov before normalisation:  38.09836085784712
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.736470411368174
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9673,     0.0007,     0.0000,     0.0002,     0.0319],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9274,     0.0002,     0.0002,     0.0720],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0078,     0.9307,     0.0122,     0.0490],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0021,     0.0186,     0.9055,     0.0735],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0240, 0.0483, 0.0020, 0.0379, 0.8877], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.14289638878054
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.46700251338546
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9338158
printing an ep nov before normalisation:  31.520168781280518
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.352]
 [0.359]
 [0.398]
 [0.331]
 [0.362]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.352]
 [0.359]
 [0.398]
 [0.331]
 [0.362]]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.669877087688114
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.035]
 [1.035]
 [1.08 ]
 [1.032]
 [1.033]] [[51.557]
 [51.557]
 [46.95 ]
 [52.228]
 [53.21 ]] [[2.805]
 [2.805]
 [2.597]
 [2.839]
 [2.894]]
printing an ep nov before normalisation:  43.29853348257829
Printing some Q and Qe and total Qs values:  [[1.168]
 [1.168]
 [0.524]
 [1.169]
 [1.267]] [[34.442]
 [34.442]
 [39.625]
 [37.372]
 [38.081]] [[2.327]
 [2.327]
 [2.042]
 [2.531]
 [2.678]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 47.39621600190083
maxi score, test score, baseline:  0.1601 1.0 1.0
printing an ep nov before normalisation:  45.445759271010054
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.04]
 [1.04]
 [1.04]
 [1.04]
 [1.04]] [[40.098]
 [40.098]
 [40.098]
 [40.098]
 [40.098]] [[2.598]
 [2.598]
 [2.598]
 [2.598]
 [2.598]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.204]
 [1.064]
 [1.064]
 [1.064]
 [1.064]] [[38.322]
 [30.191]
 [30.191]
 [30.191]
 [30.191]] [[3.   ]
 [2.139]
 [2.139]
 [2.139]
 [2.139]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.45564810144162
Printing some Q and Qe and total Qs values:  [[0.906]
 [0.918]
 [0.92 ]
 [0.908]
 [0.924]] [[31.256]
 [30.834]
 [31.106]
 [31.365]
 [29.624]] [[1.915]
 [1.901]
 [1.92 ]
 [1.924]
 [1.829]]
maxi score, test score, baseline:  0.1601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.957]
 [1.081]
 [1.081]
 [1.006]
 [1.081]] [[39.152]
 [41.211]
 [41.211]
 [35.636]
 [41.211]] [[1.968]
 [2.197]
 [2.197]
 [1.839]
 [2.197]]
printing an ep nov before normalisation:  40.76835288695494
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  1  action  0 :  tensor([    0.9996,     0.0002,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9342,     0.0008,     0.0001,     0.0634],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0004,     0.9438,     0.0150,     0.0407],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0002,     0.0462,     0.8180,     0.1352],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0027, 0.0686, 0.0606, 0.1480, 0.7202], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.74647163951418
printing an ep nov before normalisation:  35.29711901362071
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.19482757542114
siam score:  -0.93757856
maxi score, test score, baseline:  0.1601 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.762]
 [0.934]
 [0.848]
 [0.762]
 [0.762]] [[30.258]
 [31.463]
 [30.376]
 [30.258]
 [30.258]] [[0.762]
 [0.934]
 [0.848]
 [0.762]
 [0.762]]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.571]
 [0.338]
 [0.312]
 [0.326]] [[40.979]
 [41.806]
 [35.528]
 [32.932]
 [33.612]] [[1.01 ]
 [1.342]
 [0.893]
 [0.778]
 [0.815]]
Printing some Q and Qe and total Qs values:  [[0.014]
 [0.831]
 [0.009]
 [0.611]
 [0.689]] [[50.198]
 [50.323]
 [52.465]
 [51.55 ]
 [52.149]] [[0.216]
 [1.034]
 [0.23 ]
 [0.825]
 [0.908]]
line 256 mcts: sample exp_bonus 55.277967058404464
printing an ep nov before normalisation:  29.399197101593018
maxi score, test score, baseline:  0.1621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.269]
 [1.269]
 [1.269]
 [1.269]] [[33.189]
 [33.189]
 [33.189]
 [33.189]
 [33.189]] [[2.844]
 [2.844]
 [2.844]
 [2.844]
 [2.844]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 40.03450239389376
printing an ep nov before normalisation:  36.58087728719353
siam score:  -0.93490434
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.93474954
printing an ep nov before normalisation:  35.27630810815987
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.558]
 [0.338]
 [0.418]
 [0.418]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.418]
 [0.558]
 [0.338]
 [0.418]
 [0.418]]
maxi score, test score, baseline:  0.1621 1.0 1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.323343660861
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.904784202575684
printing an ep nov before normalisation:  35.32073895136516
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.204230572639155
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.10274291629071
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1621 1.0 1.0
printing an ep nov before normalisation:  36.26675987119251
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.99078275439601
printing an ep nov before normalisation:  0.0004018869503852329
line 256 mcts: sample exp_bonus 0.000708840833681279
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  23.965280212533933
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.755]
 [0.755]
 [0.713]
 [0.755]] [[29.049]
 [33.674]
 [33.674]
 [37.638]
 [33.674]] [[0.769]
 [0.755]
 [0.755]
 [0.713]
 [0.755]]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1641 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.696]
 [0.696]
 [0.696]
 [0.696]
 [0.943]] [[37.156]
 [37.156]
 [37.156]
 [37.156]
 [45.097]] [[1.11 ]
 [1.11 ]
 [1.11 ]
 [1.11 ]
 [1.513]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.959597586263754
printing an ep nov before normalisation:  25.66985073396909
printing an ep nov before normalisation:  29.642597121743275
printing an ep nov before normalisation:  25.68009100796499
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.27199606562691
printing an ep nov before normalisation:  24.67888780314216
printing an ep nov before normalisation:  38.37559849646437
Printing some Q and Qe and total Qs values:  [[1.18]
 [1.3 ]
 [1.18]
 [1.18]
 [1.18]] [[36.878]
 [39.047]
 [36.878]
 [36.878]
 [36.878]] [[1.55 ]
 [1.712]
 [1.55 ]
 [1.55 ]
 [1.55 ]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.92684937
maxi score, test score, baseline:  0.1641 1.0 1.0
printing an ep nov before normalisation:  41.064669904772614
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.10118715298242
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.974062662656415
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.006975331178836
maxi score, test score, baseline:  0.1661 1.0 1.0
siam score:  -0.924339
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92749596
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.60031578572217
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.068]
 [1.031]
 [1.068]
 [1.068]] [[39.674]
 [39.674]
 [54.966]
 [39.674]
 [39.674]] [[1.905]
 [1.905]
 [2.364]
 [1.905]
 [1.905]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.25090643801341
printing an ep nov before normalisation:  43.65653362805792
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.43226620635407
printing an ep nov before normalisation:  23.961615562438965
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.49136469418962
printing an ep nov before normalisation:  38.350753463355296
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 37.64115494989563
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.54280348193553
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.93437225
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.715]
 [0.868]
 [0.687]
 [0.693]] [[63.952]
 [59.27 ]
 [59.101]
 [63.952]
 [66.157]] [[2.447]
 [2.268]
 [2.413]
 [2.447]
 [2.55 ]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.264532371690315
siam score:  -0.93392926
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
printing an ep nov before normalisation:  50.85971930877444
printing an ep nov before normalisation:  50.05449524850016
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  19.16093945503235
printing an ep nov before normalisation:  0.05361142792239093
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.04]
 [1.04]
 [1.04]
 [1.08]
 [1.04]] [[50.483]
 [50.483]
 [50.483]
 [56.611]
 [50.483]] [[1.566]
 [1.566]
 [1.566]
 [1.687]
 [1.566]]
maxi score, test score, baseline:  0.1681 1.0 1.0
printing an ep nov before normalisation:  62.79520027570911
printing an ep nov before normalisation:  56.69190889739103
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.478672139092815
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1681 1.0 1.0
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.18462037994405
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92830354
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]] [[53.179]
 [53.179]
 [53.179]
 [53.179]
 [53.179]] [[2.18]
 [2.18]
 [2.18]
 [2.18]
 [2.18]]
printing an ep nov before normalisation:  52.578672437001416
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.24964774904646
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.41684249624574
printing an ep nov before normalisation:  44.70285893879639
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [0.994]
 [0.896]
 [0.994]] [[49.26 ]
 [49.26 ]
 [49.26 ]
 [53.538]
 [49.26 ]] [[2.375]
 [2.375]
 [2.375]
 [2.449]
 [2.375]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
printing an ep nov before normalisation:  50.182909099857284
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.70569625910599
line 256 mcts: sample exp_bonus 41.35649100757636
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1681 1.0 1.0
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9339551
siam score:  -0.93279696
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.61955213546753
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.089358046505435
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  32.02427058523585
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.33088310813075
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
printing an ep nov before normalisation:  38.005045294613936
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.880446434020996
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9979,     0.0010,     0.0000,     0.0005,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0031,     0.9629,     0.0113,     0.0003,     0.0223],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9350,     0.0410,     0.0238],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0007,     0.0009,     0.0019,     0.8304,     0.1661],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0044, 0.0286, 0.0140, 0.0920, 0.8611], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.453267574310303
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9928,     0.0013,     0.0000,     0.0012,     0.0047],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9774,     0.0012,     0.0002,     0.0211],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0114,     0.9689,     0.0001,     0.0189],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0010,     0.0349,     0.8171,     0.1466],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0015, 0.0247, 0.0278, 0.1264, 0.8196], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.58546517072028
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 30.63254216695582
printing an ep nov before normalisation:  34.73744013466252
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
maxi score, test score, baseline:  0.17409999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.424]
 [0.737]
 [0.33 ]
 [0.327]
 [0.439]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.424]
 [0.737]
 [0.33 ]
 [0.327]
 [0.439]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.582]
 [0.729]
 [0.582]
 [0.582]
 [0.569]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.582]
 [0.729]
 [0.582]
 [0.582]
 [0.569]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.318165564490286
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.17444977328152
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.73710338136118
printing an ep nov before normalisation:  34.87931906554291
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.79753042124733
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.54886555914248
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.46346450690028
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  20.101619327938067
siam score:  -0.9229336
Printing some Q and Qe and total Qs values:  [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]] [[41.048]
 [41.048]
 [41.048]
 [41.048]
 [41.048]] [[0.793]
 [0.793]
 [0.793]
 [0.793]
 [0.793]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.16 ]
 [1.326]
 [1.16 ]
 [1.218]
 [1.225]] [[32.819]
 [36.032]
 [32.819]
 [24.17 ]
 [25.998]] [[1.884]
 [2.202]
 [1.884]
 [1.531]
 [1.625]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.333]
 [0.352]
 [0.354]
 [0.354]] [[60.892]
 [70.201]
 [73.227]
 [69.386]
 [68.974]] [[1.208]
 [1.445]
 [1.549]
 [1.443]
 [1.432]]
printing an ep nov before normalisation:  55.37177563033192
siam score:  -0.9235208
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.69388765674003
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.561]
 [0.824]
 [0.561]
 [0.561]
 [0.561]] [[38.363]
 [39.601]
 [38.363]
 [38.363]
 [38.363]] [[0.934]
 [1.219]
 [0.934]
 [0.934]
 [0.934]]
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9190957
actions average: 
K:  1  action  0 :  tensor([    0.9981,     0.0000,     0.0000,     0.0002,     0.0016],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9401,     0.0004,     0.0001,     0.0578],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0074,     0.9375,     0.0143,     0.0405],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0145,     0.8036,     0.1815],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0026, 0.0806, 0.0461, 0.0966, 0.7742], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.17609999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.478342893485426
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.77168418567085
siam score:  -0.92383516
printing an ep nov before normalisation:  43.8805250470034
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.05482259619007
printing an ep nov before normalisation:  29.146902922438187
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.59227481545014
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.81955067638223
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9900,     0.0013,     0.0000,     0.0004,     0.0083],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.8937,     0.0110,     0.0022,     0.0927],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0162,     0.9497,     0.0029,     0.0311],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0048,     0.0004,     0.0101,     0.8976,     0.0871],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0347, 0.0896, 0.0376, 0.0944, 0.7438], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.83022571775831
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.04111875270007
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.65821745717869
printing an ep nov before normalisation:  29.63677167892456
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  20.234235981618216
maxi score, test score, baseline:  0.17809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.80601376125261
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
siam score:  -0.92465377
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0010,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0365,     0.9123,     0.0254,     0.0001,     0.0256],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0001,     0.9133,     0.0116,     0.0747],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0233,     0.0002,     0.0207,     0.8830,     0.0728],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0293, 0.1144, 0.0711, 0.1227, 0.6625], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.385137842649467
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.717659777574056
printing an ep nov before normalisation:  43.0152316123454
actions average: 
K:  0  action  0 :  tensor([    0.9687,     0.0001,     0.0000,     0.0003,     0.0310],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9464,     0.0117,     0.0000,     0.0416],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0007,     0.0077,     0.9168,     0.0129,     0.0619],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0021,     0.0002,     0.0002,     0.8446,     0.1529],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0034, 0.0922, 0.0548, 0.1733, 0.6763], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.91538903749534
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.9883217279207
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[46.859]
 [46.859]
 [46.859]
 [46.859]
 [46.859]] [[2.564]
 [2.564]
 [2.564]
 [2.564]
 [2.564]]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18009999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.66080927848816
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9956,     0.0007,     0.0000,     0.0004,     0.0034],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9338,     0.0112,     0.0002,     0.0539],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0006,     0.8917,     0.0336,     0.0736],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0032,     0.0005,     0.8972,     0.0988],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0027, 0.0862, 0.0697, 0.1140, 0.7274], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.49719566985712
printing an ep nov before normalisation:  45.0798905059927
siam score:  -0.9072742
Printing some Q and Qe and total Qs values:  [[1.297]
 [1.409]
 [1.358]
 [1.358]
 [1.193]] [[41.353]
 [35.534]
 [38.225]
 [38.225]
 [40.751]] [[2.363]
 [2.193]
 [2.272]
 [2.272]
 [2.229]]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9079763
printing an ep nov before normalisation:  28.851349353790283
printing an ep nov before normalisation:  67.10779327323391
printing an ep nov before normalisation:  48.995457292517465
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.711]
 [0.784]
 [0.728]
 [0.555]
 [0.748]] [[43.388]
 [36.603]
 [42.027]
 [27.325]
 [34.435]] [[0.931]
 [0.942]
 [0.936]
 [0.627]
 [0.886]]
actions average: 
K:  2  action  0 :  tensor([    0.9954,     0.0006,     0.0022,     0.0002,     0.0015],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9696,     0.0006,     0.0004,     0.0293],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0006,     0.8254,     0.0828,     0.0910],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0003,     0.0591,     0.7924,     0.1476],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0207, 0.0659, 0.1784, 0.7340], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9470,     0.0026,     0.0000,     0.0222,     0.0282],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9551,     0.0143,     0.0003,     0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9990,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0006,     0.0008,     0.8888,     0.1097],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0214, 0.0466, 0.0628, 0.1306, 0.7386], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.661194058085844
Starting evaluation
printing an ep nov before normalisation:  50.27400191047649
maxi score, test score, baseline:  0.18409999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.554]
 [0.548]
 [0.582]
 [0.583]] [[44.299]
 [34.588]
 [38.06 ]
 [36.876]
 [36.407]] [[0.535]
 [0.554]
 [0.548]
 [0.582]
 [0.583]]
printing an ep nov before normalisation:  46.80913358300935
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.541]
 [0.541]
 [0.54 ]
 [0.541]] [[36.339]
 [34.598]
 [34.598]
 [38.506]
 [34.598]] [[0.542]
 [0.541]
 [0.541]
 [0.54 ]
 [0.541]]
Printing some Q and Qe and total Qs values:  [[0.627]
 [0.627]
 [0.627]
 [0.653]
 [0.631]] [[40.012]
 [40.012]
 [40.012]
 [44.927]
 [42.175]] [[0.627]
 [0.627]
 [0.627]
 [0.653]
 [0.631]]
printing an ep nov before normalisation:  49.55650914657742
Printing some Q and Qe and total Qs values:  [[0.769]
 [0.892]
 [0.769]
 [0.668]
 [0.677]] [[38.456]
 [36.16 ]
 [38.456]
 [50.868]
 [38.609]] [[0.769]
 [0.892]
 [0.769]
 [0.668]
 [0.677]]
printing an ep nov before normalisation:  35.42365550994873
actions average: 
K:  4  action  0 :  tensor([    0.9954,     0.0035,     0.0000,     0.0002,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9758,     0.0002,     0.0001,     0.0238],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0011, 0.0311, 0.8840, 0.0141, 0.0697], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0011,     0.0004,     0.9196,     0.0786],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0025, 0.0577, 0.0815, 0.1540, 0.7043], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.24884605407715
printing an ep nov before normalisation:  29.180049002974336
printing an ep nov before normalisation:  46.93831666436062
printing an ep nov before normalisation:  50.546827967215044
printing an ep nov before normalisation:  41.336178513941846
printing an ep nov before normalisation:  33.92375469207764
printing an ep nov before normalisation:  34.65172290802002
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.44472471718331
Printing some Q and Qe and total Qs values:  [[0.817]
 [0.83 ]
 [0.819]
 [0.797]
 [0.815]] [[33.418]
 [43.467]
 [41.401]
 [47.012]
 [48.709]] [[1.241]
 [1.426]
 [1.38 ]
 [1.454]
 [1.501]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.291]
 [0.746]
 [0.853]
 [0.06 ]
 [0.895]] [[40.552]
 [36.673]
 [49.113]
 [43.248]
 [40.793]] [[0.771]
 [1.138]
 [1.529]
 [0.601]
 [1.38 ]]
printing an ep nov before normalisation:  49.666997053763794
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.21867909706757
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.23051196309523
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
actions average: 
K:  3  action  0 :  tensor([    0.9907,     0.0061,     0.0000,     0.0010,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9640,     0.0107,     0.0001,     0.0250],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9997,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0003,     0.0140,     0.8955,     0.0902],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0220, 0.0747, 0.0918, 0.8106], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92414904
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.27501240288614
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.53138590369538
Printing some Q and Qe and total Qs values:  [[1.149]
 [1.149]
 [1.149]
 [1.149]
 [1.149]] [[37.889]
 [37.889]
 [37.889]
 [37.889]
 [37.889]] [[1.337]
 [1.337]
 [1.337]
 [1.337]
 [1.337]]
printing an ep nov before normalisation:  42.267912440860094
printing an ep nov before normalisation:  34.74769153285809
printing an ep nov before normalisation:  32.49946225216686
actor:  1 policy actor:  1  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.18465448818538
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.135]
 [1.135]
 [1.135]
 [1.135]
 [1.135]] [[22.554]
 [22.554]
 [22.554]
 [22.554]
 [22.554]] [[1.54]
 [1.54]
 [1.54]
 [1.54]
 [1.54]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9444,     0.0100,     0.0001,     0.0452],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0005,     0.9665,     0.0019,     0.0305],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0001,     0.0200,     0.9595,     0.0203],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0024, 0.1442, 0.0185, 0.1503, 0.6846], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.91944176
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.65385264663274
printing an ep nov before normalisation:  67.74453314585423
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.713]
 [0.874]
 [0.605]
 [0.56 ]
 [0.82 ]] [[52.667]
 [49.586]
 [54.123]
 [56.265]
 [50.523]] [[2.106]
 [2.131]
 [2.061]
 [2.111]
 [2.118]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.295]
 [1.186]
 [1.186]
 [1.174]] [[34.985]
 [37.141]
 [34.985]
 [34.985]
 [35.668]] [[1.405]
 [1.541]
 [1.405]
 [1.405]
 [1.401]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2241 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  24.41856861114502
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2261 1.0 1.0
printing an ep nov before normalisation:  41.93450106502047
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.189]
 [1.024]
 [1.067]
 [1.167]] [[29.75 ]
 [26.975]
 [27.448]
 [29.75 ]
 [30.966]] [[2.102]
 [2.052]
 [1.917]
 [2.102]
 [2.277]]
printing an ep nov before normalisation:  66.51432568046845
Printing some Q and Qe and total Qs values:  [[1.439]
 [1.406]
 [1.354]
 [1.354]
 [1.354]] [[36.639]
 [41.684]
 [43.121]
 [43.121]
 [43.121]] [[2.566]
 [2.863]
 [2.906]
 [2.906]
 [2.906]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.9243314
Printing some Q and Qe and total Qs values:  [[1.196]
 [1.253]
 [1.196]
 [1.196]
 [1.196]] [[30.559]
 [37.997]
 [30.559]
 [30.559]
 [30.559]] [[2.2  ]
 [2.702]
 [2.2  ]
 [2.2  ]
 [2.2  ]]
printing an ep nov before normalisation:  36.087307929992676
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.38539160926519
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[1.283]
 [1.283]
 [1.283]
 [1.283]
 [1.267]] [[32.993]
 [32.993]
 [32.993]
 [32.993]
 [38.093]] [[3.015]
 [3.015]
 [3.015]
 [3.015]
 [3.267]]
printing an ep nov before normalisation:  43.40493476355424
printing an ep nov before normalisation:  33.04206848144531
Printing some Q and Qe and total Qs values:  [[1.041]
 [1.218]
 [1.041]
 [1.041]
 [1.041]] [[35.566]
 [40.644]
 [35.566]
 [35.566]
 [35.566]] [[1.96 ]
 [2.387]
 [1.96 ]
 [1.96 ]
 [1.96 ]]
actions average: 
K:  2  action  0 :  tensor([    0.9747,     0.0004,     0.0066,     0.0006,     0.0176],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9764,     0.0015,     0.0006,     0.0211],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0011,     0.9078,     0.0242,     0.0666],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0004,     0.0006,     0.9370,     0.0619],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.1629, 0.0224, 0.1175, 0.6962], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92048407
maxi score, test score, baseline:  0.2261 1.0 1.0
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  69.85363627452197
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.14102059678699
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.138]
 [0.323]
 [0.138]
 [0.138]
 [0.138]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.138]
 [0.323]
 [0.138]
 [0.138]
 [0.138]]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  63.73222353269739
line 256 mcts: sample exp_bonus 17.655114239597022
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.92009576161703
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.06293629878492
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.687992764274334
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.855]
 [0.887]
 [0.987]
 [0.954]] [[65.906]
 [58.013]
 [60.403]
 [56.758]
 [59.655]] [[1.273]
 [1.131]
 [1.18 ]
 [1.254]
 [1.241]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.834]
 [0.843]
 [0.873]
 [0.78 ]] [[35.672]
 [36.701]
 [28.92 ]
 [37.032]
 [30.375]] [[0.993]
 [1.01 ]
 [0.96 ]
 [1.052]
 [0.908]]
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.06340997298902
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.57454950301265
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2281 1.0 1.0
printing an ep nov before normalisation:  26.93264167797541
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.65877838913603
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  0 policy actor:  0  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  57.159860046144935
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.83167406891701
Printing some Q and Qe and total Qs values:  [[1.417]
 [1.326]
 [1.363]
 [1.251]
 [1.337]] [[35.691]
 [36.536]
 [36.52 ]
 [36.184]
 [36.462]] [[3.121]
 [3.108]
 [3.144]
 [3.   ]
 [3.113]]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9613,     0.0001,     0.0002,     0.0381],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0012,     0.9686,     0.0162,     0.0139],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0019,     0.0003,     0.0220,     0.8734,     0.1024],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0070, 0.0622, 0.0527, 0.1426, 0.7354], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  61.54547770838026
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.077 0.026 0.077 0.769 0.051]
siam score:  -0.92419225
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.437]
 [0.466]
 [0.442]
 [0.444]
 [0.448]] [[48.188]
 [51.576]
 [55.135]
 [53.458]
 [54.163]] [[1.181]
 [1.314]
 [1.4  ]
 [1.35 ]
 [1.376]]
siam score:  -0.9235351
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.08084124293732
printing an ep nov before normalisation:  0.08710966595231184
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  58.776942133331964
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.308]
 [1.307]
 [1.003]
 [1.293]] [[25.884]
 [33.642]
 [31.375]
 [28.892]
 [32.827]] [[1.932]
 [2.495]
 [2.365]
 [1.919]
 [2.433]]
printing an ep nov before normalisation:  0.3126279327506154
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.0000],
        [0.0040],
        [0.3363],
        [0.2540],
        [0.2653],
        [0.0000],
        [0.4902],
        [0.1632],
        [0.2196]], dtype=torch.float64)
0.0 0.0
0.96059601 0.96059601
0.0 0.004043145514971047
0.0 0.3362681370147455
0.0 0.2539734607212963
0.0 0.26532428760160803
0.0 0.0
0.0 0.49022348417442996
0.0 0.1631650732227277
0.0 0.2195925797179307
maxi score, test score, baseline:  0.2301 1.0 1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.54614332416561
printing an ep nov before normalisation:  62.33812558750796
actions average: 
K:  1  action  0 :  tensor([    0.9966,     0.0001,     0.0021,     0.0002,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0014,     0.9690,     0.0001,     0.0002,     0.0292],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9983,     0.0009,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0022, 0.0024, 0.0155, 0.7726, 0.2073], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0361, 0.0733, 0.0211, 0.1163, 0.7532], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.2742182428169
printing an ep nov before normalisation:  41.04656175256229
printing an ep nov before normalisation:  51.182637899418204
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.016]
 [1.006]
 [0.988]
 [1.02 ]
 [1.016]] [[41.294]
 [38.558]
 [40.463]
 [41.917]
 [42.524]] [[2.415]
 [2.238]
 [2.336]
 [2.457]
 [2.489]]
printing an ep nov before normalisation:  45.857491940965346
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.00604481820019
printing an ep nov before normalisation:  33.056745529174805
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.81036186218262
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.47379006765816
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9563,     0.0011,     0.0000,     0.0233,     0.0192],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9439,     0.0114,     0.0003,     0.0440],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0004,     0.9079,     0.0139,     0.0778],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0011,     0.0004,     0.0168,     0.9391,     0.0427],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0217, 0.0302, 0.0234, 0.0820, 0.8427], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2321 1.0 1.0
printing an ep nov before normalisation:  36.605807692995064
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.961923076953582
actions average: 
K:  0  action  0 :  tensor([    0.9662,     0.0008,     0.0000,     0.0004,     0.0326],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9995,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0091,     0.8839,     0.0371,     0.0697],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0360,     0.8443,     0.1194],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0088, 0.0650, 0.0612, 0.0513, 0.8138], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.465421388731
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.019952602581625
Printing some Q and Qe and total Qs values:  [[1.211]
 [1.137]
 [1.018]
 [1.099]
 [1.137]] [[36.349]
 [35.264]
 [37.153]
 [43.606]
 [35.264]] [[1.82 ]
 [1.713]
 [1.652]
 [1.934]
 [1.713]]
printing an ep nov before normalisation:  66.76476319034768
printing an ep nov before normalisation:  32.69388879802893
printing an ep nov before normalisation:  33.252762008750466
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9197233
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2341 1.0 1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9991,     0.0002,     0.0000,     0.0003,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9553,     0.0003,     0.0008,     0.0429],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0279,     0.8660,     0.0190,     0.0870],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0001,     0.0251,     0.8459,     0.1287],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0131, 0.0502, 0.0192, 0.0865, 0.8310], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.20940172892908
printing an ep nov before normalisation:  24.48401927947998
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.84144000866396
siam score:  -0.92102504
printing an ep nov before normalisation:  29.522434906419477
actor:  0 policy actor:  0  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.92313594
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.26787883134723
Printing some Q and Qe and total Qs values:  [[0.565]
 [0.794]
 [0.565]
 [0.565]
 [0.565]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.565]
 [0.794]
 [0.565]
 [0.565]
 [0.565]]
siam score:  -0.92320466
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
printing an ep nov before normalisation:  54.012069520889405
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.9925,     0.0031,     0.0001,     0.0002,     0.0041],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0011,     0.9622,     0.0090,     0.0002,     0.0274],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0139,     0.9631,     0.0006,     0.0221],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0001,     0.0039,     0.9248,     0.0711],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0034, 0.0525, 0.0545, 0.1241, 0.7656], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 41.49027063310146
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.075]
 [1.026]
 [1.025]
 [1.03 ]
 [1.162]] [[39.336]
 [37.861]
 [27.831]
 [28.816]
 [34.028]] [[1.4  ]
 [1.33 ]
 [1.191]
 [1.21 ]
 [1.413]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.611388683319092
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0002,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9554,     0.0002,     0.0008,     0.0436],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9999,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0120,     0.0009,     0.0137,     0.8731,     0.1004],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0259, 0.0151, 0.0392, 0.1316, 0.7882], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.9981,     0.0001,     0.0000,     0.0009,     0.0009],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9855,     0.0011,     0.0002,     0.0131],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0115,     0.9715,     0.0002,     0.0168],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0011,     0.9308,     0.0679],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0008, 0.0364, 0.0764, 0.1664, 0.7200], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.925]
 [1.176]
 [1.105]
 [1.105]
 [1.081]] [[41.186]
 [38.839]
 [36.689]
 [36.689]
 [38.15 ]] [[2.021]
 [2.177]
 [2.016]
 [2.016]
 [2.052]]
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.215]
 [1.215]
 [1.215]
 [1.215]] [[32.694]
 [31.224]
 [31.224]
 [31.224]
 [32.813]] [[2.873]
 [2.61 ]
 [2.61 ]
 [2.61 ]
 [2.745]]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9986,     0.0000,     0.0000,     0.0008,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9578,     0.0141,     0.0002,     0.0278],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0020,     0.0006,     0.9392,     0.0111,     0.0470],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0002,     0.0008,     0.9974,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0522, 0.0265, 0.0468, 0.0449, 0.8297], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.062]
 [1.053]
 [1.035]
 [1.055]
 [1.057]] [[34.301]
 [28.455]
 [34.608]
 [31.64 ]
 [30.674]] [[1.236]
 [1.174]
 [1.211]
 [1.204]
 [1.198]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92304856
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2673],
        [0.0000],
        [0.2391],
        [0.0000],
        [0.6371],
        [0.2491],
        [0.4872],
        [0.4416],
        [0.5094],
        [0.0000]], dtype=torch.float64)
0.0 0.2672511643295684
0.0 0.0
0.0 0.23909965395106314
0.96059601 0.96059601
0.0 0.6371408417666666
0.0 0.2490594480045961
0.0 0.4872274535444213
0.0 0.44155924751401154
0.0 0.5094390559493032
0.970299 0.970299
printing an ep nov before normalisation:  26.325191020778433
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  24.74921226477509
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  66.13047404067956
printing an ep nov before normalisation:  54.21583352112081
printing an ep nov before normalisation:  31.28359953680029
printing an ep nov before normalisation:  26.064611524183054
Printing some Q and Qe and total Qs values:  [[0.409]
 [0.459]
 [0.662]
 [0.396]
 [0.409]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.409]
 [0.459]
 [0.662]
 [0.396]
 [0.409]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  32.14705296484773
Printing some Q and Qe and total Qs values:  [[1.137]
 [1.199]
 [1.137]
 [1.137]
 [1.137]] [[29.624]
 [32.057]
 [29.624]
 [29.624]
 [29.624]] [[1.783]
 [1.947]
 [1.783]
 [1.783]
 [1.783]]
printing an ep nov before normalisation:  46.02955027887343
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.706]
 [0.706]
 [0.706]
 [0.706]] [[64.716]
 [63.056]
 [63.056]
 [63.056]
 [63.056]] [[2.039]
 [1.976]
 [1.976]
 [1.976]
 [1.976]]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9131233
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.625]
 [0.566]
 [0.54 ]
 [0.584]] [[27.334]
 [40.229]
 [27.334]
 [26.543]
 [32.785]] [[0.566]
 [0.625]
 [0.566]
 [0.54 ]
 [0.584]]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.174]
 [1.337]
 [1.174]
 [1.174]
 [1.174]] [[32.486]
 [39.491]
 [32.486]
 [32.486]
 [32.486]] [[2.308]
 [2.936]
 [2.308]
 [2.308]
 [2.308]]
printing an ep nov before normalisation:  47.35126120018358
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.28818548887806
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.921315
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9217606
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.052]
 [1.062]
 [1.062]
 [1.062]
 [1.062]] [[42.986]
 [28.638]
 [28.638]
 [28.638]
 [28.638]] [[1.528]
 [1.306]
 [1.306]
 [1.306]
 [1.306]]
printing an ep nov before normalisation:  50.390681229715675
printing an ep nov before normalisation:  41.457221055478136
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
printing an ep nov before normalisation:  41.744200847768724
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.98385674499205
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9979,     0.0004,     0.0005,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0021, 0.9179, 0.0093, 0.0011, 0.0696], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0001,     0.8913,     0.0395,     0.0687],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0002,     0.0161,     0.8661,     0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0010, 0.0336, 0.0783, 0.0561, 0.8310], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  71.08717754735369
printing an ep nov before normalisation:  28.94977331161499
Printing some Q and Qe and total Qs values:  [[1.102]
 [0.952]
 [0.951]
 [1.032]
 [0.951]] [[44.337]
 [29.62 ]
 [38.969]
 [45.723]
 [38.969]] [[1.756]
 [1.252]
 [1.476]
 [1.72 ]
 [1.476]]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  54.27977659048527
Printing some Q and Qe and total Qs values:  [[0.671]
 [0.697]
 [0.678]
 [0.724]
 [0.843]] [[36.694]
 [33.288]
 [36.589]
 [33.922]
 [31.961]] [[1.931]
 [1.716]
 [1.93 ]
 [1.788]
 [1.769]]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 42.51535654329634
printing an ep nov before normalisation:  43.50595522676319
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
actions average: 
K:  2  action  0 :  tensor([    0.9930,     0.0030,     0.0004,     0.0004,     0.0033],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9452,     0.0119,     0.0006,     0.0422],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0082,     0.9430,     0.0174,     0.0311],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0003,     0.0008,     0.9515,     0.0469],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0362, 0.0959, 0.0425, 0.1861, 0.6393], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.78006687331675
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9219643
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.321002847049073
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  70.2682858030317
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.95580291748047
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[38.153]
 [38.153]
 [38.153]
 [38.153]
 [38.153]] [[13.772]
 [13.772]
 [13.772]
 [13.772]
 [13.772]]
printing an ep nov before normalisation:  51.93648815155029
printing an ep nov before normalisation:  32.34272009934782
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actions average: 
K:  1  action  0 :  tensor([    0.9992,     0.0002,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9612,     0.0129,     0.0042,     0.0214],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9915,     0.0004,     0.0080],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0008,     0.9703,     0.0288],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0152, 0.0406, 0.0373, 0.1266, 0.7803], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.298]
 [1.363]
 [1.298]
 [1.298]
 [1.298]] [[34.831]
 [39.203]
 [34.831]
 [34.831]
 [34.831]] [[2.745]
 [3.144]
 [2.745]
 [2.745]
 [2.745]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  78.5442007876327
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.39647843036754
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.692354595070324
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 34.566190242767334
printing an ep nov before normalisation:  31.31622430268548
printing an ep nov before normalisation:  40.38987209219941
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.92110807
Printing some Q and Qe and total Qs values:  [[0.66 ]
 [0.809]
 [0.727]
 [0.556]
 [0.67 ]] [[46.28 ]
 [44.964]
 [39.467]
 [48.085]
 [44.787]] [[1.772]
 [1.862]
 [1.531]
 [1.748]
 [1.715]]
printing an ep nov before normalisation:  36.752361410889286
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.311773028319987
printing an ep nov before normalisation:  33.589349534650026
printing an ep nov before normalisation:  60.92066338769277
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
printing an ep nov before normalisation:  43.50145823934352
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.37189169893294
siam score:  -0.9215834
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.323]
 [0.241]
 [0.241]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.241]
 [0.241]
 [0.323]
 [0.241]
 [0.241]]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.56 ]
 [0.588]
 [0.592]
 [0.583]] [[39.909]
 [41.249]
 [34.939]
 [36.728]
 [39.226]] [[0.075]
 [0.56 ]
 [0.588]
 [0.592]
 [0.583]]
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.92512035051957
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.448]
 [0.438]
 [0.518]
 [0.448]
 [0.448]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.448]
 [0.438]
 [0.518]
 [0.448]
 [0.448]]
printing an ep nov before normalisation:  33.08542490005493
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.895]
 [0.895]
 [0.913]
 [0.879]] [[26.106]
 [26.106]
 [26.106]
 [39.374]
 [27.875]] [[1.152]
 [1.152]
 [1.152]
 [1.419]
 [1.17 ]]
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.302]
 [1.23 ]
 [1.263]
 [1.274]] [[35.741]
 [29.332]
 [28.345]
 [28.366]
 [33.622]] [[1.546]
 [1.583]
 [1.494]
 [1.528]
 [1.632]]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.65050530433655
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  29.76164665140148
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9905,     0.0007,     0.0000,     0.0010,     0.0077],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9961,     0.0005,     0.0003,     0.0029],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0025, 0.0114, 0.9263, 0.0116, 0.0482], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0003,     0.1019,     0.8711,     0.0261],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0094, 0.0020, 0.1112, 0.0957, 0.7817], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
printing an ep nov before normalisation:  46.75034656862949
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.731521677102478
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.18605327606201
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.72347816793852
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.38720167942984
maxi score, test score, baseline:  0.2501 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9534,     0.0065,     0.0005,     0.0393],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0075,     0.9212,     0.0128,     0.0578],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0014,     0.0001,     0.0033,     0.9143,     0.0810],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0112, 0.0460, 0.0295, 0.1197, 0.7936], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.61027486007726
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.44528983228698
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9982,     0.0001,     0.0000,     0.0004,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9591,     0.0004,     0.0001,     0.0401],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0010,     0.9663,     0.0027,     0.0299],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0002,     0.0177,     0.8854,     0.0961],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0026, 0.0922, 0.0485, 0.1548, 0.7020], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2501 1.0 1.0
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.105599572706666
maxi score, test score, baseline:  0.2501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.35105558105511
printing an ep nov before normalisation:  43.24942565410899
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.97822094876754
actor:  0 policy actor:  0  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.396]
 [0.396]
 [0.408]
 [0.396]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.396]
 [0.396]
 [0.396]
 [0.408]
 [0.396]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.05737839255286
maxi score, test score, baseline:  0.2541 1.0 1.0
printing an ep nov before normalisation:  31.377880954320073
Printing some Q and Qe and total Qs values:  [[1.181]
 [1.142]
 [1.181]
 [1.181]
 [1.181]] [[45.714]
 [45.07 ]
 [45.714]
 [45.714]
 [45.714]] [[2.748]
 [2.675]
 [2.748]
 [2.748]
 [2.748]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.374]
 [1.379]
 [1.364]
 [1.253]
 [1.303]] [[31.406]
 [35.907]
 [38.414]
 [40.768]
 [38.29 ]] [[2.355]
 [2.634]
 [2.771]
 [2.803]
 [2.702]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.91513276
line 256 mcts: sample exp_bonus 35.805773076246425
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.321109283070875
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  55.31719863732556
Printing some Q and Qe and total Qs values:  [[1.099]
 [1.012]
 [1.028]
 [1.07 ]
 [1.012]] [[39.798]
 [33.72 ]
 [33.439]
 [34.487]
 [33.72 ]] [[1.998]
 [1.69 ]
 [1.696]
 [1.776]
 [1.69 ]]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2541 1.0 1.0
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.746252989999896
printing an ep nov before normalisation:  30.447942076577938
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2541 1.0 1.0
siam score:  -0.909935
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
siam score:  -0.9165698
printing an ep nov before normalisation:  54.69072766357667
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.936]
 [0.936]
 [0.936]
 [1.04 ]
 [0.936]] [[41.154]
 [41.154]
 [41.154]
 [43.145]
 [41.154]] [[1.614]
 [1.614]
 [1.614]
 [1.778]
 [1.614]]
siam score:  -0.91913307
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2561 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
printing an ep nov before normalisation:  31.61809965182109
maxi score, test score, baseline:  0.2561 1.0 1.0
printing an ep nov before normalisation:  38.820238676674606
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.837306628684644
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
siam score:  -0.915638
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.547258329734735
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.605088678363213
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.93192888720802
printing an ep nov before normalisation:  40.65427157575299
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.91430116
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.3708854799251
siam score:  -0.91356474
maxi score, test score, baseline:  0.2561 1.0 1.0
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.444]
 [0.444]
 [0.444]
 [0.444]
 [0.444]] [[52.821]
 [52.821]
 [52.821]
 [52.821]
 [52.821]] [[1.742]
 [1.742]
 [1.742]
 [1.742]
 [1.742]]
siam score:  -0.9147924
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.2041601944382
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.63091920270633
printing an ep nov before normalisation:  28.50027533323235
maxi score, test score, baseline:  0.2561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2561 1.0 1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9155943
printing an ep nov before normalisation:  40.67658123189502
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
line 256 mcts: sample exp_bonus 29.174892957144618
maxi score, test score, baseline:  0.2601 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.84648110561039
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.829440474212745
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.42536449432373
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.098737091422485
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.12071223059072
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.612770630981586
printing an ep nov before normalisation:  58.14322611874068
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.28177307060742
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.49093598379788
printing an ep nov before normalisation:  38.749918937683105
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.45374352087678
printing an ep nov before normalisation:  28.8549650287381
actions average: 
K:  3  action  0 :  tensor([    0.9607,     0.0000,     0.0002,     0.0003,     0.0389],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9797,     0.0005,     0.0001,     0.0198],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0086,     0.8995,     0.0319,     0.0596],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0027,     0.9195,     0.0772],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0057, 0.0280, 0.0048, 0.0985, 0.8630], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.220557439499718
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.707870244529786
printing an ep nov before normalisation:  27.38709847132365
printing an ep nov before normalisation:  37.204880389336104
printing an ep nov before normalisation:  33.85557393052265
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.513517379760742
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.349]
 [1.438]
 [1.366]
 [1.366]
 [1.345]] [[37.203]
 [33.754]
 [32.492]
 [32.492]
 [34.361]] [[1.642]
 [1.685]
 [1.596]
 [1.596]
 [1.6  ]]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.90613234
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.02246822641694
printing an ep nov before normalisation:  45.96268978373749
Printing some Q and Qe and total Qs values:  [[0.46 ]
 [0.419]
 [0.408]
 [0.419]
 [0.419]] [[32.285]
 [34.499]
 [41.572]
 [34.499]
 [34.499]] [[0.46 ]
 [0.419]
 [0.408]
 [0.419]
 [0.419]]
printing an ep nov before normalisation:  33.38317772904897
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9038093
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.025]
 [1.243]
 [1.243]
 [1.243]] [[34.253]
 [36.248]
 [34.253]
 [34.253]
 [34.253]] [[1.676]
 [1.508]
 [1.676]
 [1.676]
 [1.676]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2601 1.0 1.0
maxi score, test score, baseline:  0.2601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.961]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.822]
 [0.822]
 [0.822]
 [0.822]
 [0.961]]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9945,     0.0006,     0.0024,     0.0004,     0.0021],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9115,     0.0154,     0.0002,     0.0727],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.9475,     0.0202,     0.0321],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0433,     0.8858,     0.0707],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0020, 0.1109, 0.0394, 0.0984, 0.7494], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
siam score:  -0.9058621
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2621 1.0 1.0
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.826]
 [0.834]
 [0.861]
 [0.934]
 [0.913]] [[55.639]
 [48.861]
 [45.308]
 [43.647]
 [49.992]] [[1.853]
 [1.67 ]
 [1.597]
 [1.623]
 [1.781]]
printing an ep nov before normalisation:  44.97651942545518
printing an ep nov before normalisation:  46.65289060262139
printing an ep nov before normalisation:  40.10338325125316
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.28670406341553
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.453923691381505
maxi score, test score, baseline:  0.2621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.36813976783916
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.40611438403073
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2641 1.0 1.0
line 256 mcts: sample exp_bonus 30.21296982231558
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.297]
 [0.297]
 [0.297]
 [0.297]
 [0.297]] [[60.344]
 [60.344]
 [60.344]
 [60.344]
 [60.344]] [[1.285]
 [1.285]
 [1.285]
 [1.285]
 [1.285]]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.90724146
printing an ep nov before normalisation:  50.20274036954045
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.0001147782285215726
Printing some Q and Qe and total Qs values:  [[1.176]
 [1.107]
 [1.061]
 [1.104]
 [1.119]] [[44.671]
 [47.216]
 [55.395]
 [55.461]
 [44.522]] [[1.356]
 [1.305]
 [1.315]
 [1.358]
 [1.298]]
printing an ep nov before normalisation:  22.50169277191162
printing an ep nov before normalisation:  34.50816657982159
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
printing an ep nov before normalisation:  24.64190006213235
printing an ep nov before normalisation:  25.861196517944336
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.746]
 [0.746]
 [0.746]
 [0.746]] [[36.911]
 [36.911]
 [36.911]
 [36.911]
 [36.911]] [[1.543]
 [1.543]
 [1.543]
 [1.543]
 [1.543]]
Printing some Q and Qe and total Qs values:  [[0.651]
 [0.651]
 [0.651]
 [0.651]
 [0.651]] [[74.694]
 [74.694]
 [74.694]
 [74.694]
 [74.694]] [[1.984]
 [1.984]
 [1.984]
 [1.984]
 [1.984]]
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0005,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9970,     0.0002,     0.0018,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9336,     0.0165,     0.0496],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0003,     0.0161,     0.8759,     0.1074],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0058, 0.0243, 0.0040, 0.2570, 0.7089], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.19613699043368
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9075229
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.173]
 [0.022]
 [1.005]
 [1.023]
 [1.067]] [[30.604]
 [27.069]
 [34.481]
 [38.849]
 [32.243]] [[1.592]
 [0.357]
 [1.518]
 [1.641]
 [1.526]]
siam score:  -0.9096677
printing an ep nov before normalisation:  45.28149432476189
Printing some Q and Qe and total Qs values:  [[1.012]
 [1.012]
 [1.012]
 [0.951]
 [0.957]] [[39.272]
 [39.272]
 [39.272]
 [40.605]
 [38.688]] [[2.022]
 [2.022]
 [2.022]
 [2.019]
 [1.94 ]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
printing an ep nov before normalisation:  37.64098482280381
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
printing an ep nov before normalisation:  54.61863054650409
using explorer policy with actor:  1
siam score:  -0.90953535
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3629],
        [0.2863],
        [0.5857],
        [0.9033],
        [0.5028],
        [0.4711],
        [0.6354],
        [0.0000],
        [0.3198],
        [0.4263]], dtype=torch.float64)
0.0 0.36291646158464724
0.0 0.28630952333671184
0.0 0.5857402098202865
0.0 0.9033273981161632
0.0 0.502799045667775
0.0 0.47105091834351465
0.0 0.6353707896819814
0.0 0.0
0.0 0.31982269849787454
0.0 0.4263327873281909
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.416053892439145
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.98804701172425
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.443]
 [0.479]
 [0.443]
 [0.443]
 [0.443]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.443]
 [0.479]
 [0.443]
 [0.443]
 [0.443]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.85483545831882
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.747]
 [0.747]
 [0.747]
 [0.747]
 [0.747]] [[40.302]
 [40.302]
 [40.302]
 [40.302]
 [40.302]] [[0.946]
 [0.946]
 [0.946]
 [0.946]
 [0.946]]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.605726718902588
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9072826
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.97862501528939
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.17008018493652
maxi score, test score, baseline:  0.2661 1.0 1.0
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.758]
 [0.618]
 [0.508]
 [0.618]] [[50.927]
 [41.459]
 [50.927]
 [51.881]
 [50.927]] [[2.018]
 [1.731]
 [2.018]
 [1.951]
 [2.018]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.2661 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.848]
 [0.145]
 [1.035]
 [0.998]] [[34.556]
 [29.995]
 [36.127]
 [42.562]
 [33.767]] [[0.645]
 [1.249]
 [0.755]
 [1.865]
 [1.528]]
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.171]
 [0.308]
 [0.192]
 [0.175]
 [0.255]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.171]
 [0.308]
 [0.192]
 [0.175]
 [0.255]]
printing an ep nov before normalisation:  41.03952816426113
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.048483576092575
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.168]
 [0.055]
 [0.346]
 [0.504]] [[42.226]
 [36.29 ]
 [35.334]
 [37.847]
 [34.366]] [[0.002]
 [0.168]
 [0.055]
 [0.346]
 [0.504]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  68.27382153988752
printing an ep nov before normalisation:  33.73248099747598
printing an ep nov before normalisation:  41.267538367120736
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  16.67882919047904
Printing some Q and Qe and total Qs values:  [[0.392]
 [0.136]
 [0.392]
 [0.45 ]
 [0.501]] [[19.436]
 [20.58 ]
 [19.436]
 [20.946]
 [21.995]] [[0.392]
 [0.136]
 [0.392]
 [0.45 ]
 [0.501]]
printing an ep nov before normalisation:  18.066413402557373
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.393697887778025
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.497]
 [0.573]
 [0.497]
 [0.497]] [[41.562]
 [41.562]
 [38.48 ]
 [41.562]
 [41.562]] [[0.497]
 [0.497]
 [0.573]
 [0.497]
 [0.497]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  26.15084171295166
line 256 mcts: sample exp_bonus 31.737829511269304
printing an ep nov before normalisation:  30.998500314516193
printing an ep nov before normalisation:  41.53862953186035
printing an ep nov before normalisation:  56.169447898864746
Printing some Q and Qe and total Qs values:  [[0.827]
 [0.907]
 [0.827]
 [0.827]
 [0.827]] [[32.463]
 [36.739]
 [32.463]
 [32.463]
 [32.463]] [[0.827]
 [0.907]
 [0.827]
 [0.827]
 [0.827]]
printing an ep nov before normalisation:  32.55317705155327
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.934]
 [0.902]
 [0.836]
 [0.932]] [[37.609]
 [31.424]
 [35.35 ]
 [31.349]
 [35.144]] [[0.942]
 [0.934]
 [0.902]
 [0.836]
 [0.932]]
printing an ep nov before normalisation:  35.06062060789812
maxi score, test score, baseline:  0.2681 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.954]
 [0.974]
 [0.954]
 [0.954]
 [0.954]] [[42.812]
 [36.583]
 [42.812]
 [42.812]
 [42.812]] [[0.954]
 [0.974]
 [0.954]
 [0.954]
 [0.954]]
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  0.058461834981926586
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.987950800208534
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.06552331681077
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.51454636265894
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0000],
        [0.2633],
        [0.2317],
        [0.3799],
        [0.3782],
        [0.0000],
        [0.4398],
        [0.2961],
        [0.5222],
        [0.4414]], dtype=torch.float64)
0.0 0.0
0.0 0.263348333177276
0.0 0.23169225847527516
0.0 0.37992482221791285
0.0 0.37823739348655155
0.0 0.0
0.0 0.43982553905468325
0.0 0.29607070861632895
0.0 0.5221572478649014
0.0 0.4413866618189705
printing an ep nov before normalisation:  60.04191624885097
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  42.84232407127379
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.03367759541174
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.39372965482283
printing an ep nov before normalisation:  51.192021376823575
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.695595247012765
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.57150040512992
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.89982665
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.451]
 [1.411]
 [1.411]
 [1.411]
 [1.411]] [[55.029]
 [48.003]
 [48.003]
 [48.003]
 [48.003]] [[2.451]
 [2.224]
 [2.224]
 [2.224]
 [2.224]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  5.710082719920138
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.324]
 [0.648]
 [0.553]
 [0.414]
 [0.483]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.324]
 [0.648]
 [0.553]
 [0.414]
 [0.483]]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.44704452728591
printing an ep nov before normalisation:  22.64000830707207
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
printing an ep nov before normalisation:  33.755245208740234
Printing some Q and Qe and total Qs values:  [[1.048]
 [0.969]
 [0.969]
 [1.068]
 [1.116]] [[49.65 ]
 [35.215]
 [35.215]
 [44.374]
 [40.189]] [[2.219]
 [1.584]
 [1.584]
 [2.036]
 [1.922]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.91772142697199
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.748]
 [0.749]
 [0.767]
 [0.748]
 [0.686]] [[47.111]
 [48.606]
 [48.596]
 [47.111]
 [52.757]] [[1.163]
 [1.195]
 [1.214]
 [1.163]
 [1.22 ]]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  47.12929450452636
printing an ep nov before normalisation:  20.45770138951181
siam score:  -0.90210325
Printing some Q and Qe and total Qs values:  [[0.467]
 [0.48 ]
 [0.507]
 [0.01 ]
 [0.448]] [[38.173]
 [42.899]
 [33.883]
 [31.925]
 [37.751]] [[0.467]
 [0.48 ]
 [0.507]
 [0.01 ]
 [0.448]]
actions average: 
K:  3  action  0 :  tensor([    0.9944,     0.0004,     0.0007,     0.0006,     0.0039],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9700,     0.0006,     0.0005,     0.0287],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0124,     0.9360,     0.0165,     0.0349],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0209,     0.8938,     0.0850],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0140, 0.0998, 0.0743, 0.0890, 0.7228], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.949504734878985
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.756]
 [0.716]
 [0.808]
 [0.756]
 [0.756]] [[56.011]
 [57.914]
 [50.122]
 [56.011]
 [56.011]] [[2.654]
 [2.716]
 [2.388]
 [2.654]
 [2.654]]
siam score:  -0.9050081
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.02 ]
 [0.948]
 [0.994]
 [0.982]] [[28.524]
 [28.856]
 [35.22 ]
 [35.761]
 [35.146]] [[1.803]
 [1.035]
 [2.418]
 [2.503]
 [2.447]]
printing an ep nov before normalisation:  60.0016549825115
printing an ep nov before normalisation:  42.0832049463908
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.74567987529713
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.9042223
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.725589091753196
siam score:  -0.8928847
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.89345175
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  64.2952939791451
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.157997104154056
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.21931495148668
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8939207
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.0861955870121
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.965088367462158
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.356]
 [1.306]
 [1.29 ]
 [0.314]
 [1.223]] [[31.339]
 [33.735]
 [32.323]
 [29.655]
 [29.069]] [[1.658]
 [2.803]
 [2.672]
 [1.479]
 [2.341]]
maxi score, test score, baseline:  0.3161 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3161 1.0 1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
printing an ep nov before normalisation:  49.97020452765018
printing an ep nov before normalisation:  48.000240389712694
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3161 1.0 1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.165742788671196
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  60.041625048499036
printing an ep nov before normalisation:  42.724717697373826
printing an ep nov before normalisation:  30.527400970458984
UNIT TEST: sample policy line 217 mcts : [0.026 0.308 0.564 0.077 0.026]
printing an ep nov before normalisation:  36.28660804673042
maxi score, test score, baseline:  0.3181 1.0 1.0
printing an ep nov before normalisation:  51.638977022585124
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.153]
 [1.266]
 [1.192]
 [1.192]
 [1.192]] [[53.204]
 [43.955]
 [50.264]
 [50.264]
 [50.264]] [[1.918]
 [1.823]
 [1.89 ]
 [1.89 ]
 [1.89 ]]
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.376399517059326
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.6091632093848
siam score:  -0.8997891
printing an ep nov before normalisation:  39.93880544240168
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.624880807331486
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.276]
 [1.32 ]
 [1.276]
 [1.276]
 [1.238]] [[33.848]
 [34.375]
 [33.848]
 [33.848]
 [32.292]] [[2.294]
 [2.365]
 [2.294]
 [2.294]
 [2.175]]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.36800160985342
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.18249442904008
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.65532823192077
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.12602377318707
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
UNIT TEST: sample policy line 217 mcts : [0.128 0.179 0.179 0.179 0.333]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.436]
 [1.436]
 [1.477]
 [1.436]
 [1.436]] [[30.442]
 [30.442]
 [25.794]
 [30.442]
 [30.442]] [[3.152]
 [3.152]
 [2.733]
 [3.152]
 [3.152]]
printing an ep nov before normalisation:  0.42025217934266834
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.97790850987646
maxi score, test score, baseline:  0.3181 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([    0.9635,     0.0001,     0.0000,     0.0003,     0.0361],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9495,     0.0119,     0.0001,     0.0382],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0000,     0.9994,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0002,     0.0229,     0.8943,     0.0823],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0102, 0.0592, 0.0291, 0.0866, 0.8148], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.078]
 [1.141]
 [1.129]
 [1.098]
 [1.087]] [[39.311]
 [32.465]
 [33.408]
 [33.035]
 [32.745]] [[1.885]
 [1.713]
 [1.733]
 [1.688]
 [1.668]]
maxi score, test score, baseline:  0.3181 1.0 1.0
printing an ep nov before normalisation:  49.07803327806168
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.681]
 [0.566]
 [0.566]
 [0.566]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.566]
 [0.681]
 [0.566]
 [0.566]
 [0.566]]
using explorer policy with actor:  1
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9819,     0.0045,     0.0007,     0.0002,     0.0127],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9843,     0.0004,     0.0018,     0.0131],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.8917,     0.0317,     0.0763],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0013,     0.0002,     0.0017,     0.8243,     0.1724],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0046,     0.0170,     0.0005,     0.0964,     0.8815],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3181 1.0 1.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
actions average: 
K:  4  action  0 :  tensor([    0.9766,     0.0026,     0.0000,     0.0104,     0.0104],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9829,     0.0001,     0.0000,     0.0168],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.9973,     0.0002,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0007,     0.0134,     0.9055,     0.0801],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0374, 0.0381, 0.0336, 0.1216, 0.7693], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.83 ]
 [0.736]
 [0.737]
 [0.647]
 [0.727]] [[52.149]
 [55.471]
 [56.823]
 [54.817]
 [56.728]] [[2.215]
 [2.265]
 [2.324]
 [2.147]
 [2.31 ]]
printing an ep nov before normalisation:  54.59871706127614
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.01979737645682
main train batch thing paused
add a thread
Adding thread: now have 2 threads
Printing some Q and Qe and total Qs values:  [[0.447]
 [0.475]
 [0.486]
 [0.479]
 [0.475]] [[49.603]
 [51.69 ]
 [51.626]
 [54.726]
 [51.69 ]] [[2.015]
 [2.149]
 [2.156]
 [2.307]
 [2.149]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.692]
 [0.601]
 [0.601]
 [0.601]] [[26.655]
 [36.1  ]
 [26.655]
 [26.655]
 [26.655]] [[1.557]
 [2.292]
 [1.557]
 [1.557]
 [1.557]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.52573965102415
printing an ep nov before normalisation:  32.08255140048819
printing an ep nov before normalisation:  46.073904037475586
maxi score, test score, baseline:  0.3181 1.0 1.0
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.188]
 [1.132]
 [1.132]
 [1.132]
 [1.132]] [[75.311]
 [72.686]
 [72.686]
 [72.686]
 [72.686]] [[1.819]
 [1.735]
 [1.735]
 [1.735]
 [1.735]]
printing an ep nov before normalisation:  50.528093595798204
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  0.0001253512081689223
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.53482392093135
printing an ep nov before normalisation:  48.1709623336792
printing an ep nov before normalisation:  22.792129516601562
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.292]
 [0.333]
 [0.312]
 [0.319]] [[25.911]
 [35.739]
 [44.923]
 [33.675]
 [34.677]] [[0.186]
 [0.292]
 [0.333]
 [0.312]
 [0.319]]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.47842788696289
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
Printing some Q and Qe and total Qs values:  [[0.433]
 [0.515]
 [0.457]
 [0.486]
 [0.476]] [[31.916]
 [35.314]
 [31.487]
 [30.788]
 [32.47 ]] [[0.433]
 [0.515]
 [0.457]
 [0.486]
 [0.476]]
maxi score, test score, baseline:  0.3181 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.23 ]
 [1.337]
 [0.633]
 [1.205]
 [1.335]] [[25.103]
 [23.918]
 [21.175]
 [21.318]
 [21.72 ]] [[2.415]
 [2.423]
 [1.492]
 [2.075]
 [2.239]]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  46.18685689556762
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.7612226282652
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Sims:  40 1 epoch:  163546 pick best:  False frame count:  163546
printing an ep nov before normalisation:  39.63815388127526
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.97044028668899
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.42391439409247
printing an ep nov before normalisation:  33.38190797349081
printing an ep nov before normalisation:  49.23613959254814
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.081]
 [1.081]
 [1.081]
 [1.081]] [[39.17]
 [39.17]
 [39.17]
 [39.17]
 [39.17]] [[1.698]
 [1.698]
 [1.698]
 [1.698]
 [1.698]]
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  25.450416312516115
printing an ep nov before normalisation:  33.126023514930075
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.495]
 [0.596]
 [0.596]
 [0.498]
 [0.624]] [[64.65 ]
 [55.097]
 [55.097]
 [45.789]
 [48.966]] [[0.495]
 [0.596]
 [0.596]
 [0.498]
 [0.624]]
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.29896560669066
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  60.391812004352985
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.    0.59  0.    0.256 0.154]
printing an ep nov before normalisation:  38.85980295668118
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.522912897359824
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.721]
 [0.45 ]
 [0.627]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.572]
 [0.572]
 [0.721]
 [0.45 ]
 [0.627]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.258]
 [1.352]
 [1.358]
 [1.377]
 [1.366]] [[39.124]
 [32.395]
 [32.945]
 [32.367]
 [36.498]] [[2.166]
 [2.001]
 [2.028]
 [2.025]
 [2.173]]
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.146360874176025
Printing some Q and Qe and total Qs values:  [[1.375]
 [1.275]
 [1.375]
 [1.375]
 [1.375]] [[18.571]
 [27.6  ]
 [18.571]
 [18.571]
 [18.571]] [[2.135]
 [2.405]
 [2.135]
 [2.135]
 [2.135]]
maxi score, test score, baseline:  0.3221 1.0 1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.89365417
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.57668738348148
printing an ep nov before normalisation:  36.03689614784653
printing an ep nov before normalisation:  30.77840238029011
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 42.37934482789761
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.181765837147935
printing an ep nov before normalisation:  34.12510912656423
maxi score, test score, baseline:  0.3221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.323]
 [1.326]
 [1.323]
 [1.323]
 [1.34 ]] [[33.702]
 [33.278]
 [33.702]
 [33.702]
 [33.648]] [[2.688]
 [2.659]
 [2.688]
 [2.688]
 [2.701]]
printing an ep nov before normalisation:  31.566319419243296
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.124]
 [0.925]
 [0.971]
 [1.033]
 [1.07 ]] [[26.868]
 [26.13 ]
 [35.996]
 [31.515]
 [27.699]] [[1.782]
 [1.549]
 [2.045]
 [1.902]
 [1.765]]
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.172]
 [1.125]
 [1.125]
 [1.143]] [[41.511]
 [37.242]
 [34.563]
 [34.563]
 [36.057]] [[2.241]
 [2.115]
 [1.959]
 [1.959]
 [2.038]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.773785158783404
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.88159644509726
printing an ep nov before normalisation:  39.6474631962786
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.51680269410739
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.476172924041748
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.315537600093826
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.3916974067688
printing an ep nov before normalisation:  42.745788697447196
siam score:  -0.8901159
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
printing an ep nov before normalisation:  37.136347024295404
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.94839493515778
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  70.34773931932821
printing an ep nov before normalisation:  53.12357110268176
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9425,     0.0005,     0.0001,     0.0569],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0007,     0.9321,     0.0216,     0.0454],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0076, 0.0020, 0.0110, 0.8069, 0.1725], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0142, 0.0410, 0.0630, 0.1112, 0.7707], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  24.560378240781617
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3261 1.0 1.0
printing an ep nov before normalisation:  35.616580505166255
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.21100450226886
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  62.91709949875564
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.415]
 [1.415]
 [1.415]
 [1.338]
 [1.415]] [[29.312]
 [29.312]
 [29.312]
 [44.247]
 [29.312]] [[2.485]
 [2.485]
 [2.485]
 [2.955]
 [2.485]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  32.64477357115379
printing an ep nov before normalisation:  33.58336632734831
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.19694709777832
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  59.222599354467846
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.89018005
line 256 mcts: sample exp_bonus 42.62098731511102
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.378]
 [1.378]
 [1.378]
 [1.378]
 [1.378]] [[38.837]
 [38.837]
 [38.837]
 [38.837]
 [38.837]] [[3.375]
 [3.375]
 [3.375]
 [3.375]
 [3.375]]
actor:  1 policy actor:  1  step number:  102 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.292165984860326
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.161897060322346
printing an ep nov before normalisation:  38.10833158460974
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.38539056986246
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.42038764707725
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.2471204876595
printing an ep nov before normalisation:  41.10040768905883
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
UNIT TEST: sample policy line 217 mcts : [0.026 0.846 0.026 0.026 0.077]
Printing some Q and Qe and total Qs values:  [[0.945]
 [0.945]
 [0.945]
 [1.024]
 [0.973]] [[38.949]
 [38.949]
 [38.949]
 [38.916]
 [38.277]] [[1.422]
 [1.422]
 [1.422]
 [1.5  ]
 [1.432]]
actions average: 
K:  2  action  0 :  tensor([    0.9919,     0.0001,     0.0000,     0.0032,     0.0048],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9667,     0.0004,     0.0001,     0.0328],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9621,     0.0169,     0.0206],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0002,     0.0447,     0.8387,     0.1160],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0156, 0.0796, 0.0923, 0.8111], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  23.078811478209854
printing an ep nov before normalisation:  25.27544984163779
printing an ep nov before normalisation:  33.47623715513652
printing an ep nov before normalisation:  74.32179805563958
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.984]
 [0.984]
 [0.984]
 [1.033]
 [0.984]] [[30.191]
 [30.191]
 [30.191]
 [38.932]
 [30.191]] [[1.78 ]
 [1.78 ]
 [1.78 ]
 [2.238]
 [1.78 ]]
printing an ep nov before normalisation:  46.12154483795166
printing an ep nov before normalisation:  25.37887709382526
Printing some Q and Qe and total Qs values:  [[0.722]
 [0.577]
 [0.577]
 [0.54 ]
 [0.577]] [[48.629]
 [39.059]
 [39.059]
 [39.344]
 [39.059]] [[2.008]
 [1.428]
 [1.428]
 [1.404]
 [1.428]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.8765476998087
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.817]
 [0.683]
 [0.683]
 [0.683]] [[33.191]
 [44.963]
 [33.191]
 [33.191]
 [33.191]] [[1.439]
 [2.262]
 [1.439]
 [1.439]
 [1.439]]
siam score:  -0.8928783
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.58618661781349
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9935,     0.0012,     0.0025,     0.0002,     0.0026],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9383,     0.0004,     0.0009,     0.0602],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0003,     0.0012,     0.8650,     0.0545,     0.0790],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0007,     0.0019,     0.0001,     0.9174,     0.0800],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0334, 0.0291, 0.0443, 0.0716, 0.8216], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.814505577087402
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.879]
 [0.879]
 [0.865]
 [0.877]] [[40.205]
 [39.966]
 [39.966]
 [35.49 ]
 [40.668]] [[1.954]
 [1.961]
 [1.961]
 [1.717]
 [1.995]]
printing an ep nov before normalisation:  25.796101996320026
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  76.38032762186049
printing an ep nov before normalisation:  39.20889390512914
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88688505
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.56547735535447
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  3  action  0 :  tensor([    0.9692,     0.0002,     0.0000,     0.0008,     0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9786,     0.0000,     0.0005,     0.0205],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0010,     0.0002,     0.9116,     0.0389,     0.0483],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0001,     0.0000,     0.9388,     0.0605],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0092, 0.0463, 0.0557, 0.1048, 0.7841], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.781]
 [0.781]
 [0.781]
 [0.781]
 [0.781]] [[32.856]
 [32.856]
 [32.856]
 [32.856]
 [32.856]] [[1.927]
 [1.927]
 [1.927]
 [1.927]
 [1.927]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.62296585906647
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.88725764
maxi score, test score, baseline:  0.3281 1.0 1.0
printing an ep nov before normalisation:  44.38113178601793
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.02734510627829
printing an ep nov before normalisation:  40.325014401348156
printing an ep nov before normalisation:  25.19547700881958
Printing some Q and Qe and total Qs values:  [[1.212]
 [1.318]
 [0.645]
 [1.121]
 [1.203]] [[32.458]
 [24.97 ]
 [27.54 ]
 [23.124]
 [25.195]] [[1.794]
 [1.694]
 [1.092]
 [1.446]
 [1.585]]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  30.544021249481386
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.809073329449845
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.445236400434435
printing an ep nov before normalisation:  25.80502435808807
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.649125551841585
printing an ep nov before normalisation:  41.117786270995396
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.80815601348877
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.643548238487256
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  68.1680163436562
printing an ep nov before normalisation:  37.31389225801868
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 40.759332694510675
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.19 ]
 [1.028]
 [1.027]
 [0.878]] [[36.219]
 [32.17 ]
 [33.512]
 [34.686]
 [33.062]] [[1.981]
 [1.926]
 [1.825]
 [1.878]
 [1.655]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.79 ]
 [0.799]
 [0.872]
 [0.893]] [[45.528]
 [38.571]
 [41.943]
 [41.337]
 [41.34 ]] [[1.826]
 [1.591]
 [1.72 ]
 [1.771]
 [1.792]]
actions average: 
K:  1  action  0 :  tensor([    0.9488,     0.0013,     0.0001,     0.0018,     0.0480],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9843,     0.0003,     0.0007,     0.0146],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0043,     0.0002,     0.8777,     0.0221,     0.0957],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0009,     0.0003,     0.0011,     0.8963,     0.1015],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0139, 0.0665, 0.0959, 0.0444, 0.7792], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9871,     0.0022,     0.0000,     0.0044,     0.0063],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9919,     0.0000,     0.0004,     0.0076],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0009, 0.0019, 0.9055, 0.0233, 0.0684], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0004,     0.0404,     0.8427,     0.1161],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0252, 0.0057, 0.0977, 0.1555, 0.7159], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  31.233317887808887
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.471910336267776
Printing some Q and Qe and total Qs values:  [[0.446]
 [0.032]
 [0.377]
 [0.352]
 [0.379]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.446]
 [0.032]
 [0.377]
 [0.352]
 [0.379]]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3301 1.0 1.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.538]
 [0.575]
 [0.594]
 [0.601]
 [0.582]] [[34.848]
 [22.505]
 [20.313]
 [31.953]
 [23.858]] [[0.538]
 [0.575]
 [0.594]
 [0.601]
 [0.582]]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.79168905312843
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.71362921352003
printing an ep nov before normalisation:  32.886600494384766
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  4  action  0 :  tensor([    0.9959,     0.0002,     0.0000,     0.0001,     0.0038],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9762,     0.0002,     0.0010,     0.0225],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0004,     0.9839,     0.0145,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0007,     0.0173,     0.9591,     0.0227],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0020, 0.0074, 0.0022, 0.1004, 0.8880], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.082]
 [1.082]
 [1.082]
 [1.082]
 [1.082]] [[45.666]
 [45.666]
 [45.666]
 [45.666]
 [45.666]] [[1.965]
 [1.965]
 [1.965]
 [1.965]
 [1.965]]
printing an ep nov before normalisation:  28.61537093946064
printing an ep nov before normalisation:  54.77924172974127
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.101089267222726
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.032853723856313
printing an ep nov before normalisation:  17.5492262840271
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  26.49520763983325
actor:  1 policy actor:  1  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.49495179726047
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.962258936390654
Printing some Q and Qe and total Qs values:  [[1.13 ]
 [1.011]
 [1.011]
 [1.011]
 [1.011]] [[32.456]
 [25.261]
 [25.261]
 [25.261]
 [25.261]] [[1.46 ]
 [1.182]
 [1.182]
 [1.182]
 [1.182]]
printing an ep nov before normalisation:  33.558255997588454
printing an ep nov before normalisation:  30.0246757411756
printing an ep nov before normalisation:  45.28265598171548
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.738586089983215
maxi score, test score, baseline:  0.3341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.919]
 [0.903]
 [0.919]
 [1.024]
 [0.99 ]] [[31.851]
 [27.909]
 [31.851]
 [32.352]
 [33.079]] [[1.519]
 [1.361]
 [1.519]
 [1.642]
 [1.634]]
siam score:  -0.8802207
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.22639657487444
Printing some Q and Qe and total Qs values:  [[0.384]
 [0.37 ]
 [0.389]
 [0.384]
 [0.384]] [[34.301]
 [32.209]
 [34.016]
 [34.301]
 [34.301]] [[0.384]
 [0.37 ]
 [0.389]
 [0.384]
 [0.384]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.23305437539389
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.055]
 [0.996]
 [0.984]
 [0.996]
 [0.996]] [[39.66 ]
 [41.13 ]
 [41.854]
 [41.13 ]
 [41.13 ]] [[1.799]
 [1.794]
 [1.809]
 [1.794]
 [1.794]]
UNIT TEST: sample policy line 217 mcts : [0.231 0.385 0.205 0.128 0.051]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.92274267168017
actor:  1 policy actor:  1  step number:  97 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.06 ]
 [1.109]
 [1.109]
 [1.087]
 [1.095]] [[38.529]
 [38.508]
 [34.758]
 [35.587]
 [35.618]] [[2.418]
 [2.467]
 [2.245]
 [2.272]
 [2.282]]
printing an ep nov before normalisation:  43.64436663980395
printing an ep nov before normalisation:  23.506946170680354
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3361 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.732]
 [0.72 ]
 [0.656]
 [0.7  ]] [[25.372]
 [29.01 ]
 [27.614]
 [24.969]
 [28.371]] [[1.481]
 [1.687]
 [1.586]
 [1.353]
 [1.615]]
printing an ep nov before normalisation:  33.83331720079764
siam score:  -0.8805068
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  76.33252143859863
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
printing an ep nov before normalisation:  42.32651959307962
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.88941000943252
printing an ep nov before normalisation:  38.48049551396576
actor:  0 policy actor:  0  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3401 1.0 1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.94838111505595
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.28518486022949
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.539]
 [0.713]
 [0.53 ]
 [0.539]
 [0.484]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.539]
 [0.713]
 [0.53 ]
 [0.539]
 [0.484]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.513]
 [0.79 ]
 [0.549]
 [0.528]
 [0.553]] [[20.303]
 [29.862]
 [27.455]
 [22.683]
 [19.713]] [[0.765]
 [1.261]
 [0.965]
 [0.834]
 [0.792]]
printing an ep nov before normalisation:  22.807846069335938
Printing some Q and Qe and total Qs values:  [[0.876]
 [0.921]
 [0.924]
 [0.997]
 [0.924]] [[42.876]
 [38.381]
 [28.922]
 [41.36 ]
 [28.922]] [[1.385]
 [1.325]
 [1.107]
 [1.47 ]
 [1.107]]
printing an ep nov before normalisation:  36.19177982329368
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  37.78466619060627
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.501]
 [0.412]
 [0.494]
 [0.431]
 [0.501]] [[36.374]
 [24.761]
 [26.275]
 [26.495]
 [36.374]] [[1.638]
 [0.997]
 [1.151]
 [1.098]
 [1.638]]
maxi score, test score, baseline:  0.3421 1.0 1.0
siam score:  -0.87708646
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.083]
 [1.231]
 [1.168]
 [1.168]
 [1.101]] [[47.356]
 [36.59 ]
 [39.183]
 [39.183]
 [41.228]] [[2.415]
 [2.099]
 [2.148]
 [2.148]
 [2.169]]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.145227909088135
printing an ep nov before normalisation:  28.544788052445945
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  45.11274357475983
printing an ep nov before normalisation:  43.28332089994367
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
maxi score, test score, baseline:  0.3421 1.0 1.0
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.717812806832015
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.83607799658464
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3421 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.313114764059506
maxi score, test score, baseline:  0.3421 1.0 1.0
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.78697850235837
printing an ep nov before normalisation:  51.12312359871605
Printing some Q and Qe and total Qs values:  [[0.477]
 [0.477]
 [0.476]
 [0.476]
 [0.474]] [[25.26 ]
 [20.528]
 [20.304]
 [23.461]
 [20.717]] [[0.477]
 [0.477]
 [0.476]
 [0.476]
 [0.474]]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  21.120697836709653
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.3389340662194
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.6  ]
 [0.683]
 [0.681]
 [0.6  ]
 [0.64 ]] [[38.583]
 [41.519]
 [33.614]
 [38.583]
 [37.571]] [[2.108]
 [2.396]
 [1.841]
 [2.108]
 [2.077]]
printing an ep nov before normalisation:  31.634110687186027
actions average: 
K:  1  action  0 :  tensor([    0.9734,     0.0002,     0.0001,     0.0002,     0.0261],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9980,     0.0003,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0001,     0.9996,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0011,     0.0003,     0.0004,     0.9042,     0.0939],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0031, 0.0247, 0.0221, 0.1526, 0.7974], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.648040844704305
printing an ep nov before normalisation:  26.741580963134766
printing an ep nov before normalisation:  24.844199233602655
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.87062323
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.791299469989454
Starting evaluation
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.915771937377286
printing an ep nov before normalisation:  56.355941022664894
printing an ep nov before normalisation:  36.87552703208838
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.214148998260498
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.835]
 [0.765]
 [0.765]
 [0.712]] [[29.838]
 [35.073]
 [29.838]
 [29.838]
 [53.748]] [[0.765]
 [0.835]
 [0.765]
 [0.765]
 [0.712]]
maxi score, test score, baseline:  0.34409999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.544811030410425
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  32.314443588256836
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.233]
 [0.68 ]
 [0.018]
 [0.686]
 [0.54 ]] [[28.029]
 [36.774]
 [28.051]
 [30.022]
 [26.767]] [[0.34 ]
 [0.86 ]
 [0.125]
 [0.81 ]
 [0.636]]
printing an ep nov before normalisation:  31.043338179599044
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.816]
 [0.833]
 [0.997]
 [0.831]] [[30.539]
 [30.175]
 [29.874]
 [32.497]
 [28.635]] [[1.05 ]
 [0.995]
 [1.009]
 [1.205]
 [0.992]]
printing an ep nov before normalisation:  37.50704908851744
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.23303666112529
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  43.92974540728052
printing an ep nov before normalisation:  44.00645780155965
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.948]
 [0.948]
 [0.948]
 [1.041]
 [0.948]] [[33.021]
 [33.021]
 [33.021]
 [44.08 ]
 [33.021]] [[1.735]
 [1.735]
 [1.735]
 [2.238]
 [1.735]]
Printing some Q and Qe and total Qs values:  [[1.061]
 [1.061]
 [1.061]
 [1.115]
 [0.99 ]] [[36.829]
 [36.829]
 [36.829]
 [35.596]
 [37.553]] [[1.932]
 [1.932]
 [1.932]
 [1.945]
 [1.885]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.84515596020942
printing an ep nov before normalisation:  35.12214682063804
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.713081062509744
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [1.03 ]
 [1.008]
 [1.03 ]
 [1.03 ]] [[32.73 ]
 [32.73 ]
 [38.016]
 [32.73 ]
 [32.73 ]] [[1.74 ]
 [1.74 ]
 [1.914]
 [1.74 ]
 [1.74 ]]
printing an ep nov before normalisation:  33.078030838055135
actor:  1 policy actor:  1  step number:  102 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.31844336850001
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.56643867492676
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.35 ]
 [1.414]
 [1.333]
 [1.328]
 [1.341]] [[30.429]
 [26.233]
 [31.076]
 [26.641]
 [29.542]] [[1.573]
 [1.584]
 [1.563]
 [1.503]
 [1.552]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.326]
 [1.254]
 [1.214]
 [1.255]
 [1.309]] [[22.229]
 [15.842]
 [21.916]
 [17.051]
 [20.813]] [[1.544]
 [1.369]
 [1.427]
 [1.39 ]
 [1.504]]
printing an ep nov before normalisation:  23.058369190467644
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.05606740422174
printing an ep nov before normalisation:  26.67094560062039
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.132619183841484
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.40434883849358
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.073]
 [1.073]
 [1.073]
 [1.039]
 [1.073]] [[55.476]
 [55.476]
 [55.476]
 [57.709]
 [55.476]] [[2.19]
 [2.19]
 [2.19]
 [2.22]
 [2.19]]
printing an ep nov before normalisation:  30.04777026375034
printing an ep nov before normalisation:  45.55706060483074
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.414]
 [1.422]
 [1.382]
 [1.382]
 [1.382]] [[40.45 ]
 [38.827]
 [45.746]
 [45.746]
 [45.746]] [[2.225]
 [2.166]
 [2.415]
 [2.415]
 [2.415]]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.361027591770217
maxi score, test score, baseline:  0.3921 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
siam score:  -0.8618137
printing an ep nov before normalisation:  47.66222508824476
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.047392950960415
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.107]
 [0.918]
 [0.914]
 [1.016]
 [0.918]] [[38.912]
 [37.33 ]
 [45.154]
 [40.607]
 [37.33 ]] [[2.085]
 [1.81 ]
 [2.236]
 [2.088]
 [1.81 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.83205821815264
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.207]
 [1.119]
 [1.109]
 [1.173]] [[30.865]
 [32.084]
 [31.321]
 [30.267]
 [30.613]] [[2.131]
 [2.298]
 [2.163]
 [2.09 ]
 [2.175]]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.53400224890118
printing an ep nov before normalisation:  32.053291374128264
printing an ep nov before normalisation:  48.40008889558044
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.926592019549126
printing an ep nov before normalisation:  51.382986436919815
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.53955590355903
maxi score, test score, baseline:  0.3921 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.133]
 [1.065]
 [1.065]
 [1.077]
 [1.127]] [[40.313]
 [32.355]
 [32.355]
 [31.989]
 [38.465]] [[1.798]
 [1.47 ]
 [1.47 ]
 [1.47 ]
 [1.731]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  3.3901330230219173
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  55.35741720265246
printing an ep nov before normalisation:  73.20078269347312
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.65617827324339
printing an ep nov before normalisation:  35.515630139512524
printing an ep nov before normalisation:  37.444192915268516
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.35509170699348
actor:  1 policy actor:  1  step number:  101 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.374]
 [1.374]
 [1.374]
 [1.374]] [[35.503]
 [25.729]
 [25.729]
 [25.729]
 [25.729]] [[2.323]
 [2.266]
 [2.266]
 [2.266]
 [2.266]]
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
printing an ep nov before normalisation:  31.42396278365601
maxi score, test score, baseline:  0.3921 1.0 1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.86959547
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.49836515224601
printing an ep nov before normalisation:  54.74554657253046
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.02833283653354
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.9978,     0.0004,     0.0001,     0.0004,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9228,     0.0002,     0.0000,     0.0766],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0013,     0.0167,     0.9631,     0.0000,     0.0189],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0093, 0.0032, 0.0145, 0.8406, 0.1324], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0381,     0.0266,     0.0005,     0.0614,     0.8734],
       grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  22.901571249704897
printing an ep nov before normalisation:  44.067068099975586
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.054]
 [1.22 ]
 [1.054]
 [1.079]
 [1.054]] [[35.024]
 [43.541]
 [35.024]
 [22.356]
 [35.024]] [[1.998]
 [2.493]
 [1.998]
 [1.533]
 [1.998]]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.027]
 [1.027]
 [1.027]
 [1.1  ]
 [1.004]] [[42.403]
 [42.403]
 [42.403]
 [49.343]
 [46.183]] [[1.704]
 [1.704]
 [1.704]
 [1.987]
 [1.795]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.70750148908083
printing an ep nov before normalisation:  42.324110806628745
Printing some Q and Qe and total Qs values:  [[0.723]
 [0.769]
 [0.723]
 [0.623]
 [0.782]] [[38.538]
 [50.986]
 [38.538]
 [32.721]
 [50.684]] [[1.169]
 [1.573]
 [1.169]
 [0.901]
 [1.577]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.546919566685915
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.39418436720947
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.9745979309082
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.838805640635016
Printing some Q and Qe and total Qs values:  [[1.229]
 [1.107]
 [1.107]
 [1.09 ]
 [1.107]] [[29.921]
 [28.413]
 [28.413]
 [29.275]
 [28.413]] [[1.37 ]
 [1.234]
 [1.234]
 [1.225]
 [1.234]]
printing an ep nov before normalisation:  33.345005782695
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  53.10279459958373
siam score:  -0.8712352
printing an ep nov before normalisation:  41.19594670057768
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  30.92444459081131
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.343338408662454
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.777]
 [0.757]
 [0.757]
 [0.757]
 [0.757]] [[68.859]
 [51.475]
 [51.475]
 [51.475]
 [51.475]] [[2.021]
 [1.523]
 [1.523]
 [1.523]
 [1.523]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.54598808288574
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.52683688239956
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.15463230050873
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4001 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.942]
 [0.942]
 [0.942]
 [0.972]
 [0.942]] [[40.279]
 [40.279]
 [40.279]
 [38.51 ]
 [40.279]] [[2.925]
 [2.925]
 [2.925]
 [2.796]
 [2.925]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.95786746932154
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.82 ]
 [0.846]
 [0.841]
 [0.84 ]
 [0.831]] [[41.105]
 [40.701]
 [39.726]
 [37.62 ]
 [38.315]] [[1.51 ]
 [1.525]
 [1.491]
 [1.429]
 [1.441]]
line 256 mcts: sample exp_bonus 44.52191944044377
printing an ep nov before normalisation:  47.95648158201997
printing an ep nov before normalisation:  51.40794664939833
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  44.48030981837013
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.49382084884058
printing an ep nov before normalisation:  38.06859525031189
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]] [[41.956]
 [41.956]
 [41.956]
 [41.956]
 [41.956]] [[0.478]
 [0.478]
 [0.478]
 [0.478]
 [0.478]]
maxi score, test score, baseline:  0.4001 1.0 1.0
printing an ep nov before normalisation:  49.777037037867686
printing an ep nov before normalisation:  37.28639496297465
printing an ep nov before normalisation:  55.52293407768014
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.33772473418269
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.86140900230955
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.3173065502408
printing an ep nov before normalisation:  55.93095209873072
using explorer policy with actor:  1
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  1.5731809608558933
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4021 1.0 1.0
printing an ep nov before normalisation:  28.595068745365197
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  15.761546880076217
printing an ep nov before normalisation:  43.8203670597198
printing an ep nov before normalisation:  43.03030353638048
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.024]
 [1.011]
 [0.996]
 [1.016]] [[42.505]
 [47.984]
 [49.621]
 [38.865]
 [51.421]] [[1.578]
 [1.617]
 [1.635]
 [1.419]
 [1.674]]
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.759]
 [0.026]
 [0.637]
 [0.639]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.046]
 [0.759]
 [0.026]
 [0.637]
 [0.639]]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  41.20381126580506
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.17537788836378
siam score:  -0.854655
printing an ep nov before normalisation:  37.098972244488586
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.32303727515496
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.496]
 [0.554]
 [0.495]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.554]
 [0.554]
 [0.496]
 [0.554]
 [0.495]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.81 ]
 [0.881]
 [0.745]
 [0.81 ]
 [0.81 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.81 ]
 [0.881]
 [0.745]
 [0.81 ]
 [0.81 ]]
actions average: 
K:  2  action  0 :  tensor([    0.9594,     0.0000,     0.0000,     0.0005,     0.0400],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0024,     0.9568,     0.0116,     0.0008,     0.0284],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9448,     0.0267,     0.0283],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0002,     0.0111,     0.8670,     0.1213],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0028, 0.0706, 0.0311, 0.1345, 0.7611], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.983]
 [0.983]
 [0.983]
 [1.025]
 [0.983]] [[32.384]
 [32.384]
 [32.384]
 [40.487]
 [32.384]] [[1.189]
 [1.189]
 [1.189]
 [1.358]
 [1.189]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
siam score:  -0.8457051
printing an ep nov before normalisation:  18.206578650855633
printing an ep nov before normalisation:  18.402348309286282
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.743472071428926
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.891]
 [0.868]
 [0.847]
 [0.891]] [[35.126]
 [35.126]
 [40.558]
 [34.999]
 [35.126]] [[2.151]
 [2.151]
 [2.383]
 [2.101]
 [2.151]]
printing an ep nov before normalisation:  42.34442122430917
siam score:  -0.84077126
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.472]
 [0.577]
 [0.491]
 [0.496]
 [0.497]] [[32.368]
 [35.349]
 [34.828]
 [34.728]
 [33.642]] [[1.34 ]
 [1.618]
 [1.501]
 [1.5  ]
 [1.438]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4021 1.0 1.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  16.210832595825195
UNIT TEST: sample policy line 217 mcts : [0.051 0.667 0.    0.128 0.154]
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.143]
 [1.178]
 [1.178]
 [1.178]] [[43.716]
 [44.86 ]
 [43.716]
 [43.716]
 [43.716]] [[2.639]
 [2.672]
 [2.639]
 [2.639]
 [2.639]]
printing an ep nov before normalisation:  38.228624236656074
Printing some Q and Qe and total Qs values:  [[0.749]
 [0.676]
 [0.618]
 [0.652]
 [0.745]] [[30.905]
 [25.383]
 [34.32 ]
 [35.791]
 [30.444]] [[0.749]
 [0.676]
 [0.618]
 [0.652]
 [0.745]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.423291228253824
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.765]
 [0.76 ]
 [0.744]
 [0.767]] [[42.762]
 [42.762]
 [33.521]
 [40.358]
 [38.054]] [[2.518]
 [2.518]
 [1.84 ]
 [2.323]
 [2.178]]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  117 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  26.857568536485946
printing an ep nov before normalisation:  0.00039442653815058293
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  64.60971224906378
printing an ep nov before normalisation:  26.20018720626831
siam score:  -0.84595454
printing an ep nov before normalisation:  25.16080616113438
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.516445366453176
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.36774166926627
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.8812837600708
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.487081838176145
printing an ep nov before normalisation:  39.81387111597804
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.465588546010004
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.833771782043506
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  51.99244988240623
Printing some Q and Qe and total Qs values:  [[1.439]
 [1.357]
 [1.339]
 [1.371]
 [1.41 ]] [[35.908]
 [36.252]
 [36.036]
 [37.089]
 [35.55 ]] [[1.96 ]
 [1.888]
 [1.864]
 [1.926]
 [1.921]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.12998871264984
printing an ep nov before normalisation:  59.96734223494158
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.851]
 [0.851]
 [0.884]
 [0.851]] [[42.831]
 [43.192]
 [43.192]
 [44.886]
 [43.192]] [[1.706]
 [1.718]
 [1.718]
 [1.818]
 [1.718]]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  29.67769412830584
Printing some Q and Qe and total Qs values:  [[1.391]
 [1.127]
 [0.22 ]
 [1.071]
 [1.235]] [[26.014]
 [25.369]
 [28.552]
 [25.928]
 [25.099]] [[2.34 ]
 [2.053]
 [1.262]
 [2.017]
 [2.151]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  40.454468727111816
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.054]
 [0.952]
 [1.054]
 [1.054]
 [1.074]] [[28.139]
 [23.084]
 [28.139]
 [36.247]
 [32.986]] [[1.341]
 [1.14 ]
 [1.341]
 [1.497]
 [1.454]]
maxi score, test score, baseline:  0.4041 1.0 1.0
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
UNIT TEST: sample policy line 217 mcts : [0.    0.231 0.    0.513 0.256]
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.026 0.897 0.026]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.85003054
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.11618352097692
actions average: 
K:  2  action  0 :  tensor([    0.9912,     0.0005,     0.0001,     0.0025,     0.0058],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9985,     0.0002,     0.0001,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0001,     0.9165,     0.0451,     0.0382],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0003,     0.0011,     0.9336,     0.0649],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0159, 0.0049, 0.0044, 0.0038, 0.9709], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.941966013217126
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.14335101972293
printing an ep nov before normalisation:  28.582859913886775
printing an ep nov before normalisation:  29.064839550672634
Printing some Q and Qe and total Qs values:  [[0.68 ]
 [0.675]
 [0.675]
 [0.675]
 [0.675]] [[39.646]
 [41.465]
 [41.465]
 [41.465]
 [41.276]] [[0.68 ]
 [0.675]
 [0.675]
 [0.675]
 [0.675]]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.29858587646456
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.70906433353791
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.855744
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  37.53166198730469
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 52.00937014531127
UNIT TEST: sample policy line 217 mcts : [0.487 0.282 0.128 0.026 0.077]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.06355446501184
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.542]
 [0.689]
 [0.641]
 [0.566]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.542]
 [0.689]
 [0.641]
 [0.566]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.773]
 [0.773]
 [0.797]
 [0.807]
 [0.771]] [[48.297]
 [48.297]
 [60.379]
 [56.821]
 [55.729]] [[1.792]
 [1.792]
 [2.28 ]
 [2.153]
 [2.075]]
Printing some Q and Qe and total Qs values:  [[1.266]
 [1.266]
 [1.266]
 [1.266]
 [1.266]] [[21.085]
 [21.085]
 [21.085]
 [21.085]
 [21.085]] [[8.287]
 [8.287]
 [8.287]
 [8.287]
 [8.287]]
actor:  1 policy actor:  1  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  70.1480762907685
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
line 256 mcts: sample exp_bonus 57.487852220166616
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.1332266855652
Printing some Q and Qe and total Qs values:  [[0.732]
 [0.869]
 [0.738]
 [0.793]
 [0.771]] [[51.614]
 [40.925]
 [37.954]
 [40.729]
 [40.563]] [[1.399]
 [1.312]
 [1.118]
 [1.232]
 [1.206]]
printing an ep nov before normalisation:  42.238914944656926
printing an ep nov before normalisation:  30.786354485491998
Printing some Q and Qe and total Qs values:  [[1.044]
 [0.976]
 [0.984]
 [0.994]
 [1.027]] [[38.022]
 [19.154]
 [20.509]
 [21.863]
 [33.644]] [[1.329]
 [1.069]
 [1.091]
 [1.115]
 [1.268]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  46.64111590742829
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  67.51085195889331
printing an ep nov before normalisation:  42.43071037695786
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.57871843095568
printing an ep nov before normalisation:  44.39348208530718
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  61.00226635864294
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.424]
 [1.424]
 [1.293]
 [1.424]
 [1.424]] [[27.271]
 [27.271]
 [23.887]
 [27.271]
 [27.271]] [[2.363]
 [2.363]
 [2.026]
 [2.363]
 [2.363]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.91410805844273
printing an ep nov before normalisation:  41.66071887913943
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
printing an ep nov before normalisation:  53.98767808475342
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.031]
 [0.603]
 [0.579]
 [0.602]] [[41.339]
 [41.544]
 [38.867]
 [41.823]
 [39.364]] [[1.387]
 [0.694]
 [1.191]
 [1.25 ]
 [1.203]]
printing an ep nov before normalisation:  36.664739664110186
printing an ep nov before normalisation:  28.500096797943115
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.657]
 [0.7  ]
 [0.7  ]
 [0.695]] [[47.28 ]
 [50.25 ]
 [47.28 ]
 [47.28 ]
 [50.739]] [[1.971]
 [2.057]
 [1.971]
 [1.971]
 [2.116]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.27850418881085
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  46.21422450279213
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8462399
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  28.776029988455285
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.110310209352065
printing an ep nov before normalisation:  28.201720054575564
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.2773799944836
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
printing an ep nov before normalisation:  33.23988247653299
printing an ep nov before normalisation:  33.51463726311334
actor:  0 policy actor:  0  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.57037564676248
Printing some Q and Qe and total Qs values:  [[0.16 ]
 [0.723]
 [0.875]
 [0.756]
 [0.776]] [[33.386]
 [31.301]
 [28.14 ]
 [29.93 ]
 [31.918]] [[1.672]
 [2.077]
 [1.989]
 [2.005]
 [2.176]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
printing an ep nov before normalisation:  54.49590919860634
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.0
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  39.544380932189405
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.772803041768967
printing an ep nov before normalisation:  41.73399248650713
printing an ep nov before normalisation:  42.332338788083504
printing an ep nov before normalisation:  40.963728194428924
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.393]
 [1.393]
 [1.393]
 [1.393]
 [1.393]] [[41.32]
 [41.32]
 [41.32]
 [41.32]
 [41.32]] [[2.3]
 [2.3]
 [2.3]
 [2.3]
 [2.3]]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.635362066406415
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
printing an ep nov before normalisation:  55.00799500078359
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20036
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.66567805918658
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 24.381825368537058
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  28.658844471655584
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.95223127652511
actions average: 
K:  4  action  0 :  tensor([    0.9886,     0.0005,     0.0000,     0.0001,     0.0107],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9547,     0.0001,     0.0000,     0.0444],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0066,     0.9040,     0.0287,     0.0606],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0050, 0.0037, 0.0130, 0.9254, 0.0529], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0057, 0.0826, 0.0201, 0.1011, 0.7905], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  39.62075383018491
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.69249497407212
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 44.69463282109035
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.262]
 [0.266]
 [0.193]
 [0.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.262]
 [0.266]
 [0.193]
 [0.199]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 34.5987119330678
Printing some Q and Qe and total Qs values:  [[1.38]
 [1.38]
 [1.38]
 [1.38]
 [1.38]] [[42.314]
 [42.314]
 [42.314]
 [42.314]
 [42.314]] [[2.6]
 [2.6]
 [2.6]
 [2.6]
 [2.6]]
printing an ep nov before normalisation:  48.63328556044025
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.938]
 [0.913]
 [0.913]
 [0.898]] [[47.358]
 [42.272]
 [47.358]
 [47.358]
 [48.952]] [[0.913]
 [0.938]
 [0.913]
 [0.913]
 [0.898]]
actor:  0 policy actor:  0  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.883]
 [0.883]
 [0.883]
 [1.076]
 [0.859]] [[22.241]
 [22.241]
 [22.241]
 [34.307]
 [25.235]] [[1.042]
 [1.042]
 [1.042]
 [1.4  ]
 [1.059]]
Printing some Q and Qe and total Qs values:  [[1.242]
 [1.241]
 [1.241]
 [1.166]
 [1.224]] [[42.737]
 [31.468]
 [31.468]
 [46.177]
 [31.808]] [[1.687]
 [1.496]
 [1.496]
 [1.669]
 [1.484]]
Printing some Q and Qe and total Qs values:  [[1.172]
 [1.084]
 [1.022]
 [1.084]
 [1.095]] [[31.589]
 [35.682]
 [32.224]
 [35.682]
 [30.29 ]] [[1.815]
 [1.881]
 [1.689]
 [1.881]
 [1.69 ]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.95282489597961
printing an ep nov before normalisation:  50.94045770944771
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [1.006]
 [0.006]
 [1.122]
 [1.091]] [[32.55 ]
 [37.947]
 [35.54 ]
 [33.534]
 [37.213]] [[1.549]
 [1.76 ]
 [0.656]
 [1.684]
 [1.814]]
actor:  1 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  36.12596035003662
printing an ep nov before normalisation:  49.78898220048187
printing an ep nov before normalisation:  70.11283621033488
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
printing an ep nov before normalisation:  29.076309204101562
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.054]
 [1.054]
 [1.054]
 [1.054]
 [1.054]] [[46.439]
 [46.439]
 [46.439]
 [46.439]
 [46.439]] [[1.834]
 [1.834]
 [1.834]
 [1.834]
 [1.834]]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.622]
 [0.604]
 [0.622]
 [0.618]
 [0.622]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.622]
 [0.604]
 [0.622]
 [0.618]
 [0.622]]
Printing some Q and Qe and total Qs values:  [[0.626]
 [0.626]
 [0.739]
 [0.626]
 [0.626]] [[20.196]
 [20.196]
 [33.354]
 [20.196]
 [20.196]] [[1.078]
 [1.078]
 [1.752]
 [1.078]
 [1.078]]
printing an ep nov before normalisation:  33.8285061487921
actions average: 
K:  0  action  0 :  tensor([    0.9849,     0.0006,     0.0000,     0.0010,     0.0135],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9614,     0.0010,     0.0013,     0.0358],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0015,     0.0007,     0.9947,     0.0016,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0007,     0.9298,     0.0691],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0277,     0.0380,     0.1157,     0.8180],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
printing an ep nov before normalisation:  37.06234221108871
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  2  action  0 :  tensor([    0.9668,     0.0005,     0.0001,     0.0024,     0.0303],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9978,     0.0001,     0.0003,     0.0018],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0003,     0.9985,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0005,     0.0004,     0.9967,     0.0024],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0365, 0.0337, 0.0027, 0.0390, 0.8882], grad_fn=<DivBackward0>)
siam score:  -0.83934736
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.07283051889571
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  87.6192034046862
printing an ep nov before normalisation:  0.012822880453882135
printing an ep nov before normalisation:  30.738023562042073
printing an ep nov before normalisation:  23.938735326131187
Printing some Q and Qe and total Qs values:  [[0.937]
 [0.852]
 [0.852]
 [0.852]
 [0.852]] [[62.581]
 [49.365]
 [49.365]
 [49.365]
 [49.365]] [[1.876]
 [1.49 ]
 [1.49 ]
 [1.49 ]
 [1.49 ]]
printing an ep nov before normalisation:  34.60105169811939
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8452343
actor:  0 policy actor:  0  step number:  114 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  1  action  0 :  tensor([    0.9949,     0.0008,     0.0004,     0.0012,     0.0027],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9927,     0.0004,     0.0032,     0.0031],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0013, 0.0474, 0.9028, 0.0387, 0.0097], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0002,     0.0148,     0.9369,     0.0481],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0408, 0.0410, 0.0894, 0.0255, 0.8034], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.857415187640385
printing an ep nov before normalisation:  31.395394173551
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.578]
 [0.573]
 [0.601]
 [0.583]] [[34.964]
 [36.665]
 [38.064]
 [44.957]
 [39.815]] [[0.594]
 [0.578]
 [0.573]
 [0.601]
 [0.583]]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.2459],
        [0.4980],
        [0.5675],
        [0.6166],
        [0.6979],
        [0.4980],
        [0.0000],
        [0.8949],
        [0.7791],
        [0.4368]], dtype=torch.float64)
0.0 0.2458907341554109
0.0 0.4979813090913839
0.0 0.567454876561253
0.0 0.616566451496925
0.0 0.697928383046639
0.0 0.4979813090913839
0.0 0.0
0.0 0.8948562891782226
0.0 0.779084348685765
0.0 0.43679538257328043
printing an ep nov before normalisation:  44.78220914798101
printing an ep nov before normalisation:  47.3618546873909
printing an ep nov before normalisation:  53.69838799231695
printing an ep nov before normalisation:  48.63144082527538
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.009]
 [1.068]
 [1.068]
 [1.068]] [[54.973]
 [45.773]
 [51.435]
 [51.435]
 [51.435]] [[2.503]
 [1.999]
 [2.32 ]
 [2.32 ]
 [2.32 ]]
printing an ep nov before normalisation:  43.154763477625856
printing an ep nov before normalisation:  47.06261126026712
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  48.74134851973349
printing an ep nov before normalisation:  48.64865672249595
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.761]
 [0.761]
 [0.761]
 [0.761]] [[40.247]
 [30.105]
 [30.105]
 [30.105]
 [30.105]] [[2.14 ]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.454112308738964
siam score:  -0.84855604
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  57.876119040209005
printing an ep nov before normalisation:  48.48754782224744
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
printing an ep nov before normalisation:  30.918640992234714
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
printing an ep nov before normalisation:  60.827884801597946
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.279]
 [0.225]
 [0.225]
 [0.225]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.225]
 [0.279]
 [0.225]
 [0.225]
 [0.225]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.438]
 [0.551]
 [0.438]
 [0.438]
 [0.455]] [[49.429]
 [43.578]
 [49.429]
 [49.429]
 [53.847]] [[0.438]
 [0.551]
 [0.438]
 [0.438]
 [0.455]]
printing an ep nov before normalisation:  45.4840814383219
line 256 mcts: sample exp_bonus 49.73599149142851
line 256 mcts: sample exp_bonus 30.315713483878206
printing an ep nov before normalisation:  39.6522856740385
printing an ep nov before normalisation:  54.75248540235031
printing an ep nov before normalisation:  38.82357454570005
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.10637872850883
printing an ep nov before normalisation:  23.288442822088868
printing an ep nov before normalisation:  30.675179475117474
printing an ep nov before normalisation:  41.645887399555605
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4221 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  34.5105885496749
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  23.510743336772954
printing an ep nov before normalisation:  0.46436669337992953
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  1.6589641398127242
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.935]
 [0.888]
 [0.888]
 [0.858]] [[40.865]
 [40.512]
 [35.362]
 [35.362]
 [37.812]] [[0.904]
 [0.935]
 [0.888]
 [0.888]
 [0.858]]
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4501 1.0 1.0
maxi score, test score, baseline:  0.4501 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.541]
 [0.581]
 [0.525]
 [0.476]
 [0.51 ]] [[34.627]
 [31.057]
 [27.897]
 [28.357]
 [28.319]] [[0.541]
 [0.581]
 [0.525]
 [0.476]
 [0.51 ]]
printing an ep nov before normalisation:  25.965280031378068
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  35.63512995857932
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4521 1.0 1.0
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  45.552953052822666
maxi score, test score, baseline:  0.4521 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  41.47238499622636
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0001,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0110,     0.9682,     0.0006,     0.0003,     0.0199],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0012,     0.9983,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0003,     0.8861,     0.1129],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0204, 0.0290, 0.0009, 0.0493, 0.9004], grad_fn=<DivBackward0>)
siam score:  -0.84537065
maxi score, test score, baseline:  0.4541 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  0 policy actor:  0  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  43.312466973540275
printing an ep nov before normalisation:  33.4892201423645
printing an ep nov before normalisation:  48.94044352738416
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4561 1.0 1.0
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  43.017462295079724
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  1  action  0 :  tensor([    0.9442,     0.0002,     0.0007,     0.0015,     0.0535],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9531,     0.0002,     0.0002,     0.0464],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0015,     0.0001,     0.9494,     0.0002,     0.0487],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0010,     0.0001,     0.9594,     0.0391],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0341, 0.0271, 0.0034, 0.0403, 0.8951], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  58.57707703015178
siam score:  -0.8432478
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  40.27512241829962
maxi score, test score, baseline:  0.4561 1.0 1.0
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  0.7370182577142259
line 256 mcts: sample exp_bonus 36.336057703800286
maxi score, test score, baseline:  0.4561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.107]
 [0.016]
 [1.026]
 [1.07 ]
 [1.075]] [[42.706]
 [40.487]
 [36.229]
 [37.092]
 [35.556]] [[2.25 ]
 [1.056]
 [1.866]
 [1.95 ]
 [1.885]]
printing an ep nov before normalisation:  41.211626423355824
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4561 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  33.835214367717604
printing an ep nov before normalisation:  33.86832856374015
siam score:  -0.8466255
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.51136779785156
printing an ep nov before normalisation:  41.79671476633697
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  27.334011867772183
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.453]
 [0.573]
 [0.453]
 [0.453]
 [0.459]] [[32.55 ]
 [31.288]
 [32.55 ]
 [32.55 ]
 [35.387]] [[0.453]
 [0.573]
 [0.453]
 [0.453]
 [0.459]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  15.488980494958033
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.784]
 [0.089]
 [0.647]
 [0.776]] [[38.669]
 [36.011]
 [34.997]
 [32.572]
 [36.744]] [[1.237]
 [1.84 ]
 [1.078]
 [1.475]
 [1.881]]
maxi score, test score, baseline:  0.4581 1.0 1.0
printing an ep nov before normalisation:  30.44351100921631
siam score:  -0.8437868
printing an ep nov before normalisation:  73.18315321968444
printing an ep nov before normalisation:  64.19967562631393
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.866]
 [0.849]
 [0.866]
 [0.931]] [[49.059]
 [49.059]
 [54.819]
 [49.059]
 [51.589]] [[2.186]
 [2.186]
 [2.405]
 [2.186]
 [2.355]]
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0001,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9980,     0.0002,     0.0001,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9540,     0.0194,     0.0264],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0005,     0.0002,     0.9950,     0.0039],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0170, 0.0133, 0.0659, 0.9024], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  30.635766983032227
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8437426
printing an ep nov before normalisation:  64.78434198262376
printing an ep nov before normalisation:  60.36428853142026
printing an ep nov before normalisation:  36.886476875077335
printing an ep nov before normalisation:  34.92741827646407
printing an ep nov before normalisation:  40.80856655420702
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.094842030141095
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.83 ]
 [0.856]
 [0.993]
 [0.922]] [[36.757]
 [29.162]
 [33.522]
 [26.292]
 [31.164]] [[2.569]
 [1.927]
 [2.358]
 [1.822]
 [2.205]]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[1.06]
 [1.06]
 [1.06]
 [1.06]
 [1.06]] [[29.885]
 [29.885]
 [29.885]
 [29.885]
 [29.885]] [[2.153]
 [2.153]
 [2.153]
 [2.153]
 [2.153]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 30.278428632160875
printing an ep nov before normalisation:  28.531536109090563
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9949,     0.0031,     0.0000,     0.0001,     0.0018],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9870,     0.0001,     0.0001,     0.0126],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0193,     0.9409,     0.0005,     0.0388],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0003,     0.0003,     0.9215,     0.0772],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0279, 0.0536, 0.0285, 0.1135, 0.7765], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.40655959452708
maxi score, test score, baseline:  0.4581 1.0 1.0
printing an ep nov before normalisation:  38.75592060242117
printing an ep nov before normalisation:  31.371020516296397
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.832]
 [0.765]
 [0.765]
 [0.765]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.765]
 [0.832]
 [0.765]
 [0.765]
 [0.765]]
printing an ep nov before normalisation:  23.5921049118042
siam score:  -0.84029114
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
printing an ep nov before normalisation:  34.13947082640576
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.40474023353406
Printing some Q and Qe and total Qs values:  [[0.824]
 [0.824]
 [0.824]
 [0.824]
 [0.898]] [[38.151]
 [38.151]
 [38.151]
 [38.151]
 [36.342]] [[2.824]
 [2.824]
 [2.824]
 [2.824]
 [2.726]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.566]
 [0.566]
 [0.598]
 [0.582]
 [0.566]] [[35.051]
 [35.051]
 [36.347]
 [37.402]
 [35.051]] [[2.062]
 [2.062]
 [2.191]
 [2.252]
 [2.062]]
line 256 mcts: sample exp_bonus 31.944740788972233
printing an ep nov before normalisation:  34.25510674915521
Printing some Q and Qe and total Qs values:  [[1.263]
 [1.259]
 [1.237]
 [1.114]
 [1.293]] [[23.738]
 [30.318]
 [27.096]
 [25.117]
 [23.076]] [[2.069]
 [2.506]
 [2.268]
 [2.012]
 [2.054]]
printing an ep nov before normalisation:  32.22043528302403
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.37349013572557
printing an ep nov before normalisation:  44.416707782648636
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  45.16674995422363
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  23.844852817730075
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.825369327476544
maxi score, test score, baseline:  0.4601 1.0 1.0
line 256 mcts: sample exp_bonus 27.165465934348703
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  36.92748905667534
printing an ep nov before normalisation:  40.1148796081543
printing an ep nov before normalisation:  28.582748233836618
printing an ep nov before normalisation:  29.865038607740864
printing an ep nov before normalisation:  32.90764520952098
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.594]
 [0.71 ]
 [0.594]
 [0.594]
 [0.594]] [[47.232]
 [50.876]
 [47.232]
 [47.232]
 [47.232]] [[2.402]
 [2.706]
 [2.402]
 [2.402]
 [2.402]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.703516483306885
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.782]
 [0.825]
 [0.758]
 [0.914]] [[39.197]
 [36.295]
 [36.812]
 [35.398]
 [35.992]] [[2.131]
 [2.004]
 [2.079]
 [1.924]
 [2.117]]
printing an ep nov before normalisation:  43.940577415900414
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  31.05063817835251
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.119]
 [0.119]
 [0.119]
 [0.119]
 [0.119]] [[11.341]
 [11.341]
 [11.341]
 [11.341]
 [11.341]] [[0.47]
 [0.47]
 [0.47]
 [0.47]
 [0.47]]
printing an ep nov before normalisation:  28.341534570652083
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.31122491866117
printing an ep nov before normalisation:  22.6416235374111
printing an ep nov before normalisation:  37.65083060026686
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.137867010532506
printing an ep nov before normalisation:  34.1644812931148
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.83290666
printing an ep nov before normalisation:  41.67516360335145
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  47.76528144328649
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  54.41896128273132
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.9682,     0.0005,     0.0000,     0.0004,     0.0309],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9594,     0.0002,     0.0015,     0.0383],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0018,     0.9912,     0.0008,     0.0062],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0022,     0.0006,     0.9023,     0.0948],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0021, 0.0316, 0.0244, 0.0300, 0.9118], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  49.767678900232276
maxi score, test score, baseline:  0.4581 1.0 1.0
printing an ep nov before normalisation:  36.58269830580383
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  56.682188218870714
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]] [[52.296]
 [52.296]
 [52.296]
 [52.296]
 [52.296]] [[0.234]
 [0.234]
 [0.234]
 [0.234]
 [0.234]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.64576246440986
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.755]
 [0.689]
 [0.689]
 [0.689]] [[27.677]
 [40.446]
 [28.986]
 [28.986]
 [28.986]] [[0.916]
 [1.297]
 [1.003]
 [1.003]
 [1.003]]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.586]
 [0.499]
 [0.458]
 [0.502]] [[28.894]
 [35.867]
 [28.894]
 [40.724]
 [40.168]] [[0.499]
 [0.586]
 [0.499]
 [0.458]
 [0.502]]
siam score:  -0.825279
actions average: 
K:  2  action  0 :  tensor([    0.9462,     0.0160,     0.0001,     0.0007,     0.0370],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9714,     0.0007,     0.0005,     0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0086,     0.9811,     0.0001,     0.0097],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0009,     0.0011,     0.0003,     0.9034,     0.0943],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0523, 0.0482, 0.0017, 0.0227, 0.8751], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  42.83560790066433
line 256 mcts: sample exp_bonus 41.48843301112532
printing an ep nov before normalisation:  44.16066781744342
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.86630621383288
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  17.915066480636597
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  55.28335092212896
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  52.02386933987672
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  32.30444539313882
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  38.72483816048495
printing an ep nov before normalisation:  33.56621637434461
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.66144875126961
Printing some Q and Qe and total Qs values:  [[0.484]
 [0.555]
 [0.484]
 [0.457]
 [0.493]] [[25.053]
 [32.775]
 [25.053]
 [21.756]
 [27.431]] [[0.484]
 [0.555]
 [0.484]
 [0.457]
 [0.493]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.396]
 [0.432]
 [0.471]
 [0.416]
 [0.432]] [[36.081]
 [33.467]
 [32.569]
 [42.371]
 [33.467]] [[0.396]
 [0.432]
 [0.471]
 [0.416]
 [0.432]]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  34.93771329576743
printing an ep nov before normalisation:  57.37366235993747
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.553]
 [0.62 ]
 [0.528]
 [0.528]] [[38.845]
 [34.29 ]
 [33.663]
 [37.644]
 [37.644]] [[1.657]
 [1.447]
 [1.484]
 [1.579]
 [1.579]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  37.059526443481445
siam score:  -0.8258362
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
siam score:  -0.8248594
maxi score, test score, baseline:  0.4581 1.0 1.0
maxi score, test score, baseline:  0.4581 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  0 policy actor:  0  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4601 1.0 1.0
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  27.103388389208078
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
actions average: 
K:  0  action  0 :  tensor([    0.9223,     0.0000,     0.0000,     0.0008,     0.0768],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9773,     0.0050,     0.0002,     0.0169],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0000,     0.9991,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0012,     0.0001,     0.0002,     0.8670,     0.1315],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0246, 0.0463, 0.0164, 0.0385, 0.8742], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
line 256 mcts: sample exp_bonus 47.658312455466856
printing an ep nov before normalisation:  32.0289715180639
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
siam score:  -0.821968
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  50.63229705775794
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
maxi score, test score, baseline:  0.4601 1.0 1.0
printing an ep nov before normalisation:  30.737926792122813
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.627]
 [0.664]
 [0.664]] [[26.24 ]
 [26.24 ]
 [31.009]
 [26.24 ]
 [26.24 ]] [[0.664]
 [0.664]
 [0.627]
 [0.664]
 [0.664]]
Printing some Q and Qe and total Qs values:  [[0.946]
 [0.908]
 [0.957]
 [1.015]
 [0.946]] [[13.526]
 [13.034]
 [11.381]
 [21.337]
 [10.663]] [[1.021]
 [0.978]
 [1.012]
 [1.161]
 [0.994]]
printing an ep nov before normalisation:  29.099323962219685
printing an ep nov before normalisation:  32.23485543698744
printing an ep nov before normalisation:  44.76636308091684
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.8244885
maxi score, test score, baseline:  0.4621 1.0 1.0
printing an ep nov before normalisation:  32.532252115588776
maxi score, test score, baseline:  0.4621 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.4621 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.638]
 [0.583]
 [0.586]
 [0.59 ]] [[47.136]
 [45.998]
 [50.723]
 [49.576]
 [48.62 ]] [[1.74 ]
 [1.724]
 [1.913]
 [1.858]
 [1.812]]
printing an ep nov before normalisation:  54.55447551619911
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4641 1.0 1.0
probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.772]
 [0.785]
 [0.85 ]
 [0.928]] [[58.838]
 [56.324]
 [46.588]
 [58.199]
 [50.432]] [[2.324]
 [2.25 ]
 [1.849]
 [2.408]
 [2.155]]
