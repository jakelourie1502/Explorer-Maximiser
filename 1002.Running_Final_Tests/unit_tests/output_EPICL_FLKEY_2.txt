append_mcts_svs:False
dirichlet_alpha:0.3
noise:0.3
dirichlet_alpha:0.5
noise:0.5
sims:{-1: 6, 6000: 40}
c1:1
c2:19652
temperature_init:1
temperature_changes:{-1: 1, 3000000.0: 0.5}
manual_over_ride_play_limit:None
exponent_node_n:1
ucb_denom_k:1
use_policy:True
model_expV_on_dones:True
norm_Qs_OnMaxi:True
norm_Qs_OnAll:True
norm_each_turn:False
state_channels:64
res_block_kernel_size:3
res_block_channels:[32, 32, 64, 64, 64]
res_block_ds:[False, True, False, False, False]
conv1:{'channels': 32, 'kernel_size': 3, 'stride': 2, 'padding': 1}
conv2:{'channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}
reward_support:[-1, 1, 51]
conv1:{'kernel_size': 3, 'stride': 1, 'padding': 1}
res_blocks:[64]
reward_conv_channels:32
reward_hidden_dim:128
terminal_conv_channels:32
terminal_hidden_dim:64
value_support:[-1, 1, 51]
res_block:[64]
value_conv_channels:32
value_hidden_dim:128
policy_conv_channels:32
policy_hidden_dim:128
expV_conv_channels:32
expV_hidden_dim:128
expV_support:[-10, 10, 51]
proj_l1:256
proj_out:128
pred_hid:256
replay_buffer_size:50000
replay_buffer_size_exploration:200000
all_time_buffer_size:200000
batch_size:128
play_workers:2
min_workers:1
max_workers:6
lr:0.001
lr_warmup:1000
lr_decay:1
lr_decay_step:100000
optimizer:<class 'torch.optim.rmsprop.RMSprop'>
momentum:0.9
l2:0.0001
rho:0.99
k:5
value:0.25
dones:2.0
siam:2
rdn:0.5
expV:0.5
train_start_batch_multiple:2
prioritised_replay:True
ep_to_batch_ratio:[15, 16]
main_to_rdn_ratio:2
train_to_RS_ratio:4
on_policy_expV:False
rgb_im:True
channels:3
state_size:[6, 6]
timesteps_in_obs:2
store_prev_actions:True
env:<class 'game_play.frozen_lake_KEY.gridWorld'>
same_env_each_time:True
env_size:[7, 7]
observable_size:[7, 7]
game_modes:2
env_map:[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
max_steps:120
actions_size:5
optimal_score:1
total_frames:305000
exp_gamma:0.95
atari_env:False
memory_size:30
reward_clipping:False
image_size:[48, 48]
deque_length:3
PRESET_CONFIG:7
VK:True
use_two_heads:True
follow_better_policy:0.5
use_siam:True
exploration_type:episodic
rdn_beta:[0.3333333333333333, 2, 6]
explorer_percentage:0.8
reward_exploration:False
train_dones:True
contrast_vector:True
norm_state_vecs:False
RND_output_vector:256
RND_loss:cosine
prediction_bias:True
distance_measure:False
update_play_model:16
gamma:0.99
calc_n_step_rewards_after_frames:10000
N_steps_reward:5
start_frame_count:0
load_in_model:False
log_states:True
log_metrics:False
exploration_logger_dims:(2, 49)
device_train:cpu
device_selfplay:cpu
eval_x_frames:10000
eval_count:20
detach_expV_calc:True
use_new_episode_expV:False
start_training_expV_min:10000
start_training_expV_max:20000
start_training_expV_siam_override:0.8
value_only:False
[['H' 'H' 'H' 'H' 'H' 'F' 'G']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'F']
 ['S' 'F' 'H' 'H' 'H' 'H' 'H']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'K' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F' 'F']]
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
printing an ep nov before normalisation:  8.10954082658995
printing an ep nov before normalisation:  6.77434989278396
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[6.788]
 [6.788]
 [6.788]
 [6.788]
 [6.788]] [[4.527]
 [4.527]
 [4.527]
 [4.527]
 [4.527]]
printing an ep nov before normalisation:  7.8565816004887274
printing an ep nov before normalisation:  7.712465819111003
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
from probs:  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
Starting evaluation
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
siam score:  0.002892382781613957
Printing some Q and Qe and total Qs values:  [[-0.001]
 [-0.001]
 [-0.001]
 [-0.001]
 [-0.001]] [[10.22 ]
 [ 7.784]
 [10.207]
 [ 7.784]
 [ 7.784]] [[0.374]
 [0.141]
 [0.372]
 [0.141]
 [0.141]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.03565384043857777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.0620745845080819
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([0.3239, 0.1353, 0.1574, 0.2264, 0.1570], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.1411, 0.6219, 0.0805, 0.0416, 0.1150], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.1603, 0.1291, 0.2448, 0.2718, 0.1940], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.2219, 0.0518, 0.1948, 0.3591, 0.1724], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.2171, 0.1507, 0.1842, 0.2620, 0.1861], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 2 threads
Frames:  1604 train batches done:  37 episodes:  107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.3369, 0.1286, 0.1562, 0.1999, 0.1785], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1034, 0.7360, 0.0400, 0.0277, 0.0929], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.1287, 0.0258, 0.3208, 0.2883, 0.2364], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1814, 0.0886, 0.2423, 0.2782, 0.2096], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.2161, 0.2507, 0.1399, 0.1976, 0.1957], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  1604 train batches done:  97 episodes:  107
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.47854355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.4970644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5318778
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([0.5088, 0.0862, 0.0844, 0.1854, 0.1352], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.1132, 0.5495, 0.1686, 0.0276, 0.1411], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0438, 0.0258, 0.5249, 0.2278, 0.1777], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.2340, 0.0866, 0.1359, 0.3628, 0.1806], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1559, 0.1439, 0.2662, 0.1752, 0.2588], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.5726235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.08200070868719
printing an ep nov before normalisation:  82.20237420773685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.766210026211205
siam score:  -0.6166484
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([0.7211, 0.1393, 0.0046, 0.0816, 0.0534], grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.1243, 0.6832, 0.0567, 0.0265, 0.1094], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0091, 0.0320, 0.4966, 0.2253, 0.2369], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.2015, 0.0259, 0.0949, 0.4867, 0.1909], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1208, 0.1514, 0.2288, 0.3011, 0.1978], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6383237
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.2 0.4 0.  0.2 0.2]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9129,     0.0120,     0.0001,     0.0273,     0.0476],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0230, 0.9348, 0.0053, 0.0010, 0.0358], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0068, 0.0027, 0.7352, 0.0910, 0.1643], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0303, 0.0026, 0.0552, 0.5943, 0.3177], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0434, 0.0162, 0.0875, 0.4077, 0.4453], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.7800817489624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: False
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.7790,     0.0300,     0.0006,     0.0620,     0.1284],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0596, 0.8218, 0.0029, 0.0101, 0.1056], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0055, 0.0223, 0.5696, 0.1527, 0.2500], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1423, 0.0412, 0.0609, 0.4805, 0.2751], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1919, 0.1601, 0.0761, 0.1900, 0.3818], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6734848
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6781035
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.579]
 [59.434]
 [70.245]
 [ 0.   ]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.6346134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.59407747
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9087,     0.0032,     0.0002,     0.0490,     0.0389],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0141, 0.9400, 0.0062, 0.0031, 0.0366], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0241,     0.8146,     0.0615,     0.0996],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0373,     0.0006,     0.0417,     0.6811,     0.2393],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0980, 0.0197, 0.1225, 0.3336, 0.4262], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  111.04740497610692
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6266644
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.590675354003906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6924135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9042,     0.0025,     0.0000,     0.0240,     0.0693],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0100,     0.9683,     0.0009,     0.0004,     0.0204],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0726,     0.7872,     0.0595,     0.0804],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0119, 0.0031, 0.0495, 0.6466, 0.2889], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0540, 0.0199, 0.1380, 0.2413, 0.5469], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.4 0.4 0.  0.2 0. ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.6959706
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7046043
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.45955456832669
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.83926825006808
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7000146
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.7959,     0.0195,     0.0004,     0.0608,     0.1234],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0303, 0.8141, 0.0628, 0.0027, 0.0901], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0071,     0.8019,     0.0941,     0.0967],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0152,     0.0005,     0.0620,     0.5168,     0.4055],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0706, 0.0384, 0.0919, 0.2884, 0.5107], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  123.39981396185779
printing an ep nov before normalisation:  35.97921643938337
siam score:  -0.6611921
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
main train batch thing paused
add a thread
Adding thread: now have 2 threads
printing an ep nov before normalisation:  68.97129020351649
printing an ep nov before normalisation:  42.81679630279541
printing an ep nov before normalisation:  38.69553430184676
actions average: 
K:  4  action  0 :  tensor([0.7995, 0.1234, 0.0008, 0.0041, 0.0722], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0073, 0.8596, 0.0451, 0.0178, 0.0701], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0049,     0.7630,     0.1405,     0.0916],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1350, 0.0025, 0.0347, 0.5222, 0.3056], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0316, 0.2830, 0.1347, 0.1400, 0.4106], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9143,     0.0229,     0.0001,     0.0114,     0.0513],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0017,     0.9748,     0.0004,     0.0008,     0.0223],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0024,     0.9121,     0.0092,     0.0762],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0027, 0.0007, 0.0897, 0.5913, 0.3156], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0326, 0.0282, 0.1108, 0.2199, 0.6085], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.61627961872738
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.25056254595503
printing an ep nov before normalisation:  25.914797847734878
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.65876404094222
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  113.89180464548146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.93038868904114
printing an ep nov before normalisation:  55.93039512634277
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.67989454652872
main train batch thing paused
add a thread
Adding thread: now have 3 threads
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  49.13492109683261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.79 ]
 [69.094]
 [69.094]
 [69.094]
 [69.094]] [[2.   ]
 [1.733]
 [1.733]
 [1.733]
 [1.733]]
printing an ep nov before normalisation:  50.99122798694386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.439]
 [38.987]
 [36.328]
 [35.602]
 [42.369]] [[0.662]
 [0.972]
 [0.886]
 [0.862]
 [1.082]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  105.44680735007823
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 92.571]
 [ 99.104]
 [102.756]
 [ 88.472]
 [ 96.786]] [[1.115]
 [1.213]
 [1.268]
 [1.053]
 [1.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7328404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.933]
 [56.912]
 [50.933]
 [50.653]
 [50.933]] [[1.302]
 [1.592]
 [1.302]
 [1.289]
 [1.302]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.68737531419889
printing an ep nov before normalisation:  7.9786288553435725
siam score:  -0.725596
printing an ep nov before normalisation:  49.25717099956449
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  80.67129878961143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.764067298165173
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.122389793395996
printing an ep nov before normalisation:  39.18984731038412
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.232]
 [50.836]
 [33.554]
 [27.64 ]
 [23.531]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.285032419294645
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9939,     0.0000,     0.0000,     0.0034,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0014, 0.9706, 0.0045, 0.0022, 0.0214], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0005,     0.9865,     0.0040,     0.0090],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0002,     0.0202,     0.7734,     0.2058],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0025, 0.0029, 0.0499, 0.3400, 0.6046], grad_fn=<DivBackward0>)
siam score:  -0.7415858
siam score:  -0.7444626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74290055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.903484638240087
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.74134315343505
printing an ep nov before normalisation:  59.89990148670461
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.08586862566955
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.649]
 [32.649]
 [32.649]
 [32.649]
 [32.649]] [[0.081]
 [0.081]
 [0.081]
 [0.081]
 [0.081]]
printing an ep nov before normalisation:  53.46598627365841
printing an ep nov before normalisation:  68.73156616098498
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.531]
 [46.52 ]
 [37.113]
 [27.061]
 [28.256]] [[0.257]
 [0.842]
 [0.603]
 [0.347]
 [0.377]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.270636558532715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74042344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7384183
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.04933938309901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.73574847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74805224
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.18613662797355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.14227123068347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.80021910591926
actions average: 
K:  2  action  0 :  tensor([0.9421, 0.0074, 0.0154, 0.0024, 0.0327], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0344, 0.9141, 0.0209, 0.0100, 0.0207], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0003,     0.7449,     0.1364,     0.1184],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.1800,     0.0015,     0.0003,     0.5761,     0.2422],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0490, 0.1215, 0.0627, 0.3507, 0.4162], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.74260485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.75074095
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.32164502590723
printing an ep nov before normalisation:  68.53875770163533
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.955]
 [59.766]
 [62.919]
 [60.955]
 [60.955]] [[0.942]
 [0.907]
 [1.   ]
 [0.942]
 [0.942]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.23142610269932
printing an ep nov before normalisation:  42.68816187438653
printing an ep nov before normalisation:  14.388615816557959
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.597339884132225
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.30314447575698
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.132510525387055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.760979
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.36346616409908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.13595771789551
siam score:  -0.76390684
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.392]
 [71.233]
 [76.234]
 [72.011]
 [73.638]] [[1.321]
 [1.713]
 [1.879]
 [1.739]
 [1.793]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.50299112236134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  86.8807463425171
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.855]
 [50.638]
 [48.161]
 [50.638]
 [50.638]] [[1.061]
 [1.13 ]
 [1.034]
 [1.13 ]
 [1.13 ]]
Starting evaluation
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  48.58329170247846
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.653]
 [40.475]
 [35.863]
 [34.502]
 [34.461]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
siam score:  -0.75392187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.38031697682995
siam score:  -0.75177693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.27148533868017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.571]
 [53.411]
 [44.592]
 [53.016]
 [51.914]] [[1.367]
 [1.44 ]
 [1.091]
 [1.424]
 [1.381]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.666]
 [57.015]
 [66.869]
 [71.56 ]
 [70.901]] [[0.284]
 [0.454]
 [0.616]
 [0.693]
 [0.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.20269038640049
line 256 mcts: sample exp_bonus 43.11397444273192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.14534950256348
printing an ep nov before normalisation:  67.59640528412598
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[121.073]
 [121.073]
 [121.073]
 [118.397]
 [124.932]] [[1.265]
 [1.265]
 [1.265]
 [1.218]
 [1.333]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  107.97668203018047
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.73940658569336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.33061489099896
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.28 ]
 [63.73 ]
 [59.697]
 [59.28 ]
 [59.485]] [[1.176]
 [1.264]
 [1.184]
 [1.176]
 [1.18 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.10667252629966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  93.79721816169203
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.58230590360934
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.308 0.59  0.026 0.051 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.8251702444894
printing an ep nov before normalisation:  75.08421588574184
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.76556534
printing an ep nov before normalisation:  70.51379126611343
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.39191438151165
actions average: 
K:  2  action  0 :  tensor([    0.9649,     0.0194,     0.0000,     0.0004,     0.0153],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0466,     0.9261,     0.0002,     0.0001,     0.0270],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0147,     0.8234,     0.0383,     0.1236],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0334, 0.0009, 0.0462, 0.6453, 0.2741], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0156, 0.0106, 0.0583, 0.3223, 0.5932], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.73581208130749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.8628411824852
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.16232933191561
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  1  action  0 :  tensor([    0.9732,     0.0014,     0.0000,     0.0073,     0.0181],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0056, 0.9176, 0.0523, 0.0047, 0.0199], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9320,     0.0331,     0.0348],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0365, 0.0288, 0.0058, 0.6460, 0.2829], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0098, 0.0159, 0.0182, 0.3577, 0.5985], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [89.97 ]
 [ 0.   ]
 [93.495]
 [89.291]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  56.68546694161736
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.43712425231934
printing an ep nov before normalisation:  64.60314822485802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  118.22977714158958
printing an ep nov before normalisation:  58.951563749650504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  117.94084696591078
printing an ep nov before normalisation:  23.946604532113156
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77326006
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[102.761]
 [102.761]
 [ 94.843]
 [102.761]
 [102.761]] [[0.574]
 [0.574]
 [0.487]
 [0.574]
 [0.574]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 2 threads
Frames:  12156 train batches done:  1421 episodes:  862
printing an ep nov before normalisation:  96.49076752363146
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.79176606699467
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  84.6486728152676
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.089]
 [60.664]
 [52.205]
 [60.664]
 [60.664]] [[0.888]
 [1.   ]
 [0.734]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.9132, 0.0068, 0.0044, 0.0204, 0.0551], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9471,     0.0001,     0.0010,     0.0509],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0003,     0.9049,     0.0404,     0.0543],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0116,     0.0683,     0.6501,     0.2697],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0007, 0.0017, 0.1252, 0.2967, 0.5757], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.26083706254285
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76701826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.48234374992958
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.58036994934082
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.453]
 [73.452]
 [59.306]
 [61.338]
 [61.41 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.079]
 [57.079]
 [57.079]
 [63.26 ]
 [57.079]] [[1.019]
 [1.019]
 [1.019]
 [1.207]
 [1.019]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  71.04678485371245
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.545]
 [60.713]
 [53.946]
 [49.451]
 [48.094]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  7.877434433129906
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[103.121]
 [ 74.103]
 [ 40.825]
 [ 36.526]
 [ 44.003]] [[0.443]
 [0.25 ]
 [0.029]
 [0.   ]
 [0.05 ]]
UNIT TEST: sample policy line 217 mcts : [0.41  0.256 0.103 0.051 0.179]
printing an ep nov before normalisation:  79.37893333542151
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.744]
 [61.399]
 [64.244]
 [50.305]
 [57.324]] [[0.136]
 [0.212]
 [0.228]
 [0.15 ]
 [0.189]]
printing an ep nov before normalisation:  90.64382573658567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.58557820650469
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.948]
 [27.035]
 [31.354]
 [29.203]
 [28.643]] [[0.796]
 [0.3  ]
 [0.39 ]
 [0.345]
 [0.334]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  109.76645746249528
UNIT TEST: sample policy line 217 mcts : [0.205 0.282 0.179 0.077 0.256]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.86098350268597
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  91.53507723460795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.37087603134448
printing an ep nov before normalisation:  81.44458770751953
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.161]
 [66.161]
 [66.161]
 [66.161]
 [66.161]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
actions average: 
K:  0  action  0 :  tensor([0.9552, 0.0050, 0.0058, 0.0127, 0.0213], grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0018, 0.9033, 0.0237, 0.0219, 0.0493], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0006,     0.9993,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0961,     0.5949,     0.3088],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0002,     0.0029,     0.1926,     0.2803,     0.5239],
       grad_fn=<DivBackward0>)
siam score:  -0.7609668
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.688]
 [60.404]
 [81.55 ]
 [43.276]
 [44.431]] [[0.597]
 [0.671]
 [1.093]
 [0.329]
 [0.352]]
printing an ep nov before normalisation:  79.28778819187454
printing an ep nov before normalisation:  22.08656076853572
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  85.47944535008631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
deleting a thread, now have 1 threads
Frames:  13726 train batches done:  1608 episodes:  964
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9911,     0.0020,     0.0000,     0.0063,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0471,     0.9499,     0.0005,     0.0003,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0023,     0.9040,     0.0334,     0.0603],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0806,     0.0006,     0.0131,     0.6313,     0.2745],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0392, 0.0066, 0.0651, 0.3402, 0.5489], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.411]
 [60.411]
 [60.411]
 [60.411]
 [60.411]] [[0.384]
 [0.384]
 [0.384]
 [0.384]
 [0.384]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.718]
 [62.519]
 [62.519]
 [62.519]
 [62.519]] [[0.46 ]
 [0.351]
 [0.351]
 [0.351]
 [0.351]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  119.50697365052217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.19475891514318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.92255087403434
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.203]
 [71.743]
 [80.745]
 [73.203]
 [73.203]] [[1.442]
 [1.413]
 [1.59 ]
 [1.442]
 [1.442]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 92.363]
 [108.565]
 [108.565]
 [108.565]
 [108.565]] [[0.7  ]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
printing an ep nov before normalisation:  58.92055446925256
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.96479194203181
printing an ep nov before normalisation:  29.02458097200582
actions average: 
K:  3  action  0 :  tensor([    0.9666,     0.0004,     0.0000,     0.0198,     0.0132],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0461, 0.8812, 0.0167, 0.0105, 0.0456], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0029,     0.8804,     0.0614,     0.0554],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0165, 0.0039, 0.0016, 0.7664, 0.2116], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0099, 0.2077, 0.0028, 0.3311, 0.4484], grad_fn=<DivBackward0>)
siam score:  -0.75862944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7620639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7635429
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.79290895983361
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.415]
 [38.415]
 [38.415]
 [62.722]
 [38.415]] [[0.286]
 [0.286]
 [0.286]
 [1.128]
 [0.286]]
printing an ep nov before normalisation:  45.122779977488285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.74515399679
printing an ep nov before normalisation:  39.3400218747165
printing an ep nov before normalisation:  37.04272985458374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.49890835659412
printing an ep nov before normalisation:  85.00749668200021
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.6417336628319
printing an ep nov before normalisation:  35.72292952813914
printing an ep nov before normalisation:  83.47287178039551
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  103.53956216899286
printing an ep nov before normalisation:  12.50453772434246
printing an ep nov before normalisation:  42.65584118283488
printing an ep nov before normalisation:  53.83524043825231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.80638263465144
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9628,     0.0009,     0.0000,     0.0041,     0.0322],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9918,     0.0021,     0.0004,     0.0056],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0232,     0.9416,     0.0016,     0.0335],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0001,     0.0004,     0.7989,     0.2004],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0118, 0.0345, 0.0949, 0.3306, 0.5282], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.08484465800592
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.32374360323021
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[89.812]
 [78.389]
 [72.235]
 [81.089]
 [80.935]] [[1.379]
 [1.06 ]
 [0.888]
 [1.135]
 [1.131]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.77859822746967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.87705988021231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.866914527623976
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.128 0.128 0.103 0.205 0.436]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [82.116]
 [83.578]
 [ 0.   ]
 [ 0.   ]] [[-0.185]
 [ 0.307]
 [ 0.316]
 [-0.185]
 [-0.185]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77652067
printing an ep nov before normalisation:  39.88342856527868
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.77467406
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.31031589005378
printing an ep nov before normalisation:  115.2288627834729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.76836116042188
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.89299621428134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  82.43794096443568
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.31748819351196
printing an ep nov before normalisation:  99.1180451619429
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.919288286482583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.733]
 [57.23 ]
 [53.291]
 [58.077]
 [56.286]] [[1.308]
 [1.121]
 [0.958]
 [1.156]
 [1.082]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.15974831029746
printing an ep nov before normalisation:  57.61557950947181
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.429]
 [82.873]
 [99.319]
 [83.429]
 [83.429]] [[1.157]
 [1.141]
 [1.607]
 [1.157]
 [1.157]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  99.1113071847974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 76.54713093665288
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.147]
 [43.073]
 [45.461]
 [43.118]
 [42.495]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.3272047812261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.323244656878984
actions average: 
K:  2  action  0 :  tensor([    0.9894,     0.0005,     0.0003,     0.0071,     0.0028],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9847,     0.0002,     0.0004,     0.0147],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9352,     0.0316,     0.0333],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0100,     0.0000,     0.0139,     0.7907,     0.1853],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.0014, 0.0213, 0.3682, 0.6079], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.02297878265381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.81091289059255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.57014313549514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.62246665301245
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7590553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  136.53298194776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.85314411040186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
siam score:  -0.76498544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.217480386522254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.8802, 0.0065, 0.0034, 0.0539, 0.0560], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0013, 0.9761, 0.0026, 0.0015, 0.0185], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0008,     0.9559,     0.0009,     0.0421],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0001,     0.0359,     0.5731,     0.3907],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0239, 0.0160, 0.0632, 0.2218, 0.6750], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  79.26114212873644
printing an ep nov before normalisation:  56.618392082640774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  3  action  0 :  tensor([    0.9936,     0.0014,     0.0000,     0.0007,     0.0044],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0304,     0.9468,     0.0035,     0.0004,     0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0036,     0.8719,     0.0430,     0.0814],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0110, 0.0167, 0.0849, 0.6921, 0.1953], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0265, 0.1213, 0.0553, 0.3418, 0.4551], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  139.59402913882712
printing an ep nov before normalisation:  24.978957229419937
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.10684582294944
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  96.7547845778108
printing an ep nov before normalisation:  0.0497073310228302
printing an ep nov before normalisation:  53.81976287879286
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.538]
 [69.611]
 [66.924]
 [65.778]
 [66.103]] [[1.329]
 [1.795]
 [1.726]
 [1.697]
 [1.705]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.94930012389213
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.681023229122104
printing an ep nov before normalisation:  74.47265895274727
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.046018999728936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.72638047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  78.29646328336392
printing an ep nov before normalisation:  72.90665978833077
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  101.07954837391006
printing an ep nov before normalisation:  50.03269863692802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.188]
 [53.572]
 [58.327]
 [65.285]
 [59.831]] [[0.701]
 [0.472]
 [0.558]
 [0.684]
 [0.585]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  81.03841131872724
printing an ep nov before normalisation:  52.809130595387145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  99.20643544400976
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  108.62556968911863
printing an ep nov before normalisation:  97.75693346379221
printing an ep nov before normalisation:  42.3337984085083
printing an ep nov before normalisation:  29.67215061187744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  83.5654032205184
printing an ep nov before normalisation:  69.22792196273804
printing an ep nov before normalisation:  73.97087858816911
printing an ep nov before normalisation:  87.01809770019342
printing an ep nov before normalisation:  94.55028778834591
printing an ep nov before normalisation:  43.6525821685791
siam score:  -0.76227224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76417595
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.5066309258976
UNIT TEST: sample policy line 217 mcts : [0.795 0.    0.205 0.    0.   ]
printing an ep nov before normalisation:  61.60220015556828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.85996580123901
printing an ep nov before normalisation:  88.47006691826714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.68546346566285
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.744]
 [77.485]
 [69.069]
 [69.069]
 [69.893]] [[0.95 ]
 [0.927]
 [0.776]
 [0.776]
 [0.791]]
printing an ep nov before normalisation:  75.73667561201839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.61225088683871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.462 0.179 0.103 0.179 0.077]
printing an ep nov before normalisation:  54.69170475512857
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.669]
 [55.193]
 [62.379]
 [62.517]
 [65.201]] [[0.836]
 [0.77 ]
 [0.963]
 [0.967]
 [1.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.93963477950276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.821263502253814
printing an ep nov before normalisation:  65.2005428262241
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 58.948969053666325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  76.37963828983207
printing an ep nov before normalisation:  80.06485303243002
printing an ep nov before normalisation:  67.12229507526231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.33366096155146
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  305000
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
STARTED EXPV TRAINING ON FRAME NO.  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.08970355987549
printing an ep nov before normalisation:  54.40969719257753
printing an ep nov before normalisation:  34.04694820810727
printing an ep nov before normalisation:  81.5278864427791
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  139.40813082344465
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.2307658516217
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.312668899810713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.244461373462286
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.155137727729365
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  81.30065373011998
printing an ep nov before normalisation:  73.29915824786656
printing an ep nov before normalisation:  65.0444795465608
printing an ep nov before normalisation:  78.29747933303598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.566]
 [39.011]
 [40.511]
 [42.172]
 [36.728]] [[1.205]
 [0.976]
 [1.051]
 [1.135]
 [0.861]]
printing an ep nov before normalisation:  50.12589899056361
printing an ep nov before normalisation:  58.538899023268236
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 42.188960405872656
printing an ep nov before normalisation:  40.95452951805604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.154064107002302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.168186202078374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.20466709136963
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.09462384751567043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[112.772]
 [112.772]
 [112.772]
 [112.772]
 [112.772]] [[1.604]
 [1.604]
 [1.604]
 [1.604]
 [1.604]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.76913863
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  105.53126801843078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.342]
 [59.079]
 [65.929]
 [55.688]
 [54.656]] [[0.325]
 [0.323]
 [0.395]
 [0.287]
 [0.276]]
printing an ep nov before normalisation:  27.042842165772186
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.21 ]
 [35.808]
 [32.992]
 [28.381]
 [22.85 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.19191642100228
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.154 0.41  0.103 0.179 0.154]
line 256 mcts: sample exp_bonus 124.69771347586216
printing an ep nov before normalisation:  61.90091903557744
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  123.5330313396925
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.926411283850474
printing an ep nov before normalisation:  52.4319301390501
siam score:  -0.7838445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.911038704005477
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78117645
line 256 mcts: sample exp_bonus 62.97505779624124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.174]
 [52.277]
 [51.81 ]
 [40.303]
 [37.733]] [[0.701]
 [1.351]
 [1.333]
 [0.896]
 [0.798]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.8330820560502161
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  False
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.32880131898688586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 64.93330688379774
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.751053915402053
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.67844861868663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.23969413629573
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.73947721172499
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  133.33732406581913
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.56318879238762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.63387672700631
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.685]
 [40.685]
 [40.685]
 [40.685]
 [40.685]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  83.51092996943507
siam score:  -0.78380644
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.51514824398403
printing an ep nov before normalisation:  16.664533615112305
printing an ep nov before normalisation:  25.636673147935632
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.73033666610718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.83508189660486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.892]
 [60.939]
 [60.939]
 [60.939]
 [60.939]] [[1.227]
 [1.161]
 [1.161]
 [1.161]
 [1.161]]
printing an ep nov before normalisation:  44.38482933823115
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[86.224]
 [86.224]
 [86.224]
 [86.224]
 [86.224]] [[1.241]
 [1.241]
 [1.241]
 [1.241]
 [1.241]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.779707
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.929]
 [52.034]
 [58.772]
 [39.255]
 [40.002]] [[0.893]
 [1.139]
 [1.409]
 [0.625]
 [0.655]]
printing an ep nov before normalisation:  56.39672300447095
printing an ep nov before normalisation:  63.3987328160368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.16 ]
 [46.733]
 [47.42 ]
 [46.745]
 [47.447]] [[1.613]
 [1.517]
 [1.563]
 [1.518]
 [1.565]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.55233742872569
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[102.516]
 [102.516]
 [102.516]
 [102.516]
 [102.516]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.832193186555465
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7875514
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.46340652585943
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.972]
 [56.891]
 [20.297]
 [20.328]
 [20.999]] [[0.094]
 [0.254]
 [0.052]
 [0.052]
 [0.056]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  93.93372530032553
printing an ep nov before normalisation:  39.66589204235781
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 100.48035650782622
line 256 mcts: sample exp_bonus 80.20383068570028
actions average: 
K:  4  action  0 :  tensor([    0.9964,     0.0002,     0.0000,     0.0012,     0.0023],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9748,     0.0028,     0.0016,     0.0200],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0005,     0.9531,     0.0196,     0.0268],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0005,     0.0352,     0.7058,     0.2579],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0040, 0.0427, 0.0524, 0.2129, 0.6881], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.077 0.103 0.128 0.256 0.436]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.3918494937596
siam score:  -0.76560557
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 66.81833083148106
printing an ep nov before normalisation:  39.142396450042725
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.23631266999249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77646095
using explorer policy with actor:  1
siam score:  -0.7786525
printing an ep nov before normalisation:  75.04788626209303
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.22065481796019
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77910066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.13314810751115
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.434]
 [41.059]
 [30.165]
 [28.16 ]
 [30.196]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  27.0827040147249
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.76 ]
 [47.071]
 [49.76 ]
 [40.587]
 [49.76 ]] [[2.194]
 [2.   ]
 [2.194]
 [1.533]
 [2.194]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.363]
 [55.158]
 [41.928]
 [40.587]
 [40.416]] [[1.411]
 [1.443]
 [0.911]
 [0.857]
 [0.85 ]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 48.89282755559086
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.77290577939744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.413771629333496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7900264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86675321253531
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7909685
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  112.61938277783183
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7926665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.043]
 [54.144]
 [54.149]
 [48.289]
 [48.547]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.77831316
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7760007
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.242]
 [46.852]
 [59.164]
 [54.051]
 [53.618]] [[0.207]
 [0.174]
 [0.266]
 [0.228]
 [0.225]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.29658177372757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.58664477551027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.437550342190754
printing an ep nov before normalisation:  62.166214290331254
actions average: 
K:  4  action  0 :  tensor([0.9821, 0.0038, 0.0014, 0.0053, 0.0074], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0005,     0.9678,     0.0105,     0.0001,     0.0210],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0119,     0.9389,     0.0255,     0.0237],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0666, 0.0009, 0.1385, 0.5378, 0.2561], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0074, 0.1145, 0.0350, 0.1735, 0.6696], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.80716781604223
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 60.881885628057766
printing an ep nov before normalisation:  59.70565466335475
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77485406
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  63.87058118941437
printing an ep nov before normalisation:  81.83461111061155
printing an ep nov before normalisation:  118.60109949756678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.57172037919272
printing an ep nov before normalisation:  69.06736375393432
printing an ep nov before normalisation:  98.17139668628495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.13032935378453
actions average: 
K:  0  action  0 :  tensor([    0.9967,     0.0001,     0.0000,     0.0005,     0.0028],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9822,     0.0006,     0.0012,     0.0157],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0021, 0.0015, 0.9536, 0.0251, 0.0177], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0070, 0.0013, 0.0089, 0.7346, 0.2482], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0102, 0.0046, 0.0670, 0.3538, 0.5645], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79758924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.583]
 [50.18 ]
 [46.81 ]
 [47.582]
 [46.863]] [[1.14 ]
 [1.042]
 [0.904]
 [0.936]
 [0.906]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.908]
 [66.581]
 [37.908]
 [54.808]
 [37.908]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  72.34653472457181
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  107.72995847598506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  130.1041792007138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7957535
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.358859062194824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.31726477742555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.74889503935624
printing an ep nov before normalisation:  73.76198768615723
printing an ep nov before normalisation:  55.95121383666992
printing an ep nov before normalisation:  64.59995990924035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.99603291013112
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.74 ]
 [56.823]
 [61.396]
 [57.681]
 [56.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  60.41237008221748
actions average: 
K:  3  action  0 :  tensor([    0.9912,     0.0014,     0.0000,     0.0040,     0.0034],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0015, 0.9443, 0.0024, 0.0014, 0.0504], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9976,     0.0006,     0.0018],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0001,     0.0042,     0.7896,     0.2057],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0026, 0.0141, 0.0236, 0.2295, 0.7302], grad_fn=<DivBackward0>)
actions average: 
K:  0  action  0 :  tensor([    0.9610,     0.0014,     0.0001,     0.0229,     0.0147],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0012,     0.9838,     0.0013,     0.0003,     0.0135],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0105,     0.9525,     0.0006,     0.0364],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0003,     0.0049,     0.7307,     0.2634],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0531, 0.0008, 0.0169, 0.3332, 0.5959], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.77790695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  96.79139340643827
printing an ep nov before normalisation:  66.34551525115967
printing an ep nov before normalisation:  40.419891931856924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  55.73544473509473
printing an ep nov before normalisation:  111.60398857902592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.86738615459342
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.01693534851074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.03128004553847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.26484209083895
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  84.14798204955665
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.460302721475575
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.274]
 [81.281]
 [60.903]
 [86.956]
 [80.371]] [[0.597]
 [0.579]
 [0.394]
 [0.631]
 [0.571]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.541205406188965
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.271]
 [46.786]
 [33.756]
 [40.271]
 [40.271]] [[0.978]
 [1.137]
 [0.82 ]
 [0.978]
 [0.978]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.79960088239069
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  108.14870624879465
printing an ep nov before normalisation:  0.05959019721249856
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.6236867660192
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.84451293945312
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.03892125542342
printing an ep nov before normalisation:  67.51084777722261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.76592207
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.46744221075711
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.753]
 [45.646]
 [54.752]
 [61.432]
 [45.646]] [[0.448]
 [0.403]
 [0.483]
 [0.542]
 [0.403]]
siam score:  -0.76443404
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.828]
 [51.901]
 [39.995]
 [55.423]
 [51.934]] [[1.355]
 [1.02 ]
 [0.686]
 [1.119]
 [1.021]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.360587096262805
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.77903722097583
using explorer policy with actor:  1
siam score:  -0.7689975
siam score:  -0.7704095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9842,     0.0001,     0.0000,     0.0085,     0.0072],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9948,     0.0014,     0.0000,     0.0036],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0180,     0.9415,     0.0253,     0.0151],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0009, 0.0041, 0.0021, 0.7576, 0.2353], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0488, 0.0441, 0.0143, 0.2250, 0.6679], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  8.701633175637369
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.07566677965459
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.30020203033304
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.84612595471526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9887,     0.0016,     0.0001,     0.0047,     0.0049],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9929,     0.0001,     0.0001,     0.0062],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0025,     0.9315,     0.0390,     0.0269],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0023,     0.0005,     0.0280,     0.7684,     0.2007],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0007, 0.0082, 0.3826, 0.6071], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 89.11247998342914
printing an ep nov before normalisation:  78.48165863652332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.40840868810155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[83.049]
 [40.138]
 [40.138]
 [40.138]
 [40.138]] [[1.31 ]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[119.934]
 [ 74.02 ]
 [ 74.02 ]
 [ 74.02 ]
 [ 74.02 ]] [[1.272]
 [0.583]
 [0.583]
 [0.583]
 [0.583]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  108.6265215145275
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.509]
 [53.937]
 [56.426]
 [61.58 ]
 [58.591]] [[1.076]
 [0.648]
 [0.702]
 [0.815]
 [0.75 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.70188283920288
printing an ep nov before normalisation:  55.41986888263446
printing an ep nov before normalisation:  63.82928399309348
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.109]
 [67.109]
 [67.109]
 [67.109]
 [67.109]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
printing an ep nov before normalisation:  49.038797645264886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.7596718
printing an ep nov before normalisation:  70.48827365414765
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.205 0.436 0.103 0.231 0.026]
printing an ep nov before normalisation:  40.32027819951576
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.736853043479364
printing an ep nov before normalisation:  45.92424392700195
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.24187863290396
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.47036405425908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.94559833554213
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.968]
 [46.409]
 [51.349]
 [46.575]
 [45.488]] [[0.808]
 [0.706]
 [0.848]
 [0.711]
 [0.68 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.78912646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.036781311035156
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.66886077073026
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  7.004135498306283
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  72.42683410644531
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.7204932598336
siam score:  -0.7924802
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.89888192669838
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.77446288878009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79372025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.174409359290685
Starting evaluation
siam score:  -0.7877864
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  92.19546437116688
printing an ep nov before normalisation:  92.6095392138658
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.716]
 [56.391]
 [84.831]
 [65.853]
 [64.359]] [[0.099]
 [0.126]
 [0.228]
 [0.16 ]
 [0.155]]
printing an ep nov before normalisation:  73.98809667702146
printing an ep nov before normalisation:  34.14596346269341
printing an ep nov before normalisation:  27.941339633855826
siam score:  -0.78534055
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.12137144728763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.817]
 [16.817]
 [16.817]
 [17.   ]
 [17.596]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.6257219907878
line 256 mcts: sample exp_bonus 38.98671042910975
printing an ep nov before normalisation:  46.111962471922695
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.88]
 [29.88]
 [29.88]
 [29.88]
 [29.88]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.77826935904367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.45892632565805
printing an ep nov before normalisation:  52.52687885162448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9294,     0.0095,     0.0001,     0.0187,     0.0422],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0025,     0.9886,     0.0000,     0.0001,     0.0088],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0006,     0.9428,     0.0177,     0.0389],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0092, 0.0012, 0.0075, 0.7362, 0.2459], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0296, 0.0272, 0.0710, 0.1951, 0.6770], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  80.49191382666694
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.248]
 [50.248]
 [71.151]
 [50.248]
 [50.248]] [[0.583]
 [0.583]
 [1.106]
 [0.583]
 [0.583]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.264549255371094
printing an ep nov before normalisation:  29.470166820597353
actions average: 
K:  3  action  0 :  tensor([    0.9553,     0.0006,     0.0001,     0.0192,     0.0248],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9743,     0.0169,     0.0000,     0.0087],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0042,     0.9495,     0.0314,     0.0145],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0004,     0.0324,     0.6238,     0.3432],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0003,     0.1982,     0.1649,     0.1861,     0.4505],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.60688354339766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.03520339276641
printing an ep nov before normalisation:  66.68881971239122
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.31 ]
 [43.562]
 [55.116]
 [59.151]
 [57.81 ]] [[1.252]
 [0.738]
 [1.093]
 [1.217]
 [1.175]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.706]
 [45.106]
 [54.171]
 [49.545]
 [50.458]] [[0.349]
 [0.217]
 [0.305]
 [0.26 ]
 [0.269]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 2.2695511617701468e-11
0.0 8.995172286904403e-12
0.0 5.500893817066579e-12
0.0 9.44493090106838e-12
0.0 5.172224064053752e-11
0.0 3.120632846185396e-11
0.0 2.0671597854970465e-11
0.0 1.5188002441726094e-11
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.226]
 [60.605]
 [47.662]
 [47.662]
 [47.662]] [[1.049]
 [0.849]
 [0.458]
 [0.458]
 [0.458]]
printing an ep nov before normalisation:  69.53652677635762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.539]
 [30.561]
 [44.78 ]
 [46.996]
 [39.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.7875992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.19662666320801
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.03021176659199
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.48774022655126
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.90100009140812
printing an ep nov before normalisation:  47.2155531959115
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.169]
 [66.358]
 [66.358]
 [71.039]
 [66.358]] [[1.646]
 [1.694]
 [1.694]
 [1.883]
 [1.694]]
printing an ep nov before normalisation:  69.73386129892985
printing an ep nov before normalisation:  71.94091796875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9790,     0.0098,     0.0003,     0.0012,     0.0098],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9990,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9687,     0.0107,     0.0204],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0068, 0.0061, 0.0146, 0.6724, 0.3001], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0009, 0.0009, 0.0267, 0.3524, 0.6190], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.46635439601086
siam score:  -0.79931074
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.765436083263715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.3475623296947
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.09101213750922
printing an ep nov before normalisation:  61.698039025059416
printing an ep nov before normalisation:  51.1976593159263
printing an ep nov before normalisation:  33.748302002653084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.075]
 [27.075]
 [29.831]
 [27.075]
 [27.075]] [[1.373]
 [1.373]
 [1.546]
 [1.373]
 [1.373]]
printing an ep nov before normalisation:  26.324853705799857
printing an ep nov before normalisation:  92.27045378381335
printing an ep nov before normalisation:  97.56522109229411
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.465]
 [35.106]
 [78.98 ]
 [50.19 ]
 [39.052]] [[0.346]
 [0.231]
 [1.385]
 [0.628]
 [0.335]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[91.365]
 [73.861]
 [73.861]
 [73.861]
 [73.861]] [[0.883]
 [0.614]
 [0.614]
 [0.614]
 [0.614]]
printing an ep nov before normalisation:  47.00128192815882
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.744]
 [52.995]
 [70.873]
 [27.878]
 [36.398]] [[0.482]
 [0.462]
 [0.666]
 [0.176]
 [0.273]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.194]
 [40.355]
 [52.581]
 [54.068]
 [39.21 ]] [[0.373]
 [0.417]
 [0.667]
 [0.697]
 [0.394]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  58.982508523123606
printing an ep nov before normalisation:  57.70114742050406
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.1178125728491
printing an ep nov before normalisation:  60.37871748352855
actions average: 
K:  2  action  0 :  tensor([    0.9961,     0.0000,     0.0000,     0.0019,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0007,     0.9859,     0.0004,     0.0001,     0.0129],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0218,     0.7925,     0.0967,     0.0890],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0210,     0.0002,     0.0005,     0.7839,     0.1944],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0265, 0.0033, 0.0320, 0.3263, 0.6119], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9468,     0.0004,     0.0000,     0.0271,     0.0257],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9796,     0.0185,     0.0000,     0.0018],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0120,     0.9807,     0.0002,     0.0072],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0006,     0.0439,     0.6943,     0.2611],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0082, 0.0103, 0.1404, 0.3312, 0.5099], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  68.4472592594668
siam score:  -0.79105914
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.02005696771050225
siam score:  -0.7921106
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  124.16012429777737
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.46913167000827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.51821479714947
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.72]
 [35.72]
 [35.72]
 [35.72]
 [35.72]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  115.95103641830494
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.712]
 [67.712]
 [89.874]
 [86.944]
 [67.712]] [[1.209]
 [1.209]
 [1.609]
 [1.556]
 [1.209]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  13.800888314605686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[114.976]
 [ 88.11 ]
 [ 88.11 ]
 [ 88.11 ]
 [ 88.11 ]] [[0.581]
 [0.445]
 [0.445]
 [0.445]
 [0.445]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9858,     0.0007,     0.0002,     0.0132],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0126,     0.0008,     0.9833,     0.0031,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0004,     0.0007,     0.7516,     0.2469],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0358, 0.0009, 0.0029, 0.3605, 0.5998], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  12.756004866425826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  0.01900966420180339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.955850900930024
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.174046129278125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.525293032612254
siam score:  -0.78398573
printing an ep nov before normalisation:  52.83881305500135
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.05883115791738
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.08855112082816
siam score:  -0.78635097
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.566]
 [38.482]
 [51.815]
 [37.386]
 [37.85 ]] [[0.492]
 [0.651]
 [1.084]
 [0.616]
 [0.631]]
printing an ep nov before normalisation:  58.77916793454289
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.558617690310435
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.614085247444066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.18261424310805
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  105.98929640043478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.94095676929047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.17386536214061
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.56157872486398
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8029542
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 55.8160086644524
printing an ep nov before normalisation:  66.2464630126366
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.50982804548757
using explorer policy with actor:  1
printing an ep nov before normalisation:  82.62919699608456
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.303]
 [55.301]
 [57.355]
 [54.473]
 [55.301]] [[0.843]
 [1.079]
 [1.139]
 [1.054]
 [1.079]]
siam score:  -0.7996075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.86169338226318
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  86.18364064330916
printing an ep nov before normalisation:  72.67375416559388
printing an ep nov before normalisation:  51.105881665704096
using explorer policy with actor:  1
siam score:  -0.75805587
printing an ep nov before normalisation:  21.621575054734592
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [85.344]
 [ 0.   ]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.8774,     0.0016,     0.0000,     0.0638,     0.0572],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0231, 0.8663, 0.0133, 0.0047, 0.0926], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0077, 0.0017, 0.8898, 0.0289, 0.0720], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0038, 0.0011, 0.0012, 0.8103, 0.1835], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0251, 0.0012, 0.0024, 0.2467, 0.7246], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  65.82931108886272
siam score:  -0.74596757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.676]
 [69.676]
 [69.676]
 [69.676]
 [69.676]] [[0.243]
 [0.243]
 [0.243]
 [0.243]
 [0.243]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.19550704956055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.1276041003849
siam score:  -0.77114594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.955]
 [70.36 ]
 [74.444]
 [72.853]
 [60.533]] [[1.023]
 [1.034]
 [1.143]
 [1.1  ]
 [0.771]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.03254239923244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.60680464298129
printing an ep nov before normalisation:  42.54645911970904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  132.30317620294556
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.052]
 [59.2  ]
 [61.66 ]
 [59.2  ]
 [61.364]] [[0.665]
 [0.63 ]
 [0.676]
 [0.63 ]
 [0.671]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.304]
 [36.131]
 [40.983]
 [27.694]
 [30.337]] [[0.147]
 [0.207]
 [0.25 ]
 [0.132]
 [0.156]]
siam score:  -0.78108114
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.361827090747557
printing an ep nov before normalisation:  96.10272303994402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.0742260314738
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8009541
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.221]
 [57.806]
 [49.542]
 [45.817]
 [45.423]] [[0.603]
 [0.987]
 [0.769]
 [0.671]
 [0.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.726]
 [50.42 ]
 [48.567]
 [41.373]
 [35.537]] [[0.146]
 [0.107]
 [0.101]
 [0.078]
 [0.06 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.79142665863037
printing an ep nov before normalisation:  67.16181221543778
printing an ep nov before normalisation:  87.04222365805026
printing an ep nov before normalisation:  40.65950870513916
printing an ep nov before normalisation:  48.34097526953269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.71883987338617
printing an ep nov before normalisation:  80.2155748812708
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.178]
 [75.178]
 [79.259]
 [75.178]
 [75.178]] [[0.931]
 [0.931]
 [0.981]
 [0.931]
 [0.931]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.961]
 [47.829]
 [53.384]
 [42.522]
 [42.385]] [[0.63 ]
 [0.684]
 [0.842]
 [0.532]
 [0.528]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.68222061423586
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  71.78255671633707
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[104.753]
 [ 65.09 ]
 [ 55.297]
 [ 55.297]
 [ 55.297]] [[1.115]
 [0.585]
 [0.454]
 [0.454]
 [0.454]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.30402052057849
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.011]
 [36.152]
 [51.042]
 [36.995]
 [36.275]] [[0.556]
 [0.657]
 [1.135]
 [0.684]
 [0.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.7957364
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.452016830444336
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.65221977233887
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.18494845065398
printing an ep nov before normalisation:  129.37995606152222
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[136.067]
 [122.235]
 [122.235]
 [122.235]
 [122.235]] [[1.838]
 [1.596]
 [1.596]
 [1.596]
 [1.596]]
printing an ep nov before normalisation:  70.22557539940802
siam score:  -0.80196536
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.40744845841558
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.527]
 [64.527]
 [62.959]
 [64.527]
 [64.527]] [[1.753]
 [1.753]
 [1.691]
 [1.753]
 [1.753]]
printing an ep nov before normalisation:  86.32198272724301
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.349]
 [49.573]
 [50.137]
 [45.657]
 [45.593]] [[1.309]
 [1.532]
 [1.549]
 [1.411]
 [1.409]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.79750893028258
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.13377475738525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  88.8466034014382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.92297894420362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9951,     0.0005,     0.0000,     0.0004,     0.0041],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9644,     0.0240,     0.0012,     0.0103],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9985,     0.0000,     0.0015],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0005,     0.0182,     0.6614,     0.3195],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0011, 0.0136, 0.0538, 0.2848, 0.6467], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.9580,     0.0007,     0.0000,     0.0002,     0.0411],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9987,     0.0004,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0149,     0.9212,     0.0155,     0.0484],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0009,     0.0313,     0.6737,     0.2937],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0016, 0.0059, 0.0023, 0.2407, 0.7495], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  69.77793836876708
siam score:  -0.80811197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  105.88315853979773
actions average: 
K:  0  action  0 :  tensor([    0.9967,     0.0014,     0.0000,     0.0009,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9972,     0.0000,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0043,     0.9086,     0.0272,     0.0599],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0037,     0.0071,     0.0005,     0.7048,     0.2839],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0044, 0.0324, 0.0465, 0.2605, 0.6562], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.943323003776364
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.51 ]
 [65.22 ]
 [68.091]
 [65.22 ]
 [65.22 ]] [[1.72 ]
 [1.71 ]
 [1.808]
 [1.71 ]
 [1.71 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.003865111209134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.80474335
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.82216382777156
printing an ep nov before normalisation:  37.23435401916504
printing an ep nov before normalisation:  46.372737884521484
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.628494727667945
printing an ep nov before normalisation:  94.27687967108706
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 63.315790384214935
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.273]
 [77.273]
 [77.273]
 [77.273]
 [77.273]] [[1.371]
 [1.371]
 [1.371]
 [1.371]
 [1.371]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  94.30470753097335
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.536107628652516
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.381328315739445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9921,     0.0018,     0.0000,     0.0006,     0.0056],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0067,     0.9909,     0.0000,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0171,     0.9337,     0.0044,     0.0440],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0317,     0.0014,     0.0006,     0.7072,     0.2590],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0535, 0.0488, 0.0086, 0.3286, 0.5606], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
siam score:  -0.80120504
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.64681343044246
printing an ep nov before normalisation:  42.95490741729736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.59966044368021
actions average: 
K:  4  action  0 :  tensor([0.9648, 0.0158, 0.0058, 0.0019, 0.0117], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9878,     0.0005,     0.0000,     0.0115],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0073,     0.9532,     0.0002,     0.0392],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0131,     0.0003,     0.0080,     0.7291,     0.2495],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0013, 0.0083, 0.1034, 0.1688, 0.7181], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.37358592530742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.95 ]
 [75.017]
 [90.47 ]
 [68.95 ]
 [70.502]] [[0.842]
 [1.004]
 [1.415]
 [0.842]
 [0.884]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.989438743383516
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.62997809933977
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.01346849264005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  95.20646792772472
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.059184007748314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.639]
 [45.667]
 [45.667]
 [45.667]
 [45.667]] [[1.107]
 [0.829]
 [0.829]
 [0.829]
 [0.829]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.99670283679347
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9953,     0.0015,     0.0000,     0.0011,     0.0021],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9953,     0.0000,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0401,     0.8701,     0.0316,     0.0579],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0017,     0.7115,     0.2865],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0121, 0.0068, 0.0033, 0.3603, 0.6175], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 53.423710767963954
printing an ep nov before normalisation:  60.44107026834966
printing an ep nov before normalisation:  75.42426257962669
printing an ep nov before normalisation:  56.70342266240532
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7933693
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.836]
 [38.515]
 [53.836]
 [40.794]
 [40.68 ]] [[0.889]
 [0.753]
 [1.379]
 [0.846]
 [0.841]]
printing an ep nov before normalisation:  40.77678951936327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 3.8056497994543085e-13
0.0 -2.5688136204716776e-12
0.0 -2.3179867022191184e-12
0.0 -9.808197473566764e-12
0.0 1.3579250492361575e-12
0.0 -3.874843441325295e-12
0.0 0.0
0.0 -5.059784410452344e-12
0.0 0.0
0.0 -2.5428660086969674e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.394]
 [40.394]
 [41.78 ]
 [40.394]
 [40.394]] [[1.706]
 [1.706]
 [1.778]
 [1.706]
 [1.706]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.824]
 [31.824]
 [31.824]
 [31.824]
 [31.824]] [[1.09]
 [1.09]
 [1.09]
 [1.09]
 [1.09]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.42258225881058
printing an ep nov before normalisation:  60.896362251818786
printing an ep nov before normalisation:  76.67225747532483
printing an ep nov before normalisation:  47.90470035240463
printing an ep nov before normalisation:  52.56276567229278
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0097, 0.9315, 0.0023, 0.0048, 0.0517], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0063,     0.9510,     0.0163,     0.0263],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0012,     0.0001,     0.0312,     0.6339,     0.3336],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0034, 0.0141, 0.0873, 0.2653, 0.6299], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.51231129964192
printing an ep nov before normalisation:  107.87162502647006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.095]
 [47.58 ]
 [56.352]
 [45.374]
 [46.585]] [[0.394]
 [0.357]
 [0.487]
 [0.325]
 [0.343]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79946303
siam score:  -0.79970914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.982]
 [64.741]
 [43.818]
 [40.895]
 [42.506]] [[0.813]
 [0.847]
 [0.444]
 [0.388]
 [0.419]]
printing an ep nov before normalisation:  82.76570655973049
printing an ep nov before normalisation:  70.9138621388844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.63843945094518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.07939587315352
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.88222625965184
printing an ep nov before normalisation:  43.49362816421692
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.55654721361435
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.42078546785493
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9631,     0.0001,     0.0000,     0.0195,     0.0172],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0185, 0.9637, 0.0010, 0.0014, 0.0153], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0005,     0.8906,     0.0423,     0.0667],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0059,     0.0005,     0.0250,     0.7274,     0.2412],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1432, 0.0074, 0.0496, 0.1559, 0.6438], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.699]
 [44.802]
 [46.68 ]
 [48.405]
 [43.137]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.91685676574707
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.01522932639639
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.336]
 [47.602]
 [47.602]
 [47.602]
 [47.602]] [[0.209]
 [0.255]
 [0.255]
 [0.255]
 [0.255]]
printing an ep nov before normalisation:  65.88048934936523
printing an ep nov before normalisation:  56.94727897644043
printing an ep nov before normalisation:  34.88272270201571
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  71.9173804608148
printing an ep nov before normalisation:  49.389154768864
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.584907750978118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.521]
 [45.227]
 [50.452]
 [47.307]
 [46.976]] [[0.763]
 [0.966]
 [1.19 ]
 [1.055]
 [1.041]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.425601486926936
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.84 ]
 [41.047]
 [56.221]
 [51.77 ]
 [35.258]] [[0.645]
 [0.712]
 [1.173]
 [1.038]
 [0.536]]
printing an ep nov before normalisation:  60.03640277531158
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.9546, 0.0261, 0.0034, 0.0068, 0.0091], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0019,     0.9852,     0.0051,     0.0002,     0.0077],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9771,     0.0116,     0.0113],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0008,     0.0002,     0.0145,     0.7874,     0.1972],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0332, 0.0527, 0.0378, 0.2876, 0.5887], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  54.756479263305664
printing an ep nov before normalisation:  51.52560822779215
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  49.32857612895785
printing an ep nov before normalisation:  76.68309495995875
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.107]
 [26.645]
 [26.645]
 [26.645]
 [26.645]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  31.25113391544358
printing an ep nov before normalisation:  40.246343561672475
printing an ep nov before normalisation:  29.611742985486558
printing an ep nov before normalisation:  41.49353704817486
printing an ep nov before normalisation:  85.76403002765733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.117188245782174
printing an ep nov before normalisation:  67.45624559117506
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.72845410147398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.019439697265625
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.639]
 [36.426]
 [46.136]
 [35.299]
 [35.144]] [[0.44 ]
 [0.761]
 [1.22 ]
 [0.708]
 [0.7  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0090,     0.9715,     0.0080,     0.0002,     0.0113],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9781,     0.0033,     0.0186],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0005,     0.0002,     0.7620,     0.2364],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0093, 0.0071, 0.0459, 0.3173, 0.6204], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 118.91210165315333
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80731124
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.87546714127936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9992,     0.0005,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9988,     0.0001,     0.0000,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9750,     0.0145,     0.0105],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0007,     0.7873,     0.2117],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0079, 0.0164, 0.0708, 0.2938, 0.6111], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.102]
 [37.645]
 [47.18 ]
 [44.559]
 [44.559]] [[1.412]
 [0.849]
 [1.248]
 [1.138]
 [1.138]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.109]
 [52.125]
 [66.058]
 [72.28 ]
 [52.125]] [[0.921]
 [0.895]
 [1.26 ]
 [1.424]
 [0.895]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.8459,     0.0006,     0.0000,     0.0038,     0.1497],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.8708,     0.1127,     0.0012,     0.0146],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9319,     0.0201,     0.0478],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0007,     0.0426,     0.6635,     0.2931],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0024,     0.1023,     0.2309,     0.6641],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.58127962119593
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.984755768274475
printing an ep nov before normalisation:  42.65109434609171
siam score:  -0.8153717
printing an ep nov before normalisation:  56.82778120088732
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.235]
 [49.634]
 [52.028]
 [36.247]
 [41.621]] [[0.209]
 [0.514]
 [0.559]
 [0.265]
 [0.365]]
siam score:  -0.8160408
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  81.79982247864744
printing an ep nov before normalisation:  46.56016139647165
printing an ep nov before normalisation:  50.245097180758975
printing an ep nov before normalisation:  39.03445687928021
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.39115902628219
printing an ep nov before normalisation:  56.783187835838035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  93.06969366779352
printing an ep nov before normalisation:  58.08799536657098
printing an ep nov before normalisation:  86.30891033157296
siam score:  -0.8070445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.197178329457415
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.87296166988711
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.652]
 [34.652]
 [34.652]
 [34.652]
 [34.652]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.03078246251206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.75050227376763
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.4108131767572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.9433008100908
printing an ep nov before normalisation:  11.076090335845947
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.777]
 [22.777]
 [28.924]
 [22.777]
 [22.777]] [[1.425]
 [1.425]
 [2.   ]
 [1.425]
 [1.425]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.067]
 [14.083]
 [26.492]
 [12.259]
 [12.008]] [[0.441]
 [0.348]
 [0.929]
 [0.263]
 [0.251]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.776]
 [77.774]
 [77.774]
 [77.774]
 [77.774]] [[1.643]
 [1.557]
 [1.557]
 [1.557]
 [1.557]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  77.98489645215562
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.455]
 [54.836]
 [50.008]
 [56.714]
 [56.537]] [[0.963]
 [0.941]
 [0.768]
 [1.009]
 [1.002]]
printing an ep nov before normalisation:  63.391651602130565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.20786718761852
printing an ep nov before normalisation:  65.12184376368944
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7604234
siam score:  -0.7659706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.99981619948684
printing an ep nov before normalisation:  45.96844403543485
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.408]
 [56.492]
 [68.531]
 [60.269]
 [58.558]] [[0.28 ]
 [0.375]
 [0.516]
 [0.419]
 [0.399]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.09609020841319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.8278694152832
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.79683661479649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.358896493911743
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.38078204445075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.26500210100138
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.87032402283816
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7777167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.11592710879988
printing an ep nov before normalisation:  49.79452013406487
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.98971326010568
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.52241788591658
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.671632919771426
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8057264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.501]
 [47.501]
 [47.501]
 [47.501]
 [47.501]] [[0.886]
 [0.886]
 [0.886]
 [0.886]
 [0.886]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0004,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9947,     0.0018,     0.0000,     0.0024],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0026,     0.9489,     0.0210,     0.0274],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0002,     0.0001,     0.8347,     0.1647],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0033, 0.0448, 0.0640, 0.1969, 0.6909], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  95.36401876137187
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.55480504366986
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.059]
 [59.177]
 [57.454]
 [64.846]
 [60.39 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  42.93306137531623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 2.6194979592978314e-10
0.0 4.357296050570498e-10
0.0 1.6599552543664946e-09
0.0 1.1390655855614019e-09
0.0 5.800502249990671e-10
0.0 0.0
0.0 4.809822410240157e-10
0.0 1.800418329283897e-10
0.0 0.0
siam score:  -0.8018872
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.63577041136083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.77653560612492
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.67467065644587
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.86284256211252
siam score:  -0.8081032
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.98]
 [41.98]
 [41.98]
 [41.98]
 [41.98]] [[0.373]
 [0.373]
 [0.373]
 [0.373]
 [0.373]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.18121395338091
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8108445
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.58968717650231
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.5284566925475
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  16.78186969047708
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.73019798813759
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.175959836718505
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  37.893260214818675
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.1354107910174
actions average: 
K:  1  action  0 :  tensor([    0.9990,     0.0001,     0.0000,     0.0005,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9719,     0.0213,     0.0000,     0.0062],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0157,     0.9576,     0.0022,     0.0244],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0022,     0.0002,     0.0146,     0.8403,     0.1427],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0025, 0.0010, 0.0482, 0.2106, 0.7378], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.37594747543335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8054736
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
siam score:  -0.80626035
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.488]
 [59.883]
 [53.488]
 [53.488]
 [53.93 ]] [[1.251]
 [1.551]
 [1.251]
 [1.251]
 [1.271]]
printing an ep nov before normalisation:  20.196055165126232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.808774471282959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.823]
 [18.26 ]
 [20.286]
 [18.26 ]
 [18.26 ]] [[0.857]
 [0.732]
 [0.894]
 [0.732]
 [0.732]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  72.6298186358871
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.56407951387225
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.53605317965455
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.703]
 [37.703]
 [37.703]
 [37.703]
 [37.703]] [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]]
printing an ep nov before normalisation:  45.67635406026728
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9851,     0.0019,     0.0002,     0.0048,     0.0080],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9954,     0.0022,     0.0000,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0044,     0.8981,     0.0495,     0.0481],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0013,     0.0005,     0.0002,     0.6048,     0.3932],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0320, 0.0018, 0.0980, 0.1913, 0.6770], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.50988810160844
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8103918
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.242727341935234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.661]
 [30.661]
 [30.661]
 [30.661]
 [30.661]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.373422622680664
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.70618144176569
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.13132617665818
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.457176366552844
siam score:  -0.8062822
printing an ep nov before normalisation:  41.76367831717047
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.672]
 [60.74 ]
 [58.751]
 [60.526]
 [61.144]] [[1.273]
 [1.317]
 [1.274]
 [1.313]
 [1.326]]
siam score:  -0.80804825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.215]
 [69.244]
 [52.515]
 [52.515]
 [52.515]] [[0.702]
 [1.08 ]
 [0.709]
 [0.709]
 [0.709]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.9238768801328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[94.83 ]
 [80.665]
 [66.72 ]
 [80.665]
 [80.665]] [[2.   ]
 [1.522]
 [1.051]
 [1.522]
 [1.522]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.231280017291965
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.352]
 [24.863]
 [27.497]
 [24.863]
 [24.863]] [[0.997]
 [0.531]
 [0.648]
 [0.531]
 [0.531]]
siam score:  -0.789814
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.340725421905518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  53.114135189551725
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.929]
 [60.697]
 [46.003]
 [55.375]
 [52.062]] [[0.898]
 [1.241]
 [0.941]
 [1.132]
 [1.065]]
printing an ep nov before normalisation:  67.41567377376984
printing an ep nov before normalisation:  24.34595823287964
printing an ep nov before normalisation:  30.422341719440634
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.31147701831935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.846]
 [46.732]
 [64.532]
 [53.604]
 [52.309]] [[0.118]
 [0.195]
 [0.3  ]
 [0.235]
 [0.227]]
printing an ep nov before normalisation:  52.79354181791543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.01800997971759
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 55.37989392067907
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0108, 0.9555, 0.0078, 0.0172, 0.0087], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0004,     0.9464,     0.0402,     0.0130],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0003,     0.0008,     0.7137,     0.2848],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0013, 0.0023, 0.0309, 0.2740, 0.6915], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.28186736161118
printing an ep nov before normalisation:  59.72586030847
line 256 mcts: sample exp_bonus 44.21384393950669
printing an ep nov before normalisation:  19.25191635124958
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.65173530578613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.66365541730608
printing an ep nov before normalisation:  85.75885103485672
printing an ep nov before normalisation:  49.1051078483823
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  107.90492529993412
printing an ep nov before normalisation:  53.76286936190449
printing an ep nov before normalisation:  84.41965677395895
siam score:  -0.8010445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  84.99053006086434
printing an ep nov before normalisation:  40.17093603774242
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.68211751495181
printing an ep nov before normalisation:  55.88981813906569
printing an ep nov before normalisation:  44.60578658291857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.3268702521728
printing an ep nov before normalisation:  30.210584027311114
printing an ep nov before normalisation:  73.19047220385224
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.77418130060164
printing an ep nov before normalisation:  24.896879196166992
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  53.26090377710037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.05818462961476
printing an ep nov before normalisation:  58.22397271975998
printing an ep nov before normalisation:  19.795551300048828
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 23.315843146801093
printing an ep nov before normalisation:  44.009830552136776
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.258]
 [43.102]
 [38.941]
 [61.047]
 [47.371]] [[0.94 ]
 [0.936]
 [0.846]
 [1.326]
 [1.029]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.993107190094634
printing an ep nov before normalisation:  46.94796244361435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.236277186263774
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.139901987569072
printing an ep nov before normalisation:  64.78904492666193
printing an ep nov before normalisation:  81.2270023173939
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [46.467]
 [ 0.   ]
 [ 0.   ]] [[-0.492]
 [-0.492]
 [ 0.482]
 [-0.492]
 [-0.492]]
printing an ep nov before normalisation:  59.19243618673379
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.665]
 [80.665]
 [68.74 ]
 [80.665]
 [80.665]] [[1.667]
 [1.667]
 [1.348]
 [1.667]
 [1.667]]
siam score:  -0.811933
printing an ep nov before normalisation:  62.24832130165637
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  13.671789531724965
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.062]
 [27.843]
 [28.508]
 [27.318]
 [28.026]] [[1.093]
 [0.567]
 [0.594]
 [0.546]
 [0.575]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.08603639114301
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.057574373908054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.971]
 [65.88 ]
 [65.88 ]
 [69.396]
 [65.88 ]] [[1.081]
 [0.953]
 [0.953]
 [1.027]
 [0.953]]
printing an ep nov before normalisation:  30.794492072526193
printing an ep nov before normalisation:  63.56947078530073
printing an ep nov before normalisation:  61.3534671194831
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.45730089636203
printing an ep nov before normalisation:  69.36014659723666
siam score:  -0.81246966
actions average: 
K:  2  action  0 :  tensor([    0.9660,     0.0010,     0.0001,     0.0204,     0.0124],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0018,     0.9207,     0.0062,     0.0004,     0.0709],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0081,     0.8917,     0.0016,     0.0983],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0005,     0.0369,     0.8566,     0.1059],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0025,     0.0566,     0.3411,     0.5992],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.116098029195925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.004323959350586
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.883]
 [42.883]
 [42.883]
 [42.883]
 [42.883]] [[0.432]
 [0.432]
 [0.432]
 [0.432]
 [0.432]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.469]
 [22.54 ]
 [22.54 ]
 [22.54 ]
 [22.54 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9240,     0.0005,     0.0001,     0.0026,     0.0728],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0049, 0.8586, 0.0062, 0.0097, 0.1206], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0143,     0.9200,     0.0248,     0.0409],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0011,     0.0000,     0.0009,     0.8150,     0.1829],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0079, 0.0811, 0.0532, 0.1889, 0.6688], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.83164626747478
UNIT TEST: sample policy line 217 mcts : [0.718 0.    0.103 0.051 0.128]
printing an ep nov before normalisation:  42.5236728000544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.21126603638179
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.8354611136834
printing an ep nov before normalisation:  81.94399034663579
printing an ep nov before normalisation:  50.82131862640381
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  6.377790026726871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([0.9885, 0.0014, 0.0068, 0.0015, 0.0019], grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9796,     0.0014,     0.0001,     0.0186],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0009,     0.0067,     0.9172,     0.0347,     0.0405],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0002,     0.0295,     0.6864,     0.2834],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0009,     0.0004,     0.0104,     0.3111,     0.6772],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.56225778761464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.842873929393136
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.79621917
siam score:  -0.7941925
siam score:  -0.7916652
printing an ep nov before normalisation:  15.335414693186067
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.52314221470908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.203143888027626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.7840639
printing an ep nov before normalisation:  35.823325944666635
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.46137622102626
printing an ep nov before normalisation:  32.075565453638575
printing an ep nov before normalisation:  37.874895708578656
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.17471018941705552
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.69499402568913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.68290178349878
printing an ep nov before normalisation:  61.13580324308916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.74925190759832
printing an ep nov before normalisation:  38.51185896821059
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.205 0.179 0.462 0.077 0.077]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.22217666864448
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  100.41972692315052
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  18.61225234137641
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[15.201]
 [27.886]
 [20.643]
 [15.797]
 [17.093]] [[0.29 ]
 [0.768]
 [0.495]
 [0.312]
 [0.361]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.85204402555911
printing an ep nov before normalisation:  67.54666599724605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[88.166]
 [88.166]
 [88.166]
 [88.166]
 [88.166]] [[1.238]
 [1.238]
 [1.238]
 [1.238]
 [1.238]]
printing an ep nov before normalisation:  66.21106370819665
printing an ep nov before normalisation:  35.61883638271488
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.35652189219392
printing an ep nov before normalisation:  53.40581392952079
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.809467577224694
printing an ep nov before normalisation:  28.33037009757822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.95201356545607
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.40500507346624
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.48328525179684
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.438]
 [28.438]
 [31.065]
 [28.438]
 [28.438]] [[1.489]
 [1.489]
 [1.667]
 [1.489]
 [1.489]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.732]
 [16.67 ]
 [24.419]
 [19.027]
 [18.346]] [[0.807]
 [0.639]
 [0.959]
 [0.737]
 [0.708]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.396109277284395
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9987,     0.0003,     0.0000,     0.0005,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9554,     0.0325,     0.0004,     0.0114],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0026,     0.9750,     0.0127,     0.0096],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0027,     0.7175,     0.2795],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0029, 0.0055, 0.0057, 0.2546, 0.7314], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.929]
 [43.314]
 [45.835]
 [41.159]
 [41.243]] [[0.092]
 [0.205]
 [0.226]
 [0.187]
 [0.187]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.548571036105386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.19508360821462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.341]
 [45.854]
 [51.804]
 [30.341]
 [30.341]] [[0.129]
 [0.378]
 [0.473]
 [0.129]
 [0.129]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.419201727896066
printing an ep nov before normalisation:  37.173410932722994
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.242636409036237
siam score:  -0.80700713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.80865616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.896]
 [16.25 ]
 [16.93 ]
 [13.129]
 [15.455]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  39.22793874738338
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.73840173085531
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.987]
 [48.238]
 [50.388]
 [29.921]
 [41.987]] [[0.172]
 [0.217]
 [0.233]
 [0.086]
 [0.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.94139077588198
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.579]
 [58.536]
 [54.822]
 [59.579]
 [59.579]] [[0.295]
 [0.286]
 [0.253]
 [0.295]
 [0.295]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.045783195968
printing an ep nov before normalisation:  31.52041832605998
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.471614837646484
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.348472989071105
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.0144742918704
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[13.867]
 [14.214]
 [18.822]
 [18.285]
 [13.867]] [[0.497]
 [0.523]
 [0.872]
 [0.831]
 [0.497]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.035610766699854
printing an ep nov before normalisation:  17.16195583343506
printing an ep nov before normalisation:  33.218005895737065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.423]
 [38.081]
 [38.87 ]
 [41.627]
 [42.334]] [[0.921]
 [0.501]
 [0.523]
 [0.598]
 [0.617]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.74787639515336
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.11 ]
 [54.125]
 [61.749]
 [54.125]
 [54.125]] [[0.619]
 [0.646]
 [0.853]
 [0.646]
 [0.646]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  89.92524564364952
printing an ep nov before normalisation:  29.843981856737685
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.438500665175354
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.884791218956224
siam score:  -0.7804841
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.35999752246457
printing an ep nov before normalisation:  44.96185205586724
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.30821204861447
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9330,     0.0284,     0.0000,     0.0143,     0.0242],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0364,     0.8989,     0.0002,     0.0008,     0.0638],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0364,     0.8617,     0.0515,     0.0503],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0075,     0.0005,     0.0654,     0.7174,     0.2092],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1177, 0.1091, 0.0183, 0.2044, 0.5505], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.5922508507685
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[19.628]
 [17.389]
 [43.983]
 [17.108]
 [18.362]] [[0.26 ]
 [0.196]
 [0.959]
 [0.188]
 [0.224]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.838]
 [64.838]
 [64.838]
 [64.838]
 [64.838]] [[0.931]
 [0.931]
 [0.931]
 [0.931]
 [0.931]]
printing an ep nov before normalisation:  21.499092208412094
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  115.67911746076754
printing an ep nov before normalisation:  40.52041390725556
printing an ep nov before normalisation:  56.15677905442506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.56842949666203
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.715]
 [27.36 ]
 [38.568]
 [50.247]
 [48.707]] [[0.665]
 [0.566]
 [0.821]
 [1.085]
 [1.051]]
printing an ep nov before normalisation:  97.43132203005904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.98416090011597
printing an ep nov before normalisation:  32.631648755209895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  69.88457105440503
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.817592
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9850,     0.0032,     0.0000,     0.0013,     0.0105],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9969,     0.0002,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0204,     0.9581,     0.0026,     0.0189],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0000,     0.0137,     0.8642,     0.1220],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0166, 0.0416, 0.0696, 0.2222, 0.6499], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  87.90381051491612
printing an ep nov before normalisation:  45.89185668853496
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.603]
 [34.919]
 [38.653]
 [34.919]
 [34.919]] [[0.921]
 [0.804]
 [0.897]
 [0.804]
 [0.804]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.6979836610537
siam score:  -0.8122498
printing an ep nov before normalisation:  57.20585553572795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.420804602989776
printing an ep nov before normalisation:  17.30397481837991
printing an ep nov before normalisation:  29.345803805054953
printing an ep nov before normalisation:  29.677025649971412
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.558]
 [25.105]
 [26.392]
 [27.306]
 [24.015]] [[0.877]
 [1.022]
 [1.074]
 [1.112]
 [0.977]]
printing an ep nov before normalisation:  24.746761841430434
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.504]
 [15.022]
 [19.765]
 [18.851]
 [18.678]] [[0.93 ]
 [0.681]
 [0.896]
 [0.855]
 [0.847]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.719]
 [36.476]
 [41.874]
 [36.476]
 [36.476]] [[0.103]
 [0.085]
 [0.115]
 [0.085]
 [0.085]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.04951429671057
siam score:  -0.8099549
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.468]
 [39.514]
 [39.514]
 [39.514]
 [39.514]] [[1.292]
 [0.874]
 [0.874]
 [0.874]
 [0.874]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.47]
 [42.47]
 [42.47]
 [42.47]
 [42.47]] [[70.798]
 [70.798]
 [70.798]
 [70.798]
 [70.798]]
printing an ep nov before normalisation:  16.219838293185518
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  71.1227408592423
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.343938653443935
printing an ep nov before normalisation:  24.0640190045246
printing an ep nov before normalisation:  24.682680329387114
actions average: 
K:  3  action  0 :  tensor([    0.9686,     0.0287,     0.0000,     0.0004,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0014,     0.9876,     0.0003,     0.0002,     0.0106],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0134,     0.9347,     0.0049,     0.0469],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0041, 0.0044, 0.0483, 0.7548, 0.1884], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0674, 0.0062, 0.0459, 0.2321, 0.6483], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.617640225543038
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.089]
 [55.007]
 [47.039]
 [39.327]
 [48.451]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 85.8923534880956
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.12583159925222
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.957]
 [23.957]
 [23.957]
 [23.957]
 [23.957]] [[0.814]
 [0.814]
 [0.814]
 [0.814]
 [0.814]]
printing an ep nov before normalisation:  54.27421987106919
siam score:  -0.81759644
printing an ep nov before normalisation:  57.45518069534293
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.117686073730454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 35.454501279269046
printing an ep nov before normalisation:  41.585917472839355
printing an ep nov before normalisation:  44.66638658559766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.26073296864827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.580596446990967
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.966653738202034
printing an ep nov before normalisation:  40.70839583746367
siam score:  -0.8013905
printing an ep nov before normalisation:  89.45714685948889
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.602]
 [43.602]
 [45.66 ]
 [43.602]
 [43.602]] [[0.304]
 [0.304]
 [0.333]
 [0.304]
 [0.304]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.53136954149715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.62670785425389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.35977716101225
using explorer policy with actor:  1
printing an ep nov before normalisation:  7.022368412316382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.66520881405233
printing an ep nov before normalisation:  44.56409878777196
siam score:  -0.80912966
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.947055044943355
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.98331920961975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.55832380108719
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.19422921111789
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.73090855259433
printing an ep nov before normalisation:  24.63778882437782
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.598]
 [71.192]
 [73.962]
 [67.489]
 [67.716]] [[0.2  ]
 [0.505]
 [0.543]
 [0.455]
 [0.458]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.671]
 [57.039]
 [66.528]
 [65.394]
 [66.444]] [[0.619]
 [0.903]
 [1.09 ]
 [1.068]
 [1.088]]
printing an ep nov before normalisation:  47.834550102188985
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81530786
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.685192186579634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.66184273713223
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.511363873689575
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.558]
 [73.558]
 [78.139]
 [73.558]
 [73.558]] [[1.184]
 [1.184]
 [1.288]
 [1.184]
 [1.184]]
printing an ep nov before normalisation:  44.54513093634401
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.051 0.231 0.436 0.026 0.256]
actions average: 
K:  1  action  0 :  tensor([    0.9989,     0.0006,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9758,     0.0162,     0.0000,     0.0063],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0056,     0.9489,     0.0150,     0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0187,     0.0003,     0.0006,     0.7942,     0.1862],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0031,     0.0004,     0.0305,     0.2589,     0.7072],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.214071642638686
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81836075
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8151735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.857835114002672
printing an ep nov before normalisation:  38.506612630121374
actions average: 
K:  3  action  0 :  tensor([    0.9923,     0.0062,     0.0000,     0.0003,     0.0012],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9880,     0.0011,     0.0000,     0.0102],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0000,     0.0221,     0.8285,     0.1493],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0006, 0.0555, 0.1579, 0.2869, 0.4991], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.93514437759394
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.249950939631454
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.678527111752125
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9913,     0.0008,     0.0000,     0.0003,     0.0076],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9772,     0.0150,     0.0000,     0.0073],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0036, 0.0021, 0.9223, 0.0243, 0.0476], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0008,     0.0405,     0.8266,     0.1318],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0008, 0.0193, 0.0421, 0.3111, 0.6267], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.71379280090332
siam score:  -0.81524736
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.622]
 [59.622]
 [59.622]
 [59.622]
 [59.622]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.48973297908014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9837,     0.0140,     0.0000,     0.0003,     0.0020],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9986,     0.0001,     0.0000,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9347,     0.0363,     0.0289],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0022,     0.0001,     0.0078,     0.8219,     0.1679],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0071, 0.0673, 0.0155, 0.1989, 0.7112], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.642898944439313
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.995458329383045
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8287827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.15344572067261
printing an ep nov before normalisation:  18.710716602534617
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  37.44742207594781
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0020,     0.9881,     0.0056,     0.0000,     0.0043],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0062,     0.9657,     0.0011,     0.0270],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0001,     0.0061,     0.8601,     0.1328],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0777, 0.0054, 0.0919, 0.3381, 0.4869], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.50318431854248
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.483032284250893
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.27 ]
 [19.413]
 [21.462]
 [19.413]
 [22.982]] [[0.958]
 [0.496]
 [0.616]
 [0.496]
 [0.706]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.139858774777835
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.528931997197255
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[63.026]
 [62.961]
 [56.132]
 [56.132]
 [54.598]] [[0.732]
 [0.73 ]
 [0.593]
 [0.593]
 [0.562]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.83469969613406
printing an ep nov before normalisation:  39.79661898424533
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  20.085503572667864
printing an ep nov before normalisation:  17.390312837283503
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.595]
 [36.595]
 [36.595]
 [63.458]
 [36.595]] [[0.337]
 [0.337]
 [0.337]
 [1.321]
 [0.337]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.348697993110044
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.66345099877419
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.18988433942625
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.552413505567724
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.72915129674322
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.220587143812836
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.4663678718066
printing an ep nov before normalisation:  44.87270403183653
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.813837
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.315]
 [72.137]
 [68.315]
 [65.336]
 [68.315]] [[1.174]
 [1.24 ]
 [1.174]
 [1.123]
 [1.174]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.24011578822795
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.87511027969515
printing an ep nov before normalisation:  46.08773365915417
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0031,     0.9172,     0.0371,     0.0004,     0.0423],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0055,     0.9568,     0.0160,     0.0212],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0008,     0.0002,     0.0449,     0.7686,     0.1855],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0123,     0.0006,     0.0007,     0.3177,     0.6686],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.363]
 [50.415]
 [50.415]
 [50.415]
 [50.415]] [[0.809]
 [1.   ]
 [1.   ]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.14947564916951706
siam score:  -0.81185424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 93.0852413580071
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.067]
 [43.997]
 [43.21 ]
 [43.553]
 [42.253]] [[1.005]
 [1.16 ]
 [1.118]
 [1.137]
 [1.068]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.697433948516846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.11878346026012
siam score:  -0.81499964
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [42.723]
 [32.368]
 [23.789]] [[-0.169]
 [-0.169]
 [ 0.351]
 [ 0.225]
 [ 0.12 ]]
printing an ep nov before normalisation:  22.49264349758647
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8191768
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.442]
 [58.442]
 [58.442]
 [58.442]
 [58.442]] [[1.719]
 [1.719]
 [1.719]
 [1.719]
 [1.719]]
printing an ep nov before normalisation:  37.593584060668945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.79984361546666
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.755179483995825
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.93018985186836
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.791128118951015
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.527926304705936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.1486527188434
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.305]
 [76.305]
 [76.305]
 [76.305]
 [76.305]] [[1.367]
 [1.367]
 [1.367]
 [1.367]
 [1.367]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[77.735]
 [87.385]
 [85.949]
 [87.385]
 [87.385]] [[1.773]
 [1.993]
 [1.96 ]
 [1.993]
 [1.993]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.36546749245504
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.967580110098506
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.05307102203369
printing an ep nov before normalisation:  27.429686492846027
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.115]
 [59.64 ]
 [49.543]
 [65.612]
 [60.359]] [[1.351]
 [1.075]
 [0.747]
 [1.269]
 [1.099]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.83714268568844
printing an ep nov before normalisation:  33.99120025153046
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.98342624923484
printing an ep nov before normalisation:  7.175901649058574
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.264564301815064
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.059]
 [48.408]
 [45.059]
 [45.059]
 [45.059]] [[1.447]
 [1.667]
 [1.447]
 [1.447]
 [1.447]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.723]
 [42.723]
 [42.723]
 [42.723]
 [42.723]] [[0.174]
 [0.174]
 [0.174]
 [0.174]
 [0.174]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.391]
 [36.204]
 [35.245]
 [31.855]
 [35.033]] [[1.362]
 [1.423]
 [1.351]
 [1.099]
 [1.336]]
printing an ep nov before normalisation:  29.11960505969522
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9872,     0.0005,     0.0000,     0.0080,     0.0043],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0051,     0.9498,     0.0141,     0.0000,     0.0309],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0004,     0.9015,     0.0154,     0.0827],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0449,     0.0002,     0.0031,     0.6912,     0.2606],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0062, 0.0017, 0.0253, 0.2434, 0.7234], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.0199353786273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.2116777182029
printing an ep nov before normalisation:  47.56186854565239
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.325087246838077
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.68519465385102
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.874]
 [42.67 ]
 [48.343]
 [30.03 ]
 [28.562]] [[0.166]
 [0.376]
 [0.447]
 [0.218]
 [0.2  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.101991176605225
printing an ep nov before normalisation:  75.35509760473724
printing an ep nov before normalisation:  39.31537628173828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.221245765686035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.5730204751531
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.88178443680004
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  37.03452599131167
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.79974799487297
siam score:  -0.82872546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.48755876496439
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.49502229727239
printing an ep nov before normalisation:  49.17789159701098
printing an ep nov before normalisation:  73.5547234306091
printing an ep nov before normalisation:  85.99886928087535
printing an ep nov before normalisation:  58.28280001755572
printing an ep nov before normalisation:  67.41575926542575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0242,     0.9588,     0.0045,     0.0000,     0.0124],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0106,     0.9364,     0.0140,     0.0388],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0274,     0.0002,     0.0013,     0.7972,     0.1739],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0549, 0.0280, 0.0330, 0.2325, 0.6515], grad_fn=<DivBackward0>)
siam score:  -0.8279345
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.56942793514457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.4334775316057
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9933,     0.0030,     0.0000,     0.0004,     0.0033],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9723,     0.0242,     0.0000,     0.0034],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0183,     0.9412,     0.0219,     0.0185],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0001,     0.0008,     0.8222,     0.1764],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0010, 0.0830, 0.0033, 0.2193, 0.6934], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.125]
 [66.701]
 [68.467]
 [53.378]
 [61.189]] [[0.341]
 [0.54 ]
 [0.57 ]
 [0.311]
 [0.445]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.56420263705777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.614]
 [30.136]
 [29.996]
 [26.947]
 [28.597]] [[0.172]
 [0.218]
 [0.216]
 [0.176]
 [0.198]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.05072192312653
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.943]
 [40.243]
 [40.045]
 [38.137]
 [39.134]] [[0.174]
 [0.254]
 [0.253]
 [0.234]
 [0.244]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.195]
 [62.195]
 [75.644]
 [62.195]
 [62.195]] [[1.294]
 [1.294]
 [1.617]
 [1.294]
 [1.294]]
printing an ep nov before normalisation:  70.483393259676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.69755015577574
printing an ep nov before normalisation:  42.316642765827396
printing an ep nov before normalisation:  48.17171483648608
siam score:  -0.82799804
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.414]
 [40.414]
 [40.414]
 [40.414]
 [40.414]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.908452279890284
printing an ep nov before normalisation:  69.71295211563351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  3.666950299219991
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.73345368205395
printing an ep nov before normalisation:  40.1096211129177
siam score:  -0.8250798
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.4554473372572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.973006926781814
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.9024772644043
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.95761543292207
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.98883628845215
printing an ep nov before normalisation:  43.022929107036596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  90.864627516759
actions average: 
K:  4  action  0 :  tensor([    0.9238,     0.0308,     0.0000,     0.0005,     0.0449],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.8861,     0.0517,     0.0002,     0.0617],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0294,     0.8653,     0.0466,     0.0586],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0014,     0.0003,     0.1169,     0.6292,     0.2521],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0023, 0.0142, 0.1003, 0.2253, 0.6580], grad_fn=<DivBackward0>)
siam score:  -0.8145725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.547140156328837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.81433296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.236]
 [34.905]
 [28.879]
 [21.298]
 [29.727]] [[0.195]
 [0.475]
 [0.36 ]
 [0.215]
 [0.376]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.46725808335798
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[79.157]
 [89.506]
 [89.533]
 [89.506]
 [89.506]] [[1.308]
 [1.579]
 [1.58 ]
 [1.579]
 [1.579]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.268]
 [51.19 ]
 [44.007]
 [44.007]
 [44.007]] [[1.293]
 [1.4  ]
 [1.204]
 [1.204]
 [1.204]]
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0002,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([0.0057, 0.9588, 0.0143, 0.0030, 0.0182], grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0121,     0.9563,     0.0112,     0.0201],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0013,     0.0002,     0.0003,     0.6580,     0.3403],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.1165, 0.1119, 0.0024, 0.1087, 0.6605], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 72.844809128856
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.723]
 [36.428]
 [42.295]
 [37.434]
 [37.434]] [[0.364]
 [0.719]
 [0.933]
 [0.755]
 [0.755]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0011,     0.9596,     0.0304,     0.0001,     0.0087],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0184,     0.9065,     0.0257,     0.0492],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0000,     0.0045,     0.7972,     0.1977],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0179,     0.0007,     0.0058,     0.2241,     0.7516],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.251790029500995
printing an ep nov before normalisation:  35.239877700805664
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9627,     0.0301,     0.0001,     0.0070],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0004,     0.9563,     0.0145,     0.0288],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0084,     0.8108,     0.1806],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0002,     0.0054,     0.0374,     0.2050,     0.7520],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.661]
 [53.59 ]
 [55.661]
 [55.661]
 [55.661]] [[1.317]
 [1.241]
 [1.317]
 [1.317]
 [1.317]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.21626806259155
printing an ep nov before normalisation:  24.81439383305845
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  98.36515125951084
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.082]
 [49.082]
 [49.082]
 [49.082]
 [49.082]] [[1.243]
 [1.243]
 [1.243]
 [1.243]
 [1.243]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.31251746380334
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.3324747782328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  95.62221966084354
printing an ep nov before normalisation:  58.94952074978715
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.82059908682852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.282]
 [38.676]
 [39.871]
 [41.282]
 [41.282]] [[1.309]
 [1.165]
 [1.231]
 [1.309]
 [1.309]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.613]
 [56.313]
 [50.102]
 [48.405]
 [52.513]] [[0.636]
 [0.769]
 [0.684]
 [0.661]
 [0.717]]
UNIT TEST: sample policy line 217 mcts : [0.103 0.308 0.282 0.154 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.931135890131394
siam score:  -0.8230621
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.35]
 [40.35]
 [40.35]
 [40.35]
 [40.35]] [[1.37]
 [1.37]
 [1.37]
 [1.37]
 [1.37]]
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9529,     0.0335,     0.0000,     0.0129],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0010,     0.9477,     0.0188,     0.0323],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0066,     0.0005,     0.0871,     0.6684,     0.2374],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0087, 0.0135, 0.0564, 0.1691, 0.7523], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.82670766
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.25650249459222
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.686072078787966
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9818,     0.0097,     0.0000,     0.0082],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0019,     0.9878,     0.0006,     0.0096],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0181,     0.0002,     0.0245,     0.6683,     0.2890],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0041, 0.0017, 0.0631, 0.3149, 0.6162], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.048587283888637
printing an ep nov before normalisation:  35.21243287756357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  19.327651578879568
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.747]
 [48.747]
 [53.406]
 [48.747]
 [48.747]] [[1.364]
 [1.364]
 [1.592]
 [1.364]
 [1.364]]
printing an ep nov before normalisation:  27.1345853805542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.99697439162034
printing an ep nov before normalisation:  58.457632063759775
actions average: 
K:  3  action  0 :  tensor([    0.9968,     0.0004,     0.0000,     0.0011,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9488,     0.0432,     0.0000,     0.0080],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0094,     0.9427,     0.0107,     0.0372],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0003,     0.0306,     0.7473,     0.2213],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0158, 0.0414, 0.1518, 0.2053, 0.5857], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.025]
 [43.992]
 [44.025]
 [44.025]
 [44.025]] [[0.613]
 [0.612]
 [0.613]
 [0.613]
 [0.613]]
printing an ep nov before normalisation:  72.43265480466819
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.141]
 [62.488]
 [65.649]
 [65.649]
 [65.649]] [[1.375]
 [1.033]
 [1.118]
 [1.118]
 [1.118]]
printing an ep nov before normalisation:  28.710836828649974
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.13208172792084
printing an ep nov before normalisation:  46.64143038083254
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.525]
 [47.337]
 [44.388]
 [38.398]
 [41.544]] [[1.14 ]
 [1.472]
 [1.269]
 [0.855]
 [1.072]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.503]
 [36.259]
 [37.352]
 [36.259]
 [36.259]] [[1.509]
 [1.361]
 [1.433]
 [1.361]
 [1.361]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.52108253485553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.41245841853423
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.46 ]
 [57.802]
 [36.983]
 [41.186]
 [28.54 ]] [[0.433]
 [0.49 ]
 [0.268]
 [0.313]
 [0.179]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.247]
 [43.938]
 [42.559]
 [43.286]
 [48.247]] [[0.559]
 [0.48 ]
 [0.455]
 [0.468]
 [0.559]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9977,     0.0020,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9951,     0.0034,     0.0000,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0511,     0.8967,     0.0004,     0.0517],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0331,     0.7209,     0.2456],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0202, 0.0171, 0.0021, 0.2965, 0.6640], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.58341093250527
printing an ep nov before normalisation:  62.12405994645717
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.408]
 [43.408]
 [43.408]
 [43.408]
 [43.408]] [[0.571]
 [0.571]
 [0.571]
 [0.571]
 [0.571]]
siam score:  -0.82185435
printing an ep nov before normalisation:  61.147468536829706
siam score:  -0.8203956
printing an ep nov before normalisation:  33.423991203308105
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.714]
 [42.425]
 [43.403]
 [43.403]
 [43.403]] [[2.   ]
 [1.329]
 [1.375]
 [1.375]
 [1.375]]
printing an ep nov before normalisation:  90.23980234756364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9996,     0.0002,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9744,     0.0026,     0.0000,     0.0224],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0128,     0.9011,     0.0221,     0.0637],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0031, 0.0033, 0.0187, 0.6608, 0.3141], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0476, 0.0452, 0.1776, 0.1712, 0.5584], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  36.23779325546744
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.145]
 [26.557]
 [28.048]
 [28.67 ]
 [26.544]] [[0.236]
 [0.222]
 [0.258]
 [0.273]
 [0.222]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.969]
 [77.008]
 [79.146]
 [77.008]
 [77.008]] [[0.761]
 [0.95 ]
 [1.   ]
 [0.95 ]
 [0.95 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.258260252720035
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.031]
 [58.832]
 [58.832]
 [58.832]
 [58.832]] [[1.355]
 [1.346]
 [1.346]
 [1.346]
 [1.346]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8346436
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  58.86213183403015
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.01776885986328
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.90645235052713
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.903]
 [36.903]
 [36.903]
 [36.903]
 [36.903]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.13442077266018
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.78402159261115
printing an ep nov before normalisation:  38.97602558135986
printing an ep nov before normalisation:  55.169418736315784
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.013]
 [35.013]
 [61.279]
 [35.013]
 [35.013]] [[0.401]
 [0.401]
 [0.921]
 [0.401]
 [0.401]]
printing an ep nov before normalisation:  45.03920555114746
siam score:  -0.8293677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8310553
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.653]
 [37.399]
 [31.571]
 [31.571]
 [31.571]] [[0.982]
 [0.933]
 [0.704]
 [0.704]
 [0.704]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.575]
 [61.796]
 [43.717]
 [52.266]
 [43.717]] [[1.147]
 [1.522]
 [1.077]
 [1.288]
 [1.077]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.324]
 [48.324]
 [48.324]
 [48.324]
 [48.324]] [[48.324]
 [48.324]
 [48.324]
 [48.324]
 [48.324]]
actions average: 
K:  1  action  0 :  tensor([    0.9604,     0.0000,     0.0000,     0.0348,     0.0048],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0060,     0.9502,     0.0213,     0.0000,     0.0226],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0017, 0.0071, 0.9173, 0.0116, 0.0623], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0004,     0.0002,     0.7597,     0.2393],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0432, 0.0021, 0.0442, 0.1798, 0.7307], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[85.207]
 [85.207]
 [85.207]
 [85.207]
 [85.207]] [[1.874]
 [1.874]
 [1.874]
 [1.874]
 [1.874]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9713,     0.0111,     0.0005,     0.0021,     0.0150],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9859,     0.0008,     0.0000,     0.0133],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0157,     0.9043,     0.0194,     0.0600],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0289,     0.0007,     0.0311,     0.7324,     0.2070],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0030, 0.0350, 0.0612, 0.1810, 0.7198], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.24415317882269
printing an ep nov before normalisation:  18.017978981870897
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.26159782151912
actions average: 
K:  1  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0011,     0.9830,     0.0002,     0.0001,     0.0156],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9350,     0.0274,     0.0374],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0001,     0.0002,     0.7896,     0.2097],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0092, 0.0197, 0.0375, 0.2903, 0.6433], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  67.7074359478328
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.178]
 [60.903]
 [49.178]
 [49.178]
 [49.178]] [[1.229]
 [1.781]
 [1.229]
 [1.229]
 [1.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.09707455018462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.31920407418232
printing an ep nov before normalisation:  60.10210197808704
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.006418611525305096
printing an ep nov before normalisation:  40.86677124414085
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.356]
 [66.356]
 [73.151]
 [66.356]
 [66.356]] [[1.072]
 [1.072]
 [1.235]
 [1.072]
 [1.072]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.76714915604374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.86563598915325
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  72.93552945877076
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.156]
 [30.095]
 [35.66 ]
 [23.243]
 [23.243]] [[0.222]
 [0.263]
 [0.34 ]
 [0.168]
 [0.168]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.834]
 [30.834]
 [50.183]
 [30.834]
 [30.834]] [[0.158]
 [0.158]
 [0.316]
 [0.158]
 [0.158]]
actions average: 
K:  1  action  0 :  tensor([    0.9985,     0.0005,     0.0000,     0.0003,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9535,     0.0305,     0.0001,     0.0150],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0050,     0.8920,     0.0424,     0.0606],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0233, 0.0132, 0.0269, 0.7773, 0.1592], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0035, 0.0025, 0.0143, 0.1232, 0.8565], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.887905218255185
printing an ep nov before normalisation:  47.70144122177433
printing an ep nov before normalisation:  30.03974225785997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.193]
 [41.641]
 [59.274]
 [42.193]
 [42.193]] [[1.011]
 [0.998]
 [1.421]
 [1.011]
 [1.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  3  action  0 :  tensor([    0.9920,     0.0010,     0.0000,     0.0001,     0.0070],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0020,     0.9942,     0.0018,     0.0000,     0.0020],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0064, 0.0113, 0.9246, 0.0100, 0.0477], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0204,     0.0003,     0.0006,     0.8175,     0.1610],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0725, 0.0039, 0.0359, 0.1794, 0.7083], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.461448651040573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.395]
 [58.395]
 [58.395]
 [58.395]
 [58.395]] [[1.295]
 [1.295]
 [1.295]
 [1.295]
 [1.295]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.58 ]
 [57.948]
 [64.115]
 [60.034]
 [62.395]] [[1.408]
 [1.468]
 [1.624]
 [1.521]
 [1.58 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.47861227932017
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.96941716314678
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [42.477]
 [ 0.   ]
 [ 0.   ]] [[-0.377]
 [-0.377]
 [ 0.556]
 [-0.377]
 [-0.377]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.06790047314118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0086,     0.9317,     0.0335,     0.0001,     0.0260],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0049,     0.9548,     0.0048,     0.0351],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0137,     0.0001,     0.0005,     0.7179,     0.2677],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.1276, 0.0015, 0.0239, 0.2260, 0.6211], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.49356774893398
printing an ep nov before normalisation:  54.457449488039884
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.2097093624898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.88329025322297
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.439]
 [33.571]
 [39.882]
 [40.684]
 [35.684]] [[0.803]
 [0.726]
 [0.986]
 [1.019]
 [0.813]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.84933921740261
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 30.864850664136867
siam score:  -0.8228949
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 43.99724853316579
printing an ep nov before normalisation:  27.903762446766905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0007217291931738146
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 22.759184555612205
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8220212
printing an ep nov before normalisation:  58.91060999394278
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.002149858808494
printing an ep nov before normalisation:  62.736049945250684
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.48973080626314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.62574002056748
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.48481714763114
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.333 0.359 0.256]
printing an ep nov before normalisation:  39.44812007812356
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.49127602478917
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.28087542602378
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8194287
actions average: 
K:  4  action  0 :  tensor([    0.9707,     0.0153,     0.0000,     0.0044,     0.0096],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9639,     0.0284,     0.0000,     0.0076],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0076,     0.9823,     0.0003,     0.0098],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0008,     0.0002,     0.0451,     0.7122,     0.2416],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0009, 0.0026, 0.1255, 0.2752, 0.5958], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.467]
 [40.789]
 [40.789]
 [40.789]
 [40.789]] [[2.   ]
 [1.549]
 [1.549]
 [1.549]
 [1.549]]
printing an ep nov before normalisation:  36.994522945788724
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.68976925297718
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
siam score:  -0.8125542
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.51732554192515
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.774]
 [18.774]
 [19.568]
 [21.657]
 [18.774]] [[0.178]
 [0.178]
 [0.193]
 [0.233]
 [0.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  62.268818302824876
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.073]
 [54.007]
 [57.528]
 [57.155]
 [57.11 ]] [[0.5  ]
 [0.655]
 [0.747]
 [0.737]
 [0.736]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.726]
 [45.262]
 [49.063]
 [47.306]
 [44.815]] [[0.964]
 [1.325]
 [1.535]
 [1.438]
 [1.3  ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.97310687298592
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.949148178100586
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.775514853467655
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.921]
 [68.059]
 [68.059]
 [68.059]
 [68.059]] [[1.333]
 [1.261]
 [1.261]
 [1.261]
 [1.261]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  70.53870891938543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.429]
 [54.666]
 [53.075]
 [61.029]
 [56.809]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.22742824646186
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.62782145794191
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.029922014283734
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.89996220686948
printing an ep nov before normalisation:  42.1243521472654
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.17 ]
 [43.993]
 [59.684]
 [52.501]
 [48.441]] [[0.745]
 [0.976]
 [1.507]
 [1.264]
 [1.127]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  53.307254270993845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.804]
 [46.804]
 [47.674]
 [46.804]
 [46.804]] [[1.213]
 [1.213]
 [1.246]
 [1.213]
 [1.213]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.385725809424166
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.021]
 [57.09 ]
 [56.143]
 [50.4  ]
 [47.33 ]] [[0.212]
 [0.225]
 [0.219]
 [0.181]
 [0.161]]
printing an ep nov before normalisation:  52.8114668586558
printing an ep nov before normalisation:  52.9857070256175
siam score:  -0.82839495
siam score:  -0.8288806
printing an ep nov before normalisation:  48.700846545483714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8335222
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.059794974681054
printing an ep nov before normalisation:  32.10406303405762
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.54731117509937
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0080,     0.9888,     0.0000,     0.0000,     0.0032],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0025,     0.9775,     0.0006,     0.0194],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0313, 0.0008, 0.0280, 0.6662, 0.2737], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0217, 0.0013, 0.0852, 0.2478, 0.6440], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.846]
 [54.868]
 [39.456]
 [36.186]
 [39.918]] [[0.515]
 [0.873]
 [0.506]
 [0.428]
 [0.517]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.297630693623915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.928]
 [48.113]
 [48.188]
 [46.707]
 [42.792]] [[1.1  ]
 [1.476]
 [1.481]
 [1.391]
 [1.153]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  40.6554799520021
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.83514302378935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.564538372413686
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.6  ]
 [29.6  ]
 [32.947]
 [29.6  ]
 [29.6  ]] [[1.393]
 [1.393]
 [1.686]
 [1.393]
 [1.393]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  105.33577948848004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.711331367492676
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8312414
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.579]
 [41.579]
 [41.579]
 [41.579]
 [41.579]] [[0.181]
 [0.181]
 [0.181]
 [0.181]
 [0.181]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.199]
 [48.699]
 [60.088]
 [54.038]
 [53.12 ]] [[0.197]
 [0.202]
 [0.303]
 [0.249]
 [0.241]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.82153374
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.647]
 [29.883]
 [41.159]
 [23.056]
 [47.557]] [[0.019]
 [0.476]
 [0.935]
 [0.198]
 [1.195]]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9995,     0.0001,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0367,     0.8788,     0.0216,     0.0001,     0.0627],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0025,     0.9118,     0.0356,     0.0493],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0590,     0.0000,     0.0011,     0.7745,     0.1654],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0061, 0.0246, 0.2162, 0.7517], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.56661742104994
printing an ep nov before normalisation:  39.228272438049316
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.336]
 [49.021]
 [49.021]
 [49.021]
 [49.021]] [[0.538]
 [0.392]
 [0.392]
 [0.392]
 [0.392]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.116]
 [38.116]
 [38.116]
 [38.116]
 [38.116]] [[1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.579584054723924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.108736090636086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.689671017610905
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.82778364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.53 ]
 [22.536]
 [27.55 ]
 [39.187]
 [37.025]] [[0.174]
 [0.071]
 [0.145]
 [0.316]
 [0.285]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.88514196207811
printing an ep nov before normalisation:  43.22237176267189
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.64471123633848
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 25.252983570098877
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.02376539797474
printing an ep nov before normalisation:  38.67408739776807
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.26501510429343
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  67.91549730908942
printing an ep nov before normalisation:  49.951820150354564
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.798]
 [64.17 ]
 [61.258]
 [53.051]
 [57.375]] [[0.414]
 [0.429]
 [0.398]
 [0.309]
 [0.356]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.45526821934538
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  0  action  0 :  tensor([    0.9681,     0.0118,     0.0013,     0.0007,     0.0181],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9956,     0.0000,     0.0000,     0.0042],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0178,     0.9488,     0.0009,     0.0322],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0034,     0.7445,     0.2519],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0075, 0.0032, 0.0349, 0.2413, 0.7131], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  8.912236001512904
printing an ep nov before normalisation:  38.9556663875831
printing an ep nov before normalisation:  9.088624547509738
printing an ep nov before normalisation:  36.736750090762946
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.63]
 [41.63]
 [41.63]
 [41.63]
 [41.63]] [[0.465]
 [0.465]
 [0.465]
 [0.465]
 [0.465]]
printing an ep nov before normalisation:  28.59286174578014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00015411862477776594
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.87290811538697
printing an ep nov before normalisation:  72.04187245464416
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.56738243782558
siam score:  -0.81676257
printing an ep nov before normalisation:  52.20154705603936
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.188]
 [45.639]
 [28.983]
 [36.393]
 [39.517]] [[0.794]
 [0.82 ]
 [0.521]
 [0.654]
 [0.71 ]]
printing an ep nov before normalisation:  47.65019659892869
siam score:  -0.81759936
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.224292613905558
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.69452381077361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.35191937678519
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.456]
 [26.456]
 [26.456]
 [26.456]
 [26.456]] [[0.228]
 [0.228]
 [0.228]
 [0.228]
 [0.228]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  22.24345594989851
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.43129771116915
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.29917360584221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.39237411425612
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.55457247650932
printing an ep nov before normalisation:  41.64992809295654
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.331]
 [30.379]
 [27.739]
 [25.722]
 [27.286]] [[0.304]
 [0.305]
 [0.255]
 [0.216]
 [0.246]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.67989286020523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.83062315
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8280798
printing an ep nov before normalisation:  66.6635911059104
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.2481177639752
line 256 mcts: sample exp_bonus 39.447819637648735
printing an ep nov before normalisation:  38.94928040913812
printing an ep nov before normalisation:  57.02956110975069
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.059]
 [50.67 ]
 [41.758]
 [31.513]
 [33.986]] [[0.192]
 [0.255]
 [0.197]
 [0.131]
 [0.147]]
printing an ep nov before normalisation:  73.86338300935257
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8138344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.24489060774307
printing an ep nov before normalisation:  60.7827451494005
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.09151774202575
printing an ep nov before normalisation:  39.73148157125629
printing an ep nov before normalisation:  42.17049726923362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.60454225185913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[66.322]
 [66.322]
 [66.322]
 [66.322]
 [67.852]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.03480761084503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.984684705734253
printing an ep nov before normalisation:  61.48944294128932
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.42 ]
 [24.757]
 [24.757]
 [24.757]
 [24.757]] [[0.771]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
siam score:  -0.81535226
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.025]
 [20.025]
 [20.025]
 [20.025]
 [20.025]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  66.79280195638992
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.00450706099861
printing an ep nov before normalisation:  50.37333218548984
actions average: 
K:  0  action  0 :  tensor([    0.9844,     0.0003,     0.0002,     0.0032,     0.0120],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9810,     0.0003,     0.0002,     0.0178],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9832,     0.0007,     0.0158],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0000,     0.0316,     0.8164,     0.1519],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0092, 0.1009, 0.2218, 0.6670], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.907962462979224
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.366]
 [27.728]
 [24.984]
 [25.47 ]
 [27.011]] [[0.514]
 [0.628]
 [0.496]
 [0.519]
 [0.593]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.31967774214231
printing an ep nov before normalisation:  25.61550492212604
printing an ep nov before normalisation:  34.836720292245616
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.298278844048255
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  2  action  0 :  tensor([    0.9971,     0.0028,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9980,     0.0001,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0038, 0.0078, 0.9474, 0.0105, 0.0305], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0018,     0.0003,     0.0267,     0.7762,     0.1950],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0178, 0.0669, 0.0514, 0.2414, 0.6225], grad_fn=<DivBackward0>)
siam score:  -0.81288725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.665]
 [39.665]
 [39.665]
 [39.665]
 [39.665]] [[1.506]
 [1.506]
 [1.506]
 [1.506]
 [1.506]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.20544363481299
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.427085003288106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.29896853780595
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.104]
 [40.398]
 [40.398]
 [40.398]
 [40.398]] [[0.457]
 [0.265]
 [0.265]
 [0.265]
 [0.265]]
actions average: 
K:  4  action  0 :  tensor([    0.9987,     0.0006,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9287,     0.0549,     0.0001,     0.0163],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0174,     0.9502,     0.0092,     0.0232],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0075,     0.0235,     0.0007,     0.7860,     0.1822],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0317, 0.0253, 0.0227, 0.1718, 0.7485], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.43128245201759
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.347656946825804
printing an ep nov before normalisation:  64.74575519561768
printing an ep nov before normalisation:  67.75283147490462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.20825080955686
printing an ep nov before normalisation:  30.596914465957177
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.809800697953996
printing an ep nov before normalisation:  42.94244673940531
printing an ep nov before normalisation:  76.34990157003502
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 77.66775940383341
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.88213920593262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.00022648392302926368
printing an ep nov before normalisation:  60.18171685823123
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.6596346400214
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.419]
 [51.272]
 [63.563]
 [54.539]
 [48.292]] [[0.461]
 [0.648]
 [0.804]
 [0.69 ]
 [0.611]]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0002,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9734,     0.0173,     0.0002,     0.0082],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0164,     0.0394,     0.9132,     0.0005,     0.0305],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0018,     0.0001,     0.0345,     0.6947,     0.2689],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.1139, 0.0067, 0.0191, 0.3414, 0.5189], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.016026712545504
siam score:  -0.80897707
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.49331548195842
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.871]
 [41.877]
 [41.119]
 [32.407]
 [32.629]] [[0.227]
 [0.21 ]
 [0.203]
 [0.126]
 [0.128]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.812588
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.376]
 [35.376]
 [35.376]
 [35.376]
 [35.376]] [[0.217]
 [0.217]
 [0.217]
 [0.217]
 [0.217]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.585]
 [42.585]
 [53.834]
 [42.585]
 [42.585]] [[1.008]
 [1.008]
 [1.515]
 [1.008]
 [1.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.594]
 [58.905]
 [58.905]
 [58.905]
 [58.905]] [[1.548]
 [1.073]
 [1.073]
 [1.073]
 [1.073]]
printing an ep nov before normalisation:  42.91764259338379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -2.19387062442668e-11
0.0 -2.168355472344765e-11
0.0 -1.5830205837479845e-11
0.0 -3.076954365317458e-11
0.0 -2.075376528077839e-11
0.0 0.0
0.0 -3.0990098353662376e-11
0.0 -1.8517946015644475e-11
0.0 0.0
0.0 -2.0399147912607668e-11
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8221231
printing an ep nov before normalisation:  49.24738653127404
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.6450138092041
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.58062659914218
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.04866457754933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8326802
UNIT TEST: sample policy line 217 mcts : [0.154 0.59  0.128 0.051 0.077]
printing an ep nov before normalisation:  57.90358066558838
actions average: 
K:  0  action  0 :  tensor([    0.9922,     0.0015,     0.0001,     0.0008,     0.0054],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9990,     0.0005,     0.0000,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9793,     0.0115,     0.0093],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0002,     0.0760,     0.6178,     0.3055],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0015, 0.0016, 0.0744, 0.2436, 0.6789], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.3151455166074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82247025
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.496691097600262
actions average: 
K:  0  action  0 :  tensor([    0.9538,     0.0421,     0.0000,     0.0011,     0.0030],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9706,     0.0044,     0.0002,     0.0243],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0014,     0.9808,     0.0000,     0.0177],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0019,     0.0003,     0.0008,     0.7602,     0.2368],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0418, 0.0410, 0.0214, 0.2380, 0.6578], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.451]
 [67.105]
 [72.025]
 [67.105]
 [72.207]] [[0.858]
 [0.906]
 [0.997]
 [0.906]
 [1.   ]]
printing an ep nov before normalisation:  57.83160539538875
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.337]
 [56.337]
 [56.337]
 [56.337]
 [56.337]] [[56.337]
 [56.337]
 [56.337]
 [56.337]
 [56.337]]
siam score:  -0.82939833
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.612]
 [40.414]
 [45.789]
 [44.732]
 [37.612]] [[0.565]
 [0.683]
 [0.91 ]
 [0.865]
 [0.565]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.925380345552874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.864407790959994
printing an ep nov before normalisation:  23.786976917111932
printing an ep nov before normalisation:  59.901046734252894
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.603]
 [54.3  ]
 [44.598]
 [44.552]
 [47.775]] [[1.129]
 [0.899]
 [0.593]
 [0.591]
 [0.693]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.34806111215148
printing an ep nov before normalisation:  46.508104825449905
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9967,     0.0000,     0.0000,     0.0025,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0083,     0.9542,     0.0006,     0.0013,     0.0357],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0082,     0.9271,     0.0099,     0.0547],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0537,     0.7989,     0.1471],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0031, 0.0054, 0.0627, 0.2778, 0.6509], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.39118402141724
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.574]
 [27.147]
 [26.165]
 [25.154]
 [25.871]] [[1.192]
 [1.333]
 [1.245]
 [1.155]
 [1.219]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.893]
 [29.204]
 [29.204]
 [29.204]
 [29.204]] [[1.158]
 [0.682]
 [0.682]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.510893054422056
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.43779323122438
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.04164649763686
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.766912912743763
siam score:  -0.8263065
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.90875990715337
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.352757930755615
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.12249334268618
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.388]
 [41.327]
 [43.193]
 [39.928]
 [40.432]] [[0.717]
 [0.781]
 [0.843]
 [0.735]
 [0.752]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.21987393174255
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.588062194846025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8278651
printing an ep nov before normalisation:  33.32142844401486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.89836561745613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.65177031372699
printing an ep nov before normalisation:  36.335216716577555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 9.758]
 [17.903]
 [18.099]
 [13.851]
 [18.775]] [[0.067]
 [0.35 ]
 [0.357]
 [0.209]
 [0.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.299026935883262
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8243714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.708804752105728
printing an ep nov before normalisation:  34.253271469508235
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.135]
 [32.859]
 [32.859]
 [32.859]
 [32.859]] [[1.191]
 [0.901]
 [0.901]
 [0.901]
 [0.901]]
printing an ep nov before normalisation:  54.06302131073974
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.99 ]
 [63.586]
 [69.301]
 [63.042]
 [69.694]] [[0.928]
 [1.086]
 [1.248]
 [1.071]
 [1.26 ]]
printing an ep nov before normalisation:  47.81625113446687
actions average: 
K:  0  action  0 :  tensor([    0.9958,     0.0003,     0.0023,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9792,     0.0003,     0.0002,     0.0196],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9878,     0.0023,     0.0099],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0278,     0.0002,     0.0322,     0.7571,     0.1828],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0276,     0.0005,     0.0131,     0.2625,     0.6964],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.55492268848662
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.68491094148781
printing an ep nov before normalisation:  57.02176457132382
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.14129254723578
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.68575816390745
printing an ep nov before normalisation:  32.36315092749827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.595]
 [30.466]
 [48.823]
 [24.187]
 [23.803]] [[0.238]
 [0.408]
 [0.863]
 [0.252]
 [0.243]]
siam score:  -0.8317418
printing an ep nov before normalisation:  84.77795561417905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.753971198972295
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[64.563]
 [53.405]
 [55.864]
 [58.646]
 [58.646]] [[1.094]
 [0.785]
 [0.853]
 [0.93 ]
 [0.93 ]]
actions average: 
K:  0  action  0 :  tensor([    0.9989,     0.0001,     0.0001,     0.0006,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0049,     0.9843,     0.0000,     0.0001,     0.0106],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9793,     0.0008,     0.0199],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0005,     0.8115,     0.1878],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0356, 0.0107, 0.1627, 0.2607, 0.5304], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    0.9976,     0.0016,     0.0001,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0017,     0.9852,     0.0005,     0.0003,     0.0124],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9969,     0.0002,     0.0029],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0295,     0.7751,     0.1952],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0222, 0.0010, 0.0506, 0.2116, 0.7146], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.679758706501325
printing an ep nov before normalisation:  37.34459533144703
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  23.449325561523438
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83033556
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.775]
 [43.775]
 [58.415]
 [43.775]
 [43.775]] [[0.927]
 [0.927]
 [1.449]
 [0.927]
 [0.927]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9433,     0.0075,     0.0000,     0.0410,     0.0081],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9977,     0.0005,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0002,     0.9603,     0.0272,     0.0122],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0017,     0.0193,     0.7732,     0.2055],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0029, 0.0622, 0.0202, 0.2346, 0.6801], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.76364517211914
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.573]
 [60.327]
 [38.573]
 [38.573]
 [38.573]] [[0.288]
 [0.553]
 [0.288]
 [0.288]
 [0.288]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.11385834437871
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.370221751482575
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.531]
 [40.531]
 [40.531]
 [40.531]
 [40.531]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
printing an ep nov before normalisation:  48.8608455657959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.82692045
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0006,     0.9979,     0.0005,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0029,     0.9551,     0.0150,     0.0270],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0072,     0.9268,     0.0658],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0626, 0.0050, 0.1027, 0.2307, 0.5991], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  23.142267870334816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.982]
 [30.899]
 [31.927]
 [31.452]
 [30.633]] [[0.932]
 [0.824]
 [0.877]
 [0.853]
 [0.81 ]]
printing an ep nov before normalisation:  28.9934885318166
siam score:  -0.83076537
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83623004
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.7434537117309
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.716]
 [46.576]
 [50.908]
 [48.993]
 [51.485]] [[0.962]
 [1.038]
 [1.215]
 [1.137]
 [1.239]]
actions average: 
K:  0  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9971,     0.0001,     0.0001,     0.0019],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0026,     0.9328,     0.0033,     0.0613],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0027,     0.0000,     0.0226,     0.7708,     0.2039],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0269, 0.0010, 0.0653, 0.2116, 0.6951], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.965882858968193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.83490800857544
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8319241
printing an ep nov before normalisation:  53.01271420549261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.60237302228925
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.333]
 [26.536]
 [40.509]
 [24.427]
 [23.608]] [[0.17 ]
 [0.186]
 [0.373]
 [0.158]
 [0.147]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.21115273709905
printing an ep nov before normalisation:  44.40691866254565
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.04756829624777
printing an ep nov before normalisation:  34.97447621618205
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.374]
 [61.374]
 [61.374]
 [61.374]
 [61.374]] [[1.333]
 [1.333]
 [1.333]
 [1.333]
 [1.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.052]
 [35.052]
 [48.347]
 [35.052]
 [35.052]] [[0.437]
 [0.437]
 [0.855]
 [0.437]
 [0.437]]
actions average: 
K:  4  action  0 :  tensor([    0.9951,     0.0017,     0.0000,     0.0019,     0.0013],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9976,     0.0000,     0.0000,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0014,     0.8664,     0.0302,     0.1019],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0043,     0.0770,     0.7224,     0.1962],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.0062, 0.0830, 0.4196, 0.4899], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.56327440111677
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.816]
 [39.488]
 [39.488]
 [39.488]
 [39.488]] [[1.721]
 [1.512]
 [1.512]
 [1.512]
 [1.512]]
printing an ep nov before normalisation:  73.2529527880225
printing an ep nov before normalisation:  60.76915057736641
printing an ep nov before normalisation:  61.599656092892296
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 1.9028249359755501e-13
0.0 -7.351823494855352e-12
0.0 -9.600616570105603e-13
0.0 0.0
0.0 0.0
0.0 -2.5428660070859278e-12
0.0 0.0
0.0 0.0
0.0 -4.082424340758857e-12
0.0 -1.1892655682701808e-12
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9995,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0065,     0.9698,     0.0056,     0.0006,     0.0176],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0004,     0.0004,     0.9245,     0.0397,     0.0350],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0006,     0.0001,     0.0260,     0.7792,     0.1942],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0016, 0.0121, 0.0660, 0.1356, 0.7848], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.12616654615016
printing an ep nov before normalisation:  64.87568507063465
printing an ep nov before normalisation:  40.9907482106197
printing an ep nov before normalisation:  39.33527955141588
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.794960021972656
siam score:  -0.82983214
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.025002355371832
printing an ep nov before normalisation:  58.9504425228269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.660526678849195
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.33948998390057
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 35.13759154188835
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.586]
 [35.586]
 [39.776]
 [35.586]
 [35.586]] [[1.044]
 [1.044]
 [1.235]
 [1.044]
 [1.044]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.783715950663435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.31033781165895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.1012979389853
printing an ep nov before normalisation:  20.476198994995688
printing an ep nov before normalisation:  54.27575056793255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.67498164372409
printing an ep nov before normalisation:  49.17139485597001
printing an ep nov before normalisation:  22.668411349348776
actions average: 
K:  4  action  0 :  tensor([0.9544, 0.0179, 0.0025, 0.0072, 0.0180], grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9970,     0.0001,     0.0000,     0.0028],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0145,     0.9560,     0.0000,     0.0293],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0005,     0.0289,     0.8325,     0.1378],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0065, 0.0934, 0.1714, 0.1186, 0.6101], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.0655225730686
line 256 mcts: sample exp_bonus 33.51835690009471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9971,     0.0006,     0.0000,     0.0005,     0.0018],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9993,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0031, 0.0014, 0.9644, 0.0110, 0.0201], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0003,     0.0047,     0.7689,     0.2259],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0153, 0.0534, 0.0389, 0.2953, 0.5971], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.539]
 [42.683]
 [30.013]
 [30.013]
 [30.013]] [[0.504]
 [0.473]
 [0.257]
 [0.257]
 [0.257]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.058]
 [22.856]
 [22.856]
 [22.856]
 [22.856]] [[0.56 ]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.421826362609863
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.413]
 [50.021]
 [29.208]
 [42.555]
 [46.671]] [[1.312]
 [1.337]
 [0.493]
 [1.034]
 [1.201]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.810490878166945
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0001,     0.0004,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9993,     0.0001,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0018, 0.0127, 0.9224, 0.0266, 0.0365], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0173,     0.8346,     0.1479],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0009, 0.0140, 0.0529, 0.2257, 0.7065], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  25.734776132981807
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 31.44985330104828
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.52931327022229
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.18264543667505
printing an ep nov before normalisation:  29.72806692123413
printing an ep nov before normalisation:  40.974766123652486
line 256 mcts: sample exp_bonus 59.47820696035535
printing an ep nov before normalisation:  69.02581994953302
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.513 0.385 0.051]
printing an ep nov before normalisation:  46.67076431495018
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.779]
 [70.779]
 [70.779]
 [70.779]
 [70.779]] [[1.25]
 [1.25]
 [1.25]
 [1.25]
 [1.25]]
printing an ep nov before normalisation:  48.1668711286564
printing an ep nov before normalisation:  53.496401557626115
printing an ep nov before normalisation:  40.579864414287414
printing an ep nov before normalisation:  44.79924011965916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.63447023773732
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.26224904601217
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.407324320432526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  53.069639061559414
printing an ep nov before normalisation:  5.360708580849405
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  28.915697626816986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.10081108759385
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[17.244]
 [23.203]
 [22.171]
 [38.5  ]
 [38.5  ]] [[0.019]
 [0.093]
 [0.08 ]
 [0.284]
 [0.284]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.652883146067204
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  59.81116152087378
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.169813254359358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.74777446557906
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.709]
 [31.514]
 [31.514]
 [31.514]
 [31.514]] [[1.189]
 [1.453]
 [1.453]
 [1.453]
 [1.453]]
printing an ep nov before normalisation:  53.953652193293905
printing an ep nov before normalisation:  53.47258687740104
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.4808539064273
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.532479343675995
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  53.4996990858227
printing an ep nov before normalisation:  52.095045583135814
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9988,     0.0001,     0.0001,     0.0003,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0018,     0.8698,     0.0769,     0.0004,     0.0510],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0015,     0.9278,     0.0152,     0.0556],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0000,     0.0012,     0.7306,     0.2680],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0022, 0.0016, 0.0666, 0.2554, 0.6742], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.793794627472636
siam score:  -0.8198525
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.72340917526787
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.741]
 [48.71 ]
 [48.642]
 [48.741]
 [48.741]] [[2.   ]
 [1.998]
 [1.992]
 [2.   ]
 [2.   ]]
printing an ep nov before normalisation:  58.41821765987994
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.24 ]
 [50.219]
 [47.505]
 [47.505]
 [47.505]] [[1.744]
 [1.779]
 [1.683]
 [1.683]
 [1.683]]
printing an ep nov before normalisation:  45.4327432816849
siam score:  -0.8231906
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.472940800603965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.217]
 [29.446]
 [28.88 ]
 [30.084]
 [30.945]] [[1.069]
 [0.699]
 [0.672]
 [0.729]
 [0.77 ]]
printing an ep nov before normalisation:  30.76536390774029
printing an ep nov before normalisation:  45.32555103302002
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.81689621218108
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.678]
 [52.089]
 [59.511]
 [59.911]
 [51.91 ]] [[0.399]
 [0.677]
 [0.858]
 [0.867]
 [0.672]]
printing an ep nov before normalisation:  17.26274503392844
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.442893893514
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.731]
 [35.773]
 [51.388]
 [38.495]
 [38.495]] [[0.645]
 [0.793]
 [1.364]
 [0.892]
 [0.892]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.288882917354385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.532]
 [37.557]
 [37.557]
 [37.557]
 [37.557]] [[1.359]
 [1.131]
 [1.131]
 [1.131]
 [1.131]]
siam score:  -0.8312952
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.96365596538441
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8276643
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.299017594146335
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8291591
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8288824
printing an ep nov before normalisation:  28.32023817290288
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.607]
 [29.976]
 [31.032]
 [28.902]
 [29.373]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.412829742908738
printing an ep nov before normalisation:  35.32057320147266
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.664]
 [40.287]
 [53.033]
 [27.664]
 [27.664]] [[0.432]
 [0.692]
 [0.954]
 [0.432]
 [0.432]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.318]
 [60.647]
 [60.647]
 [60.647]
 [60.647]] [[1.143]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.97601853689223
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.375752433439736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.027724073735328
printing an ep nov before normalisation:  33.616440616250365
printing an ep nov before normalisation:  26.720545713857106
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9945,     0.0001,     0.0000,     0.0014,     0.0039],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9992,     0.0001,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0080,     0.9453,     0.0010,     0.0457],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0001,     0.0411,     0.7108,     0.2481],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0009,     0.1093,     0.2274,     0.6619],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.05754053917339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.55955710322849
printing an ep nov before normalisation:  47.614874224625154
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.182]
 [55.748]
 [54.552]
 [57.402]
 [56.034]] [[1.441]
 [1.368]
 [1.307]
 [1.453]
 [1.383]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.89 ]
 [55.525]
 [49.395]
 [55.32 ]
 [55.32 ]] [[0.223]
 [0.156]
 [0.122]
 [0.155]
 [0.155]]
printing an ep nov before normalisation:  28.216159183794307
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.15000789517993
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.40345187651612
siam score:  -0.8275003
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.397]
 [39.713]
 [38.72 ]
 [42.203]
 [40.608]] [[0.944]
 [0.743]
 [0.708]
 [0.831]
 [0.775]]
siam score:  -0.83248246
printing an ep nov before normalisation:  32.687447318471406
printing an ep nov before normalisation:  66.77048692428285
printing an ep nov before normalisation:  40.16523614045741
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.801764488220215
actions average: 
K:  2  action  0 :  tensor([    0.9939,     0.0004,     0.0001,     0.0013,     0.0043],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9752,     0.0016,     0.0000,     0.0230],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0460,     0.8776,     0.0140,     0.0624],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0012, 0.0013, 0.0492, 0.8401, 0.1081], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0014, 0.0382, 0.1291, 0.1528, 0.6785], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.02933723228284
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.817776679992676
siam score:  -0.82988656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.764]
 [42.765]
 [45.495]
 [20.554]
 [37.884]] [[0.102]
 [0.394]
 [0.427]
 [0.124]
 [0.334]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8262117
printing an ep nov before normalisation:  35.599637250324285
printing an ep nov before normalisation:  18.17701520278349
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.118]
 [29.59 ]
 [28.993]
 [13.975]
 [28.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  76.63220952858414
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[73.284]
 [73.284]
 [73.284]
 [73.284]
 [73.284]] [[0.772]
 [0.772]
 [0.772]
 [0.772]
 [0.772]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.341]
 [55.489]
 [55.489]
 [55.489]
 [55.489]] [[0.587]
 [0.475]
 [0.475]
 [0.475]
 [0.475]]
printing an ep nov before normalisation:  98.36312452111369
actions average: 
K:  1  action  0 :  tensor([    0.9970,     0.0000,     0.0000,     0.0000,     0.0030],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9987,     0.0000,     0.0000,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0089,     0.8968,     0.0250,     0.0691],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0007,     0.0004,     0.0693,     0.7272,     0.2024],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0095, 0.0161, 0.0840, 0.1609, 0.7295], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.63311484901608
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.5782434847912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9990,     0.0009,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9997,     0.0001,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0056,     0.9132,     0.0243,     0.0568],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0006,     0.0033,     0.7694,     0.2264],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0300, 0.1291, 0.0435, 0.3643, 0.4333], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  27.46446605711526
printing an ep nov before normalisation:  36.47079944610596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.92278321570269
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.297371732006404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.907]
 [55.125]
 [41.411]
 [41.411]
 [50.356]] [[0.227]
 [0.296]
 [0.164]
 [0.164]
 [0.25 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.891]
 [38.577]
 [34.265]
 [29.899]
 [34.819]] [[0.486]
 [0.689]
 [0.558]
 [0.426]
 [0.575]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.763095727823822e-10
0.0 2.8089847231071517e-09
0.0 1.0296012582325992e-09
0.0 4.591689482243758e-09
0.0 0.0
0.0 7.774942566166507e-10
0.0 5.361399455070229e-09
0.0 8.826685786827065e-10
0.0 2.4783775448837472e-09
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.32810937461748
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.678223184123176
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  68.7751078873367
actions average: 
K:  2  action  0 :  tensor([    0.9971,     0.0003,     0.0001,     0.0001,     0.0025],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9831,     0.0004,     0.0007,     0.0155],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0259,     0.9124,     0.0135,     0.0482],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0020,     0.0000,     0.0009,     0.9197,     0.0774],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0026,     0.0004,     0.1265,     0.2067,     0.6637],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.758]
 [53.758]
 [53.758]
 [53.758]
 [53.758]] [[1.207]
 [1.207]
 [1.207]
 [1.207]
 [1.207]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9719,     0.0058,     0.0001,     0.0024,     0.0198],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9974,     0.0003,     0.0001,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0104,     0.8715,     0.0819,     0.0361],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0006,     0.0349,     0.7101,     0.2541],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0010, 0.0482, 0.2748, 0.1548, 0.5211], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  32.073964502852334
printing an ep nov before normalisation:  73.5206194854
printing an ep nov before normalisation:  32.32718572775965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.29685995195054
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.92341375350952
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.54277515411377
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.0158868501659
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.465976415992344
siam score:  -0.82903594
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.59003994224713
printing an ep nov before normalisation:  58.11842282439439
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.66446146953035
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8328473
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.44689034414045
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.663019999156504
printing an ep nov before normalisation:  34.98968827608825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.761]
 [33.618]
 [30.729]
 [28.24 ]
 [30.999]] [[0.306]
 [0.32 ]
 [0.274]
 [0.234]
 [0.278]]
siam score:  -0.83318144
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  15.39190386501593
siam score:  -0.8375572
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.31195091069503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.438093979278015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.644]
 [41.675]
 [34.644]
 [34.644]
 [34.644]] [[0.444]
 [0.615]
 [0.444]
 [0.444]
 [0.444]]
siam score:  -0.83461267
printing an ep nov before normalisation:  73.39930493202864
printing an ep nov before normalisation:  64.88432884216309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.19071607545399
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.46084970275507
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.636]
 [30.906]
 [33.842]
 [34.65 ]
 [30.906]] [[0.924]
 [0.639]
 [0.764]
 [0.798]
 [0.639]]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9987,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9687,     0.0149,     0.0163],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0013,     0.0001,     0.0002,     0.7540,     0.2444],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0020, 0.0018, 0.0441, 0.2391, 0.7130], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.50521288005182
printing an ep nov before normalisation:  53.315429070544006
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.0020155611535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.49799823760986
printing an ep nov before normalisation:  34.145061160867314
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.002]
 [43.406]
 [49.985]
 [49.688]
 [42.969]] [[0.334]
 [0.483]
 [0.557]
 [0.553]
 [0.478]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9992,     0.0002,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9505,     0.0183,     0.0002,     0.0309],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0044,     0.9361,     0.0108,     0.0488],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0015, 0.0009, 0.0019, 0.6730, 0.3227], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0121, 0.0060, 0.2171, 0.0686, 0.6962], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.682]
 [44.473]
 [37.793]
 [34.13 ]
 [35.281]] [[0.392]
 [0.265]
 [0.189]
 [0.148]
 [0.161]]
siam score:  -0.83060676
UNIT TEST: sample policy line 217 mcts : [0.179 0.103 0.59  0.077 0.051]
printing an ep nov before normalisation:  45.37196387858917
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.46243403538682
printing an ep nov before normalisation:  21.42509775417644
printing an ep nov before normalisation:  30.515975952148438
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.078805923461914
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.89138476053874
siam score:  -0.82845545
siam score:  -0.8295822
printing an ep nov before normalisation:  29.36056176146926
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8270699
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83807236
printing an ep nov before normalisation:  33.68743703625052
printing an ep nov before normalisation:  55.624952038197314
printing an ep nov before normalisation:  40.69934594570235
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.074]
 [52.558]
 [40.785]
 [47.231]
 [44.902]] [[0.655]
 [0.637]
 [0.494]
 [0.572]
 [0.544]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.8981725603351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9651,     0.0276,     0.0000,     0.0073],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0115,     0.9133,     0.0221,     0.0531],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0007, 0.0014, 0.0529, 0.7088, 0.2362], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0027, 0.0133, 0.0356, 0.1758, 0.7725], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.655086823838936
siam score:  -0.83360374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.829847
line 256 mcts: sample exp_bonus 41.518012096986176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.39358327863335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.663]
 [48.663]
 [48.663]
 [48.663]
 [48.663]] [[0.178]
 [0.178]
 [0.178]
 [0.178]
 [0.178]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.818]
 [50.289]
 [41.969]
 [41.969]
 [41.969]] [[0.579]
 [0.871]
 [0.615]
 [0.615]
 [0.615]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.164140258922586
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.01737863049567
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.288]
 [32.021]
 [32.99 ]
 [41.179]
 [32.78 ]] [[0.354]
 [0.37 ]
 [0.391]
 [0.567]
 [0.386]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.539370659319964
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  59.04022405270329
siam score:  -0.8303555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8280694
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.370024154747
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.604304161618984
siam score:  -0.8346533
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.830325
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.53265209548783
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.116283601363747
printing an ep nov before normalisation:  51.865120040823655
printing an ep nov before normalisation:  35.65852783193403
printing an ep nov before normalisation:  80.48741197350215
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.82556474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.464]
 [18.707]
 [18.707]
 [18.707]
 [17.97 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.004]
 [28.022]
 [24.687]
 [24.575]
 [24.195]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0110,     0.9466,     0.0270,     0.0001,     0.0154],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0036, 0.0233, 0.8370, 0.0246, 0.1114], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0022,     0.0005,     0.0001,     0.8169,     0.1802],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0244,     0.0232,     0.0002,     0.1796,     0.7726],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  61.22285939407998
printing an ep nov before normalisation:  53.116069813935454
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.746522877691813
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.177]
 [20.177]
 [21.792]
 [23.962]
 [20.177]] [[0.467]
 [0.467]
 [0.538]
 [0.634]
 [0.467]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.842]
 [31.696]
 [35.006]
 [33.601]
 [31.998]] [[1.274]
 [1.228]
 [1.362]
 [1.305]
 [1.24 ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.99 ]
 [43.929]
 [34.99 ]
 [34.99 ]
 [34.99 ]] [[0.826]
 [1.229]
 [0.826]
 [0.826]
 [0.826]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.86681583401891
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  63.100359132807434
printing an ep nov before normalisation:  60.68705685163439
printing an ep nov before normalisation:  35.34178790996394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.440692793514
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.446]
 [39.446]
 [48.684]
 [39.446]
 [39.446]] [[0.704]
 [0.704]
 [1.018]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.97087805235772
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.897]
 [28.897]
 [55.52 ]
 [30.711]
 [28.897]] [[0.378]
 [0.378]
 [0.915]
 [0.414]
 [0.378]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.71 ]
 [48.654]
 [46.723]
 [40.71 ]
 [40.71 ]] [[0.88 ]
 [1.219]
 [1.137]
 [0.88 ]
 [0.88 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.731322765350342
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.83]
 [29.83]
 [29.83]
 [29.83]
 [29.83]] [[0.333]
 [0.333]
 [0.333]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.60824124101939
siam score:  -0.83118767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.762101444482575
printing an ep nov before normalisation:  53.253290577636754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83077985
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.77131892928349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.574]
 [28.344]
 [57.127]
 [55.243]
 [41.776]] [[0.125]
 [0.139]
 [0.656]
 [0.622]
 [0.38 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.7070602675821
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8301108
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.66 ]
 [42.899]
 [30.687]
 [24.492]
 [28.3  ]] [[0.382]
 [0.462]
 [0.275]
 [0.179]
 [0.238]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.788]
 [35.299]
 [35.221]
 [28.813]
 [33.831]] [[0.372]
 [0.432]
 [0.43 ]
 [0.276]
 [0.397]]
printing an ep nov before normalisation:  57.970023936924804
printing an ep nov before normalisation:  37.54327649056617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9960,     0.0003,     0.0003,     0.0027],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0007,     0.9419,     0.0076,     0.0495],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0101,     0.0002,     0.0161,     0.7942,     0.1793],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0089, 0.0171, 0.0966, 0.3020, 0.5755], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.03885683667357
actions average: 
K:  0  action  0 :  tensor([    0.9662,     0.0001,     0.0000,     0.0332,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9876,     0.0002,     0.0001,     0.0119],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0006,     0.8879,     0.0548,     0.0567],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0001,     0.0006,     0.8397,     0.1595],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0004,     0.0028,     0.0226,     0.3125,     0.6616],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.822625572253045
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8391716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.30211169570117
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.585]
 [41.585]
 [47.127]
 [40.742]
 [41.585]] [[0.92 ]
 [0.92 ]
 [1.143]
 [0.886]
 [0.92 ]]
siam score:  -0.8428202
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.940749600027026
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.673739354284294
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.311]
 [47.147]
 [42.171]
 [43.933]
 [44.256]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.778]
 [27.778]
 [27.778]
 [27.778]
 [27.778]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.319706904421054
printing an ep nov before normalisation:  41.27524362590326
printing an ep nov before normalisation:  42.43886314884173
printing an ep nov before normalisation:  21.686382066346823
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.559594664525754
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[16.315]
 [19.726]
 [24.411]
 [21.662]
 [11.386]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  17.160109129599732
printing an ep nov before normalisation:  28.991455731130912
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.304909467697144
actions average: 
K:  2  action  0 :  tensor([    0.9727,     0.0170,     0.0000,     0.0001,     0.0101],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9776,     0.0026,     0.0002,     0.0194],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0002,     0.9580,     0.0165,     0.0252],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0091,     0.0004,     0.0188,     0.8268,     0.1449],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0008, 0.0072, 0.0855, 0.2763, 0.6301], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.64176321029663
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.897]
 [33.026]
 [33.026]
 [33.026]
 [33.026]] [[1.334]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.713852510941905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0268,     0.9186,     0.0220,     0.0001,     0.0325],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0213,     0.9019,     0.0134,     0.0634],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0002,     0.1063,     0.7739,     0.1190],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0074,     0.0003,     0.0361,     0.1423,     0.8139],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.318294561720045
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.07 ]
 [56.577]
 [40.322]
 [40.322]
 [40.322]] [[0.498]
 [0.779]
 [0.441]
 [0.441]
 [0.441]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.557342919339526
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9841,     0.0002,     0.0000,     0.0054,     0.0103],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9584,     0.0179,     0.0001,     0.0233],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0072,     0.9392,     0.0139,     0.0397],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0004,     0.0558,     0.7540,     0.1893],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0024,     0.0004,     0.2106,     0.1834,     0.6033],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  79.72755118483886
printing an ep nov before normalisation:  38.70582103729248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.7331942530114
actions average: 
K:  2  action  0 :  tensor([    0.9467,     0.0526,     0.0005,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0058,     0.9525,     0.0319,     0.0000,     0.0098],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0049,     0.9075,     0.0172,     0.0700],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0078, 0.0016, 0.0041, 0.7925, 0.1939], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0425, 0.0061, 0.0286, 0.1888, 0.7339], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.828179859737915
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.663]
 [46.078]
 [49.508]
 [39.257]
 [36.519]] [[0.489]
 [0.653]
 [0.713]
 [0.534]
 [0.487]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.662]
 [35.168]
 [24.831]
 [24.831]
 [24.831]] [[0.222]
 [0.404]
 [0.183]
 [0.183]
 [0.183]]
printing an ep nov before normalisation:  35.9922176412176
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8299141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.829892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.14851855078429
printing an ep nov before normalisation:  42.235660509267625
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.032522488872125
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.322234520514144
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.01768427461361
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.35786641518375
printing an ep nov before normalisation:  39.79855347076088
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.31671423406936
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8321816
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.883057599969504
actions average: 
K:  4  action  0 :  tensor([    0.9980,     0.0001,     0.0000,     0.0004,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9420,     0.0297,     0.0000,     0.0279],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0052,     0.9176,     0.0221,     0.0551],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0003,     0.0750,     0.6825,     0.2416],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0029, 0.0025, 0.0379, 0.1957, 0.7610], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.137706907990875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.254305581512604
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.806]
 [59.248]
 [66.724]
 [63.987]
 [71.563]] [[0.638]
 [0.81 ]
 [1.01 ]
 [0.937]
 [1.139]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.905]
 [55.438]
 [36.152]
 [35.097]
 [34.502]] [[0.389]
 [0.978]
 [0.515]
 [0.49 ]
 [0.476]]
printing an ep nov before normalisation:  26.32629632949829
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9990,     0.0001,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9572,     0.0316,     0.0001,     0.0108],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0067,     0.8902,     0.0418,     0.0613],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0005,     0.0010,     0.8008,     0.1974],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0016, 0.0025, 0.0105, 0.1948, 0.7907], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.284166761732386
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.745205402374268
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.257]
 [23.698]
 [23.698]
 [23.698]
 [23.698]] [[1.371]
 [0.853]
 [0.853]
 [0.853]
 [0.853]]
printing an ep nov before normalisation:  21.2152172017469
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 0.0
0.0 -1.2454853923321173e-11
0.0 -3.6326657304210987e-12
0.0 0.0
0.0 -6.3095944015651094e-12
0.0 -9.41898328325227e-12
0.0 0.0
0.0 0.0
0.0 -9.860092694296865e-12
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -7.014504539971698e-12
0.0 -4.082424342369897e-12
0.0 -4.082424342369897e-12
0.0 -5.059784408438544e-12
0.0 0.0
0.0 -6.7074577941934665e-12
0.0 -5.085732018602215e-12
0.0 -5.215470084121306e-12
0.0 0.0
0.0 -4.765711463311788e-12
printing an ep nov before normalisation:  54.835478164747315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9829,     0.0001,     0.0000,     0.0150,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0038,     0.9808,     0.0002,     0.0001,     0.0152],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0035, 0.0115, 0.8771, 0.0198, 0.0882], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0001,     0.0239,     0.7724,     0.2028],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0308, 0.0012, 0.0233, 0.2328, 0.7119], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.61771556360656
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.83425623
printing an ep nov before normalisation:  41.52575725472768
printing an ep nov before normalisation:  59.23129626449598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.966]
 [44.131]
 [44.131]
 [45.108]
 [44.131]] [[1.694]
 [1.768]
 [1.768]
 [1.83 ]
 [1.768]]
line 256 mcts: sample exp_bonus 35.62965691762932
printing an ep nov before normalisation:  38.19244886693666
printing an ep nov before normalisation:  18.164825889297557
printing an ep nov before normalisation:  13.150070772251443
printing an ep nov before normalisation:  23.97562545057211
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.268807010505906
siam score:  -0.8326141
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.38 ]
 [55.792]
 [39.38 ]
 [39.38 ]
 [39.38 ]] [[0.737]
 [1.412]
 [0.737]
 [0.737]
 [0.737]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  65.4579158065261
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.83550537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.9042366267686
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.83541026773841
siam score:  -0.8301193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.672]
 [23.672]
 [61.298]
 [23.672]
 [23.672]] [[0.238]
 [0.238]
 [0.96 ]
 [0.238]
 [0.238]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  74.98788604771754
printing an ep nov before normalisation:  97.96864740591214
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.947]
 [32.947]
 [33.227]
 [32.947]
 [32.947]] [[0.326]
 [0.326]
 [0.33 ]
 [0.326]
 [0.326]]
siam score:  -0.8396683
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8421506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.959]
 [55.959]
 [55.959]
 [55.959]
 [55.959]] [[0.499]
 [0.499]
 [0.499]
 [0.499]
 [0.499]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8439749
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.26669510947959
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.43020057678223
printing an ep nov before normalisation:  6.8121867535032266
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.300456140638346
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.035]
 [49.265]
 [35.984]
 [43.517]
 [34.112]] [[0.722]
 [1.045]
 [0.763]
 [0.923]
 [0.724]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.942585706710815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.45079229575803
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.341]
 [31.81 ]
 [35.585]
 [32.524]
 [36.272]] [[0.422]
 [0.505]
 [0.632]
 [0.529]
 [0.655]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.18212648290582
printing an ep nov before normalisation:  40.0444772060144
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.69203887057034
using explorer policy with actor:  1
printing an ep nov before normalisation:  57.86430926046768
printing an ep nov before normalisation:  48.313276942444716
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.555]
 [31.759]
 [35.569]
 [34.461]
 [33.555]] [[0.428]
 [0.384]
 [0.476]
 [0.45 ]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.375044462286645
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.31321620941162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.91438556898635
siam score:  -0.8281613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.11574735550916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.723]
 [36.925]
 [36.925]
 [58.511]
 [36.925]] [[1.354]
 [0.458]
 [0.458]
 [1.238]
 [0.458]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9853,     0.0026,     0.0003,     0.0004,     0.0113],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9799,     0.0012,     0.0005,     0.0182],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0068,     0.9441,     0.0303,     0.0187],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0439,     0.9088,     0.0473],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0333, 0.0934, 0.1658, 0.7064], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8403164
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.73478143275869
line 256 mcts: sample exp_bonus 33.9422876342916
printing an ep nov before normalisation:  46.120598702026754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.10175842981027
printing an ep nov before normalisation:  60.08519422531493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.67341397747673
actions average: 
K:  4  action  0 :  tensor([    0.9971,     0.0002,     0.0000,     0.0000,     0.0027],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9193,     0.0544,     0.0002,     0.0259],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0030,     0.0018,     0.9224,     0.0003,     0.0724],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0004,     0.0481,     0.7629,     0.1884],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0094, 0.0603, 0.1152, 0.0932, 0.7220], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9926,     0.0010,     0.0000,     0.0009,     0.0056],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9909,     0.0005,     0.0002,     0.0081],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0507,     0.8719,     0.0168,     0.0606],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0002,     0.0705,     0.6912,     0.2379],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0325, 0.0976, 0.2661, 0.6030], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.251]
 [31.56 ]
 [37.318]
 [27.902]
 [32.251]] [[0.483]
 [0.464]
 [0.625]
 [0.361]
 [0.483]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.59477138519287
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.044]
 [38.937]
 [36.92 ]
 [38.937]
 [38.937]] [[1.063]
 [0.799]
 [0.724]
 [0.799]
 [0.799]]
actions average: 
K:  3  action  0 :  tensor([    0.9979,     0.0005,     0.0000,     0.0001,     0.0016],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0008,     0.9414,     0.0276,     0.0003,     0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9476,     0.0176,     0.0348],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0039,     0.0003,     0.0270,     0.7711,     0.1977],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0230, 0.0238, 0.1613, 0.1236, 0.6684], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.085340369962964
printing an ep nov before normalisation:  44.6030380434567
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.756]
 [33.756]
 [40.519]
 [49.732]
 [33.756]] [[0.444]
 [0.444]
 [0.675]
 [0.989]
 [0.444]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.135627979902818
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.45480868553763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  61.94482391728544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0008,     0.9856,     0.0002,     0.0001,     0.0134],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0190,     0.9507,     0.0010,     0.0293],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0023,     0.0003,     0.0047,     0.7511,     0.2415],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0085, 0.0013, 0.1039, 0.2037, 0.6827], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  45.51783027433728
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9963,     0.0000,     0.0000,     0.0025,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0017,     0.9697,     0.0074,     0.0002,     0.0210],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0036,     0.0002,     0.9280,     0.0269,     0.0414],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0142,     0.0005,     0.0167,     0.7841,     0.1844],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0024, 0.0460, 0.0511, 0.3319, 0.5686], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.54904863258494
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.012133659914806
printing an ep nov before normalisation:  20.83369493484497
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.421]
 [30.357]
 [33.615]
 [33.973]
 [35.802]] [[0.709]
 [0.523]
 [0.623]
 [0.634]
 [0.69 ]]
siam score:  -0.8301839
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.914]
 [76.914]
 [76.914]
 [76.914]
 [76.914]] [[0.988]
 [0.988]
 [0.988]
 [0.988]
 [0.988]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  2  action  0 :  tensor([    0.9956,     0.0001,     0.0001,     0.0019,     0.0024],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9755,     0.0045,     0.0020,     0.0175],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9747,     0.0010,     0.0242],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0001,     0.0001,     0.7663,     0.2325],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0013, 0.0053, 0.0652, 0.2885, 0.6396], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8326962
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.734]
 [24.342]
 [24.342]
 [24.342]
 [34.242]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Sims:  40 1 epoch:  95574 pick best:  False frame count:  95574
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [ 0.   ]
 [27.676]
 [ 0.   ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
line 256 mcts: sample exp_bonus 22.96975415339901
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9775,     0.0012,     0.0001,     0.0210],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0018,     0.0000,     0.9425,     0.0275,     0.0281],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0231,     0.7619,     0.2143],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0044, 0.0245, 0.1732, 0.1239, 0.6740], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.49464133741509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.935]
 [32.135]
 [23.454]
 [27.69 ]
 [27.853]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  25.581954951071754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.29818002605692
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.841]
 [37.841]
 [40.157]
 [42.958]
 [37.841]] [[1.157]
 [1.157]
 [1.276]
 [1.42 ]
 [1.157]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.25186491012573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.973161697387695
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.693467445300108
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.383]
 [43.644]
 [43.644]
 [43.644]
 [43.644]] [[0.481]
 [0.295]
 [0.295]
 [0.295]
 [0.295]]
printing an ep nov before normalisation:  35.399250984191895
siam score:  -0.83165896
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  21.93340301513672
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.749]
 [21.933]
 [25.727]
 [21.933]
 [21.933]] [[0.167]
 [0.157]
 [0.202]
 [0.157]
 [0.157]]
printing an ep nov before normalisation:  24.253129959106445
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.996367996578446
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.584989070892334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.68917357313251
printing an ep nov before normalisation:  31.75127876497182
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.79295063018799
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.485027386064957
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.8850584347762
printing an ep nov before normalisation:  34.81368402913639
siam score:  -0.8422247
printing an ep nov before normalisation:  37.07381415685839
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8365271
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.608]
 [41.608]
 [41.608]
 [48.598]
 [41.608]] [[1.459]
 [1.459]
 [1.459]
 [1.827]
 [1.459]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.539]
 [28.872]
 [44.47 ]
 [49.901]
 [44.419]] [[0.744]
 [0.543]
 [0.837]
 [0.939]
 [0.836]]
printing an ep nov before normalisation:  44.19661563223221
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.942]
 [56.036]
 [56.848]
 [43.574]
 [48.2  ]] [[0.44 ]
 [0.574]
 [0.582]
 [0.446]
 [0.493]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.10714826997925
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.18725193871392
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83429754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.441]
 [56.441]
 [56.441]
 [56.441]
 [56.441]] [[0.875]
 [0.875]
 [0.875]
 [0.875]
 [0.875]]
printing an ep nov before normalisation:  43.45091809159662
printing an ep nov before normalisation:  52.478802791188635
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.85298441023336
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.10855516869741
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.561315059661865
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.39380359649658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 44.275466202204655
siam score:  -0.8450219
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.39090892567349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.88647747039795
printing an ep nov before normalisation:  31.975369940915495
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.99030065536499
printing an ep nov before normalisation:  28.278508151126278
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.31 ]
 [54.584]
 [48.796]
 [56.101]
 [55.409]] [[1.34 ]
 [1.197]
 [0.975]
 [1.256]
 [1.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9162,     0.0374,     0.0001,     0.0002,     0.0462],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0120,     0.9390,     0.0295,     0.0001,     0.0194],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9464,     0.0250,     0.0286],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0304,     0.0003,     0.0591,     0.7752,     0.1350],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0506, 0.0007, 0.1886, 0.1935, 0.5666], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9406,     0.0486,     0.0000,     0.0002,     0.0107],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9981,     0.0004,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0004,     0.9028,     0.0242,     0.0725],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0012,     0.0004,     0.0199,     0.8373,     0.1411],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0027, 0.0022, 0.0672, 0.1964, 0.7315], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.99556464907
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  74.4014194374722
printing an ep nov before normalisation:  30.13365832027026
printing an ep nov before normalisation:  42.54064596753008
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  107.36575469620362
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8414343
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.45641489273515
printing an ep nov before normalisation:  35.5489760828919
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9967,     0.0001,     0.0002,     0.0004,     0.0026],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0023,     0.9810,     0.0001,     0.0001,     0.0165],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0118,     0.9698,     0.0024,     0.0160],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0018,     0.0002,     0.0007,     0.8365,     0.1608],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0005,     0.0026,     0.0037,     0.0793,     0.9140],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.75799096592989
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.06849449506866
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.238]
 [35.363]
 [45.169]
 [45.999]
 [44.533]] [[0.736]
 [0.575]
 [0.844]
 [0.867]
 [0.827]]
UNIT TEST: sample policy line 217 mcts : [0.154 0.436 0.128 0.128 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.73595256967185
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.78356431160804
printing an ep nov before normalisation:  32.53523019174439
printing an ep nov before normalisation:  56.58746667652988
printing an ep nov before normalisation:  48.9938367465631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.37104486021226
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.681]
 [62.371]
 [62.371]
 [44.929]
 [62.371]] [[1.065]
 [0.805]
 [0.805]
 [0.465]
 [0.805]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.95071177666131
siam score:  -0.8317304
printing an ep nov before normalisation:  37.790015886356855
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.60154650588393
siam score:  -0.83220017
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.895]
 [58.589]
 [55.895]
 [55.895]
 [55.895]] [[1.123]
 [1.227]
 [1.123]
 [1.123]
 [1.123]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.972]
 [32.165]
 [28.204]
 [29.613]
 [30.254]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8299185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.65532684326172
siam score:  -0.8321469
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.374]
 [41.374]
 [41.374]
 [41.374]
 [41.374]] [[68.97]
 [68.97]
 [68.97]
 [68.97]
 [68.97]]
printing an ep nov before normalisation:  35.347584272100924
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83249754
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.50621470392441
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.861]
 [59.861]
 [59.861]
 [70.097]
 [59.861]] [[0.794]
 [0.794]
 [0.794]
 [0.995]
 [0.794]]
printing an ep nov before normalisation:  41.04268146395024
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.097]
 [56.302]
 [65.838]
 [50.151]
 [59.814]] [[0.65 ]
 [0.71 ]
 [0.97 ]
 [0.542]
 [0.806]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.206]
 [56.101]
 [48.017]
 [56.159]
 [54.756]] [[1.613]
 [1.607]
 [1.206]
 [1.61 ]
 [1.541]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.49987305249907
actions average: 
K:  4  action  0 :  tensor([    0.7711,     0.0072,     0.0002,     0.0398,     0.1817],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9840,     0.0005,     0.0001,     0.0151],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0329,     0.8907,     0.0303,     0.0461],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0003,     0.0031,     0.8140,     0.1826],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0013, 0.0706, 0.0705, 0.1783, 0.6793], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.979]
 [65.94 ]
 [50.898]
 [50.898]
 [52.428]] [[0.4  ]
 [0.599]
 [0.462]
 [0.462]
 [0.476]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.775]
 [70.747]
 [60.775]
 [60.775]
 [60.775]] [[0.536]
 [0.667]
 [0.536]
 [0.536]
 [0.536]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  80.2652250215181
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.549]
 [41.539]
 [43.783]
 [45.839]
 [42.834]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
line 256 mcts: sample exp_bonus 28.85206432670182
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.881]
 [77.558]
 [69.786]
 [64.289]
 [69.786]] [[1.556]
 [1.636]
 [1.403]
 [1.237]
 [1.403]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.432502027269564
actions average: 
K:  3  action  0 :  tensor([    0.9931,     0.0001,     0.0025,     0.0005,     0.0038],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9950,     0.0027,     0.0007,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0021,     0.9180,     0.0123,     0.0675],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0001,     0.0022,     0.8067,     0.1907],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0909, 0.1228, 0.1093, 0.6759], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  43.768387159096314
printing an ep nov before normalisation:  49.27931504391351
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.32862304748518
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.34879293246638
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.205566014284585
printing an ep nov before normalisation:  25.60779403386216
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
siam score:  -0.8447995
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8456063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.739091093383145
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.839]
 [33.161]
 [39.592]
 [39.931]
 [33.161]] [[1.014]
 [0.322]
 [0.548]
 [0.56 ]
 [0.322]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8471089
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  53.340965448689424
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.01774095418308
printing an ep nov before normalisation:  50.40331289588615
siam score:  -0.84358996
printing an ep nov before normalisation:  47.364585250649306
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.6590511119193
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.77963401037616
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.227]
 [22.291]
 [27.616]
 [20.007]
 [25.493]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.14892473134009
printing an ep nov before normalisation:  53.1335789790263
line 256 mcts: sample exp_bonus 56.11173593485298
printing an ep nov before normalisation:  25.907506942749023
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.29977245291043
siam score:  -0.84184295
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.685172759838444
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.716]
 [37.716]
 [37.716]
 [37.716]
 [37.716]] [[0.84]
 [0.84]
 [0.84]
 [0.84]
 [0.84]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[68.369]
 [68.369]
 [68.369]
 [68.369]
 [68.369]] [[1.459]
 [1.459]
 [1.459]
 [1.459]
 [1.459]]
printing an ep nov before normalisation:  38.43619140591331
printing an ep nov before normalisation:  37.43492378974812
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9992,     0.0003,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9978,     0.0000,     0.0002,     0.0020],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0196,     0.8764,     0.0384,     0.0656],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0001,     0.0213,     0.7590,     0.2190],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0013, 0.0006, 0.1229, 0.3923, 0.4828], grad_fn=<DivBackward0>)
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 4.654309720158326e-10
0.0 0.0
0.0 8.310985640834449e-09
0.0 3.149417397240568e-09
0.0 0.0
0.0 8.146858343185879e-10
0.0 1.0205230536190282e-08
0.0 0.0
0.0 0.0
0.0 4.0883058027868095e-10
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.109]
 [54.327]
 [46.109]
 [46.109]
 [46.109]] [[1.393]
 [1.847]
 [1.393]
 [1.393]
 [1.393]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.812462989145246
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[59.886]
 [62.499]
 [54.445]
 [54.445]
 [54.445]] [[1.639]
 [1.759]
 [1.39 ]
 [1.39 ]
 [1.39 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.84064749748161
siam score:  -0.825524
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.8827075357956
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.205]
 [35.655]
 [47.205]
 [47.205]
 [46.873]] [[1.667]
 [1.107]
 [1.667]
 [1.667]
 [1.651]]
printing an ep nov before normalisation:  38.74673784619058
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 5.619583814247477
printing an ep nov before normalisation:  42.67449676990509
printing an ep nov before normalisation:  30.84261772727685
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.056]
 [19.229]
 [24.264]
 [22.462]
 [20.061]] [[0.297]
 [0.271]
 [0.427]
 [0.371]
 [0.297]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9748,     0.0068,     0.0005,     0.0177],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0001,     0.9240,     0.0207,     0.0551],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0003,     0.0586,     0.6928,     0.2477],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0019, 0.0041, 0.1464, 0.1024, 0.7453], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.63435868963826
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.205]
 [70.142]
 [72.623]
 [70.142]
 [65.937]] [[1.526]
 [1.897]
 [2.   ]
 [1.897]
 [1.723]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.697]
 [50.522]
 [49.936]
 [44.643]
 [51.697]] [[1.91 ]
 [1.833]
 [1.794]
 [1.445]
 [1.91 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9709,     0.0214,     0.0000,     0.0013,     0.0065],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9962,     0.0005,     0.0004,     0.0028],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0025, 0.0087, 0.9155, 0.0129, 0.0604], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0046,     0.0330,     0.8532,     0.1090],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0007,     0.0525,     0.0450,     0.0420,     0.8597],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.97697728715937
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.67626057937582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.325831623224985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.66363600175359
printing an ep nov before normalisation:  64.13378125436151
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.076885175919955
printing an ep nov before normalisation:  38.19592319831534
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.18450038252788
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.874]
 [61.874]
 [61.874]
 [61.874]
 [61.874]] [[1.895]
 [1.895]
 [1.895]
 [1.895]
 [1.895]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84482497
printing an ep nov before normalisation:  28.16051082269035
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  101.17347702400623
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.185]
 [69.474]
 [90.115]
 [90.833]
 [72.185]] [[1.195]
 [1.13 ]
 [1.63 ]
 [1.647]
 [1.195]]
printing an ep nov before normalisation:  37.78336524963379
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.169]
 [47.87 ]
 [47.87 ]
 [47.557]
 [48.012]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.99744097555986
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.32910806262255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.27469340913347
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.019033432006836
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.963]
 [40.963]
 [43.703]
 [40.963]
 [36.473]] [[0.971]
 [0.971]
 [1.083]
 [0.971]
 [0.786]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  49.28480295873504
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.954]
 [25.453]
 [26.423]
 [30.802]
 [27.805]] [[1.611]
 [1.032]
 [1.107]
 [1.445]
 [1.214]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.54842340382607
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.54099992587111
printing an ep nov before normalisation:  32.15975046157837
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.44506922616472
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.10348551115849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9556,     0.0017,     0.0000,     0.0004,     0.0423],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0004,     0.9749,     0.0055,     0.0001,     0.0191],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0008,     0.9438,     0.0016,     0.0538],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0007,     0.0016,     0.8026,     0.1947],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0086, 0.0017, 0.0476, 0.3326, 0.6095], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.84378225
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.426529645960727
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.75 ]
 [34.018]
 [30.922]
 [26.189]
 [30.922]] [[0.41 ]
 [0.788]
 [0.662]
 [0.469]
 [0.662]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.5532841152615
printing an ep nov before normalisation:  33.03271288891481
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  74.89197696365244
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.70796497855952
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.112]
 [50.112]
 [50.112]
 [50.112]
 [50.112]] [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]]
printing an ep nov before normalisation:  37.32438564300537
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.89262989589147
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.65831424216888
siam score:  -0.85107636
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.61 ]
 [47.014]
 [49.281]
 [36.505]
 [36.505]] [[0.801]
 [0.909]
 [0.982]
 [0.574]
 [0.574]]
printing an ep nov before normalisation:  45.687618255615234
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9628,     0.0041,     0.0004,     0.0101,     0.0226],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9993,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0078,     0.9217,     0.0186,     0.0519],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0005,     0.0002,     0.0177,     0.7739,     0.2077],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0384,     0.1559,     0.1674,     0.6379],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.03947272395832
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.99763384933675
printing an ep nov before normalisation:  50.16829621277638
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.84324896
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  25.82872941294141
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.43477077558757
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.333772582980934
siam score:  -0.8443934
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.124]
 [47.51 ]
 [49.275]
 [48.22 ]
 [42.124]] [[1.015]
 [1.301]
 [1.395]
 [1.339]
 [1.015]]
printing an ep nov before normalisation:  33.4866972126448
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.74 ]
 [34.444]
 [44.9  ]
 [31.74 ]
 [31.74 ]] [[0.699]
 [0.805]
 [1.215]
 [0.699]
 [0.699]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.679950848000292
printing an ep nov before normalisation:  37.385156194771916
printing an ep nov before normalisation:  35.73584258556366
printing an ep nov before normalisation:  53.23301315307617
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8349556
printing an ep nov before normalisation:  41.08499142776841
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.008519350441247298
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.48939036181288
printing an ep nov before normalisation:  37.965584581179606
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.225369453430176
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.50239819626144
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.676517387059285
printing an ep nov before normalisation:  76.36874579762315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.82801420575501
printing an ep nov before normalisation:  49.516849517822266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.692 0.051 0.077 0.154 0.026]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[76.31]
 [76.31]
 [76.31]
 [76.31]
 [76.31]] [[1.603]
 [1.603]
 [1.603]
 [1.603]
 [1.603]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.0464186668396
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.29025228050364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.925067800731824
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.381]
 [28.454]
 [28.883]
 [33.598]
 [28.153]] [[0.494]
 [0.621]
 [0.639]
 [0.833]
 [0.609]]
siam score:  -0.8361833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.644170458251985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.787283722200655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.611]
 [38.044]
 [46.39 ]
 [41.07 ]
 [40.196]] [[0.724]
 [0.824]
 [1.168]
 [0.949]
 [0.913]]
actions average: 
K:  3  action  0 :  tensor([    0.9702,     0.0004,     0.0002,     0.0179,     0.0113],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9701,     0.0031,     0.0004,     0.0257],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0118,     0.8528,     0.0512,     0.0841],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0002,     0.0249,     0.8732,     0.1017],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0006,     0.0011,     0.0729,     0.2186,     0.7068],
       grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.25638011703975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.24642622513383
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.302]
 [34.871]
 [30.235]
 [31.87 ]
 [31.322]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.544]
 [38.33 ]
 [73.476]
 [40.245]
 [35.728]] [[0.064]
 [0.187]
 [0.627]
 [0.211]
 [0.154]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.593]
 [32.593]
 [55.81 ]
 [34.842]
 [36.182]] [[0.129]
 [0.129]
 [0.271]
 [0.143]
 [0.151]]
printing an ep nov before normalisation:  31.34874369853611
printing an ep nov before normalisation:  24.309150965458038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.34272894304398
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8363324
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.169933940945704
siam score:  -0.83637315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  40.08249768847906
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.2006314458947
printing an ep nov before normalisation:  37.95110156519557
printing an ep nov before normalisation:  33.43652685765591
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.629482806158876
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.42]
 [57.42]
 [57.42]
 [57.42]
 [57.42]] [[0.509]
 [0.509]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.17189677344967
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8408476
printing an ep nov before normalisation:  41.591286550881044
printing an ep nov before normalisation:  24.753579096682085
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9991,     0.0004,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0026,     0.9669,     0.0080,     0.0002,     0.0222],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0155,     0.9168,     0.0242,     0.0435],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0016, 0.0010, 0.0232, 0.7795, 0.1947], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.1905, 0.1181, 0.1425, 0.5477], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.413682977177935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8537117
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.795419681084006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.178]
 [19.94 ]
 [20.085]
 [19.632]
 [19.286]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.322]
 [56.322]
 [56.322]
 [56.322]
 [56.322]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 65.44656791778579
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0019,     0.9969,     0.0000,     0.0003,     0.0009],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0005,     0.9582,     0.0053,     0.0358],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0002,     0.0002,     0.8349,     0.1641],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0038, 0.0387, 0.0759, 0.2519, 0.6297], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.08153937643063
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.88300445311938
printing an ep nov before normalisation:  52.65405582615129
printing an ep nov before normalisation:  24.494490528515207
printing an ep nov before normalisation:  43.54740744126022
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.533]
 [60.572]
 [61.533]
 [61.533]
 [61.177]] [[1.562]
 [1.522]
 [1.562]
 [1.562]
 [1.547]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.6482923138782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8438004
printing an ep nov before normalisation:  64.52180785251313
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.6325912729108
siam score:  -0.84313846
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8433073
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  0.0020021901627842453
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84312165
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.337208144362993
actions average: 
K:  4  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0017,     0.9785,     0.0016,     0.0001,     0.0181],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0092, 0.0011, 0.9348, 0.0026, 0.0523], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0008,     0.0007,     0.8393,     0.1587],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0018, 0.0710, 0.1239, 0.2816, 0.5217], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
printing an ep nov before normalisation:  27.120621947106827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.185]
 [24.678]
 [36.569]
 [27.165]
 [28.515]] [[0.127]
 [0.116]
 [0.171]
 [0.127]
 [0.133]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9986,     0.0011,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9616,     0.0267,     0.0000,     0.0115],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0006,     0.9404,     0.0213,     0.0377],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0011,     0.0004,     0.0248,     0.8595,     0.1143],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0019, 0.0718, 0.1679, 0.1361, 0.6223], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.546]
 [60.112]
 [48.575]
 [52.041]
 [48.221]] [[1.023]
 [1.255]
 [0.847]
 [0.969]
 [0.834]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.86682331002592
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
line 256 mcts: sample exp_bonus 37.668379655576935
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  57.549698396819586
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  0  action  0 :  tensor([    0.9940,     0.0041,     0.0001,     0.0002,     0.0015],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9354,     0.0485,     0.0003,     0.0158],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9755,     0.0002,     0.0240],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0006,     0.0303,     0.8018,     0.1672],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0025, 0.0670, 0.0799, 0.2139, 0.6367], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  23.980448246002197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.29111028281779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.87913828754101
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.37974107747667
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.95105240130392
printing an ep nov before normalisation:  50.77901590179509
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.0048997069305
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.789]
 [33.281]
 [31.651]
 [31.651]
 [31.651]] [[1.08 ]
 [0.749]
 [0.678]
 [0.678]
 [0.678]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.905685777297776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.39098642439961
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.851]
 [ 0.   ]
 [44.122]
 [ 0.   ]
 [ 0.   ]] [[ 0.482]
 [-0.441]
 [ 0.581]
 [-0.441]
 [-0.441]]
printing an ep nov before normalisation:  89.00545263285598
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 48.10844923374547
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.74620723724365
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.10822339327623
printing an ep nov before normalisation:  43.69797425751015
printing an ep nov before normalisation:  49.87506012930638
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.647]
 [51.374]
 [61.828]
 [53.647]
 [53.647]] [[1.145]
 [1.07 ]
 [1.413]
 [1.145]
 [1.145]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.19343730679493
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.83819025662029
line 256 mcts: sample exp_bonus 36.21028199565965
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.09518949998827
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.936]
 [36.936]
 [36.936]
 [36.936]
 [36.936]] [[0.359]
 [0.359]
 [0.359]
 [0.359]
 [0.359]]
printing an ep nov before normalisation:  51.73295654705228
printing an ep nov before normalisation:  46.90149052568769
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.177]
 [34.177]
 [36.585]
 [34.177]
 [34.177]] [[0.868]
 [0.868]
 [0.985]
 [0.868]
 [0.868]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.38338741766043
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.633]
 [50.218]
 [29.633]
 [29.633]
 [29.633]] [[0.441]
 [1.025]
 [0.441]
 [0.441]
 [0.441]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.92930154400309
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9805,     0.0004,     0.0002,     0.0188],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0033,     0.9608,     0.0018,     0.0339],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0002,     0.0250,     0.7375,     0.2369],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0280, 0.0017, 0.0376, 0.2674, 0.6653], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.980605225630654
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [44.506]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.556]
 [ 0.751]
 [-0.556]
 [-0.556]
 [-0.556]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.36824085222961
line 256 mcts: sample exp_bonus 34.231810512293386
printing an ep nov before normalisation:  35.00757189398866
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  27.757694377027317
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8514867
printing an ep nov before normalisation:  23.289820509338497
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.37263732486301
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.808]
 [29.808]
 [29.808]
 [39.489]
 [29.808]] [[0.537]
 [0.537]
 [0.537]
 [0.869]
 [0.537]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.69770034501319
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.494]
 [25.855]
 [32.189]
 [27.639]
 [32.027]] [[0.847]
 [0.654]
 [0.814]
 [0.699]
 [0.81 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.404]
 [48.404]
 [48.404]
 [48.404]
 [48.404]] [[80.689]
 [80.689]
 [80.689]
 [80.689]
 [80.689]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8439827
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.8163994155911
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.84096367420939
printing an ep nov before normalisation:  65.90773167229408
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.339]
 [51.531]
 [36.38 ]
 [36.38 ]
 [36.38 ]] [[0.521]
 [0.847]
 [0.473]
 [0.473]
 [0.473]]
printing an ep nov before normalisation:  36.080972635945905
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.668]
 [43.668]
 [43.668]
 [43.668]
 [43.668]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
printing an ep nov before normalisation:  18.743118080327694
printing an ep nov before normalisation:  50.043897433997444
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.393834099365385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.386320162422614
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.795]
 [45.031]
 [27.374]
 [46.84 ]
 [43.427]] [[0.771]
 [0.895]
 [0.544]
 [0.931]
 [0.863]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
printing an ep nov before normalisation:  50.97797228733114
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  30.248042563852717
printing an ep nov before normalisation:  25.033834489009212
printing an ep nov before normalisation:  26.781827343978172
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.79492131264817
printing an ep nov before normalisation:  11.30948202669913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.628982970879306
printing an ep nov before normalisation:  26.600890159606934
printing an ep nov before normalisation:  28.367723325954124
printing an ep nov before normalisation:  40.89274411469623
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.178213446210975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84675753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.20392417907715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.30036006540941
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.19796904523845
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.754]
 [45.505]
 [45.505]
 [45.505]
 [45.505]] [[1.343]
 [0.995]
 [0.995]
 [0.995]
 [0.995]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  93.39727809285891
printing an ep nov before normalisation:  65.29253971783025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[7.911]
 [9.095]
 [7.3  ]
 [9.831]
 [9.437]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.82206970682047
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84225446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.141]
 [50.141]
 [63.462]
 [50.141]
 [50.141]] [[0.584]
 [0.584]
 [0.88 ]
 [0.584]
 [0.584]]
printing an ep nov before normalisation:  27.043733596801758
printing an ep nov before normalisation:  27.636237921986215
printing an ep nov before normalisation:  33.842305879761575
printing an ep nov before normalisation:  48.58058367137828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.38915890136106
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.86898187978098
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8427366
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84438246
printing an ep nov before normalisation:  53.55469598860792
printing an ep nov before normalisation:  40.59854507446289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.823]
 [37.513]
 [51.057]
 [35.73 ]
 [37.103]] [[1.333]
 [1.313]
 [2.179]
 [1.199]
 [1.287]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8437366
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.482]
 [21.501]
 [29.544]
 [39.958]
 [26.137]] [[0.414]
 [0.254]
 [0.512]
 [0.846]
 [0.403]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.715]
 [32.715]
 [48.725]
 [42.868]
 [32.715]] [[0.592]
 [0.592]
 [1.072]
 [0.896]
 [0.592]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.992]
 [22.097]
 [26.93 ]
 [25.65 ]
 [25.383]] [[0.639]
 [0.587]
 [0.869]
 [0.794]
 [0.778]]
printing an ep nov before normalisation:  54.06392985742793
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.437289968309464
printing an ep nov before normalisation:  27.589688648640003
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.51700320870759
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.30197998661354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.38064517923437
printing an ep nov before normalisation:  62.65482905684488
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.795]
 [45.429]
 [46.074]
 [45.978]
 [45.978]] [[1.37 ]
 [1.17 ]
 [1.208]
 [1.203]
 [1.203]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.4684650909374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84909594
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.59 ]
 [41.59 ]
 [52.819]
 [41.59 ]
 [41.59 ]] [[1.202]
 [1.202]
 [1.82 ]
 [1.202]
 [1.202]]
line 256 mcts: sample exp_bonus 40.095220091767494
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.309]
 [36.309]
 [45.006]
 [36.309]
 [36.309]] [[1.235]
 [1.235]
 [1.75 ]
 [1.235]
 [1.235]]
printing an ep nov before normalisation:  33.697059840614635
printing an ep nov before normalisation:  38.781198396615444
printing an ep nov before normalisation:  44.1660688249519
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.570550631854005
printing an ep nov before normalisation:  45.3407811887358
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.656]
 [28.624]
 [27.333]
 [24.836]
 [25.274]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.358]
 [28.583]
 [29.77 ]
 [28.326]
 [27.985]] [[0.194]
 [0.209]
 [0.224]
 [0.206]
 [0.202]]
printing an ep nov before normalisation:  43.104360486729874
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.396788905069517
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.40616314600153
printing an ep nov before normalisation:  42.09318877253482
printing an ep nov before normalisation:  38.492938784618694
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.58205027687078
printing an ep nov before normalisation:  49.95688232878342
siam score:  -0.8489775
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.626]
 [41.668]
 [31.953]
 [29.626]
 [29.626]] [[0.62 ]
 [1.1  ]
 [0.713]
 [0.62 ]
 [0.62 ]]
printing an ep nov before normalisation:  31.00372076034546
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.37227691424388
printing an ep nov before normalisation:  38.89340071905106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.919648562332476
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.20906580050999
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.069]
 [39.958]
 [62.185]
 [41.825]
 [44.613]] [[0.641]
 [0.356]
 [0.75 ]
 [0.389]
 [0.438]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.88908100657982
printing an ep nov before normalisation:  29.67504425074513
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 31.69438436055139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.530433161789603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.23347550036381
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.13668135350311
printing an ep nov before normalisation:  43.96914005279541
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.74679484563119
printing an ep nov before normalisation:  59.315450296448134
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8452095
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.64843606440706
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 47.564458931154164
printing an ep nov before normalisation:  32.52022416522112
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.856]
 [35.856]
 [39.429]
 [35.856]
 [35.856]] [[1.401]
 [1.401]
 [1.667]
 [1.401]
 [1.401]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.565]
 [35.74 ]
 [35.74 ]
 [35.74 ]
 [35.74 ]] [[1.633]
 [0.654]
 [0.654]
 [0.654]
 [0.654]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.544180686944816
siam score:  -0.84326935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.77756855707828
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  25.83619405836486
printing an ep nov before normalisation:  55.53361370034359
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.97524388863621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.25873337699162
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.267]
 [44.258]
 [45.302]
 [48.371]
 [47.763]] [[1.297]
 [1.197]
 [1.249]
 [1.402]
 [1.371]]
printing an ep nov before normalisation:  32.21294584854639
printing an ep nov before normalisation:  36.16096499292981
printing an ep nov before normalisation:  36.940540863108126
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [ 0.   ]
 [60.321]
 [ 0.   ]
 [ 0.   ]] [[-0.743]
 [-0.743]
 [ 1.356]
 [-0.743]
 [-0.743]]
printing an ep nov before normalisation:  60.80142414645651
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.986]
 [51.986]
 [51.986]
 [51.986]
 [51.986]] [[1.616]
 [1.616]
 [1.616]
 [1.616]
 [1.616]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 62.03613636493567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.474144299798496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9993,     0.0001,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9961,     0.0011,     0.0001,     0.0025],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0006,     0.0085,     0.8849,     0.0398,     0.0663],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0003,     0.0398,     0.8122,     0.1474],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0016, 0.0392, 0.0641, 0.2803, 0.6148], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.27190301376882
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.431]
 [36.501]
 [37.721]
 [38.421]
 [30.358]] [[0.683]
 [0.91 ]
 [0.965]
 [0.996]
 [0.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.18217658996582
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.06054902128349
printing an ep nov before normalisation:  51.98492810561462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.231977462768555
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9686,     0.0016,     0.0003,     0.0133,     0.0162],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9906,     0.0071,     0.0001,     0.0022],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0249,     0.9665,     0.0001,     0.0084],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0334,     0.0006,     0.0440,     0.7291,     0.1929],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.1447, 0.1213, 0.1505, 0.5823], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.76229353198143
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.08895743106709
printing an ep nov before normalisation:  49.72638200234553
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.385 0.231 0.179 0.154 0.051]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.48639308614349
printing an ep nov before normalisation:  35.55181010785672
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.65 ]
 [27.65 ]
 [40.196]
 [27.65 ]
 [27.65 ]] [[0.234]
 [0.234]
 [0.436]
 [0.234]
 [0.234]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.146]
 [41.606]
 [25.166]
 [23.106]
 [38.733]] [[0.207]
 [0.416]
 [0.194]
 [0.166]
 [0.377]]
siam score:  -0.85184306
siam score:  -0.8520677
printing an ep nov before normalisation:  23.696575259660403
printing an ep nov before normalisation:  51.72208652644152
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.6052389607055
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.921]
 [28.607]
 [36.342]
 [30.613]
 [34.683]] [[0.8  ]
 [0.779]
 [1.311]
 [0.917]
 [1.197]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.948707812136153
actions average: 
K:  3  action  0 :  tensor([    0.9630,     0.0038,     0.0000,     0.0007,     0.0324],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9668,     0.0080,     0.0002,     0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0005,     0.9559,     0.0159,     0.0276],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0004,     0.0453,     0.8432,     0.1108],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0005,     0.0059,     0.1417,     0.2172,     0.6346],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.92888207053901
printing an ep nov before normalisation:  43.096981048583984
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8428055
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.76931306539066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.401]
 [45.401]
 [58.376]
 [45.401]
 [45.401]] [[0.648]
 [0.648]
 [1.   ]
 [0.648]
 [0.648]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.200878143310547
actions average: 
K:  2  action  0 :  tensor([    0.9964,     0.0002,     0.0000,     0.0013,     0.0021],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9871,     0.0121,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0013,     0.9581,     0.0003,     0.0403],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0002,     0.0006,     0.8815,     0.1176],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0277, 0.0389, 0.1889, 0.7435], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.142665369254658
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.685]
 [21.147]
 [28.731]
 [23.329]
 [22.912]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.753760015701165
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.851563
printing an ep nov before normalisation:  46.78215832090387
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.007]
 [23.828]
 [23.744]
 [23.782]
 [24.235]] [[1.667]
 [1.33 ]
 [1.321]
 [1.325]
 [1.373]]
printing an ep nov before normalisation:  56.206535245702895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.27043572636312
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.113]
 [31.56 ]
 [36.833]
 [47.55 ]
 [31.56 ]] [[0.597]
 [0.579]
 [0.753]
 [1.107]
 [0.579]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.539]
 [47.056]
 [47.867]
 [47.867]
 [47.867]] [[1.708]
 [1.379]
 [1.42 ]
 [1.42 ]
 [1.42 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.49631051927331
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  67.505094014718
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.848]
 [49.848]
 [49.848]
 [49.848]
 [49.848]] [[1.527]
 [1.527]
 [1.527]
 [1.527]
 [1.527]]
printing an ep nov before normalisation:  47.826576232910156
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.04762856265267
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.254]
 [45.854]
 [53.032]
 [36.93 ]
 [38.329]] [[0.091]
 [0.137]
 [0.172]
 [0.095]
 [0.101]]
printing an ep nov before normalisation:  57.32382263516619
printing an ep nov before normalisation:  41.85298490973634
printing an ep nov before normalisation:  35.801141481750385
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.279]
 [30.279]
 [30.279]
 [30.279]
 [30.279]] [[20.196]
 [20.196]
 [20.196]
 [20.196]
 [20.196]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.40047373137721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.333510875701904
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.079]
 [29.079]
 [29.079]
 [29.079]
 [29.079]] [[0.279]
 [0.279]
 [0.279]
 [0.279]
 [0.279]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.071702231320646
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.531637180083735
printing an ep nov before normalisation:  32.1927678905848
siam score:  -0.847034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.843607253699524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  4  action  0 :  tensor([    0.9930,     0.0060,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9816,     0.0014,     0.0000,     0.0169],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0011,     0.8313,     0.0768,     0.0908],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0016, 0.0041, 0.0443, 0.7260, 0.2241], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0073, 0.0871, 0.0097, 0.1798, 0.7162], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.157768326385174
printing an ep nov before normalisation:  65.06113620727078
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 61.364913259412276
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  31.75176540618243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8504408
printing an ep nov before normalisation:  43.08604750261932
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.5036617613207
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.867]
 [34.121]
 [31.126]
 [34.121]
 [34.121]] [[2.   ]
 [1.377]
 [1.164]
 [1.377]
 [1.377]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.23 ]
 [45.23 ]
 [ 1.989]
 [45.23 ]
 [45.23 ]] [[0.55]
 [0.55]
 [0.  ]
 [0.55]
 [0.55]]
printing an ep nov before normalisation:  40.12941360473633
printing an ep nov before normalisation:  48.42000087497388
siam score:  -0.85011894
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.41965286544974
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.94404200426362
printing an ep nov before normalisation:  51.727370678107526
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.706726288476943
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  28.309020341323436
printing an ep nov before normalisation:  36.078828143243335
printing an ep nov before normalisation:  31.16263079529729
siam score:  -0.84584296
printing an ep nov before normalisation:  44.969329833984375
printing an ep nov before normalisation:  32.499068456347985
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  50.637245386428155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.510169473381346
siam score:  -0.8411148
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.700283527374268
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.703]
 [34.849]
 [32.767]
 [33.108]
 [33.825]] [[0.19 ]
 [0.181]
 [0.16 ]
 [0.163]
 [0.171]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.08357238769531
printing an ep nov before normalisation:  31.421084276996275
actions average: 
K:  3  action  0 :  tensor([    0.9927,     0.0020,     0.0001,     0.0015,     0.0037],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0078,     0.9505,     0.0118,     0.0001,     0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0261,     0.8179,     0.0483,     0.1077],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0098,     0.0696,     0.7748,     0.1457],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0007, 0.0014, 0.1511, 0.1939, 0.6529], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.034123420715332
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  72.40761588948443
printing an ep nov before normalisation:  67.13843744484731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.80184766618783
printing an ep nov before normalisation:  42.89584159851074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8366895
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83666337
printing an ep nov before normalisation:  66.62338800031463
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.864]
 [47.365]
 [45.864]
 [45.864]
 [45.864]] [[0.888]
 [0.943]
 [0.888]
 [0.888]
 [0.888]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.64503914459636
printing an ep nov before normalisation:  28.483320051397964
printing an ep nov before normalisation:  64.87566947937012
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.79043006424663
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.722]
 [62.916]
 [64.408]
 [49.561]
 [49.561]] [[0.597]
 [0.979]
 [1.012]
 [0.682]
 [0.682]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.23913764953613
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.179 0.128 0.154 0.385 0.154]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.14904308319092
printing an ep nov before normalisation:  53.55323477683514
actions average: 
K:  2  action  0 :  tensor([    0.9792,     0.0008,     0.0001,     0.0021,     0.0178],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9608,     0.0008,     0.0006,     0.0374],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0164,     0.8476,     0.0327,     0.1032],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0003,     0.0976,     0.7614,     0.1406],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0004,     0.0515,     0.2384,     0.1265,     0.5832],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
UNIT TEST: sample policy line 217 mcts : [0.103 0.077 0.103 0.308 0.41 ]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.45637169116315
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  64.74800516090285
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.86331164544803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.054130661247335
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  19.183773683335467
printing an ep nov before normalisation:  22.934047808275775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.405215070309815
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9960,     0.0020,     0.0002,     0.0001,     0.0016],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9877,     0.0001,     0.0008,     0.0111],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0110,     0.9251,     0.0109,     0.0529],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0000,     0.0272,     0.8944,     0.0783],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0391, 0.0987, 0.0981, 0.1104, 0.6535], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  21.697455130750303
actions average: 
K:  1  action  0 :  tensor([    0.9994,     0.0001,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0009,     0.9715,     0.0005,     0.0006,     0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0126,     0.8835,     0.0180,     0.0857],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0002,     0.0200,     0.8754,     0.1044],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0023, 0.0132, 0.1451, 0.1892, 0.6503], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.77611959928197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.34306296590837
printing an ep nov before normalisation:  48.55784009422379
printing an ep nov before normalisation:  42.756578844844945
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8486652
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9993,     0.0001,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0002,     0.8844,     0.0282,     0.0872],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0033,     0.0005,     0.0005,     0.8816,     0.1141],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0052, 0.0300, 0.0976, 0.2419, 0.6252], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.0939177079394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.04599772798261
printing an ep nov before normalisation:  42.94815419419898
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.0147878591161
actions average: 
K:  3  action  0 :  tensor([    0.9374,     0.0022,     0.0004,     0.0175,     0.0426],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9364,     0.0196,     0.0007,     0.0431],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0083,     0.9794,     0.0004,     0.0119],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0000,     0.0030,     0.8997,     0.0972],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0457, 0.1079, 0.1868, 0.6585], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  105.76755702522021
siam score:  -0.8442688
printing an ep nov before normalisation:  50.84502242294699
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.772]
 [36.677]
 [36.372]
 [46.191]
 [28.772]] [[0.337]
 [0.578]
 [0.569]
 [0.869]
 [0.337]]
printing an ep nov before normalisation:  31.84281349182129
printing an ep nov before normalisation:  48.49279323972456
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.55874156951904
printing an ep nov before normalisation:  54.59622528276649
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8445537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.777048388859839e-05
printing an ep nov before normalisation:  31.764121820799016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  25.614895548132733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8534099
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.455528119339505
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.98131959142453
printing an ep nov before normalisation:  47.90926710687646
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.153117926474984
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.519]
 [32.519]
 [60.04 ]
 [32.519]
 [32.519]] [[0.466]
 [0.466]
 [1.218]
 [0.466]
 [0.466]]
printing an ep nov before normalisation:  60.70076787359013
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0018,     0.9707,     0.0166,     0.0002,     0.0107],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0006,     0.9799,     0.0017,     0.0177],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0002,     0.0006,     0.8333,     0.1649],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0042, 0.0896, 0.0811, 0.1418, 0.6832], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.818217196543564
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.43667943974985
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.315]
 [21.647]
 [24.118]
 [24.747]
 [23.046]] [[1.188]
 [1.113]
 [1.389]
 [1.46 ]
 [1.269]]
line 256 mcts: sample exp_bonus 49.69102164155829
printing an ep nov before normalisation:  42.24526517418079
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  20.07579207420349
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.774]
 [20.076]
 [25.774]
 [16.511]
 [25.774]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.412398126407915
printing an ep nov before normalisation:  42.91772761315662
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  31.818157998606136
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.97038236079244
printing an ep nov before normalisation:  47.02874030396015
printing an ep nov before normalisation:  29.9926022688751
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.375]
 [24.016]
 [21.923]
 [25.438]
 [22.582]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.4757334536633
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.392]
 [80.392]
 [84.252]
 [89.349]
 [80.392]] [[1.282]
 [1.282]
 [1.383]
 [1.517]
 [1.282]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.758]
 [40.758]
 [40.758]
 [40.758]
 [40.758]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.8520266
printing an ep nov before normalisation:  40.61033946596843
printing an ep nov before normalisation:  20.377147665122223
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.61824222663292
printing an ep nov before normalisation:  31.477365560748453
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.488]
 [23.178]
 [23.178]
 [34.975]
 [23.178]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.59420690689649
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.64665563135639
printing an ep nov before normalisation:  21.12483927807241
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8488753
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[82.317]
 [82.317]
 [82.317]
 [82.317]
 [82.317]] [[1.969]
 [1.969]
 [1.969]
 [1.969]
 [1.969]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.57318824759015
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  71.54140921241265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.524335118939526
printing an ep nov before normalisation:  43.66520605847565
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.2711067199707
printing an ep nov before normalisation:  38.61227989196777
siam score:  -0.8519169
printing an ep nov before normalisation:  44.48154104189165
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.003]
 [27.269]
 [29.859]
 [23.216]
 [23.547]] [[0.114]
 [0.103]
 [0.12 ]
 [0.077]
 [0.079]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.68112857706416
printing an ep nov before normalisation:  34.35804535673106
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.27215460011648
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.132]
 [34.108]
 [34.108]
 [34.108]
 [34.108]] [[1.667]
 [2.636]
 [2.636]
 [2.636]
 [2.636]]
printing an ep nov before normalisation:  53.00440065684408
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[61.83 ]
 [70.343]
 [61.83 ]
 [61.83 ]
 [61.83 ]] [[1.409]
 [1.667]
 [1.409]
 [1.409]
 [1.409]]
UNIT TEST: sample policy line 217 mcts : [0.026 0.026 0.872 0.051 0.026]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9530,     0.0017,     0.0000,     0.0062,     0.0391],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0036,     0.9794,     0.0007,     0.0026,     0.0136],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0122,     0.9141,     0.0125,     0.0611],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0001,     0.0441,     0.8233,     0.1319],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0003,     0.0023,     0.0902,     0.4414,     0.4656],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.84957457
printing an ep nov before normalisation:  41.251253060122416
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[75.164]
 [62.484]
 [75.164]
 [75.164]
 [75.164]] [[0.945]
 [0.697]
 [0.945]
 [0.945]
 [0.945]]
printing an ep nov before normalisation:  33.69945544415559
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.205]
 [31.753]
 [29.893]
 [34.487]
 [31.753]] [[0.705]
 [0.517]
 [0.487]
 [0.562]
 [0.517]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.449]
 [48.22 ]
 [44.548]
 [47.675]
 [48.22 ]] [[1.333]
 [1.323]
 [1.165]
 [1.3  ]
 [1.323]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.99486307539236
UNIT TEST: sample policy line 217 mcts : [0.128 0.385 0.205 0.179 0.103]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9905,     0.0002,     0.0000,     0.0040,     0.0053],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9618,     0.0110,     0.0001,     0.0268],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0016,     0.0005,     0.8975,     0.0438,     0.0566],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0005,     0.0004,     0.0196,     0.7981,     0.1813],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0037, 0.0186, 0.0589, 0.2779, 0.6409], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  16.711601179281388
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.036]
 [28.501]
 [29.71 ]
 [31.416]
 [30.069]] [[0.811]
 [0.647]
 [0.703]
 [0.783]
 [0.72 ]]
printing an ep nov before normalisation:  52.978246877757094
printing an ep nov before normalisation:  40.93964749343212
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.85473144
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.303336444278056
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.15394652850583
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.68236765916551
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.55788691932648
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.172582476837242
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.77311455101418
printing an ep nov before normalisation:  27.73194207060724
printing an ep nov before normalisation:  32.16423307146345
printing an ep nov before normalisation:  31.702916563601477
printing an ep nov before normalisation:  47.946259765590604
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.367544918458453
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  74.05547845932006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.889632545662824
actions average: 
K:  0  action  0 :  tensor([    0.9979,     0.0000,     0.0003,     0.0001,     0.0017],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0084,     0.9856,     0.0003,     0.0002,     0.0055],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0004,     0.0001,     0.9007,     0.0496,     0.0492],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0002,     0.0723,     0.8002,     0.1263],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0933, 0.0017, 0.0018, 0.1379, 0.7652], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.83652885237026
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.604]
 [47.604]
 [49.542]
 [47.604]
 [47.604]] [[1.094]
 [1.094]
 [1.178]
 [1.094]
 [1.094]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.621]
 [50.528]
 [51.485]
 [38.621]
 [38.621]] [[0.735]
 [0.961]
 [0.979]
 [0.735]
 [0.735]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
siam score:  -0.8627521
printing an ep nov before normalisation:  59.671302313913316
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.385 0.154 0.154 0.103 0.205]
printing an ep nov before normalisation:  68.1489597929495
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.55303149623391
printing an ep nov before normalisation:  43.09090277584362
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.43011758803873
actions average: 
K:  1  action  0 :  tensor([    0.9824,     0.0001,     0.0003,     0.0011,     0.0161],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9819,     0.0013,     0.0005,     0.0163],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0088,     0.9009,     0.0121,     0.0782],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0006,     0.0221,     0.8193,     0.1578],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0038, 0.0427, 0.1098, 0.2429, 0.6008], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.910154466050145
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.00349044799805
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.02804083272583
printing an ep nov before normalisation:  29.55379730052727
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.27379876604966
printing an ep nov before normalisation:  41.818122056889585
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.507]
 [39.529]
 [39.529]
 [39.529]
 [39.529]] [[1.103]
 [0.482]
 [0.482]
 [0.482]
 [0.482]]
printing an ep nov before normalisation:  38.46317612348811
siam score:  -0.8599826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.538]
 [25.538]
 [38.651]
 [25.538]
 [25.538]] [[0.407]
 [0.407]
 [0.803]
 [0.407]
 [0.407]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  73.84333951632587
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85950243
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.253644943237305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.09]
 [33.09]
 [33.09]
 [33.09]
 [33.09]] [[0.661]
 [0.661]
 [0.661]
 [0.661]
 [0.661]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.686]
 [49.686]
 [49.686]
 [51.878]
 [49.686]] [[0.968]
 [0.968]
 [0.968]
 [1.045]
 [0.968]]
printing an ep nov before normalisation:  51.15610071894139
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.67 ]
 [27.67 ]
 [27.67 ]
 [41.073]
 [27.67 ]] [[0.426]
 [0.426]
 [0.426]
 [0.966]
 [0.426]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.260947479298004
printing an ep nov before normalisation:  46.94991480310222
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.978269465802235
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.905]
 [31.905]
 [31.905]
 [31.905]
 [31.905]] [[1.16]
 [1.16]
 [1.16]
 [1.16]
 [1.16]]
printing an ep nov before normalisation:  48.5281151385186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9978,     0.0015,     0.0001,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9947,     0.0001,     0.0002,     0.0048],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0089,     0.9310,     0.0091,     0.0510],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0002,     0.0295,     0.8667,     0.1028],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0036,     0.0173,     0.0004,     0.1087,     0.8700],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.90153003462918
printing an ep nov before normalisation:  61.07227559574734
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.83348915785192
printing an ep nov before normalisation:  28.666751419990394
printing an ep nov before normalisation:  26.02163532958868
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.54868900908417
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 45.37027883529663
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.8635257
printing an ep nov before normalisation:  41.3101476550445
printing an ep nov before normalisation:  41.20610314149371
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.80633146635393
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.484520690618208
printing an ep nov before normalisation:  30.983992456642255
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9895,     0.0007,     0.0002,     0.0073,     0.0023],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0020, 0.8600, 0.0813, 0.0206, 0.0361], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0299,     0.8821,     0.0180,     0.0698],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0010,     0.0003,     0.0202,     0.8125,     0.1660],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0047, 0.0357, 0.1639, 0.2931, 0.5026], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.811]
 [34.391]
 [64.061]
 [54.811]
 [54.811]] [[1.098]
 [0.358]
 [1.434]
 [1.098]
 [1.098]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.374585973749596
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  61.98937157384524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.29520106794909
printing an ep nov before normalisation:  24.65195795960863
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.037]
 [25.847]
 [25.518]
 [23.621]
 [24.124]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.70309985168435
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.885]
 [59.473]
 [59.126]
 [60.885]
 [56.416]] [[2.   ]
 [1.915]
 [1.894]
 [2.   ]
 [1.73 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  6.237683004110863
printing an ep nov before normalisation:  83.9367646671946
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.437]
 [42.238]
 [43.504]
 [46.479]
 [38.248]] [[0.457]
 [0.415]
 [0.439]
 [0.495]
 [0.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85197854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.500438805635824
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.076]
 [52.545]
 [35.284]
 [35.732]
 [38.559]] [[0.787]
 [1.146]
 [0.414]
 [0.433]
 [0.553]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.327]
 [38.163]
 [58.763]
 [54.079]
 [40.327]] [[0.703]
 [0.64 ]
 [1.24 ]
 [1.104]
 [0.703]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.230284690856934
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.029]
 [33.029]
 [33.029]
 [41.766]
 [33.029]] [[1.341]
 [1.341]
 [1.341]
 [2.   ]
 [1.341]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.15552738661103
printing an ep nov before normalisation:  43.94845008850098
maxi score, test score, baseline:  0.0001 0.0 0.0001
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  29.035043245479603
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.379304250081375
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.239583015441895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.67314091578644
printing an ep nov before normalisation:  36.42526149749756
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.2052598490899
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.907]
 [36.907]
 [40.718]
 [39.273]
 [36.907]] [[0.959]
 [0.959]
 [1.183]
 [1.098]
 [0.959]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.15455461898517
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.76507084368742
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.325]
 [53.017]
 [33.325]
 [37.708]
 [33.325]] [[0.293]
 [0.667]
 [0.293]
 [0.376]
 [0.293]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.631]
 [49.884]
 [49.884]
 [49.123]
 [53.841]] [[0.73 ]
 [0.77 ]
 [0.77 ]
 [0.746]
 [0.896]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.78761672973633
siam score:  -0.861943
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[20.737]
 [38.125]
 [25.71 ]
 [20.737]
 [20.737]] [[0.103]
 [0.347]
 [0.173]
 [0.103]
 [0.103]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9976,     0.0001,     0.0000,     0.0002,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0037,     0.9779,     0.0015,     0.0002,     0.0167],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0022, 0.0107, 0.8970, 0.0010, 0.0891], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0004,     0.0158,     0.8388,     0.1445],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0068, 0.0563, 0.1710, 0.0986, 0.6673], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.59246911794619
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.88796385765272
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.51109838485718
printing an ep nov before normalisation:  84.10813417940822
printing an ep nov before normalisation:  46.602234840393066
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.973331685176895
printing an ep nov before normalisation:  41.55559122069529
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.65371657014331
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.85486208749691
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.113]
 [39.996]
 [38.504]
 [38.76 ]
 [38.297]] [[0.684]
 [0.714]
 [0.664]
 [0.672]
 [0.657]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.356]
 [29.356]
 [29.356]
 [35.859]
 [29.356]] [[0.428]
 [0.428]
 [0.428]
 [0.604]
 [0.428]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.980322769393354
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.17248090337586
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  33.6234171483525
printing an ep nov before normalisation:  45.10968208312988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.464347835286496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.11797861310825
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.067043389895048
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.32332906627236
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.417]
 [37.618]
 [43.138]
 [38.507]
 [37.586]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  36.604771397263285
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9233,     0.0068,     0.0002,     0.0009,     0.0688],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9075,     0.0630,     0.0001,     0.0292],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0140,     0.9168,     0.0114,     0.0577],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0001,     0.0466,     0.8236,     0.1295],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0017, 0.0950, 0.1187, 0.1238, 0.6609], grad_fn=<DivBackward0>)
siam score:  -0.85925233
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  3  action  0 :  tensor([    0.9934,     0.0000,     0.0001,     0.0037,     0.0027],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0006,     0.9882,     0.0012,     0.0002,     0.0098],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0229,     0.8499,     0.0427,     0.0845],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0001,     0.0408,     0.8374,     0.1216],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0029, 0.0007, 0.0658, 0.2908, 0.6398], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.166]
 [33.061]
 [38.859]
 [39.052]
 [38.353]] [[0.754]
 [0.896]
 [1.332]
 [1.346]
 [1.294]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.884106824248985
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.78646354980357
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.125]
 [48.961]
 [43.125]
 [43.125]
 [43.125]] [[1.323]
 [1.502]
 [1.323]
 [1.323]
 [1.323]]
printing an ep nov before normalisation:  33.5258150100708
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.934]
 [40.873]
 [39.586]
 [35.887]
 [40.934]] [[1.479]
 [1.474]
 [1.378]
 [1.101]
 [1.479]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.914914022344128
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9968,     0.0001,     0.0000,     0.0012,     0.0019],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9739,     0.0010,     0.0021,     0.0230],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0638,     0.7895,     0.0289,     0.1177],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0008,     0.0003,     0.0194,     0.7513,     0.2282],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0005,     0.0091,     0.0656,     0.2656,     0.6592],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 68.6256492400062
siam score:  -0.87039584
siam score:  -0.86745703
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.626]
 [52.982]
 [55.464]
 [54.912]
 [56.802]] [[0.21 ]
 [0.219]
 [0.236]
 [0.232]
 [0.245]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9739,     0.0008,     0.0000,     0.0163,     0.0090],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9831,     0.0132,     0.0001,     0.0035],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0028,     0.0000,     0.9842,     0.0007,     0.0123],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0028,     0.0001,     0.0667,     0.8544,     0.0759],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0023, 0.0029, 0.0572, 0.2388, 0.6988], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.978]
 [47.686]
 [40.766]
 [43.951]
 [43.951]] [[1.019]
 [1.243]
 [0.972]
 [1.096]
 [1.096]]
actions average: 
K:  3  action  0 :  tensor([    0.9844,     0.0095,     0.0000,     0.0008,     0.0053],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9675,     0.0029,     0.0001,     0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0003,     0.9289,     0.0163,     0.0543],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0017,     0.0007,     0.0522,     0.7819,     0.1635],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0032, 0.0016, 0.1040, 0.1945, 0.6968], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.15829692634886
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.26717948913574
siam score:  -0.855951
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.84582215654088
printing an ep nov before normalisation:  42.83729076385498
printing an ep nov before normalisation:  79.61373116431659
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.840775002609135
printing an ep nov before normalisation:  43.52032363842072
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.74 ]
 [44.149]
 [42.281]
 [40.897]
 [41.63 ]] [[0.494]
 [0.728]
 [0.669]
 [0.625]
 [0.648]]
printing an ep nov before normalisation:  41.13034964029993
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.45258251287323
printing an ep nov before normalisation:  41.30485656751206
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.72876787185669
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.86000276
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.118128458658852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85711485
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.72948126098568
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[29.837]
 [31.538]
 [33.823]
 [29.837]
 [29.837]] [[0.722]
 [0.8  ]
 [0.905]
 [0.722]
 [0.722]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.85513949055803
printing an ep nov before normalisation:  52.353819913437874
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.36701011657715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.85392714
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.175]
 [34.437]
 [35.983]
 [32.068]
 [34.8  ]] [[0.132]
 [0.224]
 [0.244]
 [0.194]
 [0.229]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0002,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0013,     0.9733,     0.0000,     0.0020,     0.0234],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0119,     0.9346,     0.0093,     0.0438],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([0.0008, 0.0013, 0.0147, 0.7266, 0.2566], grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0092, 0.0020, 0.1151, 0.0838, 0.7899], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.621770096989025
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.30807642955046
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.20563342447724
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.839]
 [48.809]
 [42.926]
 [44.158]
 [48.339]] [[0.19 ]
 [0.251]
 [0.191]
 [0.204]
 [0.246]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  44.377960477992495
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.74020369754567
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.8]
 [39.8]
 [39.8]
 [39.8]
 [39.8]] [[0.667]
 [0.667]
 [0.667]
 [0.667]
 [0.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9992,     0.0001,     0.0000,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9800,     0.0008,     0.0002,     0.0190],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0006,     0.0105,     0.8863,     0.0130,     0.0896],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0002,     0.0127,     0.8387,     0.1479],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0023, 0.0345, 0.1110, 0.1403, 0.7118], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.869]
 [34.869]
 [34.869]
 [34.869]
 [34.869]] [[69.738]
 [69.738]
 [69.738]
 [69.738]
 [69.738]]
printing an ep nov before normalisation:  32.29926824569702
printing an ep nov before normalisation:  52.89501296038707
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.882]
 [43.501]
 [25.163]
 [41.882]
 [29.798]] [[1.241]
 [1.33 ]
 [0.32 ]
 [1.241]
 [0.576]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.245]
 [33.933]
 [36.073]
 [38.012]
 [35.188]] [[0.977]
 [1.061]
 [1.128]
 [1.188]
 [1.1  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 51.92342856190358
siam score:  -0.8618967
printing an ep nov before normalisation:  33.62620677591195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.47049522399902
printing an ep nov before normalisation:  42.116300008677364
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8640924
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  63.91481974622688
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.12 ]
 [28.847]
 [25.72 ]
 [27.216]
 [27.892]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  33.60158920288086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.813]
 [30.647]
 [25.044]
 [23.086]
 [23.124]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  34.552344079868135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.567851066589355
printing an ep nov before normalisation:  52.711229148899804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  60.37864223575867
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8596804
printing an ep nov before normalisation:  64.02408327422778
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.538]
 [37.538]
 [61.715]
 [37.538]
 [46.35 ]] [[0.251]
 [0.251]
 [0.522]
 [0.251]
 [0.35 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.3305983057419
siam score:  -0.85890806
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.35473155975342
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[69.094]
 [57.566]
 [57.566]
 [57.566]
 [57.566]] [[1.39 ]
 [1.022]
 [1.022]
 [1.022]
 [1.022]]
printing an ep nov before normalisation:  67.86959638620944
printing an ep nov before normalisation:  40.468994989216796
printing an ep nov before normalisation:  36.76151514053345
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.826]
 [35.546]
 [35.614]
 [33.772]
 [33.765]] [[1.234]
 [1.439]
 [1.444]
 [1.305]
 [1.305]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.666]
 [54.666]
 [69.51 ]
 [54.666]
 [54.666]] [[1.195]
 [1.195]
 [1.647]
 [1.195]
 [1.195]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.8773136138916
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  16.90332094828288
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[33.211]
 [27.01 ]
 [25.806]
 [27.831]
 [28.327]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.59849717524849
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.00375210727432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.018216699913502
printing an ep nov before normalisation:  48.83935228161757
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9989,     0.0001,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0080,     0.9115,     0.0240,     0.0563],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0004,     0.0170,     0.8035,     0.1788],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0026, 0.0294, 0.2039, 0.7629], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  37.831003351195186
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.570041110576767
printing an ep nov before normalisation:  56.12995281413818
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85402155
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.20436689767881
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.965]
 [21.965]
 [21.965]
 [23.07 ]
 [23.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  31.394780906870153
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  16.391070064402182
printing an ep nov before normalisation:  54.291744232177734
line 256 mcts: sample exp_bonus 31.529406417101367
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.133]
 [27.133]
 [27.133]
 [27.133]
 [27.133]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.250526508298236
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  24.477101527698164
printing an ep nov before normalisation:  41.10076436011443
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.773]
 [48.457]
 [51.492]
 [52.025]
 [48.457]] [[1.43 ]
 [1.311]
 [1.467]
 [1.494]
 [1.311]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.09309676937038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.44239292887892
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.56336633278626
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 32.850634760555224
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.032]
 [41.296]
 [41.031]
 [27.032]
 [27.032]] [[0.345]
 [0.613]
 [0.608]
 [0.345]
 [0.345]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.823]
 [49.12 ]
 [46.141]
 [55.203]
 [49.12 ]] [[0.562]
 [0.475]
 [0.42 ]
 [0.587]
 [0.475]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
siam score:  -0.85593504
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.34896332395958
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84988344
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.866154186368334
UNIT TEST: sample policy line 217 mcts : [0.538 0.308 0.077 0.077 0.   ]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.859976222622546
printing an ep nov before normalisation:  35.77451708696523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.90701086354404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.808]
 [34.812]
 [35.808]
 [35.808]
 [35.808]] [[0.398]
 [0.376]
 [0.398]
 [0.398]
 [0.398]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85473204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.602]
 [64.143]
 [56.602]
 [56.602]
 [56.602]] [[1.343]
 [1.664]
 [1.343]
 [1.343]
 [1.343]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.06382639291805
actions average: 
K:  1  action  0 :  tensor([    0.9702,     0.0195,     0.0000,     0.0002,     0.0101],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9989,     0.0004,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0089,     0.9601,     0.0015,     0.0293],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0020,     0.0008,     0.0165,     0.9061,     0.0746],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0010, 0.0572, 0.0912, 0.1789, 0.6717], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.371]
 [38.638]
 [37.263]
 [39.04 ]
 [37.353]] [[0.849]
 [0.491]
 [0.457]
 [0.5  ]
 [0.459]]
printing an ep nov before normalisation:  73.38738778322086
line 256 mcts: sample exp_bonus 0.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.072695590929314
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.15 ]
 [34.206]
 [41.62 ]
 [33.044]
 [32.451]] [[0.118]
 [0.15 ]
 [0.209]
 [0.141]
 [0.136]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.41251087188721
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.108]
 [52.155]
 [36.646]
 [30.734]
 [34.328]] [[0.773]
 [0.975]
 [0.531]
 [0.362]
 [0.465]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.82675111189728
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.54157052054826
printing an ep nov before normalisation:  63.01222946607067
printing an ep nov before normalisation:  34.466990675280655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.46630902882796
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  1.6149153059359378
printing an ep nov before normalisation:  53.347881701107454
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.096]
 [32.096]
 [32.096]
 [37.284]
 [32.096]] [[1.174]
 [1.174]
 [1.174]
 [1.386]
 [1.174]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  98.43473564726995
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  6.132734097172131
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[67.426]
 [65.701]
 [67.426]
 [67.426]
 [59.51 ]] [[0.32 ]
 [0.306]
 [0.32 ]
 [0.32 ]
 [0.256]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9976,     0.0018,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([0.0023, 0.9636, 0.0187, 0.0019, 0.0135], grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0166,     0.8928,     0.0419,     0.0486],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0013,     0.0004,     0.0000,     0.9333,     0.0650],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0056, 0.0295, 0.0890, 0.1638, 0.7120], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8542019
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.875]
 [41.875]
 [41.875]
 [41.875]
 [41.875]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
siam score:  -0.8562851
siam score:  -0.8548038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.167583380363
siam score:  -0.85458046
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.093]
 [43.093]
 [43.093]
 [43.093]
 [43.093]] [[1.107]
 [1.107]
 [1.107]
 [1.107]
 [1.107]]
actions average: 
K:  4  action  0 :  tensor([    0.9630,     0.0309,     0.0012,     0.0001,     0.0049],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0030,     0.9952,     0.0001,     0.0003,     0.0013],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0004,     0.8763,     0.0158,     0.1074],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0030, 0.0251, 0.0014, 0.7189, 0.2516], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0030, 0.1114, 0.1975, 0.1088, 0.5792], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.962486961560266
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.1824631708821
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.6328730758605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.132]
 [34.132]
 [34.132]
 [43.197]
 [34.132]] [[0.945]
 [0.945]
 [0.945]
 [1.405]
 [0.945]]
siam score:  -0.85649645
printing an ep nov before normalisation:  51.13971516879198
siam score:  -0.85776305
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.65182412959754
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.05425503106988
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.677716985848484
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[55.995]
 [63.436]
 [51.789]
 [55.995]
 [55.995]] [[0.938]
 [1.062]
 [0.867]
 [0.938]
 [0.938]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.75 ]
 [36.545]
 [37.248]
 [32.022]
 [36.394]] [[1.122]
 [0.656]
 [0.679]
 [0.507]
 [0.651]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.39693769934634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  61.228510005186266
printing an ep nov before normalisation:  54.96315340545285
printing an ep nov before normalisation:  45.600267641077295
printing an ep nov before normalisation:  57.66688232066881
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  104.37270418095844
actions average: 
K:  0  action  0 :  tensor([    0.9985,     0.0000,     0.0000,     0.0004,     0.0010],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9839,     0.0000,     0.0012,     0.0146],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9340,     0.0023,     0.0635],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0006,     0.0002,     0.0179,     0.7499,     0.2314],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0017, 0.0603, 0.1552, 0.2121, 0.5707], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.037340007382575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.59509497046445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.09528590812525
siam score:  -0.85603577
printing an ep nov before normalisation:  17.2672210180647
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.668]
 [88.853]
 [72.606]
 [72.606]
 [72.606]] [[1.042]
 [1.225]
 [0.932]
 [0.932]
 [0.932]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.319597460174876
line 256 mcts: sample exp_bonus 28.095938799671217
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.666]
 [36.446]
 [36.446]
 [36.446]
 [33.939]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.12845420837402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.17858139819713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.345312968444055
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.1255223569967
siam score:  -0.8533218
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  89.40070607458064
printing an ep nov before normalisation:  52.48039164181074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.498]
 [57.498]
 [60.044]
 [57.498]
 [57.498]] [[1.257]
 [1.257]
 [1.333]
 [1.257]
 [1.257]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.82112216739466
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85208684
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  56.90943801914754
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.214]
 [41.904]
 [38.094]
 [41.895]
 [37.386]] [[0.892]
 [1.106]
 [0.932]
 [1.105]
 [0.9  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.31806201774207
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.59896840935793
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8587804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.881]
 [49.36 ]
 [38.732]
 [40.015]
 [42.351]] [[0.158]
 [0.279]
 [0.219]
 [0.226]
 [0.239]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.911]
 [64.821]
 [44.321]
 [41.324]
 [55.493]] [[0.654]
 [0.776]
 [0.353]
 [0.292]
 [0.584]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.866]
 [40.866]
 [40.866]
 [40.866]
 [40.866]] [[0.821]
 [0.821]
 [0.821]
 [0.821]
 [0.821]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.70119571685791
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  46.60815731782538
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.34083366394043
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  9.998312692254768
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  42.73908804155331
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.434]
 [54.784]
 [54.784]
 [55.721]
 [57.293]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.21511492406085
printing an ep nov before normalisation:  44.38968927629997
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.075440108750485
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.851004
siam score:  -0.8511383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.62706743126358
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.389041931928034
printing an ep nov before normalisation:  46.575393111638164
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.859]
 [29.901]
 [48.641]
 [35.958]
 [30.945]] [[0.776]
 [0.544]
 [1.274]
 [0.78 ]
 [0.584]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.62814235687256
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.104]
 [47.104]
 [47.104]
 [47.104]
 [47.104]] [[2.]
 [2.]
 [2.]
 [2.]
 [2.]]
printing an ep nov before normalisation:  48.481816397885794
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.948076130317496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.298982828850903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.90120787448083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84790665
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.12205172733739
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.816502797118837
actions average: 
K:  4  action  0 :  tensor([    0.9994,     0.0002,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0009,     0.9411,     0.0143,     0.0002,     0.0435],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0015, 0.0189, 0.9147, 0.0268, 0.0381], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0015, 0.0013, 0.0390, 0.8563, 0.1020], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0116, 0.0048, 0.0009, 0.1095, 0.8731], grad_fn=<DivBackward0>)
siam score:  -0.851218
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.33889059519072
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.40724177442516
siam score:  -0.8504713
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.97517466696508
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.36394631367723
printing an ep nov before normalisation:  54.03496912774684
printing an ep nov before normalisation:  39.872221170130246
using explorer policy with actor:  1
siam score:  -0.8482119
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.60875238664848
line 256 mcts: sample exp_bonus 34.577040424018975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.35264603296916
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8558733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.132912740101027
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.39647438784242
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.12816567328575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.57046313803149
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.27271342591861
siam score:  -0.85081464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8510369
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.1617692671349
siam score:  -0.8516594
printing an ep nov before normalisation:  31.271955966949463
printing an ep nov before normalisation:  33.70495009454325
printing an ep nov before normalisation:  25.076361129206862
printing an ep nov before normalisation:  35.018718242645264
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8537319
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.42296241703251
printing an ep nov before normalisation:  45.54820537567139
printing an ep nov before normalisation:  23.021656490157326
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.78948497772217
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.86569786303573
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.89343648363628
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0005511467350061139
siam score:  -0.860947
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.99712927734315
printing an ep nov before normalisation:  36.129380488587046
printing an ep nov before normalisation:  52.1642620237288
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.01832294464111
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.195]
 [49.299]
 [45.735]
 [39.321]
 [45.145]] [[1.279]
 [1.284]
 [1.101]
 [0.772]
 [1.071]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.004]
 [38.863]
 [38.085]
 [38.863]
 [38.863]] [[1.526]
 [1.587]
 [1.532]
 [1.587]
 [1.587]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.528232415517174
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.021]
 [62.853]
 [53.021]
 [53.021]
 [53.021]] [[1.353]
 [1.758]
 [1.353]
 [1.353]
 [1.353]]
printing an ep nov before normalisation:  43.26821804046631
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  82.2557568976367
printing an ep nov before normalisation:  56.88541889190674
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8508409
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[80.921]
 [80.921]
 [80.921]
 [80.921]
 [80.921]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9990,     0.0000,     0.0003,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9770,     0.0023,     0.0004,     0.0203],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0008,     0.0000,     0.9278,     0.0503,     0.0209],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0007,     0.0017,     0.0234,     0.7828,     0.1913],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0004,     0.0089,     0.1217,     0.2034,     0.6656],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.101192680417505
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[78.103]
 [78.103]
 [78.103]
 [78.103]
 [78.103]] [[0.396]
 [0.396]
 [0.396]
 [0.396]
 [0.396]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.628687381744385
printing an ep nov before normalisation:  55.72703705768189
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.16141732914024
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.96172590499391
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.36516174259009
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.86183465
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.373295530949086
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  32.70779616673096
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.249]
 [39.249]
 [39.249]
 [40.516]
 [39.249]] [[0.83 ]
 [0.83 ]
 [0.83 ]
 [0.868]
 [0.83 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[50.768]
 [53.627]
 [50.768]
 [64.777]
 [50.768]] [[0.619]
 [0.696]
 [0.619]
 [1.   ]
 [0.619]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.685638599750234
printing an ep nov before normalisation:  52.520602146353156
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.916]
 [49.63 ]
 [41.636]
 [34.916]
 [38.296]] [[0.342]
 [0.79 ]
 [0.547]
 [0.342]
 [0.445]]
printing an ep nov before normalisation:  42.5093314798097
printing an ep nov before normalisation:  60.45468552723369
printing an ep nov before normalisation:  52.9694217086457
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.73421287536621
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.2503994789397
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.831954225695796
printing an ep nov before normalisation:  51.10319776956408
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8636853
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.31008877321913
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.539]
 [47.822]
 [48.577]
 [47.431]
 [45.686]] [[1.333]
 [1.268]
 [1.296]
 [1.253]
 [1.186]]
printing an ep nov before normalisation:  59.37454545424542
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  4.409140875018238
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.75992933978716
printing an ep nov before normalisation:  39.55001315620316
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.528]
 [45.683]
 [34.448]
 [34.456]
 [38.995]] [[0.162]
 [0.142]
 [0.081]
 [0.081]
 [0.106]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9856,     0.0009,     0.0006,     0.0017,     0.0111],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0441,     0.9318,     0.0003,     0.0003,     0.0235],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0118,     0.8885,     0.0213,     0.0784],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0056,     0.0003,     0.0536,     0.7655,     0.1750],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0607, 0.0112, 0.1916, 0.1289, 0.6076], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9970,     0.0020,     0.0000,     0.0004,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9086,     0.0215,     0.0001,     0.0695],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0034,     0.9281,     0.0109,     0.0570],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0011,     0.0014,     0.8949,     0.1026],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0014, 0.0213, 0.1120, 0.1142, 0.7512], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  66.18918768407543
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.702347117278926
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.331]
 [59.364]
 [68.88 ]
 [70.817]
 [67.031]] [[1.073]
 [1.127]
 [1.382]
 [1.434]
 [1.332]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.32858977146256
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  60.222714020149155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.893744705543405
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.60872832966296
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.771]
 [28.751]
 [28.751]
 [40.912]
 [28.751]] [[0.482]
 [0.219]
 [0.219]
 [0.387]
 [0.219]]
printing an ep nov before normalisation:  27.436842155823875
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.54646143626492
printing an ep nov before normalisation:  61.36011319363312
printing an ep nov before normalisation:  67.79916002455437
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.87778564902152
siam score:  -0.85977966
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 42.05641269683838
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.36093235015869
printing an ep nov before normalisation:  32.85241740156478
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9899,     0.0018,     0.0000,     0.0021,     0.0063],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9717,     0.0004,     0.0003,     0.0274],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0025,     0.9258,     0.0133,     0.0583],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0029,     0.1164,     0.5935,     0.2869],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0003,     0.0035,     0.0091,     0.1619,     0.8252],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  58.05935733273705
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.55575907823715
printing an ep nov before normalisation:  49.6369680750597
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.925198592324726
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.057847899901606
siam score:  -0.8518447
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.03381588989681
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  68.98069768138281
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.262]
 [33.288]
 [33.288]
 [37.079]
 [36.587]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.079]
 [42.418]
 [51.079]
 [51.079]
 [46.484]] [[1.333]
 [1.008]
 [1.333]
 [1.333]
 [1.161]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.619568644362715
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.18763497904049
printing an ep nov before normalisation:  38.556321461995445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.07025909423828
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  91.40258182836635
printing an ep nov before normalisation:  36.49029287285988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.73164135609618
printing an ep nov before normalisation:  71.45902388095794
printing an ep nov before normalisation:  34.64819664437545
siam score:  -0.852162
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8522033
printing an ep nov before normalisation:  42.81923770904541
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.93 ]
 [29.331]
 [35.162]
 [38.762]
 [27.364]] [[0.139]
 [0.335]
 [0.513]
 [0.623]
 [0.274]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.537895430815457
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.351]
 [51.519]
 [54.46 ]
 [52.274]
 [53.106]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[57.199]
 [57.199]
 [57.199]
 [57.199]
 [57.199]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  63.28940806819231
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.351]
 [43.742]
 [44.238]
 [46.42 ]
 [44.56 ]] [[0.581]
 [0.698]
 [0.715]
 [0.789]
 [0.726]]
printing an ep nov before normalisation:  43.96674695797775
siam score:  -0.8509879
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.47993850708008
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.98359966278076
printing an ep nov before normalisation:  80.5244871259546
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.15395739811273
printing an ep nov before normalisation:  49.49168188604623
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.603]
 [43.854]
 [38.9  ]
 [38.9  ]
 [38.9  ]] [[0.577]
 [0.633]
 [0.509]
 [0.509]
 [0.509]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.514911331677204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  38.27782734506063
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9534,     0.0022,     0.0002,     0.0439],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0007,     0.0003,     0.9002,     0.0301,     0.0688],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0006,     0.0211,     0.8301,     0.1477],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0038, 0.0467, 0.0562, 0.1818, 0.7116], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.339037886242686
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.043]
 [45.731]
 [51.672]
 [52.6  ]
 [45.233]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  46.95070595972587
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.26825858118613
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.30092052111447
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.19365692138672
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.15044571777248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  11.321388263176857
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.61354360376782
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[62.877]
 [49.548]
 [42.665]
 [48.319]
 [49.548]] [[0.386]
 [0.255]
 [0.188]
 [0.243]
 [0.255]]
printing an ep nov before normalisation:  61.399658809623276
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.965]
 [40.99 ]
 [39.939]
 [39.709]
 [41.512]] [[1.293]
 [1.185]
 [1.127]
 [1.114]
 [1.213]]
printing an ep nov before normalisation:  78.2137219616368
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[65.109]
 [57.327]
 [58.822]
 [69.374]
 [58.822]] [[1.526]
 [1.287]
 [1.333]
 [1.657]
 [1.333]]
printing an ep nov before normalisation:  73.06520928115933
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.91297287089511
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.34718626383422
printing an ep nov before normalisation:  71.45667979264634
printing an ep nov before normalisation:  59.94311332702637
siam score:  -0.8543633
printing an ep nov before normalisation:  37.21739002769944
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[84.459]
 [58.392]
 [58.392]
 [58.392]
 [58.392]] [[1.329]
 [0.699]
 [0.699]
 [0.699]
 [0.699]]
printing an ep nov before normalisation:  32.53507067981849
printing an ep nov before normalisation:  59.98919341653417
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[30.437]
 [41.217]
 [38.287]
 [37.953]
 [30.437]] [[0.296]
 [0.496]
 [0.441]
 [0.435]
 [0.296]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  51.81183757176731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.1636950205208
printing an ep nov before normalisation:  58.3793828834619
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.03500952039446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.]
 [35.]
 [35.]
 [35.]
 [35.]] [[1.54]
 [1.54]
 [1.54]
 [1.54]
 [1.54]]
printing an ep nov before normalisation:  42.056478478038535
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.05 ]
 [27.05 ]
 [27.05 ]
 [27.05 ]
 [25.894]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  27.399178235831116
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.402280422441535
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.925]
 [25.238]
 [23.925]
 [23.925]
 [23.925]] [[0.082]
 [0.091]
 [0.082]
 [0.082]
 [0.082]]
printing an ep nov before normalisation:  50.39659555707913
siam score:  -0.85626745
printing an ep nov before normalisation:  40.11684201904153
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.813528920733184
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.252160267043024
printing an ep nov before normalisation:  69.82549986772892
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 65.3215169059359
printing an ep nov before normalisation:  53.798862189157255
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  37.50636866117061
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.39470208648733
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.65083059091958
printing an ep nov before normalisation:  45.86234092197772
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.14137029388996
printing an ep nov before normalisation:  53.37877838998563
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.15727763410858
printing an ep nov before normalisation:  37.981719970703125
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.05035749424932
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.885]
 [47.19 ]
 [36.86 ]
 [33.771]
 [37.978]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  1  action  0 :  tensor([    0.9936,     0.0004,     0.0000,     0.0039,     0.0022],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9667,     0.0115,     0.0005,     0.0213],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.8216,     0.0555,     0.1225],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0001,     0.0010,     0.9445,     0.0544],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0002,     0.1632,     0.0892,     0.2100,     0.5374],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[74.446]
 [54.215]
 [46.411]
 [46.411]
 [46.411]] [[1.084]
 [0.642]
 [0.471]
 [0.471]
 [0.471]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.01944014455888
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.394]
 [47.558]
 [53.788]
 [55.08 ]
 [55.115]] [[1.395]
 [1.353]
 [1.669]
 [1.734]
 [1.736]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.88984489440918
printing an ep nov before normalisation:  40.79071308975258
printing an ep nov before normalisation:  40.85340712842903
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.60210375088508
printing an ep nov before normalisation:  40.82394829832498
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.732974882360836
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.84912795
printing an ep nov before normalisation:  34.27195650048609
siam score:  -0.85030335
printing an ep nov before normalisation:  25.60791121891248
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.90396595001221
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9972,     0.0000,     0.0000,     0.0014,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9663,     0.0023,     0.0008,     0.0302],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0002,     0.9010,     0.0137,     0.0850],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0002,     0.0319,     0.8558,     0.1122],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0424, 0.0463, 0.1082, 0.8019], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.09566209200815
UNIT TEST: sample policy line 217 mcts : [0.128 0.385 0.051 0.026 0.41 ]
siam score:  -0.8560124
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.55120677665729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.406]
 [39.998]
 [30.832]
 [37.374]
 [46.466]] [[0.288]
 [0.372]
 [0.271]
 [0.343]
 [0.443]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.946291592128325
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.89 ]
 [49.238]
 [46.89 ]
 [46.89 ]
 [46.89 ]] [[1.025]
 [1.111]
 [1.025]
 [1.025]
 [1.025]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.92398297272868
printing an ep nov before normalisation:  75.95220376042117
printing an ep nov before normalisation:  47.113563281066575
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8621895
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.930830089818464
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.07 ]
 [27.894]
 [30.007]
 [28.406]
 [29.901]] [[0.172]
 [0.099]
 [0.136]
 [0.108]
 [0.134]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  30.98131025490407
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.69606763353084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[71.572]
 [64.581]
 [71.572]
 [71.572]
 [71.572]] [[0.707]
 [0.612]
 [0.707]
 [0.707]
 [0.707]]
printing an ep nov before normalisation:  66.04317748678272
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.56959221430761
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.237555785681888
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.47923445678083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85502106
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.948]
 [53.699]
 [49.963]
 [48.747]
 [40.308]] [[0.658]
 [1.079]
 [0.933]
 [0.885]
 [0.555]]
printing an ep nov before normalisation:  56.174631118774414
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.753]
 [45.753]
 [53.124]
 [45.753]
 [45.753]] [[1.25 ]
 [1.25 ]
 [1.564]
 [1.25 ]
 [1.25 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85618895
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.071782408510813
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.332]
 [54.436]
 [49.421]
 [47.436]
 [52.517]] [[0.211]
 [0.239]
 [0.194]
 [0.176]
 [0.222]]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.40124359475619
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85636353
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.36940281946188
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.17729584016674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9933,     0.0040,     0.0001,     0.0006,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9996,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0086,     0.8930,     0.0218,     0.0766],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0003,     0.0239,     0.6420,     0.3332],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.0501, 0.1181, 0.1180, 0.7129], grad_fn=<DivBackward0>)
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  35.288137221866386
printing an ep nov before normalisation:  36.9312592768852
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[27.434]
 [38.933]
 [31.599]
 [39.539]
 [30.09 ]] [[0.559]
 [1.048]
 [0.736]
 [1.074]
 [0.672]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 44.25270531766109
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9982,     0.0002,     0.0000,     0.0004,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9987,     0.0000,     0.0001,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0026, 0.0035, 0.9050, 0.0116, 0.0774], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0014,     0.0007,     0.0237,     0.8855,     0.0887],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0722, 0.0038, 0.2491, 0.2531, 0.4219], grad_fn=<DivBackward0>)
siam score:  -0.8608038
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.96653652191162
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.232]
 [45.648]
 [43.232]
 [43.232]
 [43.232]] [[0.647]
 [0.717]
 [0.647]
 [0.647]
 [0.647]]
printing an ep nov before normalisation:  56.61185418754819
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.24538196438419
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[46.127]
 [46.928]
 [50.632]
 [45.332]
 [46.346]] [[0.783]
 [0.809]
 [0.927]
 [0.758]
 [0.79 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.42848114060155
siam score:  -0.85854113
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.99984014756465
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.59442824530007
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85505265
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.849]
 [65.778]
 [65.778]
 [68.11 ]
 [72.499]] [[1.178]
 [1.067]
 [1.067]
 [1.118]
 [1.215]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.69351858799898
printing an ep nov before normalisation:  29.81095482590689
printing an ep nov before normalisation:  52.01505152132354
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.037716388702393
printing an ep nov before normalisation:  37.70424322126834
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 0.   ]
 [26.433]
 [27.81 ]
 [23.969]
 [ 0.   ]] [[-0.125]
 [ 0.178]
 [ 0.194]
 [ 0.15 ]
 [-0.125]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.218609674569755
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8591172
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.412]
 [43.412]
 [43.412]
 [43.412]
 [43.412]] [[0.325]
 [0.325]
 [0.325]
 [0.325]
 [0.325]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.72055530548096
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.69118863267049
printing an ep nov before normalisation:  3.754343199043433
printing an ep nov before normalisation:  6.671148104360327
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.75116142784856
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.28856923144199
printing an ep nov before normalisation:  73.4126213742844
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[72.74 ]
 [66.584]
 [66.584]
 [66.584]
 [66.584]] [[1.939]
 [1.635]
 [1.635]
 [1.635]
 [1.635]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.086809591396715
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8576249
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.003048356195016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  62.79197301435674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.562847303407544
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.40013056377767
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8636156
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9747,     0.0188,     0.0001,     0.0063],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0002,     0.9464,     0.0290,     0.0243],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0371,     0.7773,     0.1849],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0671,     0.0685,     0.1625,     0.7014],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.84675467923198
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  63.759208055586186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.201510981547756
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.88920879364014
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.72174896129461
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.186]
 [54.473]
 [39.181]
 [47.052]
 [51.578]] [[1.148]
 [1.366]
 [0.737]
 [1.061]
 [1.247]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.778110171928326
actions average: 
K:  3  action  0 :  tensor([    0.9978,     0.0006,     0.0000,     0.0001,     0.0015],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9481,     0.0259,     0.0004,     0.0254],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0103,     0.9005,     0.0148,     0.0743],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0012,     0.0001,     0.0521,     0.6892,     0.2574],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0012, 0.0251, 0.1420, 0.1030, 0.7287], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  52.25632184301093
line 256 mcts: sample exp_bonus 48.887147590690184
printing an ep nov before normalisation:  36.60151958465576
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.64296373121165
printing an ep nov before normalisation:  52.432502031409314
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.121597227818185
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.597404235823916
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.29736939863441
printing an ep nov before normalisation:  31.599158197156378
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.575]
 [26.575]
 [26.575]
 [26.575]
 [26.575]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.297]
 [45.297]
 [45.297]
 [45.297]
 [45.297]] [[1.667]
 [1.667]
 [1.667]
 [1.667]
 [1.667]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.72500696386825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.777]
 [43.786]
 [47.872]
 [42.702]
 [42.817]] [[0.338]
 [0.37 ]
 [0.434]
 [0.353]
 [0.354]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  50.106923058659945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8509402
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.539]
 [51.282]
 [52.368]
 [48.523]
 [50.843]] [[0.75 ]
 [1.116]
 [1.157]
 [1.012]
 [1.1  ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  20.58054723022822
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.885127285732985
using explorer policy with actor:  1
siam score:  -0.85142326
printing an ep nov before normalisation:  33.23961627556528
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.41  0.128 0.308 0.051 0.103]
siam score:  -0.85173804
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  66.54745336609376
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  85.05786619109311
printing an ep nov before normalisation:  24.643039663762288
printing an ep nov before normalisation:  37.07816123962402
printing an ep nov before normalisation:  55.07202337678073
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.20788919416683
printing an ep nov before normalisation:  41.197125461374945
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.98696521141239
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[60.833]
 [60.833]
 [57.857]
 [60.369]
 [60.833]] [[1.272]
 [1.272]
 [1.167]
 [1.255]
 [1.272]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.22688684425596
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.749152110468245
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  66.5206159035965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9797,     0.0117,     0.0000,     0.0034,     0.0052],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0006,     0.9920,     0.0001,     0.0003,     0.0071],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0005,     0.9323,     0.0139,     0.0533],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0001,     0.0073,     0.9413,     0.0510],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0125, 0.0067, 0.1112, 0.1959, 0.6736], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
siam score:  -0.8627463
printing an ep nov before normalisation:  35.75915230645074
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.72118496866399
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.05307274298166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.466]
 [34.466]
 [18.467]
 [34.466]
 [34.812]] [[0.388]
 [0.388]
 [0.208]
 [0.388]
 [0.392]]
printing an ep nov before normalisation:  24.74505208391582
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.586347060440524
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.08351290225983
printing an ep nov before normalisation:  73.14263312923686
printing an ep nov before normalisation:  57.75025393117445
printing an ep nov before normalisation:  41.83547019958496
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.6278463326983
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.497099668686076
UNIT TEST: sample policy line 217 mcts : [0.667 0.026 0.077 0.205 0.026]
printing an ep nov before normalisation:  53.0480789483515
printing an ep nov before normalisation:  43.564080016037316
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.22069201346733
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.34370229954351
printing an ep nov before normalisation:  38.16251754760742
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.896]
 [58.896]
 [58.896]
 [58.896]
 [58.896]] [[1.887]
 [1.887]
 [1.887]
 [1.887]
 [1.887]]
printing an ep nov before normalisation:  50.26083645912524
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.32998752593994
printing an ep nov before normalisation:  61.15260988328847
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  68.7640790285946
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0015,     0.9855,     0.0001,     0.0006,     0.0123],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0001,     0.9032,     0.0435,     0.0527],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0011,     0.0002,     0.0171,     0.8880,     0.0936],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0028, 0.0260, 0.0892, 0.2791, 0.6029], grad_fn=<DivBackward0>)
siam score:  -0.86698693
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.078]
 [40.078]
 [40.078]
 [40.078]
 [40.078]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  32.14234055371829
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.333]
 [26.333]
 [26.333]
 [26.333]
 [26.333]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  5.743452849663981
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.820815195783396
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.283]
 [50.884]
 [48.231]
 [49.563]
 [51.228]] [[1.18 ]
 [1.255]
 [1.131]
 [1.193]
 [1.271]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[21.699]
 [21.411]
 [20.853]
 [14.762]
 [19.114]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  37.42520182493529
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.0036455707916
printing an ep nov before normalisation:  29.884319305419922
printing an ep nov before normalisation:  31.091620922088623
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.99599462162282
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.86508073553402
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
siam score:  -0.86277145
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9803,     0.0001,     0.0001,     0.0110,     0.0087],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9827,     0.0001,     0.0002,     0.0168],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0017, 0.0101, 0.9309, 0.0312, 0.0262], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0002,     0.0020,     0.8393,     0.1582],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0028, 0.0752, 0.1106, 0.1335, 0.6778], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.585]
 [26.321]
 [25.616]
 [16.542]
 [17.41 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  46.431911664344305
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8569232
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.79511039849573
printing an ep nov before normalisation:  27.123098640332923
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.161]
 [42.161]
 [42.161]
 [39.424]
 [42.161]] [[1.487]
 [1.487]
 [1.487]
 [1.342]
 [1.487]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.235]
 [30.21 ]
 [24.778]
 [32.605]
 [25.835]] [[0.749]
 [0.969]
 [0.668]
 [1.102]
 [0.727]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  12.38520905263158
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.01715133817751
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.338361492500155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.210554304410664
printing an ep nov before normalisation:  49.47734342391167
using explorer policy with actor:  1
printing an ep nov before normalisation:  71.8703090268332
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.113]
 [25.113]
 [36.282]
 [25.113]
 [25.113]] [[0.172]
 [0.172]
 [0.317]
 [0.172]
 [0.172]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8546729
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  4  action  0 :  tensor([    0.9851,     0.0125,     0.0000,     0.0008,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9777,     0.0017,     0.0004,     0.0201],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0001,     0.9300,     0.0297,     0.0399],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0007,     0.0008,     0.8971,     0.1012],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0277, 0.0822, 0.0022, 0.3319, 0.5560], grad_fn=<DivBackward0>)
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
siam score:  -0.85668045
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.595255851745605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.048]
 [66.586]
 [65.321]
 [52.048]
 [52.048]] [[0.748]
 [1.378]
 [1.323]
 [0.748]
 [0.748]]
printing an ep nov before normalisation:  37.23433741369356
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.94305733386128
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85591584
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  64.37336844006803
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.212]
 [39.473]
 [30.367]
 [34.754]
 [35.649]] [[0.941]
 [0.951]
 [0.603]
 [0.771]
 [0.805]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.24331419665806
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[23.827]
 [37.074]
 [25.928]
 [25.13 ]
 [24.079]] [[0.116]
 [0.251]
 [0.137]
 [0.129]
 [0.118]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  45.60623321002876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.173]
 [37.765]
 [36.369]
 [33.461]
 [35.407]] [[0.429]
 [0.347]
 [0.326]
 [0.281]
 [0.311]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  14.899008535843166
printing an ep nov before normalisation:  38.162489571603224
printing an ep nov before normalisation:  40.0126279907084
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.078]
 [39.433]
 [22.078]
 [22.078]
 [22.078]] [[0.524]
 [1.309]
 [0.524]
 [0.524]
 [0.524]]
printing an ep nov before normalisation:  37.41939745422958
printing an ep nov before normalisation:  46.73882475454392
printing an ep nov before normalisation:  55.18453686189249
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.99 ]
 [34.373]
 [34.373]
 [34.373]
 [34.373]] [[1.33 ]
 [0.637]
 [0.637]
 [0.637]
 [0.637]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.700135659560345
siam score:  -0.8621423
printing an ep nov before normalisation:  44.286662802793565
printing an ep nov before normalisation:  37.19306945800781
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.223531433805555
printing an ep nov before normalisation:  42.008146733115204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  33.3989370694826
actions average: 
K:  2  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9770,     0.0023,     0.0001,     0.0201],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9430,     0.0205,     0.0364],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0006,     0.0277,     0.7852,     0.1863],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0090, 0.0318, 0.0562, 0.2239, 0.6791], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  48.745034328357654
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8631365
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.317963443792546
printing an ep nov before normalisation:  30.160874864253515
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.21968533419026
printing an ep nov before normalisation:  53.312822564755706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.368]
 [49.368]
 [49.368]
 [49.368]
 [49.368]] [[1.119]
 [1.119]
 [1.119]
 [1.119]
 [1.119]]
printing an ep nov before normalisation:  69.20715042649383
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.991037368774414
printing an ep nov before normalisation:  25.81335522399624
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  28.994150270083825
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.963108556207764
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8633485
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
siam score:  -0.85909677
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.65272481032404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.641348070954862
printing an ep nov before normalisation:  35.12635064748199
siam score:  -0.85706884
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85665536
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.202228413794263
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.299]
 [52.493]
 [50.478]
 [49.846]
 [50.575]] [[0.685]
 [0.955]
 [0.902]
 [0.885]
 [0.904]]
siam score:  -0.8549463
printing an ep nov before normalisation:  40.0216310391163
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.846]
 [36.344]
 [28.489]
 [39.846]
 [39.846]] [[0.333]
 [0.293]
 [0.205]
 [0.333]
 [0.333]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.834866329215572
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.485606639453092
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.020178683609686
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.811]
 [46.925]
 [45.251]
 [44.054]
 [46.787]] [[0.809]
 [0.754]
 [0.704]
 [0.669]
 [0.75 ]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  2  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9059,     0.0280,     0.0001,     0.0659],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0002,     0.0099,     0.9436,     0.0100,     0.0363],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0002,     0.0201,     0.8935,     0.0856],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0035, 0.0555, 0.1133, 0.1370, 0.6907], grad_fn=<DivBackward0>)
actions average: 
K:  3  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0010,     0.9599,     0.0165,     0.0001,     0.0225],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0108,     0.9338,     0.0118,     0.0434],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0195,     0.0003,     0.0003,     0.8326,     0.1473],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0136, 0.0131, 0.1485, 0.1502, 0.6746], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  40.598182678222656
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.08848649340909
line 256 mcts: sample exp_bonus 48.666423755765585
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.71019788893471
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.914767265319824
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0000,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9456,     0.0174,     0.0002,     0.0365],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0009,     0.9040,     0.0121,     0.0830],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0010,     0.0002,     0.0209,     0.8290,     0.1489],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0010, 0.1340, 0.2083, 0.6557], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9699,     0.0162,     0.0001,     0.0002,     0.0138],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9140,     0.0025,     0.0007,     0.0824],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0000,     0.8917,     0.0417,     0.0664],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0008,     0.0153,     0.8990,     0.0849],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0006,     0.0287,     0.0057,     0.1900,     0.7750],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8553624
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.02543067932129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8549111
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.82389151010599
printing an ep nov before normalisation:  35.34039820071658
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.963200721322927
line 256 mcts: sample exp_bonus 47.06001275157055
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[36.662]
 [36.662]
 [36.662]
 [36.662]
 [36.662]] [[1.676]
 [1.676]
 [1.676]
 [1.676]
 [1.676]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.181243744893735
printing an ep nov before normalisation:  38.19679191444076
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.58500958344731
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.94391466942678
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  65.94996768447507
printing an ep nov before normalisation:  57.28810915405326
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.68622083861439
printing an ep nov before normalisation:  4.478965428090476
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.994218003614016
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9982,     0.0003,     0.0000,     0.0001,     0.0014],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9978,     0.0000,     0.0002,     0.0018],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0066,     0.9158,     0.0241,     0.0534],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0017,     0.0004,     0.0245,     0.7893,     0.1841],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0045, 0.0481, 0.0819, 0.2822, 0.5833], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  39.03535124215684
siam score:  -0.85189956
siam score:  -0.84940755
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.433]
 [44.433]
 [55.02 ]
 [53.198]
 [44.433]] [[1.146]
 [1.146]
 [1.598]
 [1.52 ]
 [1.146]]
printing an ep nov before normalisation:  60.26454452906324
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[18.834]
 [32.436]
 [18.834]
 [18.834]
 [18.834]] [[0.286]
 [0.68 ]
 [0.286]
 [0.286]
 [0.286]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.907393672021996
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  47.15426773669905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.81601541901857
printing an ep nov before normalisation:  69.70785976007124
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 24.854143371149622
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[49.407]
 [49.407]
 [49.407]
 [49.407]
 [49.407]] [[1.]
 [1.]
 [1.]
 [1.]
 [1.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  6.435210502280597
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.58514765046055
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.553]
 [47.206]
 [47.206]
 [47.206]
 [47.206]] [[1.178]
 [1.242]
 [1.242]
 [1.242]
 [1.242]]
printing an ep nov before normalisation:  36.09489917755127
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.259]
 [46.951]
 [44.473]
 [39.781]
 [45.832]] [[1.19 ]
 [1.585]
 [1.501]
 [1.343]
 [1.547]]
printing an ep nov before normalisation:  39.94738881527493
siam score:  -0.84968114
printing an ep nov before normalisation:  11.321780367983083
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.555750846862797
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.477669583598523
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.0649178264494
printing an ep nov before normalisation:  75.28629187669503
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.036]
 [44.242]
 [30.872]
 [30.872]
 [30.872]] [[0.627]
 [1.205]
 [0.62 ]
 [0.62 ]
 [0.62 ]]
printing an ep nov before normalisation:  52.370333671569824
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.872327666302894
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.057701428504465
printing an ep nov before normalisation:  62.3938576276075
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85555863
printing an ep nov before normalisation:  45.17092921931606
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.732403313552204
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.93186366664576
siam score:  -0.8524355
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.062266679627506
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9962,     0.0003,     0.0001,     0.0001,     0.0033],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9510,     0.0046,     0.0011,     0.0434],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9620,     0.0119,     0.0260],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0273,     0.7570,     0.2153],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0040, 0.0691, 0.0774, 0.2436, 0.6060], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9911,     0.0014,     0.0006,     0.0004,     0.0066],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9987,     0.0001,     0.0001,     0.0011],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0023,     0.0006,     0.8803,     0.0202,     0.0966],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0014,     0.0764,     0.7525,     0.1690],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0008, 0.0662, 0.1436, 0.1847, 0.6047], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.52943987168909
printing an ep nov before normalisation:  1.9328645570794833
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.86 ]
 [39.5  ]
 [27.454]
 [33.08 ]
 [33.227]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.79659260577537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8501464
siam score:  -0.8510289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.475049541941445
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9840,     0.0106,     0.0000,     0.0004,     0.0050],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9815,     0.0001,     0.0002,     0.0177],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0125,     0.9756,     0.0006,     0.0112],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0005,     0.0205,     0.8548,     0.1238],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0015, 0.0613, 0.1980, 0.1497, 0.5894], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  87.04448733722522
printing an ep nov before normalisation:  60.58044609677274
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85124266
printing an ep nov before normalisation:  41.26313149285232
printing an ep nov before normalisation:  41.123170253656745
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.358706693723725
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.21904310630458
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.833945751190186
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8475314
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[56.846]
 [56.029]
 [56.846]
 [56.846]
 [56.846]] [[1.807]
 [1.771]
 [1.807]
 [1.807]
 [1.807]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.78674824007894
printing an ep nov before normalisation:  48.917859712301315
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[44.835]
 [48.841]
 [41.147]
 [44.461]
 [42.353]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  21.02331684421335
printing an ep nov before normalisation:  43.45496571537405
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.263538935816996
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.504460480533243
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8520982
printing an ep nov before normalisation:  46.53072961166135
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.84931659698486
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[32.664]
 [28.312]
 [29.307]
 [33.439]
 [27.098]] [[0.182]
 [0.148]
 [0.156]
 [0.189]
 [0.138]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.2925279975904
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  34.98140335083008
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.99839167280845
siam score:  -0.8479405
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[48.337]
 [54.89 ]
 [57.987]
 [48.337]
 [48.337]] [[0.615]
 [0.753]
 [0.818]
 [0.615]
 [0.615]]
printing an ep nov before normalisation:  2.2998319223724195
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.967311715342053
printing an ep nov before normalisation:  72.09482047631369
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[53.157]
 [53.157]
 [76.326]
 [53.157]
 [53.157]] [[1.014]
 [1.014]
 [1.62 ]
 [1.014]
 [1.014]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  62.16704399068818
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  67.60815673222851
printing an ep nov before normalisation:  49.45266793940855
line 256 mcts: sample exp_bonus 43.75567676244759
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[52.458]
 [52.203]
 [51.494]
 [55.34 ]
 [54.646]] [[1.489]
 [1.476]
 [1.441]
 [1.63 ]
 [1.596]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.092874785262794
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.080589785852766
printing an ep nov before normalisation:  36.93214390981118
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 59.19167450884495
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.197]
 [35.944]
 [21.182]
 [21.182]
 [21.182]] [[1.088]
 [1.122]
 [0.451]
 [0.451]
 [0.451]]
printing an ep nov before normalisation:  41.81968016661385
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.472]
 [39.472]
 [42.335]
 [39.472]
 [39.472]] [[1.039]
 [1.039]
 [1.226]
 [1.039]
 [1.039]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  48.2323812371839
printing an ep nov before normalisation:  43.337031254084955
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.244613971997044
printing an ep nov before normalisation:  54.91988127734335
siam score:  -0.86025894
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.52023409386887
printing an ep nov before normalisation:  40.89993953704834
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.32122679456346
printing an ep nov before normalisation:  3.279356804400777
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.155540466308594
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.13719902163339
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.58399950187455
siam score:  -0.85466367
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.89455355635532
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.84863394
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.83388316929417
line 256 mcts: sample exp_bonus 27.667327500365072
printing an ep nov before normalisation:  22.26224868533334
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  44.39715734412431
using explorer policy with actor:  1
printing an ep nov before normalisation:  50.7798577145519
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.34211104402052
actions average: 
K:  1  action  0 :  tensor([    0.9983,     0.0001,     0.0000,     0.0003,     0.0014],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9844,     0.0044,     0.0006,     0.0103],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0078,     0.9376,     0.0320,     0.0224],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0003,     0.0004,     0.8182,     0.1808],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0053, 0.0332, 0.0357, 0.2031, 0.7226], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.75365495681763
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  54.0677232299389
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8547432
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.67820992573387
printing an ep nov before normalisation:  53.495930146301546
printing an ep nov before normalisation:  45.136685268647035
printing an ep nov before normalisation:  62.93682098388672
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.003239044285976
printing an ep nov before normalisation:  45.72977918587536
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.014]
 [43.014]
 [52.042]
 [43.014]
 [43.014]] [[0.96 ]
 [0.96 ]
 [1.267]
 [0.96 ]
 [0.96 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.0074881476187727
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.152703877180144
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  35.77951061896321
printing an ep nov before normalisation:  31.392573703461117
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.395]
 [28.656]
 [28.345]
 [30.126]
 [28.611]] [[0.86 ]
 [0.721]
 [0.706]
 [0.795]
 [0.719]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Starting evaluation
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.391]
 [19.973]
 [21.819]
 [16.952]
 [16.613]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  44.97324470953344
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[14.999]
 [17.124]
 [ 8.961]
 [16.719]
 [16.036]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
siam score:  -0.8506448
printing an ep nov before normalisation:  54.383575827717614
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[25.885]
 [35.693]
 [25.885]
 [25.885]
 [25.982]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.615282062637046
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.0192708301159
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  26.99601410215302
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.24782812393244
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  43.4965515712973
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.599720231318955
actions average: 
K:  0  action  0 :  tensor([    0.9979,     0.0000,     0.0000,     0.0008,     0.0012],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9993,     0.0000,     0.0002,     0.0005],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0006,     0.9529,     0.0213,     0.0252],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0004,     0.0431,     0.7884,     0.1679],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0203,     0.0648,     0.1361,     0.7781],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.327]
 [39.406]
 [47.163]
 [47.46 ]
 [47.46 ]] [[0.834]
 [0.677]
 [0.988]
 [1.   ]
 [1.   ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.19771664301965
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.77078035537905
printing an ep nov before normalisation:  44.78623733427432
printing an ep nov before normalisation:  3.5525368689604875
printing an ep nov before normalisation:  32.74007768728342
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.63089983693309
printing an ep nov before normalisation:  24.491671267003706
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.304]
 [45.601]
 [45.601]
 [45.601]
 [45.601]] [[1.235]
 [1.171]
 [1.171]
 [1.171]
 [1.171]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.183771816310696
actions average: 
K:  3  action  0 :  tensor([    0.9979,     0.0010,     0.0000,     0.0004,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9975,     0.0000,     0.0000,     0.0023],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0016,     0.9590,     0.0151,     0.0243],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0007,     0.0005,     0.8797,     0.1189],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0039, 0.1164, 0.0834, 0.1542, 0.6422], grad_fn=<DivBackward0>)
line 256 mcts: sample exp_bonus 31.9084063529395
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.154 0.154 0.205 0.154 0.333]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8521374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.76470020839147
printing an ep nov before normalisation:  52.05566418412425
siam score:  -0.85629296
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[70.565]
 [57.532]
 [70.565]
 [70.565]
 [65.992]] [[1.333]
 [1.005]
 [1.333]
 [1.333]
 [1.218]]
printing an ep nov before normalisation:  46.781589300800825
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.630643371746466
maxi score, test score, baseline:  0.0001 0.0 0.0001
actions average: 
K:  1  action  0 :  tensor([    0.9581,     0.0001,     0.0000,     0.0320,     0.0098],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9961,     0.0010,     0.0009,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0243,     0.8880,     0.0111,     0.0765],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0001,     0.0249,     0.7785,     0.1961],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0004,     0.0502,     0.0038,     0.1522,     0.7933],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[47.321]
 [58.279]
 [54.688]
 [54.688]
 [54.688]] [[0.987]
 [1.494]
 [1.328]
 [1.328]
 [1.328]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.47125760907034
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.53555950091435
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.85590935
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 40.900852479995876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  41.954418555366544
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  53.92703214465197
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.40011692561729
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  34.904612696443166
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[24.698]
 [26.352]
 [26.352]
 [25.995]
 [25.542]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.69375621747349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[41.723]
 [41.745]
 [44.836]
 [48.514]
 [47.924]] [[0.748]
 [0.749]
 [0.888]
 [1.053]
 [1.027]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.25915667680349
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.05017082859344
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.09922052407768
printing an ep nov before normalisation:  47.54457276131967
actions average: 
K:  4  action  0 :  tensor([    0.9863,     0.0098,     0.0025,     0.0002,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0095,     0.9642,     0.0004,     0.0004,     0.0256],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.8849,     0.0391,     0.0760],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.1038, 0.0016, 0.0544, 0.6857, 0.1544], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0075, 0.1004, 0.2163, 0.1649, 0.5108], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  59.140259831481735
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.47319270966165
printing an ep nov before normalisation:  53.19994581839741
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.731499575910696
printing an ep nov before normalisation:  43.93083193869877
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.36960110130674
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.022141509628604
printing an ep nov before normalisation:  47.15229276944633
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.468]
 [43.136]
 [39.395]
 [39.813]
 [35.148]] [[0.955]
 [1.146]
 [0.951]
 [0.973]
 [0.73 ]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[40.32 ]
 [48.693]
 [63.929]
 [40.32 ]
 [40.32 ]] [[0.636]
 [0.875]
 [1.309]
 [0.636]
 [0.636]]
siam score:  -0.84594655
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  40.24945259559701
printing an ep nov before normalisation:  45.46534906502746
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  53.096701996104905
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.368]
 [33.515]
 [33.515]
 [33.515]
 [33.515]] [[1.373]
 [0.736]
 [0.736]
 [0.736]
 [0.736]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  57.7093830525514
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.80411214893673
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[35.521]
 [35.521]
 [35.521]
 [38.696]
 [35.521]] [[1.418]
 [1.418]
 [1.418]
 [1.638]
 [1.418]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[28.866]
 [28.866]
 [28.866]
 [28.866]
 [28.866]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9906,     0.0004,     0.0000,     0.0011,     0.0078],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9997,     0.0000,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9184,     0.0319,     0.0497],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0008,     0.0004,     0.0279,     0.8967,     0.0742],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0007, 0.0672, 0.1343, 0.1917, 0.6061], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[54.349]
 [54.349]
 [54.349]
 [54.349]
 [54.349]] [[0.905]
 [0.905]
 [0.905]
 [0.905]
 [0.905]]
printing an ep nov before normalisation:  61.58136072574706
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.033629568863343
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
printing an ep nov before normalisation:  46.30944337975418
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 6.970843357683991e-09
0.0 1.3279642035263832e-09
0.0 3.799595369313263e-11
0.0 1.6411622638090986e-08
0.0 0.0
0.0 0.0
0.0 6.551114780130474e-09
0.0 0.0
0.0 0.0
0.0 1.8008058136068445e-08
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 6.970843357683991e-09
0.0 2.1069461233502224e-11
0.0 2.4598336512192743e-11
0.0 1.6900544848862595e-11
0.0 0.0
0.0 0.0
0.0 4.061527866644288e-09
0.0 7.99670815960193e-10
0.0 0.0
0.0 5.014047417816222e-08
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.717968571511086
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.012863060798736
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.4172381201296
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.79666228893542
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.70770787185621
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[39.898]
 [39.898]
 [39.898]
 [39.898]
 [39.898]] [[0.346]
 [0.346]
 [0.346]
 [0.346]
 [0.346]]
siam score:  -0.85105515
printing an ep nov before normalisation:  36.079057526965634
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.937687928721846
printing an ep nov before normalisation:  20.90810763754446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.615]
 [38.615]
 [52.937]
 [38.615]
 [38.615]] [[0.326]
 [0.326]
 [0.556]
 [0.326]
 [0.326]]
printing an ep nov before normalisation:  37.38108180200578
printing an ep nov before normalisation:  32.667979501273486
printing an ep nov before normalisation:  35.756115970861984
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.73189357497988
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.80814740824982
printing an ep nov before normalisation:  31.44914408482681
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.936868075265735
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.312]
 [47.658]
 [42.312]
 [42.312]
 [42.312]] [[1.377]
 [1.658]
 [1.377]
 [1.377]
 [1.377]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8582838
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0016,     0.9772,     0.0000,     0.0001,     0.0211],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0002,     0.9558,     0.0005,     0.0434],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0010,     0.0002,     0.0004,     0.8441,     0.1543],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0038, 0.0298, 0.0959, 0.2000, 0.6705], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  51.79955670409779
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[42.244]
 [42.244]
 [42.244]
 [39.714]
 [42.244]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.313306613118854
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.67856705469881
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  39.40269771530446
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.246]
 [43.246]
 [43.246]
 [44.026]
 [43.246]] [[1.257]
 [1.257]
 [1.257]
 [1.296]
 [1.257]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.77981530343224
siam score:  -0.8537851
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.65349006652832
printing an ep nov before normalisation:  1.6371305348138776
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.877012316300316
printing an ep nov before normalisation:  54.408565285965224
maxi score, test score, baseline:  0.0001 0.0 0.0001
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.183210372924805
printing an ep nov before normalisation:  34.051821785812876
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8575549
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[37.672]
 [37.672]
 [37.672]
 [32.85 ]
 [37.672]] [[1.283]
 [1.283]
 [1.283]
 [1.118]
 [1.283]]
printing an ep nov before normalisation:  33.31592946827645
printing an ep nov before normalisation:  46.20763673398707
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  65.24222918919155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9915,     0.0061,     0.0000,     0.0002,     0.0021],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9990,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0026,     0.9236,     0.0227,     0.0511],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0071, 0.0046, 0.0186, 0.8722, 0.0975], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0031, 0.0535, 0.1500, 0.2172, 0.5762], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.475920034135004
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.36894312941314
printing an ep nov before normalisation:  52.013043565022855
printing an ep nov before normalisation:  38.43505283609129
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.63919478632085
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  4  action  0 :  tensor([    0.9973,     0.0002,     0.0000,     0.0007,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0061, 0.9822, 0.0036, 0.0014, 0.0068], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0000,     0.9131,     0.0373,     0.0496],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0001,     0.0245,     0.8074,     0.1678],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([    0.0003,     0.0881,     0.2725,     0.0884,     0.5508],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.86555004119873
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.762405750069775
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.132377306620285
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.6701263478289
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[58.531]
 [58.531]
 [58.531]
 [58.531]
 [58.531]] [[0.38]
 [0.38]
 [0.38]
 [0.38]
 [0.38]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[34.039]
 [34.039]
 [27.912]
 [26.958]
 [22.124]] [[0.902]
 [0.902]
 [0.667]
 [0.63 ]
 [0.445]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[51.997]
 [51.997]
 [51.997]
 [51.997]
 [51.997]] [[0.512]
 [0.512]
 [0.512]
 [0.512]
 [0.512]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.55183654571629
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[22.232]
 [29.587]
 [30.469]
 [29.587]
 [29.587]] [[0.324]
 [0.63 ]
 [0.667]
 [0.63 ]
 [0.63 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[26.986]
 [39.704]
 [24.26 ]
 [28.402]
 [24.091]] [[0.35 ]
 [0.629]
 [0.29 ]
 [0.381]
 [0.286]]
siam score:  -0.8498506
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.538]
 [58.198]
 [35.169]
 [58.198]
 [35.145]] [[1.607]
 [4.495]
 [2.   ]
 [4.495]
 [1.997]]
printing an ep nov before normalisation:  58.659199258129995
printing an ep nov before normalisation:  53.54529507293084
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8454529
UNIT TEST: sample policy line 217 mcts : [0.282 0.154 0.128 0.103 0.333]
printing an ep nov before normalisation:  1.3906719638369462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  19.202206134796143
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[45.766]
 [52.586]
 [45.766]
 [45.766]
 [45.766]] [[1.34]
 [1.72]
 [1.34]
 [1.34]
 [1.34]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.96046181114475
from probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[43.149]
 [41.499]
 [33.014]
 [30.619]
 [36.318]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  56.231230797596496
printing an ep nov before normalisation:  31.214116520831574
siam score:  -0.81366915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  48.17042359163876
printing an ep nov before normalisation:  46.40858512691473
printing an ep nov before normalisation:  51.5921991869755
using another actor
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[38.653]
 [43.163]
 [54.509]
 [54.509]
 [48.029]] [[1.054]
 [1.28 ]
 [1.848]
 [1.848]
 [1.524]]
printing an ep nov before normalisation:  62.2519023035758
line 256 mcts: sample exp_bonus 61.970028221540325
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0001],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0007],
        [    0.0000],
        [    0.0001],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 5.0573014700284066e-05
0.0 0.0001003252977033917
0.0 1.193072907852727e-05
0.0 1.4251541676129554e-05
0.0 0.000662299402685012
0.0 5.713178752025205e-06
0.0 5.912937669600129e-05
0.0 2.9644645011629776e-05
0.99 0.99
0.0 0.0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  64.72048069841419
printing an ep nov before normalisation:  48.34033012390137
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actions average: 
K:  4  action  0 :  tensor([    0.9975,     0.0006,     0.0000,     0.0005,     0.0014],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0082,     0.9857,     0.0014,     0.0004,     0.0043],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0008,     0.0026,     0.9399,     0.0016,     0.0552],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([0.0794, 0.0015, 0.0234, 0.7575, 0.1382], grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.1465, 0.1608, 0.0007, 0.3077, 0.3844], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
siam score:  -0.8109826
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
printing an ep nov before normalisation:  33.578291494929076
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[29.103]
 [29.103]
 [29.103]
 [29.103]
 [29.103]] [[2.006]
 [2.006]
 [2.006]
 [2.006]
 [2.006]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.08337664614326587, 0.5831167692836706]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  75.91297932705537
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
siam score:  -0.8109006
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  50.285335126711885
siam score:  -0.80569667
printing an ep nov before normalisation:  11.634180950666664
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
UNIT TEST: sample policy line 217 mcts : [0.154 0.205 0.154 0.128 0.359]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  29.049201733023438
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  52.45359661395534
line 256 mcts: sample exp_bonus 32.12272924930496
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  55.20092818684012
printing an ep nov before normalisation:  41.98587417602539
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
actions average: 
K:  0  action  0 :  tensor([    0.9940,     0.0003,     0.0005,     0.0015,     0.0036],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9811,     0.0001,     0.0005,     0.0182],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9307,     0.0117,     0.0575],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0031,     0.0002,     0.0028,     0.9045,     0.0893],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0009, 0.0892, 0.0913, 0.1653, 0.6534], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.806856478252435
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[69.796]
 [69.796]
 [69.796]
 [69.796]
 [69.796]] [[1.464]
 [1.464]
 [1.464]
 [1.464]
 [1.464]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
siam score:  -0.8075754
printing an ep nov before normalisation:  45.89195398016813
printing an ep nov before normalisation:  26.775014400482178
printing an ep nov before normalisation:  29.122857138216517
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0042,     0.9636,     0.0159,     0.0004,     0.0160],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0027,     0.9366,     0.0145,     0.0460],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0068,     0.0007,     0.0146,     0.8444,     0.1335],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0394, 0.0777, 0.0371, 0.1181, 0.7277], grad_fn=<DivBackward0>)
siam score:  -0.8119908
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  72.41020497217396
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.002]
 [0.   ]
 [0.001]
 [0.001]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  39.280840442396155
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[41.26 ]
 [41.26 ]
 [47.319]
 [40.318]
 [41.26 ]] [[1.2  ]
 [1.2  ]
 [1.479]
 [1.157]
 [1.2  ]]
printing an ep nov before normalisation:  57.65269125228505
printing an ep nov before normalisation:  56.92711370174862
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  33.033871849878835
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
siam score:  -0.82907915
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  45.65271517236142
printing an ep nov before normalisation:  48.10244916852799
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  36.249230069062975
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  64.80703055571908
printing an ep nov before normalisation:  66.34415567926008
printing an ep nov before normalisation:  60.979188629492384
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.018]
 [0.009]
 [0.009]
 [0.012]] [[30.996]
 [24.22 ]
 [23.737]
 [23.737]
 [25.121]] [[1.234]
 [0.823]
 [0.783]
 [0.783]
 [0.872]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  110.8172794468626
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  58.067705609087334
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.004]
 [0.004]
 [0.004]] [[48.647]
 [43.884]
 [50.99 ]
 [52.363]
 [48.647]] [[1.605]
 [1.385]
 [1.712]
 [1.775]
 [1.605]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  47.85728696648438
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  4.0097237214394
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  52.651836129446586
actions average: 
K:  2  action  0 :  tensor([    0.9930,     0.0045,     0.0001,     0.0002,     0.0022],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9639,     0.0030,     0.0005,     0.0326],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0142,     0.9452,     0.0004,     0.0402],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0014,     0.0002,     0.8549,     0.1429],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0006,     0.0027,     0.0821,     0.1361,     0.7786],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  57.20435686562813
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
siam score:  -0.8859232
printing an ep nov before normalisation:  35.4893123732535
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  40.099631057726164
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
siam score:  -0.89187706
printing an ep nov before normalisation:  53.729395901662855
printing an ep nov before normalisation:  53.374465355442915
rdn beta is 0 so we're just using the maxi policy
actions average: 
K:  2  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0234, 0.9119, 0.0025, 0.0019, 0.0603], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([0.0016, 0.0103, 0.9241, 0.0153, 0.0487], grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0002,     0.0001,     0.9562,     0.0435],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0027, 0.0611, 0.1674, 0.1716, 0.5972], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
UNIT TEST: sample policy line 217 mcts : [0.385 0.103 0.385 0.077 0.051]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.007]
 [0.009]
 [0.009]
 [0.009]] [[83.826]
 [75.28 ]
 [79.591]
 [83.826]
 [87.589]] [[1.785]
 [1.575]
 [1.681]
 [1.785]
 [1.876]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.06689039480037
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  74.85288942540835
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.006]
 [0.005]] [[99.623]
 [99.623]
 [99.623]
 [99.854]
 [99.623]] [[1.326]
 [1.326]
 [1.326]
 [1.331]
 [1.326]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
line 256 mcts: sample exp_bonus 60.893387410283985
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  67.20426081127034
siam score:  -0.91070175
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  55.61889726712819
printing an ep nov before normalisation:  45.089458567903264
siam score:  -0.91354537
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  0.35301278216081755
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]] [[45.824]
 [39.945]
 [43.101]
 [39.945]
 [43.842]] [[0.001]
 [0.001]
 [0.001]
 [0.001]
 [0.001]]
printing an ep nov before normalisation:  39.46267604827881
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[31.168]
 [28.907]
 [50.689]
 [50.655]
 [56.302]] [[0.131]
 [0.121]
 [0.212]
 [0.212]
 [0.236]]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.006]
 [0.01 ]
 [0.007]
 [0.007]] [[41.702]
 [44.85 ]
 [39.797]
 [39.49 ]
 [37.931]] [[2.007]
 [2.272]
 [1.849]
 [1.82 ]
 [1.688]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.008]
 [0.008]
 [0.008]] [[74.813]
 [74.813]
 [74.813]
 [74.813]
 [74.813]] [[1.408]
 [1.408]
 [1.408]
 [1.408]
 [1.408]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  47.576913627050374
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  79.84137099512127
printing an ep nov before normalisation:  68.18151584288462
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.003]
 [0.002]
 [0.002]] [[29.423]
 [32.846]
 [32.733]
 [30.277]
 [30.258]] [[0.401]
 [0.491]
 [0.488]
 [0.423]
 [0.423]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  43.22248218954127
maxi score, test score, baseline:  0.0001 0.0 0.0001
printing an ep nov before normalisation:  44.354108991307506
printing an ep nov before normalisation:  35.10634899139404
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  42.93552368409294
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  74.08408710621538
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.013]
 [0.01 ]
 [0.01 ]] [[30.102]
 [30.102]
 [26.308]
 [30.102]
 [30.102]] [[0.01 ]
 [0.01 ]
 [0.013]
 [0.01 ]
 [0.01 ]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.008]
 [0.007]
 [0.006]
 [0.01 ]] [[45.213]
 [48.021]
 [46.578]
 [44.503]
 [47.993]] [[1.402]
 [1.518]
 [1.423]
 [1.287]
 [1.518]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.008]
 [0.006]] [[34.19 ]
 [34.19 ]
 [34.19 ]
 [41.882]
 [34.19 ]] [[1.081]
 [1.081]
 [1.081]
 [1.485]
 [1.081]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.0771570272537
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  93.64395720708833
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
actions average: 
K:  0  action  0 :  tensor([    0.9958,     0.0001,     0.0002,     0.0009,     0.0029],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0007,     0.9504,     0.0115,     0.0020,     0.0355],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0005,     0.9789,     0.0014,     0.0192],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0041,     0.0001,     0.0002,     0.9121,     0.0835],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0304, 0.0008, 0.1017, 0.2309, 0.6361], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0001 0.0 0.0001
maxi score, test score, baseline:  0.0001 0.0 0.0001
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.54076001962709
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  41.564317060112344
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.018]
 [0.017]
 [0.017]
 [0.017]] [[51.872]
 [52.362]
 [41.744]
 [41.744]
 [41.744]] [[1.694]
 [1.719]
 [1.219]
 [1.219]
 [1.219]]
printing an ep nov before normalisation:  43.066346967444545
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.032]
 [0.032]
 [0.038]
 [0.032]
 [0.032]] [[55.692]
 [55.692]
 [47.443]
 [55.692]
 [55.692]] [[1.888]
 [1.888]
 [1.369]
 [1.888]
 [1.888]]
printing an ep nov before normalisation:  27.1646886392067
printing an ep nov before normalisation:  61.70642815477148
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.015]
 [0.001]
 [0.012]
 [0.013]] [[29.807]
 [25.697]
 [24.343]
 [38.504]
 [24.218]] [[0.016]
 [0.015]
 [0.001]
 [0.012]
 [0.013]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
from probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
using explorer policy with actor:  1
Starting evaluation
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  0
printing an ep nov before normalisation:  66.00155977404229
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[62.899]
 [62.899]
 [62.899]
 [62.899]
 [62.899]] [[0.017]
 [0.017]
 [0.017]
 [0.017]
 [0.017]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.015]
 [0.017]
 [0.012]
 [0.017]] [[49.197]
 [51.986]
 [44.728]
 [49.031]
 [44.728]] [[0.01 ]
 [0.015]
 [0.017]
 [0.012]
 [0.017]]
printing an ep nov before normalisation:  53.92334503765059
printing an ep nov before normalisation:  43.84134134716989
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.03 ]
 [0.016]
 [0.016]
 [0.016]] [[46.487]
 [48.597]
 [46.487]
 [46.487]
 [46.487]] [[0.016]
 [0.03 ]
 [0.016]
 [0.016]
 [0.016]]
printing an ep nov before normalisation:  56.38501744413011
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.011]
 [0.011]
 [0.011]] [[59.849]
 [60.597]
 [59.849]
 [59.849]
 [59.849]] [[0.011]
 [0.012]
 [0.011]
 [0.011]
 [0.011]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  42.03076362609863
using explorer policy with actor:  0
using explorer policy with actor:  0
printing an ep nov before normalisation:  45.90984173841726
printing an ep nov before normalisation:  35.35135510054139
printing an ep nov before normalisation:  46.80329056266474
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  59.97079365526669
printing an ep nov before normalisation:  54.492321354289444
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.011]
 [0.008]
 [0.007]
 [0.012]] [[45.541]
 [37.652]
 [53.823]
 [57.183]
 [45.541]] [[0.012]
 [0.011]
 [0.008]
 [0.007]
 [0.012]]
printing an ep nov before normalisation:  63.01731378584344
printing an ep nov before normalisation:  42.611275932890145
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]] [[57.798]
 [57.798]
 [60.898]
 [59.1  ]
 [57.798]] [[0.008]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  39.64941937514324
printing an ep nov before normalisation:  56.068429297847416
printing an ep nov before normalisation:  40.98931312561035
printing an ep nov before normalisation:  50.12536463927681
printing an ep nov before normalisation:  40.41107203411144
printing an ep nov before normalisation:  56.54978469263035
using explorer policy with actor:  0
printing an ep nov before normalisation:  37.386741638183594
printing an ep nov before normalisation:  40.81327676742347
printing an ep nov before normalisation:  30.562984943389893
printing an ep nov before normalisation:  43.40531251755614
printing an ep nov before normalisation:  54.025044248845326
printing an ep nov before normalisation:  28.593343818554455
printing an ep nov before normalisation:  28.506409618241467
printing an ep nov before normalisation:  44.546998127248564
line 256 mcts: sample exp_bonus 54.35539646248605
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
printing an ep nov before normalisation:  74.93643941549657
printing an ep nov before normalisation:  47.40399527080747
actions average: 
K:  0  action  0 :  tensor([    0.9949,     0.0003,     0.0000,     0.0002,     0.0046],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9545,     0.0004,     0.0025,     0.0422],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0129,     0.9369,     0.0089,     0.0412],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0017,     0.0005,     0.8654,     0.1322],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0025, 0.1244, 0.0283, 0.1182, 0.7266], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.78254678358768
printing an ep nov before normalisation:  55.04057088031355
Printing some Q and Qe and total Qs values:  [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.105]] [[57.955]
 [57.955]
 [57.955]
 [57.955]
 [55.756]] [[0.131]
 [0.131]
 [0.131]
 [0.131]
 [0.105]]
maxi score, test score, baseline:  0.0001 0.0 0.0001
probs:  [0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.08337165004067143, 0.5831417497966427]
actor:  0 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  50.27225663753298
maxi score, test score, baseline:  0.0021 0.0 0.0021
printing an ep nov before normalisation:  64.8275089263916
printing an ep nov before normalisation:  60.75871516580291
printing an ep nov before normalisation:  72.05978393554688
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.5831416495579818]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.102535247802734
printing an ep nov before normalisation:  36.380085945129395
printing an ep nov before normalisation:  59.69801430805092
printing an ep nov before normalisation:  55.47409354184074
maxi score, test score, baseline:  0.0021 0.0 0.0021
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.024]
 [0.02 ]
 [0.02 ]
 [0.02 ]] [[55.119]
 [71.553]
 [55.119]
 [55.119]
 [55.119]] [[1.236]
 [1.818]
 [1.236]
 [1.236]
 [1.236]]
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  97.19056300111721
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.014]
 [0.015]
 [0.009]
 [0.015]] [[47.341]
 [42.302]
 [41.947]
 [48.73 ]
 [47.025]] [[0.012]
 [0.014]
 [0.015]
 [0.009]
 [0.015]]
printing an ep nov before normalisation:  37.61698117453646
printing an ep nov before normalisation:  39.69982068519635
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.5831416495579818]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.5831416495579818]
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.5831416495579818]
printing an ep nov before normalisation:  36.3012505705688
printing an ep nov before normalisation:  50.13575498886355
printing an ep nov before normalisation:  43.81977702669835
printing an ep nov before normalisation:  41.32118907219702
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0021 0.0 0.0021
maxi score, test score, baseline:  0.0021 0.0 0.0021
probs:  [0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.08337167008840367, 0.5831416495579818]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  62.14981555818039
printing an ep nov before normalisation:  59.01128497862359
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.085]
 [0.118]
 [0.077]
 [0.11 ]] [[24.592]
 [28.76 ]
 [21.758]
 [32.896]
 [22.189]] [[0.518]
 [0.634]
 [0.458]
 [0.751]
 [0.463]]
printing an ep nov before normalisation:  19.464202102347148
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  48.071042603633956
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.9557109
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.9588675
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
line 256 mcts: sample exp_bonus 38.25564876505625
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  40.981970340914
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  35.97191004659198
printing an ep nov before normalisation:  33.34646136815049
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.1  ]
 [0.1  ]
 [0.108]
 [0.1  ]] [[27.064]
 [27.064]
 [27.064]
 [29.858]
 [27.064]] [[0.939]
 [0.939]
 [0.939]
 [1.112]
 [0.939]]
printing an ep nov before normalisation:  18.024919197360365
UNIT TEST: sample policy line 217 mcts : [0.205 0.205 0.154 0.179 0.256]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  37.75308967030418
printing an ep nov before normalisation:  45.16862279632942
printing an ep nov before normalisation:  35.932762384236646
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  23.88063669204712
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]] [[2.52 ]
 [2.574]
 [1.912]
 [2.492]
 [2.588]] [[0.002]
 [0.001]
 [0.   ]
 [0.001]
 [0.001]]
using another actor
printing an ep nov before normalisation:  49.35271655687368
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  24.66671164099653
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  55.132656147495545
line 256 mcts: sample exp_bonus 44.70381025314325
Printing some Q and Qe and total Qs values:  [[0.057]
 [0.057]
 [0.057]
 [0.056]
 [0.057]] [[63.576]
 [63.576]
 [60.224]
 [61.994]
 [63.576]] [[1.95 ]
 [1.95 ]
 [1.836]
 [1.896]
 [1.95 ]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.9529051
printing an ep nov before normalisation:  53.08911887131299
printing an ep nov before normalisation:  35.32130078282393
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.028]
 [0.023]
 [0.033]
 [0.023]] [[47.627]
 [45.168]
 [47.627]
 [50.418]
 [47.627]] [[1.854]
 [1.711]
 [1.854]
 [2.033]
 [1.854]]
printing an ep nov before normalisation:  39.7814958723325
actions average: 
K:  0  action  0 :  tensor([    0.9940,     0.0010,     0.0001,     0.0017,     0.0032],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9620,     0.0031,     0.0001,     0.0345],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0218,     0.9411,     0.0002,     0.0368],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0009,     0.0427,     0.8179,     0.1385],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0125, 0.1378, 0.0522, 0.1215, 0.6760], grad_fn=<DivBackward0>)
using another actor
printing an ep nov before normalisation:  41.33683946254345
siam score:  -0.94791055
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  43.32169105185231
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  56.88475732608934
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0002,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9359,     0.0036,     0.0005,     0.0596],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0097,     0.9467,     0.0119,     0.0317],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0029,     0.9456,     0.0512],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0051, 0.0900, 0.0285, 0.2340, 0.6425], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
from probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  22.8220239853531
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  50.64345669927849
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[ 3.042]
 [ 3.015]
 [32.701]
 [32.701]
 [32.701]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  54.91044114707374
printing an ep nov before normalisation:  32.73588326363194
printing an ep nov before normalisation:  40.682023848835335
printing an ep nov before normalisation:  50.720816063201646
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.22966958189779
printing an ep nov before normalisation:  46.887570261891234
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  32.111070692230335
printing an ep nov before normalisation:  64.68294831613865
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.007]
 [0.008]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.007]
 [0.008]
 [0.008]
 [0.008]]
printing an ep nov before normalisation:  46.1811346475666
printing an ep nov before normalisation:  53.28494324872334
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.93444145
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.40212142431009
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  28.67255484357899
printing an ep nov before normalisation:  30.055813789367676
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.017]
 [0.015]
 [0.018]] [[22.808]
 [23.679]
 [27.22 ]
 [27.328]
 [26.69 ]] [[0.837]
 [0.921]
 [1.263]
 [1.272]
 [1.213]]
actions average: 
K:  4  action  0 :  tensor([    0.9931,     0.0001,     0.0020,     0.0009,     0.0038],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9362,     0.0179,     0.0006,     0.0451],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0016, 0.0119, 0.9193, 0.0196, 0.0478], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0001,     0.0026,     0.0101,     0.8810,     0.1062],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0009, 0.2102, 0.0664, 0.2087, 0.5137], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.891170385592545
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.005]
 [0.004]
 [0.005]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  73.71089351613256
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[55.791]
 [55.791]
 [55.791]
 [55.791]
 [55.791]] [[1.905]
 [1.905]
 [1.905]
 [1.905]
 [1.905]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.57509899139404
printing an ep nov before normalisation:  60.82531676848124
printing an ep nov before normalisation:  33.19320148713955
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.006]
 [0.006]
 [0.006]
 [0.006]] [[38.985]
 [39.413]
 [38.479]
 [35.794]
 [38.651]] [[1.221]
 [1.249]
 [1.19 ]
 [1.019]
 [1.201]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  34.71049605042408
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  28.945690251033906
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.005]
 [0.005]
 [0.005]] [[55.379]
 [55.379]
 [55.379]
 [55.379]
 [55.379]] [[1.672]
 [1.672]
 [1.672]
 [1.672]
 [1.672]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  34.581204105242996
printing an ep nov before normalisation:  49.99825468435058
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.9196927
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  60.56961034574923
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[61.939]
 [44.243]
 [44.243]
 [44.243]
 [44.243]] [[1.458]
 [0.912]
 [0.912]
 [0.912]
 [0.912]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.002]
 [0.005]
 [0.004]
 [0.004]] [[51.442]
 [60.057]
 [54.016]
 [51.442]
 [51.442]] [[0.748]
 [0.872]
 [0.787]
 [0.748]
 [0.748]]
printing an ep nov before normalisation:  5.588910658793793
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  60.61767857484798
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  56.141671809116495
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  34.35094356536865
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
actions average: 
K:  1  action  0 :  tensor([    0.9991,     0.0003,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0016,     0.9365,     0.0027,     0.0004,     0.0587],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0226,     0.9039,     0.0164,     0.0566],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0013,     0.0015,     0.0005,     0.8739,     0.1228],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0051, 0.0660, 0.0353, 0.2045, 0.6891], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  37.61745005417677
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.005]
 [0.007]
 [0.005]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.005]
 [0.007]
 [0.005]
 [0.006]]
printing an ep nov before normalisation:  52.58879091615995
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  49.405427282607626
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.01 ]
 [0.012]
 [0.012]] [[79.813]
 [79.813]
 [98.958]
 [79.813]
 [79.813]] [[1.289]
 [1.289]
 [1.73 ]
 [1.289]
 [1.289]]
printing an ep nov before normalisation:  39.65909481048584
printing an ep nov before normalisation:  39.842681884765625
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.005]
 [0.006]
 [0.006]
 [0.006]] [[46.189]
 [51.994]
 [46.189]
 [46.189]
 [46.189]] [[1.45 ]
 [1.631]
 [1.45 ]
 [1.45 ]
 [1.45 ]]
printing an ep nov before normalisation:  50.28010156375444
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using explorer policy with actor:  1
siam score:  -0.92277443
printing an ep nov before normalisation:  60.058150940103744
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.028]
 [0.028]
 [0.022]
 [0.028]] [[37.513]
 [37.513]
 [37.513]
 [47.278]
 [37.513]] [[0.845]
 [0.845]
 [0.845]
 [1.185]
 [0.845]]
siam score:  -0.92406046
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.019]
 [0.021]
 [0.02 ]
 [0.019]] [[43.815]
 [45.509]
 [50.015]
 [50.554]
 [49.819]] [[1.198]
 [1.29 ]
 [1.52 ]
 [1.546]
 [1.508]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.928499
printing an ep nov before normalisation:  45.929832831058356
printing an ep nov before normalisation:  47.457348318034896
printing an ep nov before normalisation:  40.48472917315042
printing an ep nov before normalisation:  96.48045247490901
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  52.912398209141884
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
from probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.018]
 [0.018]
 [0.017]] [[73.366]
 [73.366]
 [91.461]
 [90.23 ]
 [73.366]] [[0.915]
 [0.915]
 [1.202]
 [1.183]
 [0.915]]
siam score:  -0.9312645
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
printing an ep nov before normalisation:  61.23440352622169
printing an ep nov before normalisation:  50.842732920346535
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
from probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.93189895
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
siam score:  -0.9320518
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.08337218870753921, 0.5831390564623041]
Printing some Q and Qe and total Qs values:  [[0.072]
 [0.072]
 [0.116]
 [0.072]
 [0.072]] [[49.23 ]
 [49.23 ]
 [51.972]
 [49.23 ]
 [49.23 ]] [[1.202]
 [1.202]
 [1.34 ]
 [1.202]
 [1.202]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]]
actions average: 
K:  3  action  0 :  tensor([    0.9860,     0.0001,     0.0116,     0.0001,     0.0022],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0038,     0.9673,     0.0002,     0.0009,     0.0278],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0005,     0.0002,     0.9686,     0.0033,     0.0273],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0003,     0.0145,     0.8947,     0.0904],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0102, 0.0129, 0.0856, 0.1625, 0.7287], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  50.754684350868935
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  27.14909257731182
printing an ep nov before normalisation:  31.990592565794636
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  45.04135014185941
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  25.82560191693928
printing an ep nov before normalisation:  33.553443703782435
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  62.94996698310126
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.93067331772987
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.04 ]
 [0.019]
 [0.019]
 [0.019]] [[35.427]
 [53.645]
 [35.427]
 [35.427]
 [35.427]] [[0.258]
 [0.862]
 [0.258]
 [0.258]
 [0.258]]
Printing some Q and Qe and total Qs values:  [[0.125]
 [0.132]
 [0.142]
 [0.134]
 [0.131]] [[33.039]
 [30.575]
 [30.028]
 [25.522]
 [31.687]] [[1.16 ]
 [1.033]
 [1.013]
 [0.759]
 [1.093]]
printing an ep nov before normalisation:  63.2520183455465
printing an ep nov before normalisation:  37.80117564667599
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.15780780219241
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.008]
 [0.011]
 [0.008]] [[83.956]
 [76.608]
 [75.467]
 [76.446]
 [75.467]] [[1.713]
 [1.493]
 [1.458]
 [1.49 ]
 [1.458]]
actions average: 
K:  0  action  0 :  tensor([    0.9920,     0.0008,     0.0001,     0.0028,     0.0043],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9340,     0.0363,     0.0003,     0.0293],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0314,     0.9124,     0.0002,     0.0559],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0005,     0.0253,     0.7815,     0.1924],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0007,     0.0545,     0.1578,     0.0893,     0.6976],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.021]
 [0.021]
 [0.02 ]] [[96.517]
 [96.517]
 [94.886]
 [94.405]
 [96.517]] [[1.843]
 [1.843]
 [1.804]
 [1.792]
 [1.843]]
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.009]
 [0.009]] [[60.02]
 [60.02]
 [60.02]
 [60.02]
 [60.02]] [[2.009]
 [2.009]
 [2.009]
 [2.009]
 [2.009]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  56.81810730504799
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  46.179145544999635
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.004]
 [0.004]
 [0.004]
 [0.004]]
printing an ep nov before normalisation:  1.60269852808644
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  64.58205688721087
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.017]
 [0.013]
 [0.017]] [[45.544]
 [45.544]
 [45.544]
 [50.081]
 [45.544]] [[1.566]
 [1.566]
 [1.566]
 [1.891]
 [1.566]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  71.52083082570569
printing an ep nov before normalisation:  46.60600240563306
printing an ep nov before normalisation:  48.04290338173732
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.007]
 [0.012]
 [0.01 ]
 [0.011]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.011]
 [0.007]
 [0.012]
 [0.01 ]
 [0.011]]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.008]
 [0.01 ]
 [0.01 ]
 [0.01 ]] [[48.261]
 [54.505]
 [48.261]
 [48.261]
 [48.261]] [[1.326]
 [1.639]
 [1.326]
 [1.326]
 [1.326]]
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.30458794770196
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  54.071826478138036
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  42.6926083519332
printing an ep nov before normalisation:  45.95541522745691
printing an ep nov before normalisation:  38.82756265498321
printing an ep nov before normalisation:  90.68714873274298
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.032]
 [0.027]
 [0.028]
 [0.04 ]] [[52.236]
 [53.284]
 [49.34 ]
 [42.465]
 [44.081]] [[1.255]
 [1.302]
 [1.124]
 [0.825]
 [0.908]]
printing an ep nov before normalisation:  58.3421448137742
printing an ep nov before normalisation:  51.48322504923272
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  49.70126771180245
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
actions average: 
K:  2  action  0 :  tensor([    0.9636,     0.0197,     0.0077,     0.0004,     0.0086],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0005,     0.9371,     0.0178,     0.0004,     0.0441],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0104,     0.9307,     0.0330,     0.0259],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0005,     0.0026,     0.9451,     0.0517],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0018, 0.0715, 0.0719, 0.1937, 0.6611], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  36.32211672120931
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.032]
 [0.03 ]
 [0.03 ]
 [0.03 ]] [[27.514]
 [41.152]
 [27.514]
 [27.514]
 [27.514]] [[0.413]
 [0.86 ]
 [0.413]
 [0.413]
 [0.413]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  48.49447727203369
printing an ep nov before normalisation:  52.21604678487156
UNIT TEST: sample policy line 217 mcts : [0.462 0.256 0.128 0.051 0.103]
maxi score, test score, baseline:  0.0021 0.05 0.05
actions average: 
K:  1  action  0 :  tensor([    0.9835,     0.0009,     0.0005,     0.0072,     0.0079],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9543,     0.0140,     0.0003,     0.0313],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0184,     0.8855,     0.0230,     0.0730],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0001,     0.0014,     0.9474,     0.0510],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0006,     0.0078,     0.0719,     0.2112,     0.7085],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.013]
 [0.015]
 [0.013]
 [0.013]] [[42.333]
 [34.314]
 [42.333]
 [29.189]
 [33.408]] [[2.885]
 [2.013]
 [2.885]
 [1.457]
 [1.915]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  54.35856331338063
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  71.38386836832605
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
siam score:  -0.9303275
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
siam score:  -0.93009245
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  52.85828626360488
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.029]
 [0.025]
 [0.009]
 [0.024]] [[36.218]
 [24.668]
 [31.605]
 [31.188]
 [30.2  ]] [[0.88 ]
 [0.412]
 [0.697]
 [0.663]
 [0.637]]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.025]
 [0.018]
 [0.018]
 [0.018]] [[38.909]
 [35.145]
 [40.993]
 [40.993]
 [40.993]] [[1.836]
 [1.482]
 [2.018]
 [2.018]
 [2.018]]
printing an ep nov before normalisation:  58.818230628967285
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
actions average: 
K:  1  action  0 :  tensor([    0.9067,     0.0097,     0.0001,     0.0281,     0.0555],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9137,     0.0162,     0.0006,     0.0694],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0235,     0.9379,     0.0002,     0.0385],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0009,     0.0006,     0.0405,     0.7991,     0.1589],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.0700,     0.1147,     0.0290,     0.7858],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  46.386308484157226
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.002]
 [0.002]
 [0.002]
 [0.002]] [[27.62]
 [27.62]
 [27.62]
 [27.62]
 [27.62]] [[1.805]
 [1.805]
 [1.805]
 [1.805]
 [1.805]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  48.214445985443156
printing an ep nov before normalisation:  90.98350817080231
printing an ep nov before normalisation:  97.74594545635222
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.38773819013206
printing an ep nov before normalisation:  52.26124996724915
printing an ep nov before normalisation:  45.80413176065355
printing an ep nov before normalisation:  54.06341950569138
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  67.66780045713922
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
siam score:  -0.92869765
printing an ep nov before normalisation:  32.27509682480153
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  37.02566593647554
maxi score, test score, baseline:  0.0021 0.05 0.05
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  50.73532814074168
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.008]
 [0.01 ]
 [0.009]
 [0.008]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  7.527867582242038
printing an ep nov before normalisation:  42.577369887829995
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  40.95778588426431
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[49.738]
 [37.967]
 [37.967]
 [37.967]
 [37.967]] [[1.267]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
printing an ep nov before normalisation:  40.31258894597994
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
line 256 mcts: sample exp_bonus 39.137823601347634
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  42.00059339270649
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9689,     0.0100,     0.0001,     0.0208],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0243,     0.8832,     0.0210,     0.0712],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0008,     0.0002,     0.0314,     0.8862,     0.0814],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0013, 0.0491, 0.1506, 0.2099, 0.5890], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  62.52676683279343
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[32.383]
 [32.383]
 [32.383]
 [32.383]
 [32.383]] [[64.785]
 [64.785]
 [64.785]
 [64.785]
 [64.785]]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.033]
 [0.038]
 [0.034]
 [0.038]] [[39.042]
 [51.701]
 [39.042]
 [50.003]
 [39.042]] [[0.627]
 [1.316]
 [0.627]
 [1.224]
 [0.627]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  54.8137668727162
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.037]
 [0.039]
 [0.037]
 [0.037]
 [0.037]] [[36.306]
 [48.863]
 [36.306]
 [36.306]
 [36.306]] [[0.684]
 [1.138]
 [0.684]
 [0.684]
 [0.684]]
printing an ep nov before normalisation:  35.7044801526683
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  49.73260568484432
line 256 mcts: sample exp_bonus 47.45919071070645
siam score:  -0.9353546
printing an ep nov before normalisation:  47.169251672191365
siam score:  -0.93653536
printing an ep nov before normalisation:  43.209936883175594
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  57.41761214701922
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.043]
 [0.043]
 [0.043]
 [0.043]
 [0.043]] [[52.425]
 [52.425]
 [52.425]
 [52.425]
 [52.425]] [[1.625]
 [1.625]
 [1.625]
 [1.625]
 [1.625]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.19968891143799
printing an ep nov before normalisation:  30.02150297164917
actions average: 
K:  0  action  0 :  tensor([    0.9904,     0.0008,     0.0001,     0.0001,     0.0087],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9564,     0.0149,     0.0002,     0.0283],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0171,     0.9269,     0.0257,     0.0303],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0009,     0.0100,     0.8953,     0.0937],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0005,     0.0943,     0.0026,     0.1761,     0.7266],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  66.22619365210528
printing an ep nov before normalisation:  49.65477806830864
maxi score, test score, baseline:  0.0021 0.05 0.05
siam score:  -0.9350043
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  41.13221115713571
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.019]
 [0.019]
 [0.019]] [[60.06]
 [60.06]
 [60.06]
 [60.06]
 [60.06]] [[1.947]
 [1.947]
 [1.947]
 [1.947]
 [1.947]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
siam score:  -0.93276423
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  37.993200107130605
printing an ep nov before normalisation:  56.1988543539667
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
actions average: 
K:  3  action  0 :  tensor([    0.9983,     0.0003,     0.0000,     0.0002,     0.0011],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9295,     0.0002,     0.0001,     0.0701],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0021,     0.9282,     0.0362,     0.0327],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0005,     0.0194,     0.9344,     0.0455],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0381, 0.1884, 0.2602, 0.5123], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]] [[39.737]
 [39.737]
 [39.737]
 [39.737]
 [39.737]] [[0.038]
 [0.038]
 [0.038]
 [0.038]
 [0.038]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.56627353614069
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 36.86869050044402
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.021]
 [0.005]
 [0.057]
 [0.024]] [[23.959]
 [18.617]
 [13.96 ]
 [32.894]
 [21.588]] [[0.819]
 [0.635]
 [0.466]
 [1.142]
 [0.736]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  65.84893035577714
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.028]
 [0.031]
 [0.028]
 [0.028]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.028]
 [0.031]
 [0.028]
 [0.028]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  46.01352160860226
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.03 ]
 [0.032]
 [0.03 ]
 [0.03 ]] [[53.378]
 [53.378]
 [63.339]
 [53.378]
 [53.378]] [[1.202]
 [1.202]
 [1.628]
 [1.202]
 [1.202]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  38.701801265050264
siam score:  -0.94579715
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
line 256 mcts: sample exp_bonus 44.61215229260412
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  57.354894086942664
Printing some Q and Qe and total Qs values:  [[0.027]
 [0.024]
 [0.027]
 [0.027]
 [0.027]] [[67.782]
 [64.566]
 [67.782]
 [67.782]
 [67.782]] [[2.015]
 [1.834]
 [2.015]
 [2.015]
 [2.015]]
printing an ep nov before normalisation:  52.068184697733834
UNIT TEST: sample policy line 217 mcts : [0.333 0.051 0.513 0.051 0.051]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  44.27499388782222
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[70.58]
 [62.16]
 [62.16]
 [62.16]
 [62.16]] [[1.905]
 [1.622]
 [1.622]
 [1.622]
 [1.622]]
from probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  46.85170168981022
siam score:  -0.95272744
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0002,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9125,     0.0219,     0.0006,     0.0647],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0169,     0.9270,     0.0106,     0.0453],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0003,     0.0280,     0.8139,     0.1575],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0052, 0.0226, 0.1462, 0.1968, 0.6292], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  94.04341848645383
actions average: 
K:  3  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0268,     0.8870,     0.0203,     0.0003,     0.0655],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0106,     0.9105,     0.0252,     0.0536],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0366, 0.0010, 0.0144, 0.8125, 0.1355], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0106, 0.0612, 0.0572, 0.0692, 0.8019], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
using another actor
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  38.689078418611544
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0667199568126852, 0.0667199568126852, 0.0667199568126852, 0.2666133765206481, 0.0667199568126852, 0.46650679622861096]
printing an ep nov before normalisation:  36.50899245718594
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  63.4431333844113
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  48.13598905290876
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  49.32247424527714
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0357],
        [0.0000],
        [0.0228],
        [0.0141],
        [0.0191],
        [0.0275],
        [0.0178],
        [0.0000],
        [0.0683],
        [0.0000]], dtype=torch.float64)
0.0 0.03569452390339531
0.0 0.0
0.0 0.022761618525049727
0.0 0.014115729595003169
0.0 0.01905231475675957
0.0 0.027514471062455227
0.0 0.017844354532250747
0.0 0.0
0.0 0.06825779074302267
0.0 0.0
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.017]
 [0.024]
 [0.018]
 [0.016]] [[40.396]
 [48.226]
 [44.29 ]
 [52.639]
 [40.396]] [[0.776]
 [1.137]
 [0.963]
 [1.34 ]
 [0.776]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  38.2278249942917
Printing some Q and Qe and total Qs values:  [[0.018]
 [0.022]
 [0.03 ]
 [0.026]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.018]
 [0.022]
 [0.03 ]
 [0.026]
 [0.024]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.19808668249624
printing an ep nov before normalisation:  56.99573463375319
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  38.755697448038305
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  31.56051142880134
UNIT TEST: sample policy line 217 mcts : [0.154 0.128 0.205 0.282 0.231]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.1  ]
 [0.099]
 [0.1  ]
 [0.1  ]
 [0.1  ]] [[70.912]
 [75.1  ]
 [70.912]
 [70.912]
 [70.912]] [[1.833]
 [1.967]
 [1.833]
 [1.833]
 [1.833]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.114]
 [0.023]
 [0.053]
 [0.063]] [[47.812]
 [45.645]
 [49.299]
 [47.974]
 [52.59 ]] [[1.551]
 [1.538]
 [1.629]
 [1.593]
 [1.834]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.9566539
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.083]
 [0.045]
 [0.045]
 [0.045]] [[42.459]
 [52.746]
 [42.459]
 [42.459]
 [42.459]] [[0.917]
 [1.568]
 [0.917]
 [0.917]
 [0.917]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  33.58308146288383
using explorer policy with actor:  1
siam score:  -0.9562059
printing an ep nov before normalisation:  60.11604211658357
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  48.842372746358016
printing an ep nov before normalisation:  50.93760879044444
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.111]
 [0.012]
 [0.085]
 [0.1  ]] [[42.673]
 [48.673]
 [50.456]
 [46.501]
 [48.714]] [[0.687]
 [1.008]
 [0.975]
 [0.904]
 [0.999]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.055]
 [0.047]] [[48.836]
 [48.836]
 [48.836]
 [50.43 ]
 [48.836]] [[1.035]
 [1.035]
 [1.035]
 [1.093]
 [1.035]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
printing an ep nov before normalisation:  0.9234059724497001
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  25.342849791369357
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.311]
 [0.311]
 [0.311]
 [0.311]
 [0.311]] [[35.605]
 [35.605]
 [35.605]
 [35.605]
 [35.605]] [[2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]]
printing an ep nov before normalisation:  54.681204088452105
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.081]
 [0.005]
 [0.062]
 [0.084]] [[61.224]
 [57.34 ]
 [42.288]
 [49.454]
 [58.419]] [[1.543]
 [1.46 ]
 [0.789]
 [1.129]
 [1.506]]
printing an ep nov before normalisation:  57.58879108743404
maxi score, test score, baseline:  0.0021 0.05 0.05
line 256 mcts: sample exp_bonus 57.031085728559106
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  62.066301780294516
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.046]
 [0.053]] [[55.684]
 [55.684]
 [55.684]
 [55.684]
 [74.879]] [[0.676]
 [0.676]
 [0.676]
 [0.676]
 [1.386]]
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  39.887840005021964
maxi score, test score, baseline:  0.0021 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.043]
 [0.029]
 [0.029]
 [0.029]] [[36.029]
 [37.377]
 [33.335]
 [33.335]
 [33.335]] [[0.719]
 [0.78 ]
 [0.635]
 [0.635]
 [0.635]]
printing an ep nov before normalisation:  27.645554542541504
printing an ep nov before normalisation:  55.98117816487438
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  2.733050693423138
printing an ep nov before normalisation:  42.55726635456085
siam score:  -0.9565106
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.9554477
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.068]
 [0.06 ]
 [0.063]
 [0.065]
 [0.171]] [[51.77 ]
 [49.75 ]
 [40.685]
 [50.142]
 [50.169]] [[1.618]
 [1.508]
 [1.049]
 [1.533]
 [1.64 ]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  35.12191118099501
printing an ep nov before normalisation:  24.0545392036438
printing an ep nov before normalisation:  56.49154240431491
maxi score, test score, baseline:  0.0021 0.05 0.05
UNIT TEST: sample policy line 217 mcts : [0.179 0.462 0.077 0.103 0.179]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.021]
 [0.021]
 [0.021]
 [0.021]]
actions average: 
K:  3  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9426,     0.0004,     0.0008,     0.0557],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0099,     0.9358,     0.0043,     0.0499],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0008,     0.0004,     0.9101,     0.0883],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0075, 0.0304, 0.1237, 0.2272, 0.6112], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  25.721700191497803
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  55.53433010466133
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.95130396
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  32.480886221112584
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.026]
 [0.03 ]
 [0.026]
 [0.026]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.026]
 [0.03 ]
 [0.026]
 [0.026]]
printing an ep nov before normalisation:  59.084294667393756
printing an ep nov before normalisation:  20.361184402531105
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.45 ]
 [0.476]
 [0.476]
 [0.476]] [[40.313]
 [45.591]
 [46.232]
 [46.232]
 [46.232]] [[1.503]
 [1.74 ]
 [1.797]
 [1.797]
 [1.797]]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.9503162
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
printing an ep nov before normalisation:  50.71279824858276
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.9406398
printing an ep nov before normalisation:  41.09172821044922
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  55.09530031655668
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  65.91888417190994
printing an ep nov before normalisation:  56.70127146931442
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0021 0.05 0.05
Starting evaluation
using explorer policy with actor:  1
siam score:  -0.9432362
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  42.36050135480231
printing an ep nov before normalisation:  44.69695590214514
printing an ep nov before normalisation:  38.240043086574545
printing an ep nov before normalisation:  31.25596046447754
siam score:  -0.94353485
printing an ep nov before normalisation:  45.96135944710365
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  32.804762622647786
printing an ep nov before normalisation:  26.17024177796902
printing an ep nov before normalisation:  46.21903316464855
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  30.39084525097097
printing an ep nov before normalisation:  20.645753760830058
printing an ep nov before normalisation:  64.07897469041575
siam score:  -0.9479013
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  22.41693685251685
printing an ep nov before normalisation:  32.92734112770991
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  20.64039069444823
printing an ep nov before normalisation:  28.140944593040196
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  32.256513264114616
printing an ep nov before normalisation:  48.478666236498526
Printing some Q and Qe and total Qs values:  [[0.04 ]
 [0.168]
 [0.023]
 [0.175]
 [0.14 ]] [[25.088]
 [26.582]
 [24.52 ]
 [26.759]
 [26.137]] [[0.04 ]
 [0.168]
 [0.023]
 [0.175]
 [0.14 ]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0021 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  28.11464490938953
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  31.899997572746905
maxi score, test score, baseline:  0.0021 0.05 0.05
actor:  0 policy actor:  0  step number:  116 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
line 256 mcts: sample exp_bonus 37.628689800569276
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  64.36870407427284
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  54.32829394957675
printing an ep nov before normalisation:  56.26891424548751
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.031]
 [0.041]
 [0.051]
 [0.051]
 [0.051]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.031]
 [0.041]
 [0.051]
 [0.051]
 [0.051]]
printing an ep nov before normalisation:  53.75628232959249
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.93421674
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  49.13301156914858
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  48.10383123795803
printing an ep nov before normalisation:  0.0001404804720550601
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  94.33545142185693
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  0
printing an ep nov before normalisation:  36.24387431155374
printing an ep nov before normalisation:  40.07119228738727
printing an ep nov before normalisation:  31.46231038232798
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  34.149488132496
printing an ep nov before normalisation:  47.85917892178338
actions average: 
K:  3  action  0 :  tensor([    0.9996,     0.0001,     0.0000,     0.0002,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9327,     0.0160,     0.0001,     0.0511],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0138,     0.8757,     0.0010,     0.1093],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0003,     0.0001,     0.9246,     0.0746],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0023, 0.0739, 0.0932, 0.1635, 0.6671], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  39.17018847128884
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.008]
 [0.011]
 [0.01 ]
 [0.009]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.007]
 [0.008]
 [0.011]
 [0.01 ]
 [0.009]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using another actor
printing an ep nov before normalisation:  29.818804033216573
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  67.85828599091231
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  49.426458514296584
printing an ep nov before normalisation:  41.83700011835208
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  27.856680749901606
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  22.659436784574325
printing an ep nov before normalisation:  43.36460590362549
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  22.50410556793213
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9279,     0.0145,     0.0078,     0.0497],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0004,     0.0006,     0.8720,     0.0374,     0.0896],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0003,     0.0002,     0.8893,     0.1096],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0020, 0.1482, 0.0644, 0.1191, 0.6663], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using another actor
using explorer policy with actor:  1
printing an ep nov before normalisation:  17.60867409231821
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.007]
 [0.007]
 [0.007]
 [0.007]] [[68.066]
 [68.066]
 [68.066]
 [68.066]
 [68.066]] [[1.331]
 [1.331]
 [1.331]
 [1.331]
 [1.331]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  28.440265820832487
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.008]
 [0.008]
 [0.01 ]
 [0.008]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.008]
 [0.008]
 [0.01 ]
 [0.008]
 [0.008]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.007]
 [0.007]
 [0.009]
 [0.006]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.009]
 [0.007]
 [0.007]
 [0.009]
 [0.006]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
from probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[24.353]
 [18.381]
 [18.381]
 [18.381]
 [18.381]] [[0.005]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.006]
 [0.005]
 [0.005]
 [0.005]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.004]
 [0.006]
 [0.005]
 [0.005]
 [0.005]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.012]
 [0.012]
 [0.012]
 [0.012]] [[46.948]
 [46.948]
 [46.948]
 [46.948]
 [46.948]] [[1.521]
 [1.521]
 [1.521]
 [1.521]
 [1.521]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
siam score:  -0.91724086
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
line 256 mcts: sample exp_bonus 60.195316911276095
actions average: 
K:  4  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0113,     0.8710,     0.0434,     0.0006,     0.0738],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0247,     0.8947,     0.0133,     0.0672],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0012,     0.0007,     0.0146,     0.9163,     0.0672],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0104, 0.1440, 0.0018, 0.1186, 0.7252], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  43.04055690765381
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
Printing some Q and Qe and total Qs values:  [[0.005]
 [0.007]
 [0.01 ]
 [0.009]
 [0.008]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.005]
 [0.007]
 [0.01 ]
 [0.009]
 [0.008]]
printing an ep nov before normalisation:  27.170535922050476
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  52.78517204849842
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  51.84677575520461
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
printing an ep nov before normalisation:  61.42781188967294
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714757035746026, 0.0714757035746026, 0.0714757035746026, 0.2142621482126987, 0.0714757035746026, 0.49983503748889085]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.47947301544417
printing an ep nov before normalisation:  48.80019911830407
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  60.945891859152
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
from probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.002]
 [0.003]
 [0.003]
 [0.003]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.003]
 [0.002]
 [0.003]
 [0.003]
 [0.003]]
printing an ep nov before normalisation:  39.80622693625943
printing an ep nov before normalisation:  40.99574392996884
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  55.03154754638672
line 256 mcts: sample exp_bonus 57.015586826876856
siam score:  -0.9216055
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
using explorer policy with actor:  1
printing an ep nov before normalisation:  47.19168055492952
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.034]
 [0.037]
 [0.037]
 [0.037]] [[58.399]
 [55.981]
 [45.486]
 [45.486]
 [45.486]] [[1.082]
 [1.037]
 [0.851]
 [0.851]
 [0.851]]
printing an ep nov before normalisation:  46.217278693993606
printing an ep nov before normalisation:  44.90441367574191
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
Printing some Q and Qe and total Qs values:  [[0.02 ]
 [0.02 ]
 [0.019]
 [0.02 ]
 [0.02 ]] [[88.865]
 [88.865]
 [87.453]
 [88.865]
 [88.865]] [[1.952]
 [1.952]
 [1.913]
 [1.952]
 [1.952]]
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.029]
 [0.029]
 [0.029]
 [0.029]] [[58.23]
 [58.23]
 [58.23]
 [58.23]
 [58.23]] [[2.019]
 [2.019]
 [2.019]
 [2.019]
 [2.019]]
printing an ep nov before normalisation:  41.135080027049554
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  53.747549057006836
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  57.81776728627089
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  44.464133901689365
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
Printing some Q and Qe and total Qs values:  [[0.284]
 [0.284]
 [0.284]
 [0.284]
 [0.38 ]] [[35.747]
 [35.747]
 [35.747]
 [35.747]
 [49.192]] [[0.725]
 [0.725]
 [0.725]
 [0.725]
 [1.314]]
printing an ep nov before normalisation:  40.80398082733154
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  52.09370452409205
printing an ep nov before normalisation:  47.02210006927408
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  49.15357087813783
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
actions average: 
K:  1  action  0 :  tensor([    0.9995,     0.0002,     0.0001,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9714,     0.0001,     0.0003,     0.0281],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0151,     0.9539,     0.0004,     0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0001,     0.0001,     0.8638,     0.1357],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0025, 0.0566, 0.0378, 0.1448, 0.7583], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
Printing some Q and Qe and total Qs values:  [[0.036]
 [0.001]
 [0.036]
 [0.036]
 [0.036]] [[49.055]
 [52.54 ]
 [49.055]
 [49.055]
 [49.055]] [[1.201]
 [1.334]
 [1.201]
 [1.201]
 [1.201]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
siam score:  -0.91752154
siam score:  -0.9183353
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  41.63698990865377
maxi score, test score, baseline:  0.0041 0.05 0.05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  42.57066254177166
siam score:  -0.9172778
Printing some Q and Qe and total Qs values:  [[0.029]
 [0.027]
 [0.029]
 [0.013]
 [0.028]] [[32.419]
 [24.5  ]
 [31.549]
 [37.802]
 [33.179]] [[0.397]
 [0.205]
 [0.377]
 [0.51 ]
 [0.414]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  42.08766211925984
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0002,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.9485,     0.0112,     0.0003,     0.0396],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0001,     0.9344,     0.0232,     0.0421],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0001,     0.0007,     0.0112,     0.9333,     0.0548],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0058, 0.0481, 0.0827, 0.1523, 0.7111], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.011]] [[50.521]
 [54.106]
 [50.521]
 [50.521]
 [50.277]] [[1.379]
 [1.571]
 [1.379]
 [1.379]
 [1.363]]
Printing some Q and Qe and total Qs values:  [[0.009]
 [0.009]
 [0.009]
 [0.013]
 [0.009]] [[24.663]
 [24.663]
 [24.663]
 [33.061]
 [24.663]] [[0.663]
 [0.663]
 [0.663]
 [1.091]
 [0.663]]
printing an ep nov before normalisation:  53.965379692596194
Printing some Q and Qe and total Qs values:  [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.013]
 [0.013]
 [0.013]
 [0.013]
 [0.013]]
UNIT TEST: sample policy line 217 mcts : [0.333 0.103 0.179 0.103 0.282]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  32.05369786896041
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.019]
 [0.   ]
 [0.012]
 [0.013]] [[36.419]
 [34.136]
 [12.765]
 [26.449]
 [30.351]] [[0.   ]
 [0.019]
 [0.   ]
 [0.012]
 [0.013]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.912895
printing an ep nov before normalisation:  32.02659713782268
printing an ep nov before normalisation:  71.3659733977612
using explorer policy with actor:  1
printing an ep nov before normalisation:  76.28074829099252
printing an ep nov before normalisation:  48.406033515930176
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  38.165410608543425
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  0.029244992461485708
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.494]
 [0.09 ]
 [0.303]
 [0.431]] [[20.326]
 [21.697]
 [20.25 ]
 [27.602]
 [22.373]] [[1.276]
 [1.177]
 [0.692]
 [1.319]
 [1.153]]
printing an ep nov before normalisation:  29.602444424656095
printing an ep nov before normalisation:  40.86357642720615
siam score:  -0.9150159
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actions average: 
K:  3  action  0 :  tensor([    0.9984,     0.0005,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9443,     0.0046,     0.0005,     0.0505],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9295,     0.0282,     0.0421],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0001,     0.0002,     0.9749,     0.0248],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0021, 0.0549, 0.1287, 0.1672, 0.6472], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9141581
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
from probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9147653
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.013]
 [0.015]
 [0.014]
 [0.013]] [[40.171]
 [42.127]
 [33.677]
 [32.305]
 [31.322]] [[0.35 ]
 [0.383]
 [0.256]
 [0.233]
 [0.217]]
printing an ep nov before normalisation:  45.64366254000127
actions average: 
K:  2  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9929,     0.0001,     0.0004,     0.0065],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0135,     0.9575,     0.0014,     0.0275],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0001,     0.0002,     0.0126,     0.9583,     0.0288],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.1486, 0.0319, 0.1078, 0.7105], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  56.8545260432786
siam score:  -0.91547775
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  57.94291497641818
printing an ep nov before normalisation:  44.78021169854414
printing an ep nov before normalisation:  55.94156424544497
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
actions average: 
K:  1  action  0 :  tensor([    0.9974,     0.0002,     0.0000,     0.0011,     0.0013],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0007,     0.9742,     0.0003,     0.0002,     0.0246],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0009,     0.8947,     0.0456,     0.0586],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0002,     0.0006,     0.9466,     0.0526],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.0523,     0.1403,     0.2437,     0.5632],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.67038507100646
actions average: 
K:  3  action  0 :  tensor([    0.9940,     0.0000,     0.0058,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9110,     0.0124,     0.0011,     0.0748],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0003,     0.9613,     0.0001,     0.0383],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0026,     0.0002,     0.0305,     0.8945,     0.0722],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0008, 0.0596, 0.1304, 0.1793, 0.6297], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9268523
printing an ep nov before normalisation:  43.63121185743686
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  52.18219194737073
printing an ep nov before normalisation:  32.603201750100375
printing an ep nov before normalisation:  80.65063993544777
printing an ep nov before normalisation:  68.82012437166725
siam score:  -0.92929995
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  52.05690837595369
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.017]
 [0.017]
 [0.017]
 [0.017]] [[46.982]
 [48.723]
 [48.723]
 [48.723]
 [48.723]] [[1.048]
 [1.128]
 [1.128]
 [1.128]
 [1.128]]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  42.82688416111613
printing an ep nov before normalisation:  56.76935027234711
printing an ep nov before normalisation:  55.312845445130456
siam score:  -0.92794985
using explorer policy with actor:  1
from probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  24.771490897755132
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  30.28891585652384
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  23.609916089281466
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9240215
actions average: 
K:  2  action  0 :  tensor([    0.9981,     0.0000,     0.0000,     0.0009,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([0.0017, 0.9512, 0.0210, 0.0012, 0.0250], grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0175,     0.9316,     0.0003,     0.0504],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0007,     0.0162,     0.8791,     0.1036],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0013, 0.0406, 0.0629, 0.2249, 0.6704], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  97.79278161677301
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  38.053717613220215
printing an ep nov before normalisation:  82.75498452254395
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.019]
 [0.019]
 [0.015]
 [0.012]] [[39.194]
 [ 0.   ]
 [ 0.   ]
 [35.738]
 [34.889]] [[0.022]
 [0.019]
 [0.019]
 [0.015]
 [0.012]]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  42.96372134132404
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  49.93160992903528
printing an ep nov before normalisation:  49.07775945817809
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  48.32700738068514
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.70967744065408
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9251066
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
actions average: 
K:  3  action  0 :  tensor([    0.9991,     0.0002,     0.0000,     0.0003,     0.0004],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9208,     0.0142,     0.0003,     0.0645],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0489,     0.8283,     0.0165,     0.1062],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0002,     0.0145,     0.8714,     0.1134],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0044, 0.0513, 0.0734, 0.1379, 0.7330], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
siam score:  -0.9245807
printing an ep nov before normalisation:  32.727584919150345
from probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  44.38635605636724
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.07411794142921893, 0.07411794142921893, 0.07411794142921893, 0.18517641171415622, 0.07411794142921893, 0.5183518225689681]
printing an ep nov before normalisation:  41.501687780111524
printing an ep nov before normalisation:  42.14981529090444
printing an ep nov before normalisation:  30.853592161960435
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  40.19814608805914
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  43.30236692930193
line 256 mcts: sample exp_bonus 82.96435818496401
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  49.48028271871422
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
siam score:  -0.9315091
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  62.07777141126834
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.7582933972233
printing an ep nov before normalisation:  42.13824622122627
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  45.70328205753526
from probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  56.13899230957031
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  57.11428193598052
siam score:  -0.9357369
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
Printing some Q and Qe and total Qs values:  [[0.185]
 [0.185]
 [0.185]
 [0.185]
 [0.185]] [[46.634]
 [46.634]
 [46.634]
 [46.634]
 [46.634]] [[1.731]
 [1.731]
 [1.731]
 [1.731]
 [1.731]]
printing an ep nov before normalisation:  43.464169911745685
printing an ep nov before normalisation:  0.0020354661643295913
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  56.09697795707338
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  47.12081014350731
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  58.15583637782506
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  46.48251008585534
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
Printing some Q and Qe and total Qs values:  [[0.039]
 [0.056]
 [0.039]
 [0.026]
 [0.039]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.039]
 [0.056]
 [0.039]
 [0.026]
 [0.039]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  27.81240224838257
printing an ep nov before normalisation:  60.06961022723056
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  46.10409536836172
printing an ep nov before normalisation:  27.685644083257916
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
Printing some Q and Qe and total Qs values:  [[0.045]
 [0.049]
 [0.049]
 [0.049]
 [0.049]] [[51.156]
 [48.449]
 [48.449]
 [48.449]
 [48.449]] [[1.348]
 [1.234]
 [1.234]
 [1.234]
 [1.234]]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.   ]
 [0.001]
 [0.002]
 [0.002]] [[58.873]
 [58.502]
 [58.896]
 [56.497]
 [56.497]] [[1.217]
 [1.202]
 [1.218]
 [1.121]
 [1.121]]
printing an ep nov before normalisation:  46.53729532492113
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  35.76914005044993
using another actor
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.05004461468353
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
using another actor
from probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
printing an ep nov before normalisation:  60.688379883572395
printing an ep nov before normalisation:  52.16420817319503
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.041]
 [0.041]
 [0.048]
 [0.039]
 [0.041]] [[28.434]
 [28.434]
 [41.106]
 [43.437]
 [28.434]] [[0.422]
 [0.422]
 [1.024]
 [1.124]
 [0.422]]
from probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0667173002958132, 0.0667173002958132, 0.0667173002958132, 0.16666666666666669, 0.16666666666666669, 0.46651476577922707]
Printing some Q and Qe and total Qs values:  [[0.945]
 [1.08 ]
 [0.402]
 [0.697]
 [1.17 ]] [[28.098]
 [32.945]
 [28.856]
 [29.734]
 [33.186]] [[2.117]
 [2.725]
 [1.647]
 [2.028]
 [2.839]]
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
actions average: 
K:  1  action  0 :  tensor([    0.9905,     0.0002,     0.0001,     0.0050,     0.0042],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9580,     0.0117,     0.0003,     0.0301],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0008,     0.0011,     0.9468,     0.0005,     0.0509],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0010,     0.0328,     0.8966,     0.0693],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0378, 0.0374, 0.0107, 0.1753, 0.7388], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
siam score:  -0.9201794
siam score:  -0.9200413
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  37.67729608323686
printing an ep nov before normalisation:  46.87663916807251
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.023]
 [0.028]
 [0.021]
 [0.024]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.015]
 [0.023]
 [0.028]
 [0.021]
 [0.024]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  35.430169105529785
printing an ep nov before normalisation:  57.927937118263756
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
from probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  44.107449669047064
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  51.167605126623
siam score:  -0.93004274
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.02 ]
 [0.024]
 [0.02 ]
 [0.022]] [[33.125]
 [42.591]
 [58.769]
 [37.867]
 [45.495]] [[0.291]
 [0.422]
 [0.653]
 [0.356]
 [0.465]]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.045]
 [0.024]
 [0.026]
 [0.024]] [[44.772]
 [56.931]
 [44.772]
 [55.066]
 [44.772]] [[0.6  ]
 [1.005]
 [0.6  ]
 [0.928]
 [0.6  ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  67.53030904501219
siam score:  -0.92760867
printing an ep nov before normalisation:  48.06088941633065
Printing some Q and Qe and total Qs values:  [[0.02]
 [0.02]
 [0.02]
 [0.02]
 [0.02]] [[43.206]
 [43.206]
 [43.206]
 [43.206]
 [43.206]] [[1.007]
 [1.007]
 [1.007]
 [1.007]
 [1.007]]
printing an ep nov before normalisation:  57.143910368436266
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  79.2610338265467
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
using explorer policy with actor:  1
siam score:  -0.92905784
printing an ep nov before normalisation:  44.245453568452916
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  36.0941538313394
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.022]
 [0.022]
 [0.022]
 [0.022]] [[42.406]
 [42.406]
 [42.406]
 [45.737]
 [42.406]] [[1.506]
 [1.506]
 [1.506]
 [1.695]
 [1.506]]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.021]
 [0.024]
 [0.021]] [[42.53 ]
 [42.53 ]
 [42.53 ]
 [48.671]
 [42.53 ]] [[1.151]
 [1.151]
 [1.151]
 [1.419]
 [1.151]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
printing an ep nov before normalisation:  57.57145269988344
Printing some Q and Qe and total Qs values:  [[0.03 ]
 [0.029]
 [0.029]
 [0.029]
 [0.031]] [[48.705]
 [34.827]
 [34.827]
 [42.2  ]
 [43.489]] [[1.637]
 [0.852]
 [0.852]
 [1.269]
 [1.344]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.69371557235718
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
Printing some Q and Qe and total Qs values:  [[0.047]
 [0.047]
 [0.047]
 [0.033]
 [0.047]] [[44.272]
 [44.272]
 [44.272]
 [45.567]
 [44.272]] [[1.753]
 [1.753]
 [1.753]
 [1.834]
 [1.753]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.06949194610378515, 0.06949194610378515, 0.06949194610378515, 0.15278456372911217, 0.15278456372911217, 0.4859550342304202]
UNIT TEST: sample policy line 217 mcts : [0.077 0.077 0.103 0.538 0.205]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.04 ]
 [0.035]
 [0.035]] [[ 0.   ]
 [ 0.   ]
 [35.201]
 [ 0.   ]
 [ 0.   ]] [[-0.415]
 [-0.415]
 [ 1.079]
 [-0.415]
 [-0.415]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
actions average: 
K:  3  action  0 :  tensor([    0.9559,     0.0192,     0.0000,     0.0037,     0.0212],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9618,     0.0007,     0.0010,     0.0363],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0020, 0.0014, 0.8688, 0.0312, 0.0967], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0002,     0.0015,     0.9068,     0.0915],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0269, 0.1353, 0.0458, 0.1569, 0.6351], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  21.53714418411255
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.397]
 [0.385]
 [0.323]
 [0.399]
 [0.399]] [[45.91 ]
 [38.207]
 [32.683]
 [36.179]
 [36.179]] [[1.621]
 [1.281]
 [0.984]
 [1.209]
 [1.209]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.053]
 [0.056]
 [0.041]
 [0.075]] [[43.038]
 [41.272]
 [36.119]
 [41.084]
 [39.605]] [[0.771]
 [0.739]
 [0.556]
 [0.721]
 [0.701]]
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.029]
 [0.033]
 [0.032]
 [0.03 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.026]
 [0.029]
 [0.033]
 [0.032]
 [0.03 ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
siam score:  -0.93069136
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
actions average: 
K:  2  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0008,     0.9367,     0.0068,     0.0003,     0.0553],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0190,     0.8510,     0.0271,     0.1026],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0001,     0.0026,     0.8695,     0.1272],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0012, 0.0676, 0.0650, 0.0895, 0.7768], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  62.489698489794854
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
from probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0041 0.05 0.05
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  47.92530105484137
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  43.52596493290682
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0041 0.05 0.05
printing an ep nov before normalisation:  77.68949093364506
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.038]
 [0.034]
 [0.033]
 [0.033]] [[32.293]
 [34.397]
 [42.151]
 [33.757]
 [32.785]] [[0.173]
 [0.202]
 [0.272]
 [0.192]
 [0.182]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
Printing some Q and Qe and total Qs values:  [[0.022]
 [0.023]
 [0.023]
 [0.023]
 [0.023]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.022]
 [0.023]
 [0.023]
 [0.023]
 [0.023]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0041 0.05 0.05
Printing some Q and Qe and total Qs values:  [[0.048]
 [0.042]
 [0.036]
 [0.042]
 [0.042]] [[39.701]
 [35.279]
 [39.364]
 [35.279]
 [35.279]] [[1.921]
 [1.707]
 [1.893]
 [1.707]
 [1.707]]
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.028]
 [0.024]
 [0.024]
 [0.024]] [[28.626]
 [43.458]
 [28.626]
 [28.626]
 [28.626]] [[0.56 ]
 [1.069]
 [0.56 ]
 [0.56 ]
 [0.56 ]]
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.029]
 [0.023]
 [0.023]
 [0.024]] [[27.125]
 [38.104]
 [27.125]
 [27.125]
 [26.178]] [[0.489]
 [0.855]
 [0.489]
 [0.489]
 [0.46 ]]
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.015]
 [0.015]
 [0.011]
 [0.015]
 [0.004]] [[ 0.   ]
 [ 0.   ]
 [20.488]
 [ 0.   ]
 [12.993]] [[-0.204]
 [-0.204]
 [ 0.228]
 [-0.204]
 [ 0.061]]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9487,     0.0015,     0.0006,     0.0492],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9713,     0.0002,     0.0285],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0003,     0.0277,     0.8740,     0.0977],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0113, 0.0715, 0.0388, 0.1383, 0.7401], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0041 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  29.975837943983993
using another actor
siam score:  -0.9234253
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  34.01125696042349
printing an ep nov before normalisation:  29.475747727243252
printing an ep nov before normalisation:  49.841719644601746
printing an ep nov before normalisation:  51.15388287423769
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  50.390136820740295
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  49.69680850090773
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  28.895365328946074
printing an ep nov before normalisation:  22.817696458100688
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
siam score:  -0.91714066
siam score:  -0.91879046
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([0.0039, 0.9022, 0.0226, 0.0014, 0.0699], grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0002,     0.0114,     0.9382,     0.0051,     0.0452],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0007,     0.0001,     0.1323,     0.7087,     0.1583],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0814, 0.1114, 0.1692, 0.6370], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
line 256 mcts: sample exp_bonus 60.70813700151026
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  64.05105210712206
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  66.29476127337126
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  69.98268454874714
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.019]
 [0.021]
 [0.02 ]
 [0.019]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.019]
 [0.021]
 [0.02 ]
 [0.019]]
Printing some Q and Qe and total Qs values:  [[0.012]
 [0.016]
 [0.017]
 [0.015]
 [0.017]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.012]
 [0.016]
 [0.017]
 [0.015]
 [0.017]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  58.009872471571036
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
printing an ep nov before normalisation:  46.724745945529534
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0714738962965213, 0.0714738962965213, 0.0714738962965213, 0.14286847407413034, 0.14286847407413034, 0.4998413629621754]
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using another actor
printing an ep nov before normalisation:  67.12771105506211
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.016]
 [0.016]
 [0.016]
 [0.016]
 [0.016]] [[43.273]
 [43.273]
 [43.273]
 [43.273]
 [43.273]] [[0.6]
 [0.6]
 [0.6]
 [0.6]
 [0.6]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  28.048709636498696
siam score:  -0.90738755
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
printing an ep nov before normalisation:  16.402274768006464
printing an ep nov before normalisation:  45.56563736946995
printing an ep nov before normalisation:  30.78826904296875
printing an ep nov before normalisation:  73.79527308819839
printing an ep nov before normalisation:  41.85489435675221
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  48.59289960220768
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
actions average: 
K:  2  action  0 :  tensor([    0.9996,     0.0001,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9650,     0.0001,     0.0004,     0.0342],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0103,     0.8455,     0.0304,     0.1135],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0006,     0.0002,     0.0006,     0.9456,     0.0531],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0009, 0.0376, 0.0781, 0.1295, 0.7539], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
siam score:  -0.9050726
actions average: 
K:  0  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0001,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9854,     0.0004,     0.0004,     0.0138],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0153,     0.8766,     0.0208,     0.0873],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0020,     0.0005,     0.0008,     0.9062,     0.0905],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0030, 0.1315, 0.0158, 0.1209, 0.7288], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  41.46064854828302
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.01 ]
 [0.008]
 [0.011]
 [0.008]] [[55.455]
 [41.436]
 [44.104]
 [59.041]
 [44.104]] [[1.307]
 [0.78 ]
 [0.878]
 [1.443]
 [0.878]]
Printing some Q and Qe and total Qs values:  [[0.011]
 [0.   ]
 [0.013]
 [0.011]
 [0.012]] [[49.736]
 [45.918]
 [42.044]
 [52.965]
 [48.018]] [[1.438]
 [1.219]
 [1.02 ]
 [1.615]
 [1.346]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9388,     0.0248,     0.0006,     0.0356],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0263,     0.9041,     0.0155,     0.0540],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0018,     0.0001,     0.0298,     0.8306,     0.1377],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0019, 0.1879, 0.1083, 0.1010, 0.6009], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  38.817589925149946
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.006]
 [0.007]
 [0.009]
 [0.008]
 [0.009]] [[50.092]
 [42.326]
 [34.323]
 [36.934]
 [39.156]] [[0.479]
 [0.365]
 [0.248]
 [0.286]
 [0.319]]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  30.08043306265695
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
using explorer policy with actor:  1
using explorer policy with actor:  1
using explorer policy with actor:  1
siam score:  -0.90786844
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.523]
 [1.001]
 [0.523]
 [0.523]
 [0.523]] [[38.067]
 [35.728]
 [38.067]
 [38.067]
 [38.067]] [[1.625]
 [1.975]
 [1.625]
 [1.625]
 [1.625]]
from probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  21.100103855133057
printing an ep nov before normalisation:  45.94130987302506
printing an ep nov before normalisation:  26.262142658233643
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
from probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
line 256 mcts: sample exp_bonus 44.28432076357517
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
using explorer policy with actor:  1
printing an ep nov before normalisation:  55.389132787227524
printing an ep nov before normalisation:  42.32374789398037
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  16.473162459104884
siam score:  -0.9104576
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  0.001567740042673904
from probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  29.462484399426614
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
siam score:  -0.90548974
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  84.79203785715038
printing an ep nov before normalisation:  48.04579505866029
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.019]
 [0.019]
 [0.017]
 [0.019]
 [0.019]] [[26.764]
 [26.764]
 [38.038]
 [26.764]
 [26.764]] [[0.416]
 [0.416]
 [1.127]
 [0.416]
 [0.416]]
maxi score, test score, baseline:  0.0061 0.05 0.05
printing an ep nov before normalisation:  24.35364506163387
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  42.503152594325684
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using another actor
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
siam score:  -0.912527
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
from probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  51.78671542503048
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.004]
 [0.009]
 [0.016]
 [0.012]
 [0.009]] [[33.596]
 [36.642]
 [28.496]
 [29.977]
 [30.87 ]] [[0.004]
 [0.009]
 [0.016]
 [0.012]
 [0.009]]
printing an ep nov before normalisation:  32.458339473176345
printing an ep nov before normalisation:  63.20082428306011
printing an ep nov before normalisation:  38.50813350549815
printing an ep nov before normalisation:  37.50288285988921
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
printing an ep nov before normalisation:  33.702702654522625
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  28.48321813481467
printing an ep nov before normalisation:  18.556628227233887
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  26.715967749592142
printing an ep nov before normalisation:  23.364841785884195
printing an ep nov before normalisation:  27.316489459928498
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0061 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
actor:  0 policy actor:  0  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  30.088606488575426
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.29 ]
 [0.352]
 [0.369]
 [0.361]] [[30.011]
 [36.862]
 [23.594]
 [24.159]
 [27.255]] [[0.371]
 [0.29 ]
 [0.352]
 [0.369]
 [0.361]]
printing an ep nov before normalisation:  26.201068154671827
actions average: 
K:  1  action  0 :  tensor([    0.9985,     0.0002,     0.0000,     0.0011,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9707,     0.0001,     0.0015,     0.0275],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0095,     0.9374,     0.0109,     0.0420],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0020,     0.0150,     0.9057,     0.0773],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0054,     0.0776,     0.0005,     0.1908,     0.7257],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  28.655716395619088
maxi score, test score, baseline:  0.0081 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
printing an ep nov before normalisation:  24.39378069290985
using another actor
actor:  0 policy actor:  0  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.087634754408104
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
maxi score, test score, baseline:  0.0101 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
actor:  0 policy actor:  0  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.315]
 [0.32 ]
 [0.307]
 [0.327]
 [0.332]] [[33.546]
 [29.679]
 [31.018]
 [27.962]
 [28.318]] [[0.648]
 [0.585]
 [0.596]
 [0.563]
 [0.573]]
maxi score, test score, baseline:  0.0121 0.05 0.05
probs:  [0.0667164147588074, 0.13334991603071358, 0.0667164147588074, 0.13334991603071358, 0.13334991603071358, 0.46651742239024446]
Printing some Q and Qe and total Qs values:  [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]] [[26.539]
 [26.539]
 [26.539]
 [26.539]
 [26.539]] [[0.418]
 [0.418]
 [0.418]
 [0.418]
 [0.418]]
actor:  0 policy actor:  0  step number:  115 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.06671714117703892, 0.13335015817012408, 0.06671714117703892, 0.13335015817012408, 0.13335015817012408, 0.4665152431355498]
Printing some Q and Qe and total Qs values:  [[0.371]
 [0.271]
 [0.271]
 [0.305]
 [0.271]] [[29.134]
 [19.055]
 [19.055]
 [29.556]
 [19.055]] [[0.571]
 [0.37 ]
 [0.37 ]
 [0.51 ]
 [0.37 ]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.06671714117703892, 0.13335015817012408, 0.06671714117703892, 0.13335015817012408, 0.13335015817012408, 0.4665152431355498]
printing an ep nov before normalisation:  45.569680667152
Printing some Q and Qe and total Qs values:  [[0.345]
 [0.337]
 [0.314]
 [0.348]
 [0.408]] [[40.838]
 [39.426]
 [35.306]
 [38.596]
 [36.91 ]] [[1.197]
 [1.124]
 [0.911]
 [1.097]
 [1.079]]
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.019]
 [0.026]
 [0.021]
 [0.022]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.017]
 [0.019]
 [0.026]
 [0.021]
 [0.022]]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.04 ]
 [0.043]
 [0.047]
 [0.046]] [[44.543]
 [43.506]
 [40.58 ]
 [39.048]
 [42.154]] [[1.076]
 [1.073]
 [0.953]
 [0.894]
 [1.022]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.06671714117703892, 0.13335015817012408, 0.06671714117703892, 0.13335015817012408, 0.13335015817012408, 0.4665152431355498]
printing an ep nov before normalisation:  53.42158721942017
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.06671714117703892, 0.13335015817012408, 0.06671714117703892, 0.13335015817012408, 0.13335015817012408, 0.4665152431355498]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.06671714117703892, 0.13335015817012408, 0.06671714117703892, 0.13335015817012408, 0.13335015817012408, 0.4665152431355498]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
using another actor
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  24.583624601184347
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.023]
 [0.016]
 [0.02 ]
 [0.018]] [[25.496]
 [32.476]
 [26.284]
 [26.609]
 [21.608]] [[0.017]
 [0.023]
 [0.016]
 [0.02 ]
 [0.018]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  13.324013110586463
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.298890590667725
Printing some Q and Qe and total Qs values:  [[0.413]
 [0.389]
 [0.394]
 [0.413]
 [0.362]] [[25.774]
 [29.018]
 [26.683]
 [30.552]
 [23.326]] [[0.785]
 [0.852]
 [0.791]
 [0.919]
 [0.665]]
printing an ep nov before normalisation:  44.610685855416456
using explorer policy with actor:  1
siam score:  -0.92030394
actions average: 
K:  0  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0026, 0.8368, 0.0687, 0.0023, 0.0896], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0163, 0.0092, 0.9535, 0.0010, 0.0200], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0002,     0.0001,     0.9568,     0.0421],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0073, 0.1400, 0.1036, 0.1134, 0.6357], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
actions average: 
K:  4  action  0 :  tensor([    0.9962,     0.0002,     0.0001,     0.0005,     0.0029],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0174,     0.9430,     0.0029,     0.0007,     0.0360],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0003,     0.0300,     0.9000,     0.0141,     0.0556],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0002,     0.0078,     0.9196,     0.0719],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0054, 0.0666, 0.0651, 0.1067, 0.7562], grad_fn=<DivBackward0>)
from probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
siam score:  -0.92230195
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.059]
 [0.046]
 [0.046]] [[34.546]
 [34.546]
 [45.972]
 [34.546]
 [34.546]] [[0.656]
 [0.656]
 [1.112]
 [0.656]
 [0.656]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  52.43141174316406
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
siam score:  -0.92537856
maxi score, test score, baseline:  0.0141 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.046]
 [0.046]
 [0.042]
 [0.046]] [[37.813]
 [37.813]
 [37.813]
 [42.538]
 [37.813]] [[1.502]
 [1.502]
 [1.502]
 [1.844]
 [1.502]]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.035]
 [0.041]
 [0.039]
 [0.04 ]] [[60.341]
 [51.527]
 [55.022]
 [56.947]
 [56.53 ]] [[1.663]
 [1.319]
 [1.462]
 [1.535]
 [1.52 ]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
siam score:  -0.92736524
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  49.129077054088405
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  69.78095903025218
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  33.971171379089355
maxi score, test score, baseline:  0.0141 0.2 0.2
siam score:  -0.9249617
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
line 256 mcts: sample exp_bonus 41.75121940809429
using explorer policy with actor:  1
siam score:  -0.9249994
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
siam score:  -0.9258014
using explorer policy with actor:  1
from probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  51.62630385562871
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.056]
 [0.056]
 [0.05 ]
 [0.056]] [[28.772]
 [45.218]
 [45.218]
 [24.568]
 [45.218]] [[2.051]
 [3.979]
 [3.979]
 [1.559]
 [3.979]]
printing an ep nov before normalisation:  56.02424847938663
siam score:  -0.9242154
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  24.146158695220947
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.042]
 [0.037]
 [0.037]
 [0.037]] [[48.482]
 [36.619]
 [36.485]
 [36.485]
 [36.485]] [[1.476]
 [1.002]
 [0.992]
 [0.992]
 [0.992]]
maxi score, test score, baseline:  0.0141 0.2 0.2
actions average: 
K:  2  action  0 :  tensor([    0.9989,     0.0002,     0.0000,     0.0003,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9141,     0.0140,     0.0004,     0.0712],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0007,     0.0003,     0.9729,     0.0128,     0.0133],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0005,     0.0000,     0.9983,     0.0012],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0117, 0.2076, 0.0026, 0.1056, 0.6726], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  41.47958755493164
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  12.519350341387394
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  31.95517703422116
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  98.8202829108073
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  89.60569457961324
from probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.046]
 [0.05 ]
 [0.05 ]
 [0.05 ]
 [0.05 ]] [[82.086]
 [58.369]
 [58.369]
 [58.369]
 [58.369]] [[0.964]
 [0.643]
 [0.643]
 [0.643]
 [0.643]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
Printing some Q and Qe and total Qs values:  [[0.   ]
 [0.055]
 [0.056]
 [0.054]
 [0.056]] [[44.769]
 [43.645]
 [34.05 ]
 [45.198]
 [44.817]] [[0.882]
 [0.893]
 [0.515]
 [0.953]
 [0.94 ]]
printing an ep nov before normalisation:  39.26727039127909
printing an ep nov before normalisation:  56.81888572085768
printing an ep nov before normalisation:  42.89877414703369
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.0625544962142212, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.12502179848568848, 0.4373583098430249]
printing an ep nov before normalisation:  35.84717512948325
maxi score, test score, baseline:  0.0141 0.2 0.2
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
siam score:  -0.93111205
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
printing an ep nov before normalisation:  32.86767129842919
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0001,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9990,     0.0001,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9701,     0.0004,     0.0293],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0001,     0.0005,     0.9042,     0.0949],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0102, 0.0676, 0.0368, 0.0607, 0.8247], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.056]
 [0.064]
 [0.071]
 [0.063]
 [0.059]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.056]
 [0.064]
 [0.071]
 [0.063]
 [0.059]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
siam score:  -0.9349469
printing an ep nov before normalisation:  71.8327283859253
printing an ep nov before normalisation:  79.83536534424898
line 256 mcts: sample exp_bonus 50.04937980926536
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
printing an ep nov before normalisation:  37.52991296206023
actions average: 
K:  4  action  0 :  tensor([    0.9988,     0.0000,     0.0000,     0.0002,     0.0010],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9610,     0.0063,     0.0002,     0.0318],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0136,     0.9196,     0.0021,     0.0646],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0012,     0.0001,     0.0172,     0.9241,     0.0573],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0065, 0.1360, 0.0192, 0.1240, 0.7144], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058881701591445006, 0.1176735007233841, 0.17646529985532317, 0.1176735007233841, 0.1176735007233841, 0.41163249638307947]
Printing some Q and Qe and total Qs values:  [[1.315]
 [1.315]
 [1.315]
 [1.503]
 [1.5  ]] [[34.076]
 [34.076]
 [34.076]
 [34.198]
 [17.346]] [[3.27 ]
 [3.27 ]
 [3.27 ]
 [3.466]
 [2.494]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.3264856338501
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  34.23487643550413
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.3647],
        [0.0298],
        [0.4407],
        [0.4535],
        [0.0375],
        [0.3663],
        [0.0797],
        [0.4818],
        [0.0000],
        [0.0000]], dtype=torch.float64)
0.0 0.3647496279026536
0.0 0.029768806305323112
0.0 0.4406838534790993
0.0 0.4534898063876878
0.0 0.037460079638446726
0.0 0.3663450416518461
0.0 0.07974505155269124
0.0 0.48180283960516346
0.0 0.0
0.0 0.0
actions average: 
K:  0  action  0 :  tensor([    0.9942,     0.0012,     0.0000,     0.0019,     0.0026],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0000,     0.9859,     0.0001,     0.0001,     0.0139],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0138,     0.9333,     0.0001,     0.0527],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0037,     0.9101,     0.0859],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.1298,     0.0566,     0.0907,     0.7223],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  35.44133186340332
printing an ep nov before normalisation:  20.003460709926564
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  85.823343753934
printing an ep nov before normalisation:  35.245125553423655
printing an ep nov before normalisation:  24.250202178955078
printing an ep nov before normalisation:  22.864937039610147
Printing some Q and Qe and total Qs values:  [[0.087]
 [0.102]
 [0.087]
 [0.087]
 [0.087]] [[64.47 ]
 [67.811]
 [64.47 ]
 [64.47 ]
 [64.47 ]] [[1.71]
 [1.86]
 [1.71]
 [1.71]
 [1.71]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
Printing some Q and Qe and total Qs values:  [[0.529]
 [0.53 ]
 [0.376]
 [0.547]
 [0.544]] [[34.248]
 [34.267]
 [33.355]
 [31.596]
 [32.97 ]] [[1.879]
 [1.881]
 [1.661]
 [1.703]
 [1.8  ]]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.11 ]
 [0.08 ]
 [0.08 ]
 [0.08 ]] [[44.579]
 [36.911]
 [37.339]
 [37.339]
 [37.339]] [[1.203]
 [0.909]
 [0.898]
 [0.898]
 [0.898]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.51677125286126
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  48.091205844548796
printing an ep nov before normalisation:  42.100120773697256
printing an ep nov before normalisation:  41.743846155807674
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.664]
 [0.664]
 [0.664]
 [0.664]] [[44.568]
 [44.568]
 [44.568]
 [44.568]
 [44.568]] [[2.074]
 [2.074]
 [2.074]
 [2.074]
 [2.074]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  80.03100303754496
Printing some Q and Qe and total Qs values:  [[0.086]
 [0.086]
 [0.082]
 [0.086]
 [0.086]] [[50.349]
 [50.349]
 [68.199]
 [50.349]
 [50.349]] [[0.791]
 [0.791]
 [1.124]
 [0.791]
 [0.791]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.49362512246029
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  56.738827509792756
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
from probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0002,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9776,     0.0001,     0.0001,     0.0218],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0013, 0.0215, 0.8815, 0.0118, 0.0840], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0002,     0.0014,     0.0005,     0.9538,     0.0441],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0022, 0.0067, 0.1470, 0.1779, 0.6661], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  42.13729584366512
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
siam score:  -0.94885206
Printing some Q and Qe and total Qs values:  [[0.076]
 [0.166]
 [0.092]
 [0.094]
 [0.116]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.076]
 [0.166]
 [0.092]
 [0.094]
 [0.116]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  35.4405462805897
siam score:  -0.9501288
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
printing an ep nov before normalisation:  27.540676593780518
Printing some Q and Qe and total Qs values:  [[0.763]
 [0.763]
 [0.74 ]
 [0.736]
 [0.763]] [[29.44 ]
 [29.44 ]
 [36.421]
 [35.568]
 [29.44 ]] [[1.333]
 [1.333]
 [1.521]
 [1.492]
 [1.333]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
actions average: 
K:  4  action  0 :  tensor([    0.9959,     0.0038,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9200,     0.0145,     0.0001,     0.0646],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([0.0097, 0.0491, 0.8748, 0.0139, 0.0526], grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0393,     0.0008,     0.0790,     0.8194,     0.0616],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0283, 0.0906, 0.0729, 0.1435, 0.6647], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.061458742931890575, 0.11406270479927863, 0.16666666666666669, 0.11406270479927863, 0.11406270479927863, 0.4296864760036068]
maxi score, test score, baseline:  0.0141 0.2 0.2
actor:  1 policy actor:  1  step number:  108 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.69423114351471
printing an ep nov before normalisation:  22.783334255218506
printing an ep nov before normalisation:  34.15566663138215
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.099]
 [0.108]
 [0.077]
 [0.081]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.099]
 [0.108]
 [0.077]
 [0.081]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
printing an ep nov before normalisation:  40.06978305688398
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0377],
        [0.1589],
        [0.0351],
        [0.0256],
        [0.0000],
        [0.3609],
        [0.3506],
        [0.4453],
        [0.8920],
        [0.0684]], dtype=torch.float64)
0.0 0.03770642816341825
0.0 0.15891000423672
0.0 0.03512368458308656
0.0 0.025568519436540475
0.0 0.0
0.0 0.3609468443831837
0.0 0.3506412404864579
0.0 0.4453309086299802
0.0 0.8920156188069525
0.0 0.0684096934193968
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.594425594384575
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
using explorer policy with actor:  0
printing an ep nov before normalisation:  53.72093141179335
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
printing an ep nov before normalisation:  60.330983922601774
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  36.2457366410207
line 256 mcts: sample exp_bonus 27.064756237097843
siam score:  -0.9459972
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Printing some Q and Qe and total Qs values:  [[0.836]
 [0.836]
 [1.123]
 [0.836]
 [0.836]] [[16.4  ]
 [16.4  ]
 [19.351]
 [16.4  ]
 [16.4  ]] [[1.067]
 [1.067]
 [1.433]
 [1.067]
 [1.067]]
from probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
printing an ep nov before normalisation:  25.78806377317382
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  26.775121537693416
printing an ep nov before normalisation:  30.9699739420095
actions average: 
K:  4  action  0 :  tensor([    0.9985,     0.0001,     0.0001,     0.0001,     0.0012],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9346,     0.0239,     0.0007,     0.0407],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9956,     0.0001,     0.0042],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0009,     0.0002,     0.0216,     0.8224,     0.1549],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.1096, 0.1229, 0.0734, 0.6930], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Printing some Q and Qe and total Qs values:  [[0.073]
 [0.091]
 [0.   ]
 [0.066]
 [0.08 ]] [[68.917]
 [56.163]
 [57.13 ]
 [59.219]
 [62.231]] [[1.406]
 [1.177]
 [1.105]
 [1.211]
 [1.284]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
siam score:  -0.9464082
printing an ep nov before normalisation:  40.62138132512651
Printing some Q and Qe and total Qs values:  [[0.638]
 [0.642]
 [0.656]
 [0.683]
 [0.632]] [[39.818]
 [39.11 ]
 [35.867]
 [34.244]
 [40.172]] [[1.704]
 [1.672]
 [1.523]
 [1.467]
 [1.716]]
printing an ep nov before normalisation:  51.02585079240089
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
printing an ep nov before normalisation:  65.18169279999118
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.25819835381537
siam score:  -0.94897676
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
printing an ep nov before normalisation:  29.3189166279617
printing an ep nov before normalisation:  7.041254432410398
siam score:  -0.95013994
maxi score, test score, baseline:  0.0141 0.2 0.2
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
UNIT TEST: sample policy line 217 mcts : [0.051 0.692 0.    0.154 0.103]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
Printing some Q and Qe and total Qs values:  [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.117]
 [0.117]
 [0.117]
 [0.117]
 [0.117]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 29.834058166935126
printing an ep nov before normalisation:  33.21215686126468
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.058391666489678674, 0.15833782049920606, 0.15833782049920606, 0.10836474349444236, 0.10836474349444236, 0.40820320552302447]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  48.44829087294369
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  54.4484666966238
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  30.31098278919137
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.055616765203623866, 0.15080239502908913, 0.19839520994182178, 0.10320958011635649, 0.10320958011635649, 0.3887664695927522]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.055616765203623866, 0.15080239502908913, 0.19839520994182178, 0.10320958011635649, 0.10320958011635649, 0.3887664695927522]
printing an ep nov before normalisation:  54.04929070798001
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.055616765203623866, 0.15080239502908913, 0.19839520994182178, 0.10320958011635649, 0.10320958011635649, 0.3887664695927522]
printing an ep nov before normalisation:  25.513869296815436
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.677]
 [0.618]] [[26.278]
 [26.278]
 [26.278]
 [32.387]
 [26.278]] [[1.546]
 [1.546]
 [1.546]
 [1.939]
 [1.546]]
Printing some Q and Qe and total Qs values:  [[0.572]
 [0.572]
 [0.28 ]
 [0.732]
 [0.572]] [[25.867]
 [25.867]
 [21.489]
 [32.634]
 [25.867]] [[1.026]
 [1.026]
 [0.589]
 [1.412]
 [1.026]]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.055616765203623866, 0.15080239502908913, 0.19839520994182178, 0.10320958011635649, 0.10320958011635649, 0.3887664695927522]
siam score:  -0.9525699
Printing some Q and Qe and total Qs values:  [[0.103]
 [0.103]
 [0.103]
 [0.094]
 [0.103]] [[30.15 ]
 [30.15 ]
 [30.15 ]
 [34.969]
 [30.15 ]] [[1.25 ]
 [1.25 ]
 [1.25 ]
 [1.627]
 [1.25 ]]
printing an ep nov before normalisation:  64.36436840968928
actions average: 
K:  4  action  0 :  tensor([    0.9919,     0.0059,     0.0001,     0.0002,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.8677,     0.0727,     0.0005,     0.0589],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9631,     0.0006,     0.0361],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0006,     0.0007,     0.0001,     0.9770,     0.0216],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0012, 0.0649, 0.0619, 0.1201, 0.7518], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  46.13733248644962
Printing some Q and Qe and total Qs values:  [[0.145]
 [0.001]
 [0.114]
 [0.116]
 [0.121]] [[50.55 ]
 [50.153]
 [57.134]
 [54.642]
 [51.196]] [[1.453]
 [1.287]
 [1.798]
 [1.658]
 [1.467]]
printing an ep nov before normalisation:  31.701579093933105
printing an ep nov before normalisation:  37.418904304504395
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.055616765203623866, 0.15080239502908913, 0.19839520994182178, 0.10320958011635649, 0.10320958011635649, 0.3887664695927522]
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.8650883436203
printing an ep nov before normalisation:  36.31494345996455
maxi score, test score, baseline:  0.0141 0.2 0.2
siam score:  -0.950115
printing an ep nov before normalisation:  39.567359987701
printing an ep nov before normalisation:  32.32929312456818
actions average: 
K:  1  action  0 :  tensor([    0.9941,     0.0017,     0.0000,     0.0023,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0005,     0.9605,     0.0008,     0.0002,     0.0379],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0171,     0.9484,     0.0024,     0.0321],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0004,     0.0338,     0.8483,     0.1173],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0010, 0.1647, 0.0984, 0.1344, 0.6016], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.8746338637052
siam score:  -0.94855464
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
printing an ep nov before normalisation:  41.5542745960666
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  50.86996250840999
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
printing an ep nov before normalisation:  42.29610420285177
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
printing an ep nov before normalisation:  24.607609144563895
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
printing an ep nov before normalisation:  43.61371985044273
printing an ep nov before normalisation:  1.3157194982724718
from probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
from probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
actions average: 
K:  1  action  0 :  tensor([    0.9966,     0.0005,     0.0000,     0.0009,     0.0020],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9202,     0.0242,     0.0001,     0.0548],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9494,     0.0239,     0.0267],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0003,     0.0353,     0.8412,     0.1231],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0035, 0.1306, 0.1131, 0.1076, 0.6451], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.053094187549806264, 0.1439521708432946, 0.23481015413678297, 0.09852317919655042, 0.09852317919655042, 0.37109712907701536]
maxi score, test score, baseline:  0.0141 0.2 0.2
printing an ep nov before normalisation:  27.37398386001587
Printing some Q and Qe and total Qs values:  [[0.962]
 [0.962]
 [0.962]
 [0.962]
 [0.962]] [[25.959]
 [25.959]
 [25.959]
 [25.959]
 [25.959]] [[52.88]
 [52.88]
 [52.88]
 [52.88]
 [52.88]]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  20.125449385752145
printing an ep nov before normalisation:  42.90801067814309
printing an ep nov before normalisation:  25.46721213941215
printing an ep nov before normalisation:  52.61833462386792
siam score:  -0.9562526
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
Printing some Q and Qe and total Qs values:  [[0.958]
 [1.01 ]
 [0.958]
 [0.958]
 [1.067]] [[42.039]
 [34.35 ]
 [42.039]
 [42.039]
 [41.404]] [[1.567]
 [1.441]
 [1.567]
 [1.567]
 [1.661]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
printing an ep nov before normalisation:  35.20695894557211
printing an ep nov before normalisation:  40.26886887086174
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.1571704433589
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.812]
 [0.909]
 [0.909]
 [0.909]
 [0.909]] [[47.154]
 [30.926]
 [30.926]
 [30.926]
 [30.926]] [[1.598]
 [1.312]
 [1.312]
 [1.312]
 [1.312]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
printing an ep nov before normalisation:  65.49931085530217
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
printing an ep nov before normalisation:  32.707692340710494
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
line 256 mcts: sample exp_bonus 27.78803887745395
printing an ep nov before normalisation:  31.045614291691372
printing an ep nov before normalisation:  68.74516495610824
actions average: 
K:  0  action  0 :  tensor([    0.9799,     0.0005,     0.0000,     0.0015,     0.0180],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9502,     0.0139,     0.0004,     0.0354],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0145,     0.9133,     0.0005,     0.0716],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0003,     0.0265,     0.8497,     0.1233],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0007,     0.0658,     0.0522,     0.0618,     0.8196],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
line 256 mcts: sample exp_bonus 60.03980642384906
printing an ep nov before normalisation:  40.774039250446386
Printing some Q and Qe and total Qs values:  [[0.091]
 [0.098]
 [0.091]
 [0.091]
 [0.091]] [[81.231]
 [74.261]
 [81.231]
 [81.231]
 [81.231]] [[1.94 ]
 [1.755]
 [1.94 ]
 [1.94 ]
 [1.94 ]]
using explorer policy with actor:  1
actions average: 
K:  4  action  0 :  tensor([    0.9987,     0.0000,     0.0001,     0.0002,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0068,     0.9210,     0.0049,     0.0004,     0.0669],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0027,     0.9720,     0.0125,     0.0127],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0004,     0.0003,     0.8884,     0.1104],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0306, 0.1267, 0.0341, 0.8076], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  37.60273497922611
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05561651746038025, 0.13890412936509505, 0.22219174126980987, 0.09726032341273765, 0.09726032341273765, 0.38876696507923947]
actions average: 
K:  0  action  0 :  tensor([    0.9820,     0.0028,     0.0004,     0.0025,     0.0123],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9129,     0.0175,     0.0003,     0.0693],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9513,     0.0001,     0.0486],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0023,     0.0135,     0.8756,     0.1086],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0018, 0.1033, 0.0550, 0.2161, 0.6238], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  34.791914489786606
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actions average: 
K:  3  action  0 :  tensor([    0.9958,     0.0019,     0.0004,     0.0006,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9333,     0.0383,     0.0003,     0.0280],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0000,     0.9997,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0001,     0.0004,     0.0135,     0.9044,     0.0816],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0017, 0.0484, 0.0969, 0.1362, 0.7168], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
printing an ep nov before normalisation:  35.48375329087306
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.083]
 [0.097]
 [0.083]] [[38.599]
 [38.599]
 [38.599]
 [42.034]
 [38.599]] [[1.124]
 [1.124]
 [1.124]
 [1.305]
 [1.124]]
from probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
printing an ep nov before normalisation:  24.605659509649534
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05339665127193898, 0.13335195625645263, 0.21330726124096633, 0.13335195625645263, 0.0933743037641958, 0.3732178712099936]
Printing some Q and Qe and total Qs values:  [[1.068]
 [1.204]
 [1.068]
 [1.068]
 [1.068]] [[38.893]
 [37.102]
 [38.893]
 [38.893]
 [38.893]] [[2.258]
 [2.3  ]
 [2.258]
 [2.258]
 [2.258]]
maxi score, test score, baseline:  0.0141 0.2 0.2
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.231]
 [0.226]
 [0.234]
 [0.226]] [[57.784]
 [55.884]
 [53.132]
 [59.078]
 [56.566]] [[1.176]
 [1.338]
 [1.234]
 [1.456]
 [1.357]]
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.212]
 [0.239]
 [0.126]
 [0.229]] [[39.181]
 [39.071]
 [43.738]
 [40.707]
 [38.953]] [[0.676]
 [0.877]
 [1.063]
 [0.847]
 [0.89 ]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
from probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  27.6921811029849
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
actions average: 
K:  2  action  0 :  tensor([    0.9890,     0.0001,     0.0001,     0.0005,     0.0104],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0017,     0.8931,     0.0232,     0.0007,     0.0813],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0115,     0.9635,     0.0009,     0.0240],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0007,     0.0001,     0.0009,     0.8288,     0.1695],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0025, 0.0663, 0.0673, 0.1973, 0.6665], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  72.028898606472
actions average: 
K:  4  action  0 :  tensor([    0.9991,     0.0004,     0.0000,     0.0002,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0004,     0.9326,     0.0010,     0.0003,     0.0658],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0003,     0.9490,     0.0149,     0.0358],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0007,     0.0163,     0.8909,     0.0916],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0277, 0.1985, 0.0570, 0.1011, 0.6156], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.90204811548966
siam score:  -0.94964355
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
siam score:  -0.95012695
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.09 ]
 [0.078]
 [0.077]] [[34.022]
 [31.252]
 [28.457]
 [30.297]
 [29.242]] [[1.159]
 [1.015]
 [0.885]
 [0.969]
 [0.912]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  35.78505516052246
printing an ep nov before normalisation:  35.74375691058736
printing an ep nov before normalisation:  42.51669918282793
from probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
siam score:  -0.9503989
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  41.193720235892215
printing an ep nov before normalisation:  42.315569190350644
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  68.5837444220819
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.45026199476056
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.084]
 [0.069]
 [0.075]
 [0.072]] [[26.358]
 [42.008]
 [42.778]
 [39.735]
 [40.369]] [[0.28 ]
 [0.821]
 [0.832]
 [0.734]
 [0.753]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.73284812876539
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.076]
 [0.071]
 [0.071]
 [0.071]] [[82.17 ]
 [74.906]
 [90.168]
 [90.168]
 [90.168]] [[1.686]
 [1.485]
 [1.93 ]
 [1.93 ]
 [1.93 ]]
printing an ep nov before normalisation:  66.5911678026854
printing an ep nov before normalisation:  75.78033194324615
actions average: 
K:  0  action  0 :  tensor([    0.9943,     0.0001,     0.0050,     0.0001,     0.0006],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0054,     0.9217,     0.0128,     0.0006,     0.0594],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0141,     0.9359,     0.0158,     0.0342],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0002,     0.0221,     0.9618,     0.0158],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0021, 0.0611, 0.0895, 0.2072, 0.6401], grad_fn=<DivBackward0>)
using explorer policy with actor:  0
Printing some Q and Qe and total Qs values:  [[0.085]
 [0.156]
 [0.089]
 [0.092]
 [0.112]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.085]
 [0.156]
 [0.089]
 [0.092]
 [0.112]]
from probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
siam score:  -0.9418679
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
using explorer policy with actor:  1
actions average: 
K:  2  action  0 :  tensor([    0.9673,     0.0016,     0.0007,     0.0084,     0.0220],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9342,     0.0006,     0.0002,     0.0650],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9994,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0007,     0.0397,     0.8196,     0.1397],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0007,     0.0819,     0.1105,     0.1160,     0.6909],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.067]
 [0.094]
 [0.008]
 [0.071]
 [0.073]] [[34.302]
 [27.166]
 [36.23 ]
 [48.245]
 [37.95 ]] [[0.308]
 [0.232]
 [0.278]
 [0.516]
 [0.368]]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  76.07678674657306
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.146]
 [0.082]
 [0.082]
 [0.086]] [[73.337]
 [66.098]
 [80.407]
 [84.624]
 [78.535]] [[0.827]
 [0.788]
 [0.924]
 [0.984]
 [0.902]]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  46.52403443503363
Printing some Q and Qe and total Qs values:  [[0.088]
 [0.088]
 [0.089]
 [0.087]
 [0.08 ]] [[50.672]
 [50.672]
 [50.331]
 [49.463]
 [50.426]] [[1.602]
 [1.602]
 [1.584]
 [1.535]
 [1.58 ]]
printing an ep nov before normalisation:  38.06839909963993
printing an ep nov before normalisation:  46.23432897122423
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
siam score:  -0.93918395
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
printing an ep nov before normalisation:  52.542505583301676
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.05134758356869638, 0.12822697230067656, 0.20510636103265678, 0.16666666666666669, 0.08978727793468647, 0.35886513849661716]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
printing an ep nov before normalisation:  63.14199200804065
printing an ep nov before normalisation:  60.298735324717256
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
printing an ep nov before normalisation:  89.67028442762698
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.662833254359782
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
from probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0141 0.2 0.2
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0141 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
Printing some Q and Qe and total Qs values:  [[0.647]
 [0.775]
 [0.058]
 [0.647]
 [0.754]] [[27.56 ]
 [36.75 ]
 [34.826]
 [27.56 ]
 [33.423]] [[0.647]
 [0.775]
 [0.058]
 [0.647]
 [0.754]]
printing an ep nov before normalisation:  45.4203423748298
maxi score, test score, baseline:  0.0141 0.2 0.2
actor:  0 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  4  action  0 :  tensor([    0.9963,     0.0025,     0.0002,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0009,     0.9124,     0.0090,     0.0006,     0.0772],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0010,     0.0177,     0.9442,     0.0001,     0.0369],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0019,     0.0013,     0.0005,     0.8944,     0.1019],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0417, 0.0795, 0.1221, 0.1234, 0.6333], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
printing an ep nov before normalisation:  58.54916400539072
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
Printing some Q and Qe and total Qs values:  [[0.065]
 [0.066]
 [0.059]
 [0.069]
 [0.065]] [[59.465]
 [62.964]
 [48.919]
 [56.618]
 [54.451]] [[0.891]
 [0.953]
 [0.702]
 [0.846]
 [0.804]]
printing an ep nov before normalisation:  64.09052658356327
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04945033250880896, 0.12348170145061384, 0.23452875486332114, 0.16049738592151624, 0.08646601697971139, 0.34557580827602846]
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.93520635
Printing some Q and Qe and total Qs values:  [[0.155]
 [0.145]
 [0.155]
 [0.155]
 [0.155]] [[56.785]
 [53.074]
 [56.785]
 [56.785]
 [56.785]] [[1.763]
 [1.595]
 [1.763]
 [1.763]
 [1.763]]
printing an ep nov before normalisation:  58.58201995288562
line 256 mcts: sample exp_bonus 41.26248162508406
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.047688628525282085, 0.11907545141011283, 0.22615568573735897, 0.19046227429494358, 0.08338203996769745, 0.333235920064605]
line 256 mcts: sample exp_bonus 45.00648281632887
line 256 mcts: sample exp_bonus 38.344813181628616
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
siam score:  -0.93349355
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
printing an ep nov before normalisation:  29.366335843585848
from probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
printing an ep nov before normalisation:  40.14542201981635
maxi score, test score, baseline:  0.0161 0.2 0.2
from probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
actions average: 
K:  1  action  0 :  tensor([    0.9939,     0.0000,     0.0000,     0.0039,     0.0021],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9730,     0.0002,     0.0002,     0.0265],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9599,     0.0241,     0.0160],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0140,     0.9373,     0.0484],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0012, 0.0611, 0.0235, 0.1083, 0.8060], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
printing an ep nov before normalisation:  53.27076659414747
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.65863099380606
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
printing an ep nov before normalisation:  58.52986060242648
using explorer policy with actor:  1
siam score:  -0.9361119
printing an ep nov before normalisation:  47.20298350977832
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
line 256 mcts: sample exp_bonus 42.89068192170425
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
Printing some Q and Qe and total Qs values:  [[0.002]
 [0.049]
 [0.   ]
 [0.054]
 [0.05 ]] [[40.079]
 [47.358]
 [40.091]
 [36.827]
 [45.078]] [[0.382]
 [0.6  ]
 [0.381]
 [0.358]
 [0.548]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04604844659527791, 0.11497314377892864, 0.25282253814623, 0.18389784096257938, 0.08051079518710326, 0.3217472353298808]
printing an ep nov before normalisation:  38.090411835482705
printing an ep nov before normalisation:  28.679975947934636
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  60.993041785504126
Printing some Q and Qe and total Qs values:  [[0.139]
 [0.106]
 [0.117]
 [0.101]
 [0.106]] [[42.175]
 [47.818]
 [51.552]
 [51.959]
 [47.818]] [[0.833]
 [0.996]
 [1.135]
 [1.134]
 [0.996]]
maxi score, test score, baseline:  0.0161 0.2 0.2
printing an ep nov before normalisation:  36.93955051762621
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.033]
 [0.033]
 [0.042]
 [0.038]
 [0.037]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.033]
 [0.033]
 [0.042]
 [0.038]
 [0.037]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  53.712787887290084
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  50.095348788938956
printing an ep nov before normalisation:  35.7540850892803
maxi score, test score, baseline:  0.0161 0.2 0.2
using another actor
printing an ep nov before normalisation:  51.65217184231289
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  25.59513676249839
using explorer policy with actor:  1
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  48.58695267619422
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
siam score:  -0.92900085
Printing some Q and Qe and total Qs values:  [[0.052]
 [0.056]
 [0.088]
 [0.073]
 [0.054]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.052]
 [0.056]
 [0.088]
 [0.073]
 [0.054]]
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  45.2411908808971
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.283]
 [0.16 ]
 [0.224]
 [0.283]
 [0.258]] [[65.892]
 [69.75 ]
 [50.805]
 [65.892]
 [67.088]] [[1.081]
 [1.004]
 [0.839]
 [1.081]
 [1.07 ]]
maxi score, test score, baseline:  0.0161 0.2 0.2
printing an ep nov before normalisation:  45.34853260137896
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  35.346726585357445
Printing some Q and Qe and total Qs values:  [[0.075]
 [0.075]
 [0.075]
 [0.075]
 [0.075]] [[28.378]
 [28.378]
 [28.378]
 [28.378]
 [28.378]] [[0.5]
 [0.5]
 [0.5]
 [0.5]
 [0.5]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
printing an ep nov before normalisation:  30.72523880214711
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.001]
 [0.002]
 [0.001]
 [0.001]] [[46.204]
 [ 1.796]
 [ 2.042]
 [46.204]
 [ 1.856]] [[1.668]
 [0.036]
 [0.047]
 [1.668]
 [0.039]]
Printing some Q and Qe and total Qs values:  [[0.051]
 [0.073]
 [0.063]
 [0.067]
 [0.063]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.051]
 [0.073]
 [0.063]
 [0.067]
 [0.063]]
from probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
printing an ep nov before normalisation:  58.020898486019874
siam score:  -0.9264599
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.044517632074337676, 0.1444577512862432, 0.2443978704981487, 0.1777711243568784, 0.07783100514497286, 0.3110246166394191]
maxi score, test score, baseline:  0.0161 0.2 0.2
printing an ep nov before normalisation:  37.73822888390011
siam score:  -0.9260561
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  50.76064654335841
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
printing an ep nov before normalisation:  46.94340102057246
printing an ep nov before normalisation:  27.36587675702887
siam score:  -0.9247366
printing an ep nov before normalisation:  37.85446038499402
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.231]
 [1.186]
 [1.326]
 [1.312]] [[25.69 ]
 [30.757]
 [28.964]
 [23.809]
 [26.166]] [[1.72 ]
 [1.671]
 [1.601]
 [1.667]
 [1.686]]
printing an ep nov before normalisation:  55.902802593147655
Printing some Q and Qe and total Qs values:  [[0.08 ]
 [0.09 ]
 [0.107]
 [0.095]
 [0.096]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.08 ]
 [0.09 ]
 [0.107]
 [0.095]
 [0.096]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
printing an ep nov before normalisation:  44.53442733897619
printing an ep nov before normalisation:  24.458166231521545
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
printing an ep nov before normalisation:  42.42047136681173
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
printing an ep nov before normalisation:  33.322162222068044
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04308559896385333, 0.13980121716605506, 0.23651683536825677, 0.2042782959675229, 0.07532413836458725, 0.3009939141697246]
actor:  1 policy actor:  1  step number:  98 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
line 256 mcts: sample exp_bonus 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
Printing some Q and Qe and total Qs values:  [[0.06 ]
 [0.071]
 [0.057]
 [0.06 ]
 [0.067]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.06 ]
 [0.071]
 [0.057]
 [0.06 ]
 [0.067]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
Printing some Q and Qe and total Qs values:  [[0.095]
 [0.077]
 [0.076]
 [0.095]
 [0.095]] [[48.722]
 [59.869]
 [63.729]
 [48.722]
 [48.722]] [[0.94 ]
 [1.234]
 [1.342]
 [0.94 ]
 [0.94 ]]
Printing some Q and Qe and total Qs values:  [[0.083]
 [0.083]
 [0.076]
 [0.083]
 [0.083]] [[46.462]
 [46.462]
 [54.965]
 [46.462]
 [46.462]] [[0.918]
 [0.918]
 [1.322]
 [0.918]
 [0.918]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
printing an ep nov before normalisation:  65.8950379388584
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
actions average: 
K:  3  action  0 :  tensor([    0.9997,     0.0002,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0030,     0.9229,     0.0302,     0.0009,     0.0429],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0210,     0.9362,     0.0032,     0.0395],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0008,     0.0000,     0.9745,     0.0244],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0050, 0.0692, 0.0618, 0.0915, 0.7724], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.82384676854003
printing an ep nov before normalisation:  31.61604651146796
printing an ep nov before normalisation:  36.138903148404424
printing an ep nov before normalisation:  33.03729361880443
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
printing an ep nov before normalisation:  58.48309529851221
actions average: 
K:  4  action  0 :  tensor([    0.9982,     0.0007,     0.0000,     0.0002,     0.0009],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0008,     0.9287,     0.0006,     0.0008,     0.0692],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0282,     0.8628,     0.0195,     0.0894],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0005,     0.0001,     0.0161,     0.8858,     0.0974],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0013, 0.1230, 0.1066, 0.0857, 0.6834], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
Printing some Q and Qe and total Qs values:  [[0.641]
 [0.559]
 [0.641]
 [0.645]
 [0.641]] [[28.577]
 [25.333]
 [28.577]
 [36.226]
 [28.577]] [[1.134]
 [0.945]
 [1.134]
 [1.389]
 [1.134]]
siam score:  -0.93095887
printing an ep nov before normalisation:  33.67244952104787
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
printing an ep nov before normalisation:  58.589010725033084
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
Printing some Q and Qe and total Qs values:  [[0.848]
 [0.848]
 [0.848]
 [0.848]
 [0.848]] [[43.546]
 [43.546]
 [43.546]
 [43.546]
 [43.546]] [[1.705]
 [1.705]
 [1.705]
 [1.705]
 [1.705]]
printing an ep nov before normalisation:  25.864316233671886
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.25 ]
 [0.673]
 [1.207]
 [1.253]] [[27.056]
 [24.926]
 [22.739]
 [18.021]
 [21.62 ]] [[1.823]
 [1.839]
 [1.183]
 [1.548]
 [1.723]]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04174308476403366, 0.13543577119100841, 0.22912845761798317, 0.22912845761798317, 0.07297398023969193, 0.29159024856929966]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
printing an ep nov before normalisation:  78.27860078582965
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
printing an ep nov before normalisation:  59.90722363289212
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
printing an ep nov before normalisation:  39.61216381617955
maxi score, test score, baseline:  0.0161 0.2 0.2
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
printing an ep nov before normalisation:  36.556667688662785
maxi score, test score, baseline:  0.0161 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
maxi score, test score, baseline:  0.0161 0.2 0.2
from probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
siam score:  -0.93192315
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.334]
 [0.388]
 [0.064]
 [0.316]] [[25.684]
 [26.091]
 [24.473]
 [24.097]
 [21.546]] [[0.028]
 [0.334]
 [0.388]
 [0.064]
 [0.316]]
printing an ep nov before normalisation:  0.7665972356909379
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.93227506
Starting evaluation
Printing some Q and Qe and total Qs values:  [[0.021]
 [0.021]
 [0.036]
 [0.021]
 [0.021]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.021]
 [0.021]
 [0.036]
 [0.021]
 [0.021]]
Printing some Q and Qe and total Qs values:  [[0.09]
 [0.09]
 [0.09]
 [0.09]
 [0.09]] [[45.554]
 [45.554]
 [45.554]
 [45.554]
 [45.554]] [[1.423]
 [1.423]
 [1.423]
 [1.423]
 [1.423]]
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
printing an ep nov before normalisation:  44.407852490743004
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.031]
 [0.043]
 [0.042]
 [0.043]] [[25.348]
 [28.449]
 [21.773]
 [20.27 ]
 [24.077]] [[0.035]
 [0.031]
 [0.043]
 [0.042]
 [0.043]]
printing an ep nov before normalisation:  24.927896690617132
printing an ep nov before normalisation:  27.441534807287944
printing an ep nov before normalisation:  30.359924225609852
printing an ep nov before normalisation:  42.58800406933177
using explorer policy with actor:  1
printing an ep nov before normalisation:  28.793200641405573
printing an ep nov before normalisation:  49.07019232670527
printing an ep nov before normalisation:  34.16260958818043
maxi score, test score, baseline:  0.018099999999999998 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
printing an ep nov before normalisation:  30.77500343322754
printing an ep nov before normalisation:  40.774739322299475
printing an ep nov before normalisation:  33.24226533409241
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]] [[21.494]
 [21.494]
 [21.494]
 [21.494]
 [21.494]] [[0.353]
 [0.353]
 [0.353]
 [0.353]
 [0.353]]
printing an ep nov before normalisation:  33.7577526514234
printing an ep nov before normalisation:  36.95477755696351
Printing some Q and Qe and total Qs values:  [[0.365]
 [0.354]
 [0.359]
 [0.357]
 [0.352]] [[24.816]
 [15.369]
 [18.001]
 [25.471]
 [18.324]] [[0.365]
 [0.354]
 [0.359]
 [0.357]
 [0.352]]
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.022099999999999998 0.2 0.2
probs:  [0.04048194990719114, 0.13133494597401352, 0.25247227406310996, 0.22218794204083586, 0.07076628192946527, 0.28275660608538417]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0241 0.2 0.2
printing an ep nov before normalisation:  31.500360405153582
printing an ep nov before normalisation:  36.08325481414795
actor:  0 policy actor:  0  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  69.05515726152022
actor:  0 policy actor:  0  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  27.808888578334912
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  34.05071258544922
printing an ep nov before normalisation:  31.756898499446724
printing an ep nov before normalisation:  26.506141408941577
actor:  0 policy actor:  0  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]] [[9.177]
 [9.177]
 [9.177]
 [9.177]
 [9.177]] [[0.003]
 [0.003]
 [0.003]
 [0.003]
 [0.003]]
actor:  0 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.92725027
from probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.127]
 [0.006]
 [0.065]
 [0.086]] [[40.34 ]
 [40.924]
 [41.842]
 [37.253]
 [40.884]] [[1.734]
 [1.815]
 [1.768]
 [1.456]
 [1.77 ]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
printing an ep nov before normalisation:  44.17598555029856
UNIT TEST: sample policy line 217 mcts : [0.077 0.026 0.718 0.026 0.154]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.081]
 [0.095]
 [0.081]
 [0.081]] [[41.381]
 [41.381]
 [58.128]
 [41.381]
 [41.381]] [[0.858]
 [0.858]
 [1.513]
 [0.858]
 [0.858]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
printing an ep nov before normalisation:  54.530970598399634
printing an ep nov before normalisation:  40.31962314074397
Printing some Q and Qe and total Qs values:  [[1.089]
 [1.149]
 [1.089]
 [1.089]
 [1.089]] [[37.329]
 [36.245]
 [37.329]
 [37.329]
 [37.329]] [[2.711]
 [2.695]
 [2.711]
 [2.711]
 [2.711]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
printing an ep nov before normalisation:  42.04571512010362
printing an ep nov before normalisation:  45.585341453552246
printing an ep nov before normalisation:  62.922629632043574
actions average: 
K:  1  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9570,     0.0136,     0.0006,     0.0286],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0137,     0.9768,     0.0002,     0.0092],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0001,     0.0140,     0.8492,     0.1364],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0045, 0.0530, 0.0544, 0.1166, 0.7715], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
printing an ep nov before normalisation:  39.372619305642964
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
line 256 mcts: sample exp_bonus 25.766106488792072
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
Printing some Q and Qe and total Qs values:  [[0.01 ]
 [0.024]
 [1.103]
 [0.733]
 [0.884]] [[39.424]
 [41.244]
 [32.875]
 [45.274]
 [44.647]] [[0.567]
 [0.631]
 [1.485]
 [1.448]
 [1.583]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04048268761780655, 0.1313351525329858, 0.25247177241989155, 0.22218761744816512, 0.07076684258953297, 0.2827559273916179]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
printing an ep nov before normalisation:  55.35104483356196
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.055]
 [0.055]
 [0.055]
 [0.055]] [[56.163]
 [56.163]
 [56.163]
 [56.163]
 [56.163]] [[1.388]
 [1.388]
 [1.388]
 [1.388]
 [1.388]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
Printing some Q and Qe and total Qs values:  [[0.141]
 [0.366]
 [0.194]
 [0.101]
 [0.148]] [[38.082]
 [37.756]
 [32.878]
 [27.372]
 [29.005]] [[1.067]
 [1.279]
 [0.885]
 [0.543]
 [0.664]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.007]
 [0.147]
 [0.011]
 [0.074]
 [0.149]] [[63.143]
 [55.354]
 [49.736]
 [55.376]
 [59.257]] [[1.532]
 [1.436]
 [1.129]
 [1.363]
 [1.557]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
printing an ep nov before normalisation:  62.47946171744834
printing an ep nov before normalisation:  58.77077579498291
actions average: 
K:  1  action  0 :  tensor([    0.9991,     0.0001,     0.0000,     0.0003,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9159,     0.0153,     0.0003,     0.0681],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0002,     0.0030,     0.9516,     0.0003,     0.0450],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0007,     0.0005,     0.9486,     0.0499],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0151, 0.0518, 0.0874, 0.2701, 0.5756], grad_fn=<DivBackward0>)
using another actor
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0000,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.8944,     0.0119,     0.0007,     0.0927],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0184,     0.9430,     0.0014,     0.0372],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0003,     0.0007,     0.9594,     0.0393],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0162, 0.0655, 0.1112, 0.1248, 0.6824], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.039295735335982365, 0.12747561087260995, 0.27444207010032257, 0.2156554864092375, 0.06868902718152489, 0.27444207010032257]
actor:  1 policy actor:  1  step number:  113 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04174359168443679, 0.12502564167259003, 0.26382905831951214, 0.20830769166074325, 0.0695042750138212, 0.29158974164889656]
printing an ep nov before normalisation:  22.04790471208855
printing an ep nov before normalisation:  37.77971658434213
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.04174359168443679, 0.12502564167259003, 0.26382905831951214, 0.20830769166074325, 0.0695042750138212, 0.29158974164889656]
printing an ep nov before normalisation:  38.148716088043564
printing an ep nov before normalisation:  41.584206974233915
printing an ep nov before normalisation:  28.245181678507368
siam score:  -0.9308171
siam score:  -0.9309973
Printing some Q and Qe and total Qs values:  [[0.089]
 [0.089]
 [0.089]
 [0.089]
 [0.089]] [[53.746]
 [53.746]
 [53.746]
 [53.746]
 [53.746]] [[0.498]
 [0.498]
 [0.498]
 [0.498]
 [0.498]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  11.441621698580207
printing an ep nov before normalisation:  77.98945089308445
using explorer policy with actor:  1
printing an ep nov before normalisation:  67.54951665250199
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.040618785775542454, 0.14865982653936322, 0.256700867303184, 0.2026803469212736, 0.06762904596649764, 0.2837111274941392]
printing an ep nov before normalisation:  43.90635013580322
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.040618785775542454, 0.14865982653936322, 0.256700867303184, 0.2026803469212736, 0.06762904596649764, 0.2837111274941392]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.040618785775542454, 0.14865982653936322, 0.256700867303184, 0.2026803469212736, 0.06762904596649764, 0.2837111274941392]
printing an ep nov before normalisation:  43.012198887858304
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.040618785775542454, 0.14865982653936322, 0.256700867303184, 0.2026803469212736, 0.06762904596649764, 0.2837111274941392]
Printing some Q and Qe and total Qs values:  [[1.459]
 [1.404]
 [1.418]
 [1.418]
 [1.385]] [[37.331]
 [40.167]
 [39.045]
 [39.045]
 [39.682]] [[2.589]
 [2.67 ]
 [2.629]
 [2.629]
 [2.628]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.040618785775542454, 0.14865982653936322, 0.256700867303184, 0.2026803469212736, 0.06762904596649764, 0.2837111274941392]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.182]
 [0.008]
 [0.069]
 [0.117]] [[48.209]
 [45.966]
 [40.482]
 [39.921]
 [43.387]] [[0.001]
 [0.182]
 [0.008]
 [0.069]
 [0.117]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
printing an ep nov before normalisation:  46.49283339029863
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
from probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03955319050312266, 0.14475055008674528, 0.24994790967036792, 0.22364856977446226, 0.06585253039902832, 0.27624724956627356]
printing an ep nov before normalisation:  29.52871065373098
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.92419726
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.084]
 [0.084]
 [0.084]
 [0.084]] [[79.83]
 [79.83]
 [79.83]
 [79.83]
 [79.83]] [[1.417]
 [1.417]
 [1.417]
 [1.417]
 [1.417]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.038542250428799815, 0.16666666666666666, 0.24354131640938678, 0.2179164331618134, 0.06416713367637318, 0.26916619965696015]
printing an ep nov before normalisation:  37.66390051580668
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  47.43851325652476
printing an ep nov before normalisation:  37.423105783790504
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  31.73279244141187
siam score:  -0.92986894
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
printing an ep nov before normalisation:  47.62300836367126
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
printing an ep nov before normalisation:  20.63039541244507
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
printing an ep nov before normalisation:  42.40846703594193
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
printing an ep nov before normalisation:  30.215318202972412
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
printing an ep nov before normalisation:  39.20268248922012
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
actions average: 
K:  1  action  0 :  tensor([    0.9970,     0.0017,     0.0001,     0.0005,     0.0007],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9820,     0.0001,     0.0002,     0.0175],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0004,     0.9320,     0.0160,     0.0516],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0002,     0.0123,     0.9098,     0.0774],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0041, 0.1757, 0.0599, 0.1541, 0.6061], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.23745510588807248, 0.06256602075283459, 0.26243926090739217]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
Printing some Q and Qe and total Qs values:  [[0.499]
 [0.498]
 [0.499]
 [0.499]
 [0.667]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.499]
 [0.498]
 [0.499]
 [0.499]
 [0.667]]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
printing an ep nov before normalisation:  67.06120320116767
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
Printing some Q and Qe and total Qs values:  [[0.408]
 [0.408]
 [0.441]
 [0.408]
 [0.408]] [[41.465]
 [41.465]
 [45.882]
 [41.465]
 [41.465]] [[0.408]
 [0.408]
 [0.441]
 [0.408]
 [0.408]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
Printing some Q and Qe and total Qs values:  [[0.568]
 [0.562]
 [0.561]
 [0.558]
 [0.562]] [[31.688]
 [31.305]
 [35.469]
 [32.148]
 [31.305]] [[0.568]
 [0.562]
 [0.561]
 [0.558]
 [0.562]]
printing an ep nov before normalisation:  37.809196405893616
maxi score, test score, baseline:  0.034100000000000005 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
printing an ep nov before normalisation:  21.275532245635986
actions average: 
K:  1  action  0 :  tensor([    0.9983,     0.0001,     0.0000,     0.0006,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0032,     0.9234,     0.0011,     0.0004,     0.0719],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0162,     0.9457,     0.0036,     0.0342],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0013,     0.0001,     0.0002,     0.9530,     0.0453],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0090, 0.0633, 0.0360, 0.0923, 0.7995], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  41.95560030158846
actor:  0 policy actor:  1  step number:  99 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03758186573351488, 0.1625026408301134, 0.23745510588807248, 0.26243926090739217, 0.06256602075283459, 0.23745510588807248]
printing an ep nov before normalisation:  32.57296237221477
printing an ep nov before normalisation:  42.83577050228944
actor:  1 policy actor:  1  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  65.6088076468479
using another actor
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03976160772039315, 0.15873510048252457, 0.23011919613980342, 0.27770859324465597, 0.06355630627281944, 0.23011919613980342]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03976160772039315, 0.15873510048252457, 0.23011919613980342, 0.27770859324465597, 0.06355630627281944, 0.23011919613980342]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03976160772039315, 0.15873510048252457, 0.23011919613980342, 0.27770859324465597, 0.06355630627281944, 0.23011919613980342]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03976160772039315, 0.15873510048252457, 0.23011919613980342, 0.27770859324465597, 0.06355630627281944, 0.23011919613980342]
printing an ep nov before normalisation:  39.48058683250427
printing an ep nov before normalisation:  46.033173531631896
printing an ep nov before normalisation:  32.039977049507286
printing an ep nov before normalisation:  39.11771172795979
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04174323350891515, 0.1553099909250529, 0.22345004537473556, 0.2915900998244182, 0.06445658499214271, 0.22345004537473556]
siam score:  -0.9370308
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04174323350891515, 0.1553099909250529, 0.2234500453747355, 0.2915900998244182, 0.0644565849921427, 0.2234500453747355]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04174323350891515, 0.1553099909250529, 0.2234500453747355, 0.2915900998244182, 0.0644565849921427, 0.2234500453747355]
Printing some Q and Qe and total Qs values:  [[0.105]
 [0.117]
 [0.14 ]
 [0.126]
 [0.117]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.105]
 [0.117]
 [0.14 ]
 [0.126]
 [0.117]]
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.105]
 [0.111]
 [0.111]
 [0.111]] [[50.96 ]
 [57.887]
 [50.96 ]
 [50.96 ]
 [50.96 ]] [[0.933]
 [1.105]
 [0.933]
 [0.933]
 [0.933]]
siam score:  -0.93772835
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04174323350891515, 0.1553099909250529, 0.2234500453747355, 0.2915900998244182, 0.0644565849921427, 0.2234500453747355]
printing an ep nov before normalisation:  26.14443063735962
Printing some Q and Qe and total Qs values:  [[0.628]
 [0.689]
 [0.644]
 [0.86 ]
 [0.711]] [[32.78 ]
 [28.256]
 [30.238]
 [37.543]
 [25.294]] [[0.903]
 [0.883]
 [0.873]
 [1.221]
 [0.851]]
printing an ep nov before normalisation:  41.635619435484415
actions average: 
K:  0  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([0.0072, 0.9134, 0.0014, 0.0026, 0.0754], grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0000,     0.9632,     0.0119,     0.0249],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0018,     0.0003,     0.0440,     0.9116,     0.0424],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0024, 0.0577, 0.1421, 0.2327, 0.5652], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
printing an ep nov before normalisation:  97.59404446029338
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9994,     0.0000,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0070,     0.9087,     0.0243,     0.0007,     0.0594],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9375,     0.0258,     0.0365],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0045,     0.0011,     0.0001,     0.8880,     0.1062],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0059, 0.1082, 0.0434, 0.1811, 0.6614], grad_fn=<DivBackward0>)
from probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
using another actor
siam score:  -0.9352757
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
printing an ep nov before normalisation:  29.270106786312947
Printing some Q and Qe and total Qs values:  [[0.175]
 [0.224]
 [0.175]
 [0.175]
 [0.175]] [[30.315]
 [34.631]
 [30.315]
 [30.315]
 [30.315]] [[0.175]
 [0.224]
 [0.175]
 [0.175]
 [0.175]]
printing an ep nov before normalisation:  16.071815490722656
Printing some Q and Qe and total Qs values:  [[0.429]
 [0.426]
 [0.438]
 [0.509]
 [0.502]] [[37.884]
 [23.944]
 [32.774]
 [30.631]
 [31.1  ]] [[0.429]
 [0.426]
 [0.438]
 [0.509]
 [0.502]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
using explorer policy with actor:  0
maxi score, test score, baseline:  0.0361 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.687]
 [0.673]
 [0.675]
 [0.701]
 [0.682]] [[47.95 ]
 [46.563]
 [46.643]
 [42.887]
 [46.95 ]] [[0.938]
 [0.91 ]
 [0.913]
 [0.903]
 [0.923]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.106]
 [0.104]
 [0.109]
 [0.108]] [[39.806]
 [49.45 ]
 [59.81 ]
 [57.849]
 [53.636]] [[0.25 ]
 [0.442]
 [0.636]
 [0.604]
 [0.523]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.36445820194882
actions average: 
K:  3  action  0 :  tensor([    0.9982,     0.0001,     0.0000,     0.0003,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9190,     0.0002,     0.0014,     0.0792],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0009, 0.0202, 0.8675, 0.0319, 0.0795], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0001,     0.0001,     0.9287,     0.0708],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([    0.0004,     0.1453,     0.0372,     0.0942,     0.7228],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
actions average: 
K:  2  action  0 :  tensor([    0.9991,     0.0000,     0.0000,     0.0000,     0.0009],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0000,     0.9974,     0.0000,     0.0007,     0.0019],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0317,     0.9035,     0.0004,     0.0643],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0067,     0.0001,     0.0382,     0.8751,     0.0799],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0020, 0.1177, 0.0805, 0.1016, 0.6983], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  38.716029787116746
using explorer policy with actor:  1
siam score:  -0.93859893
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
printing an ep nov before normalisation:  38.25925219165074
printing an ep nov before normalisation:  40.061429508756426
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.15186098795823488, 0.2406950602088256, 0.285112096334121, 0.06302691570764415, 0.2184865421461779]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
line 256 mcts: sample exp_bonus 39.75770798350011
Printing some Q and Qe and total Qs values:  [[0.078]
 [0.076]
 [0.049]
 [0.066]
 [0.051]] [[12.886]
 [12.999]
 [13.201]
 [13.831]
 [10.466]] [[0.71 ]
 [0.717]
 [0.706]
 [0.772]
 [0.495]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
maxi score, test score, baseline:  0.0361 0.35 0.35
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
Printing some Q and Qe and total Qs values:  [[0.074]
 [0.074]
 [0.109]
 [0.088]
 [0.075]] [[35.191]
 [35.191]
 [54.902]
 [38.077]
 [37.714]] [[0.604]
 [0.604]
 [1.207]
 [0.701]
 [0.678]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
maxi score, test score, baseline:  0.0361 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.196]
 [0.196]
 [0.196]
 [0.196]
 [0.196]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
printing an ep nov before normalisation:  25.685494180379855
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
printing an ep nov before normalisation:  73.19763986141201
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.03993377922741061, 0.17028760630778828, 0.23546451984797714, 0.27891579554143636, 0.06165941707414022, 0.21373888200124747]
Printing some Q and Qe and total Qs values:  [[0.099]
 [0.127]
 [0.167]
 [0.13 ]
 [0.13 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.099]
 [0.127]
 [0.167]
 [0.13 ]
 [0.13 ]]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.257745971567445
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0001,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0032,     0.9795,     0.0003,     0.0003,     0.0167],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0204,     0.9086,     0.0004,     0.0706],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0003,     0.0001,     0.9333,     0.0660],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0024, 0.1180, 0.0533, 0.0620, 0.7643], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0000,     0.0003],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9593,     0.0001,     0.0002,     0.0402],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0005,     0.0001,     0.9691,     0.0002,     0.0300],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0015,     0.0027,     0.0006,     0.9209,     0.0744],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0029, 0.0943, 0.0340, 0.0350, 0.8337], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
maxi score, test score, baseline:  0.0361 0.35 0.35
probs:  [0.04081839764499648, 0.17406950602088256, 0.24069506020882567, 0.285112096334121, 0.06302691570764417, 0.19627802408353026]
printing an ep nov before normalisation:  39.36690667948943
actor:  0 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.042825757938705085, 0.17093704282969982, 0.24780381376429667, 0.29904832772069456, 0.042825757938705085, 0.19655929980789874]
siam score:  -0.9238571
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04175837448260866, 0.16666666666666666, 0.2416116419771015, 0.2915749588507247, 0.04175837448260866, 0.2166299835402899]
printing an ep nov before normalisation:  60.24365990301985
actions average: 
K:  2  action  0 :  tensor([    0.9881,     0.0006,     0.0000,     0.0004,     0.0109],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.8768,     0.0163,     0.0008,     0.1058],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0005,     0.0156,     0.9412,     0.0003,     0.0423],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0008,     0.0002,     0.9761,     0.0226],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0704, 0.0609, 0.1435, 0.7242], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04175837448260866, 0.16666666666666666, 0.2416116419771015, 0.2915749588507247, 0.04175837448260866, 0.2166299835402899]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04175837448260866, 0.16666666666666666, 0.2416116419771015, 0.2915749588507247, 0.04175837448260866, 0.2166299835402899]
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04175837448260866, 0.16666666666666666, 0.2416116419771015, 0.2915749588507247, 0.04175837448260866, 0.2166299835402899]
printing an ep nov before normalisation:  40.52544162755909
actor:  1 policy actor:  1  step number:  103 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
maxi score, test score, baseline:  0.0381 0.35 0.35
printing an ep nov before normalisation:  39.17326843105951
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
printing an ep nov before normalisation:  34.77011680603027
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
maxi score, test score, baseline:  0.0381 0.35 0.35
printing an ep nov before normalisation:  63.448066138874964
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
printing an ep nov before normalisation:  44.23794969620755
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.225]
 [0.211]
 [0.347]
 [0.347]
 [0.578]] [[50.234]
 [51.857]
 [41.448]
 [41.448]
 [43.67 ]] [[1.119]
 [1.152]
 [0.985]
 [0.985]
 [1.28 ]]
maxi score, test score, baseline:  0.0381 0.35 0.35
actions average: 
K:  4  action  0 :  tensor([    0.9968,     0.0000,     0.0000,     0.0001,     0.0031],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0154,     0.9519,     0.0007,     0.0008,     0.0313],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0026,     0.0002,     0.9709,     0.0013,     0.0250],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0002,     0.0002,     0.9980,     0.0016],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0013, 0.1479, 0.0642, 0.0660, 0.7206], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04373909091089977, 0.16270126099712579, 0.23407856304886138, 0.3054558651005971, 0.04373909091089977, 0.2102861290316162]
printing an ep nov before normalisation:  1.3259998015655583
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04553979687812337, 0.1590962373048827, 0.22723010156093834, 0.31807525390234587, 0.04553979687812337, 0.20451881347558645]
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04553979687812337, 0.1590962373048827, 0.22723010156093834, 0.31807525390234587, 0.04553979687812337, 0.20451881347558645]
actions average: 
K:  3  action  0 :  tensor([    0.9993,     0.0000,     0.0000,     0.0001,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9084,     0.0358,     0.0002,     0.0553],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0008,     0.0003,     0.9423,     0.0192,     0.0375],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0003,     0.0007,     0.9737,     0.0247],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0009, 0.0510, 0.0284, 0.0951, 0.8246], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04553979687812337, 0.1590962373048827, 0.22723010156093834, 0.31807525390234587, 0.04553979687812337, 0.20451881347558645]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04553979687812337, 0.1590962373048827, 0.22723010156093834, 0.31807525390234587, 0.04553979687812337, 0.20451881347558645]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
maxi score, test score, baseline:  0.0381 0.35 0.35
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.105]
 [0.097]
 [0.097]
 [0.097]] [[74.82 ]
 [62.857]
 [65.456]
 [65.456]
 [65.456]] [[1.301]
 [0.986]
 [1.047]
 [1.047]
 [1.047]]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
printing an ep nov before normalisation:  47.942001906495086
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
printing an ep nov before normalisation:  38.76865377303204
printing an ep nov before normalisation:  49.365968996091745
printing an ep nov before normalisation:  38.32751051759871
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
printing an ep nov before normalisation:  37.43514454751697
printing an ep nov before normalisation:  47.619949955325154
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
printing an ep nov before normalisation:  27.624526023864746
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04453066716030898, 0.1555633939842705, 0.2443895754434397, 0.31100921153781674, 0.04453066716030898, 0.1999764847138551]
printing an ep nov before normalisation:  38.030823897186146
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.1521841666683806, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.1521841666683806, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.1521841666683806, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
Printing some Q and Qe and total Qs values:  [[0.055]
 [0.   ]
 [0.104]
 [0.083]
 [0.08 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.055]
 [0.   ]
 [0.104]
 [0.083]
 [0.08 ]]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.1521841666683806, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  34.48471546173096
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.15218416666838058, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.794]
 [0.643]] [[39.085]
 [39.085]
 [39.085]
 [42.977]
 [39.085]] [[1.737]
 [1.737]
 [1.737]
 [2.065]
 [1.737]]
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.15218416666838058, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
printing an ep nov before normalisation:  20.703198211858645
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04356541668123487, 0.15218416666838058, 0.26080291665552624, 0.30425041665038466, 0.04356541668123487, 0.1956316666632389]
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0381 0.35 0.35
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.04522331238044348, 0.1493176160543491, 0.2534119197282546, 0.3158685019325981, 0.04522331238044348, 0.1909553375239113]
using explorer policy with actor:  1
printing an ep nov before normalisation:  41.29617192827826
actor:  1 policy actor:  1  step number:  91 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0381 0.35 0.35
siam score:  -0.9274513
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.162]
 [0.224]
 [0.121]
 [0.162]
 [0.162]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.162]
 [0.224]
 [0.121]
 [0.162]
 [0.162]]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.048156616635373795, 0.1442458463904761, 0.2403350761455784, 0.3364243059006808, 0.048156616635373795, 0.18268153829251704]
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.048156616635373795, 0.1442458463904761, 0.2403350761455784, 0.3364243059006808, 0.048156616635373795, 0.18268153829251704]
Printing some Q and Qe and total Qs values:  [[0.881]
 [1.002]
 [0.949]
 [0.938]
 [0.916]] [[33.554]
 [28.515]
 [31.387]
 [30.451]
 [29.936]] [[1.883]
 [1.765]
 [1.849]
 [1.793]
 [1.747]]
printing an ep nov before normalisation:  29.03497334870079
maxi score, test score, baseline:  0.0381 0.35 0.35
probs:  [0.048156616635373795, 0.1442458463904761, 0.2403350761455784, 0.3364243059006808, 0.048156616635373795, 0.18268153829251704]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.719]
 [0.77 ]
 [0.719]
 [0.719]
 [0.719]] [[40.474]
 [41.988]
 [40.474]
 [40.474]
 [40.474]] [[0.719]
 [0.77 ]
 [0.719]
 [0.719]
 [0.719]]
actor:  0 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  57.53408206078586
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04946035014341294, 0.14199165266177116, 0.23452295518012936, 0.34556051820215933, 0.04946035014341294, 0.17900417366911447]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04946035014341294, 0.14199165266177116, 0.23452295518012936, 0.34556051820215933, 0.04946035014341294, 0.17900417366911447]
siam score:  -0.925563
printing an ep nov before normalisation:  17.28191375732422
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.048563372245958754, 0.1394120602618879, 0.24843048588100283, 0.33927917389693213, 0.048563372245958754, 0.17575153546825956]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.048563372245958754, 0.1394120602618879, 0.24843048588100283, 0.33927917389693213, 0.048563372245958754, 0.17575153546825956]
printing an ep nov before normalisation:  37.68008555499234
Printing some Q and Qe and total Qs values:  [[0.111]
 [0.111]
 [0.111]
 [0.114]
 [0.108]] [[59.331]
 [59.331]
 [62.445]
 [64.874]
 [60.728]] [[1.05 ]
 [1.05 ]
 [1.127]
 [1.191]
 [1.082]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.048563372245958754, 0.1394120602618879, 0.24843048588100283, 0.33927917389693213, 0.048563372245958754, 0.17575153546825956]
printing an ep nov before normalisation:  67.06283221600634
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.048563372245958754, 0.1394120602618879, 0.24843048588100283, 0.33927917389693213, 0.048563372245958754, 0.17575153546825956]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
using explorer policy with actor:  1
printing an ep nov before normalisation:  56.12965400235107
siam score:  -0.92323273
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
siam score:  -0.9221938
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
actions average: 
K:  3  action  0 :  tensor([    1.0000,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9477,     0.0162,     0.0022,     0.0330],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([0.0010, 0.0216, 0.9290, 0.0196, 0.0288], grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0119,     0.0004,     0.0116,     0.9471,     0.0290],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0036, 0.0013, 0.0358, 0.0913, 0.8679], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
printing an ep nov before normalisation:  37.52990222808887
printing an ep nov before normalisation:  38.65605152287943
siam score:  -0.92028165
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
printing an ep nov before normalisation:  32.43277311325073
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04978422810228953, 0.13744605702557236, 0.24264025173351172, 0.3478344464414513, 0.04978422810228953, 0.1725107885948855]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
printing an ep nov before normalisation:  45.87327209916542
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.69659696098622
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.847]
 [0.862]
 [0.862]
 [0.862]
 [0.862]] [[46.851]
 [33.096]
 [33.096]
 [33.096]
 [33.096]] [[2.055]
 [1.536]
 [1.536]
 [1.536]
 [1.536]]
printing an ep nov before normalisation:  43.92491143496467
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
printing an ep nov before normalisation:  56.5626314807436
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
actions average: 
K:  4  action  0 :  tensor([    0.9844,     0.0053,     0.0000,     0.0015,     0.0088],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9337,     0.0011,     0.0001,     0.0649],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0001,     0.9303,     0.0170,     0.0526],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0009,     0.0366,     0.8581,     0.1041],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0017, 0.1118, 0.0048, 0.0760, 0.8056], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
printing an ep nov before normalisation:  54.02167485235738
siam score:  -0.9245521
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.899]
 [0.899]
 [0.881]
 [0.899]] [[43.867]
 [30.297]
 [30.297]
 [37.099]
 [30.297]] [[1.772]
 [1.403]
 [1.403]
 [1.586]
 [1.403]]
Printing some Q and Qe and total Qs values:  [[1.226]
 [1.333]
 [1.226]
 [1.226]
 [1.226]] [[35.973]
 [31.533]
 [35.973]
 [35.973]
 [35.973]] [[2.284]
 [2.176]
 [2.284]
 [2.284]
 [2.284]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  1.718079344899479
printing an ep nov before normalisation:  25.444138050079346
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.050922334494057385, 0.13561330925450318, 0.23724247896703812, 0.3558098436316624, 0.050922334494057385, 0.16948969915868153]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05198582877027036, 0.133900712981982, 0.23219857403603592, 0.3632623887747746, 0.05198582877027036, 0.16666666666666669]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05198582877027036, 0.133900712981982, 0.23219857403603592, 0.3632623887747746, 0.05198582877027036, 0.16666666666666669]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05198582877027036, 0.133900712981982, 0.23219857403603592, 0.3632623887747746, 0.05198582877027036, 0.16666666666666669]
printing an ep nov before normalisation:  42.820881603021036
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05198582877027036, 0.133900712981982, 0.23219857403603592, 0.3632623887747746, 0.05198582877027036, 0.16666666666666669]
printing an ep nov before normalisation:  38.44370128581842
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.99336528778076
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.2445736847851971, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
printing an ep nov before normalisation:  35.00438807949344
using explorer policy with actor:  0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.2445736847851971, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
printing an ep nov before normalisation:  51.665536733672354
printing an ep nov before normalisation:  26.58740534478212
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.2445736847851971, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.24457368478519714, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.24457368478519714, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
printing an ep nov before normalisation:  28.818178176879883
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.24457368478519714, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
printing an ep nov before normalisation:  32.5969596730246
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05114936393919044, 0.1317428309583599, 0.24457368478519714, 0.3574045386120343, 0.05114936393919044, 0.16398021776602767]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  37.92851818647211
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.13033944919935
printing an ep nov before normalisation:  52.72926382176237
printing an ep nov before normalisation:  50.55484217608873
printing an ep nov before normalisation:  0.027873020332691567
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91389334
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04955486489415852, 0.12762939940916396, 0.25254865463317266, 0.3462380960511791, 0.0651697717971596, 0.15885921321516613]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04955486489415852, 0.12762939940916396, 0.25254865463317266, 0.3462380960511791, 0.0651697717971596, 0.15885921321516613]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04955486489415852, 0.12762939940916396, 0.25254865463317266, 0.3462380960511791, 0.0651697717971596, 0.15885921321516613]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04955486489415852, 0.12762939940916396, 0.25254865463317266, 0.3462380960511791, 0.0651697717971596, 0.15885921321516613]
printing an ep nov before normalisation:  58.501131910631756
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04955486489415852, 0.12762939940916396, 0.25254865463317266, 0.3462380960511791, 0.0651697717971596, 0.15885921321516613]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05057921991338317, 0.1262884243176985, 0.24742315136460302, 0.35341603753064443, 0.06572106079424624, 0.15657210607942462]
printing an ep nov before normalisation:  34.54830776592144
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05057921991338317, 0.1262884243176985, 0.24742315136460302, 0.35341603753064443, 0.06572106079424624, 0.15657210607942462]
printing an ep nov before normalisation:  32.19716787338257
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05057921991338317, 0.1262884243176985, 0.24742315136460302, 0.35341603753064443, 0.06572106079424624, 0.15657210607942462]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.049826175240296304, 0.12440521232095827, 0.25864747906614977, 0.3481423235629441, 0.0647419826564287, 0.15423682715322307]
printing an ep nov before normalisation:  44.016923904418945
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.049826175240296304, 0.12440521232095827, 0.25864747906614977, 0.3481423235629441, 0.0647419826564287, 0.15423682715322307]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.049826175240296304, 0.12440521232095827, 0.25864747906614977, 0.3481423235629441, 0.0647419826564287, 0.15423682715322307]
Printing some Q and Qe and total Qs values:  [[1.4  ]
 [1.254]
 [1.282]
 [1.282]
 [1.19 ]] [[34.561]
 [39.291]
 [39.754]
 [39.754]
 [41.972]] [[2.334]
 [2.4  ]
 [2.45 ]
 [2.45 ]
 [2.458]]
printing an ep nov before normalisation:  30.20581680375479
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05079813535309324, 0.12321596742407663, 0.25356806515184677, 0.35495303005122353, 0.06528170176728991, 0.15218310025246998]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05079813535309324, 0.12321596742407663, 0.25356806515184677, 0.35495303005122353, 0.06528170176728991, 0.15218310025246998]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.416]
 [0.008]
 [0.143]
 [0.288]] [[34.145]
 [40.549]
 [31.904]
 [30.737]
 [34.711]] [[0.749]
 [1.005]
 [0.368]
 [0.472]
 [0.723]]
siam score:  -0.9139194
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05079813535309324, 0.12321596742407663, 0.25356806515184677, 0.35495303005122353, 0.06528170176728991, 0.15218310025246998]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05079813535309324, 0.12321596742407663, 0.25356806515184677, 0.35495303005122353, 0.06528170176728991, 0.15218310025246998]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
printing an ep nov before normalisation:  39.48064018527299
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.050074232815871866, 0.12145735558166462, 0.2642236011132501, 0.34988334843220137, 0.06435085736903041, 0.1500106046879817]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  59.77731416417551
using explorer policy with actor:  1
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
siam score:  -0.90518016
printing an ep nov before normalisation:  46.16637848330686
Printing some Q and Qe and total Qs values:  [[0.147]
 [0.222]
 [0.112]
 [0.113]
 [0.161]] [[31.878]
 [35.898]
 [46.943]
 [47.953]
 [32.646]] [[0.437]
 [0.571]
 [0.622]
 [0.639]
 [0.462]]
printing an ep nov before normalisation:  33.500946380652564
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
printing an ep nov before normalisation:  63.11909336772065
line 256 mcts: sample exp_bonus 32.07539609958056
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
printing an ep nov before normalisation:  28.4103332736011
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
line 256 mcts: sample exp_bonus 22.01791171030395
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.052164501140611566, 0.1146202277911871, 0.27075954441762584, 0.36444313439348913, 0.052164501140611566, 0.14584809111647484]
line 256 mcts: sample exp_bonus 69.57996654411262
printing an ep nov before normalisation:  25.28481115733847
printing an ep nov before normalisation:  43.76455062411527
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
siam score:  -0.90886873
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
printing an ep nov before normalisation:  50.58213491163925
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  26.55238389968872
siam score:  -0.90721494
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
Printing some Q and Qe and total Qs values:  [[1.065]
 [1.208]
 [1.065]
 [1.065]
 [1.065]] [[37.883]
 [35.651]
 [37.883]
 [37.883]
 [37.883]] [[1.684]
 [1.769]
 [1.684]
 [1.684]
 [1.684]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.051363948226098674, 0.1128587313944016, 0.28196938510723474, 0.3588378640676133, 0.051363948226098674, 0.1436061229785531]
printing an ep nov before normalisation:  48.81341559650606
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05058765663290096, 0.11115061838964828, 0.2928395036598903, 0.35340246541663756, 0.05058765663290096, 0.14143209926802194]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05058765663290096, 0.11115061838964828, 0.2928395036598903, 0.35340246541663756, 0.05058765663290096, 0.14143209926802194]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.05058765663290096, 0.11115061838964828, 0.2928395036598903, 0.35340246541663756, 0.05058765663290096, 0.14143209926802194]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  29.403215678553693
printing an ep nov before normalisation:  35.71048017535674
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  28.339509963989258
printing an ep nov before normalisation:  25.691242453357074
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.9036334
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  51.55851856695508
Printing some Q and Qe and total Qs values:  [[0.313]
 [0.426]
 [0.313]
 [0.237]
 [0.328]] [[66.212]
 [57.981]
 [66.212]
 [57.464]
 [65.782]] [[0.832]
 [0.87 ]
 [0.832]
 [0.676]
 [0.844]]
Printing some Q and Qe and total Qs values:  [[1.009]
 [0.902]
 [0.891]
 [0.915]
 [0.92 ]] [[30.392]
 [29.414]
 [39.696]
 [36.02 ]
 [34.461]] [[1.321]
 [1.197]
 [1.357]
 [1.32 ]
 [1.299]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04910357563387838, 0.10788512115027252, 0.2989251440785535, 0.343011303215849, 0.04910357563387838, 0.15197128028756812]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  40.65164045804472
Printing some Q and Qe and total Qs values:  [[0.195]
 [0.237]
 [0.24 ]
 [0.145]
 [0.694]] [[34.761]
 [28.942]
 [31.559]
 [35.074]
 [30.463]] [[0.946]
 [0.777]
 [0.876]
 [0.908]
 [1.29 ]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
printing an ep nov before normalisation:  33.518732292818115
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
printing an ep nov before normalisation:  59.816967900998115
siam score:  -0.9001361
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
siam score:  -0.9004213
Printing some Q and Qe and total Qs values:  [[0.063]
 [0.064]
 [0.061]
 [0.064]
 [0.064]] [[43.128]
 [41.58 ]
 [31.204]
 [34.921]
 [39.34 ]] [[0.726]
 [0.678]
 [0.342]
 [0.464]
 [0.606]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
printing an ep nov before normalisation:  37.65890386903703
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9625,     0.0003,     0.0005,     0.0003,     0.0365],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0009,     0.9006,     0.0345,     0.0007,     0.0632],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0279,     0.9030,     0.0022,     0.0669],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0020,     0.0000,     0.0245,     0.9589,     0.0147],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0044, 0.1342, 0.1532, 0.1203, 0.5879], grad_fn=<DivBackward0>)
siam score:  -0.9018511
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
printing an ep nov before normalisation:  29.953001190815144
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
line 256 mcts: sample exp_bonus 57.71998284870169
printing an ep nov before normalisation:  38.84756792942012
Printing some Q and Qe and total Qs values:  [[0.079]
 [0.052]
 [0.079]
 [0.097]
 [0.058]] [[37.558]
 [27.797]
 [37.558]
 [45.882]
 [28.146]] [[0.479]
 [0.28 ]
 [0.479]
 [0.643]
 [0.292]]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04770430632369105, 0.11908172252947642, 0.3046630046645184, 0.3332139711468325, 0.04770430632369105, 0.14763268901179058]
line 256 mcts: sample exp_bonus 31.197500363274635
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  2  action  0 :  tensor([    0.9990,     0.0000,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0038,     0.8787,     0.0267,     0.0009,     0.0900],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0004,     0.9294,     0.0010,     0.0692],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0005,     0.0362,     0.8502,     0.1130],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0670,     0.0423,     0.0820,     0.8082],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04703423612276574, 0.11740625408976628, 0.3144479043973678, 0.32852230799076787, 0.04703423612276574, 0.14555506127656653]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04703423612276574, 0.11740625408976628, 0.3144479043973678, 0.32852230799076787, 0.04703423612276574, 0.14555506127656653]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
siam score:  -0.9097803
Printing some Q and Qe and total Qs values:  [[0.25 ]
 [0.222]
 [0.25 ]
 [0.25 ]
 [0.25 ]] [[46.355]
 [49.499]
 [46.355]
 [46.355]
 [46.355]] [[1.939]
 [2.116]
 [1.939]
 [1.939]
 [1.939]]
printing an ep nov before normalisation:  42.65454760917335
printing an ep nov before normalisation:  32.3512864112854
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046382780489539754, 0.11577733020711298, 0.3239609793598326, 0.32396097935983253, 0.046382780489539754, 0.14353515009414228]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046382780489539754, 0.11577733020711298, 0.3239609793598326, 0.32396097935983253, 0.046382780489539754, 0.14353515009414228]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046382780489539754, 0.11577733020711298, 0.3239609793598326, 0.32396097935983253, 0.046382780489539754, 0.14353515009414228]
Printing some Q and Qe and total Qs values:  [[0.101]
 [0.176]
 [0.12 ]
 [0.119]
 [0.124]] [[44.004]
 [41.375]
 [42.773]
 [43.814]
 [41.589]] [[1.329]
 [1.27 ]
 [1.285]
 [1.337]
 [1.228]]
printing an ep nov before normalisation:  68.12379197057037
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046382780489539754, 0.11577733020711298, 0.3239609793598326, 0.32396097935983253, 0.046382780489539754, 0.14353515009414228]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046382780489539754, 0.11577733020711298, 0.3239609793598326, 0.32396097935983253, 0.046382780489539754, 0.14353515009414228]
printing an ep nov before normalisation:  54.89377033509203
printing an ep nov before normalisation:  44.91076984537285
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04574917438070858, 0.11419303793879808, 0.3195246286130665, 0.31952462861306646, 0.05943794709232647, 0.14157058336203387]
printing an ep nov before normalisation:  41.381402992638584
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.68419451404204
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
from probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
printing an ep nov before normalisation:  49.55597915987512
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
printing an ep nov before normalisation:  41.416396771532526
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.046751994825110325, 0.1133712569593083, 0.3265528957887418, 0.31322904336190216, 0.060075847251949914, 0.14001896181298748]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.047702736499576445, 0.11259215295435292, 0.33321616890059297, 0.3072604023186824, 0.060680619790531735, 0.13854791953626353]
printing an ep nov before normalisation:  42.19309545861016
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.047702736499576445, 0.11259215295435292, 0.33321616890059297, 0.3072604023186824, 0.060680619790531735, 0.13854791953626353]
actions average: 
K:  1  action  0 :  tensor([    0.9606,     0.0072,     0.0000,     0.0052,     0.0269],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9855,     0.0004,     0.0018,     0.0122],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0003,     0.0001,     0.9748,     0.0005,     0.0244],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0008,     0.0012,     0.0134,     0.9056,     0.0790],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0014, 0.0902, 0.0999, 0.1574, 0.6511], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  26.25191211008583
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.047702736499576445, 0.11259215295435292, 0.33321616890059297, 0.3072604023186824, 0.060680619790531735, 0.13854791953626353]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.047702736499576445, 0.11259215295435292, 0.33321616890059297, 0.3072604023186824, 0.060680619790531735, 0.13854791953626353]
printing an ep nov before normalisation:  35.04813668340646
Printing some Q and Qe and total Qs values:  [[1.342]
 [1.342]
 [0.492]
 [1.342]
 [1.287]] [[24.136]
 [24.136]
 [20.088]
 [24.136]
 [20.578]] [[2.672]
 [2.672]
 [1.447]
 [2.672]
 [2.287]]
printing an ep nov before normalisation:  33.00872427674421
actor:  1 policy actor:  1  step number:  115 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04860535358455104, 0.11185248559282727, 0.33954216082262173, 0.30159388161765593, 0.061254779986206286, 0.1371513383961378]
printing an ep nov before normalisation:  44.9275199392178
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04860535358455104, 0.11185248559282727, 0.33954216082262173, 0.30159388161765593, 0.061254779986206286, 0.1371513383961378]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.04860535358455104, 0.11185248559282727, 0.33954216082262173, 0.30159388161765593, 0.061254779986206286, 0.1371513383961378]
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.001]
 [0.109]
 [0.126]
 [0.107]
 [0.087]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.001]
 [0.109]
 [0.126]
 [0.107]
 [0.087]]
using another actor
from probs:  [0.0494634098367281, 0.1111493344840642, 0.3455558481439414, 0.29620710842607245, 0.06180059476619531, 0.13582370434299865]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
printing an ep nov before normalisation:  45.750704592291484
printing an ep nov before normalisation:  51.480580227710746
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.0494634098367281, 0.1111493344840642, 0.3455558481439414, 0.29620710842607245, 0.06180059476619531, 0.13582370434299865]
maxi score, test score, baseline:  0.040100000000000004 0.35 0.35
probs:  [0.0494634098367281, 0.1111493344840642, 0.3455558481439414, 0.29620710842607245, 0.06180059476619531, 0.13582370434299865]
actor:  0 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.049463409836728096, 0.11114933448406421, 0.34555584814394136, 0.29620710842607245, 0.06180059476619531, 0.13582370434299862]
printing an ep nov before normalisation:  47.85340739007279
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.049463409836728096, 0.11114933448406421, 0.34555584814394136, 0.29620710842607245, 0.06180059476619531, 0.13582370434299862]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.049463409836728096, 0.11114933448406421, 0.34555584814394136, 0.29620710842607245, 0.06180059476619531, 0.13582370434299862]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  40.91149563051895
printing an ep nov before normalisation:  37.16858683534205
printing an ep nov before normalisation:  31.59843597047584
printing an ep nov before normalisation:  27.35892170828297
line 256 mcts: sample exp_bonus 37.707471986864434
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0520334582250429, 0.10394283563257066, 0.3634897226702094, 0.29860300091079967, 0.0520334582250429, 0.12989752433633453]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0520334582250429, 0.10394283563257066, 0.3634897226702094, 0.29860300091079967, 0.0520334582250429, 0.12989752433633453]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0520334582250429, 0.10394283563257066, 0.3634897226702094, 0.29860300091079967, 0.0520334582250429, 0.12989752433633453]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0520334582250429, 0.10394283563257066, 0.3634897226702094, 0.29860300091079967, 0.0520334582250429, 0.12989752433633453]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.583]
 [0.583]
 [0.583]
 [0.583]
 [0.583]] [[57.427]
 [57.427]
 [57.427]
 [57.427]
 [57.427]] [[1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05282643709530709, 0.103422094682578, 0.3690492970157504, 0.2931558106348439, 0.05282643709530709, 0.12871992347621347]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05282643709530709, 0.103422094682578, 0.3690492970157504, 0.2931558106348439, 0.05282643709530709, 0.12871992347621347]
Printing some Q and Qe and total Qs values:  [[1.371]
 [1.338]
 [1.367]
 [1.367]
 [1.357]] [[30.6  ]
 [40.755]
 [39.375]
 [39.375]
 [40.144]] [[1.818]
 [2.072]
 [2.062]
 [2.062]
 [2.074]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
printing an ep nov before normalisation:  21.399879504393695
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.241]
 [0.241]
 [0.241]
 [0.241]
 [0.241]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
Printing some Q and Qe and total Qs values:  [[0.084]
 [0.116]
 [0.178]
 [0.122]
 [0.121]] [[33.678]
 [26.524]
 [30.079]
 [27.063]
 [26.104]] [[0.935]
 [0.67 ]
 [0.88 ]
 [0.698]
 [0.658]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.093]
 [0.89 ]
 [0.085]
 [0.964]
 [0.943]] [[24.834]
 [21.726]
 [22.828]
 [35.547]
 [24.3  ]] [[0.425]
 [1.142]
 [0.365]
 [1.569]
 [1.261]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.0535802693501666, 0.10292706090645755, 0.3743344144660577, 0.28797752924254855, 0.0535802693501666, 0.12760045668460304]
printing an ep nov before normalisation:  39.239181304265465
printing an ep nov before normalisation:  36.454497223550995
actor:  1 policy actor:  1  step number:  113 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actions average: 
K:  3  action  0 :  tensor([    0.9991,     0.0004,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0015,     0.9200,     0.0008,     0.0001,     0.0776],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0145,     0.9709,     0.0007,     0.0138],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0011,     0.0188,     0.9180,     0.0617],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0121, 0.1043, 0.0949, 0.2188, 0.5699], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.052928429825084625, 0.10167338847147693, 0.3697706610266346, 0.28446698339544807, 0.052928429825084625, 0.13823210745627118]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.052928429825084625, 0.10167338847147693, 0.3697706610266346, 0.28446698339544807, 0.052928429825084625, 0.13823210745627118]
printing an ep nov before normalisation:  31.82723828671166
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0613],
        [0.6023],
        [0.4334],
        [0.1405],
        [0.3684],
        [0.1092],
        [0.1258],
        [0.3950],
        [0.0614],
        [0.5456]], dtype=torch.float64)
0.0 0.061309104754310256
0.0 0.6022643933956755
0.0 0.43335391845971877
0.0 0.14046085983116818
0.0 0.36838777714233534
0.0 0.10921213905442102
0.0 0.12579029126392438
0.0 0.3950228220765681
0.0 0.061435816551036686
0.0 0.5456008639929943
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05229229837934856, 0.10044992713190357, 0.36531688527095596, 0.29308044214212353, 0.05229229837934856, 0.13656814869631984]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05229229837934856, 0.10044992713190357, 0.36531688527095596, 0.29308044214212353, 0.05229229837934856, 0.13656814869631984]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
printing an ep nov before normalisation:  37.192131333572384
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.577]
 [0.577]
 [0.608]
 [0.577]] [[32.088]
 [27.529]
 [27.529]
 [28.565]
 [27.529]] [[0.653]
 [0.577]
 [0.577]
 [0.608]
 [0.577]]
printing an ep nov before normalisation:  40.949307856298795
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05302322442521229, 0.1000480970768486, 0.37044111482375724, 0.28814758768339377, 0.05302322442521229, 0.1353167515655758]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.9078743
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05240817706035501, 0.09888620673071907, 0.3661348773353124, 0.2964178328297663, 0.05240817706035501, 0.13374472898349216]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05240817706035501, 0.09888620673071907, 0.3661348773353124, 0.2964178328297663, 0.05240817706035501, 0.13374472898349216]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05240817706035501, 0.09888620673071907, 0.3661348773353124, 0.2964178328297663, 0.05240817706035501, 0.13374472898349216]
printing an ep nov before normalisation:  35.79918730611317
siam score:  -0.9092586
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05240817706035501, 0.09888620673071907, 0.3661348773353124, 0.2964178328297663, 0.05240817706035501, 0.13374472898349216]
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706465, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.1326001337468657]
printing an ep nov before normalisation:  56.24327955613067
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706465, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.1326001337468657]
printing an ep nov before normalisation:  10.540599016823649
printing an ep nov before normalisation:  55.88893187870212
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706465, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.1326001337468657]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.071]
 [0.002]
 [0.058]
 [0.072]] [[31.52 ]
 [29.706]
 [25.455]
 [23.162]
 [28.715]] [[0.686]
 [0.621]
 [0.395]
 [0.366]
 [0.585]]
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706465, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.1326001337468657]
siam score:  -0.9083511
siam score:  -0.90833783
printing an ep nov before normalisation:  65.51914240618639
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
printing an ep nov before normalisation:  48.60474266518158
printing an ep nov before normalisation:  30.48200581595304
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706464, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.13260013374686566]
line 256 mcts: sample exp_bonus 47.08259655235911
Starting evaluation
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05311155693399662, 0.09853360082706464, 0.3710658641854727, 0.2915772873726037, 0.05311155693399662, 0.13260013374686566]
printing an ep nov before normalisation:  42.318345312139016
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
printing an ep nov before normalisation:  40.5002901699939
printing an ep nov before normalisation:  27.702715396881104
printing an ep nov before normalisation:  33.1509981403305
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.4906099601285
printing an ep nov before normalisation:  36.16336627753901
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
printing an ep nov before normalisation:  32.51765704240428
Printing some Q and Qe and total Qs values:  [[0.428]
 [0.412]
 [0.428]
 [0.407]
 [0.418]] [[20.36 ]
 [21.775]
 [20.443]
 [20.196]
 [20.27 ]] [[0.428]
 [0.412]
 [0.428]
 [0.407]
 [0.418]]
printing an ep nov before normalisation:  25.379123947533166
printing an ep nov before normalisation:  37.56178081992224
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
maxi score, test score, baseline:  0.042100000000000005 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
printing an ep nov before normalisation:  31.730841163332528
printing an ep nov before normalisation:  35.7485032081604
printing an ep nov before normalisation:  30.867911723854874
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0441 0.35 0.35
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.06666577982514355
maxi score, test score, baseline:  0.0461 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
printing an ep nov before normalisation:  18.54687784950368
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  51.70174244731207
actions average: 
K:  2  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0003,     0.9778,     0.0005,     0.0007,     0.0208],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0013,     0.0003,     0.9255,     0.0309,     0.0420],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0097,     0.0006,     0.0125,     0.8997,     0.0775],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0019, 0.1284, 0.0824, 0.1217, 0.6657], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  49.895480646392826
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.050100000000000006 0.35 0.35
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.792668470329616
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  3  action  0 :  tensor([    0.9984,     0.0001,     0.0000,     0.0001,     0.0013],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9392,     0.0020,     0.0004,     0.0582],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0194,     0.9310,     0.0198,     0.0298],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0014,     0.0003,     0.0044,     0.9062,     0.0877],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0020, 0.1020, 0.0702, 0.1648, 0.6610], grad_fn=<DivBackward0>)
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.17 ]
 [0.319]
 [0.264]
 [0.231]
 [0.27 ]] [[31.562]
 [42.209]
 [35.729]
 [28.733]
 [30.32 ]] [[0.17 ]
 [0.319]
 [0.264]
 [0.231]
 [0.27 ]]
maxi score, test score, baseline:  0.06810000000000001 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.07010000000000001 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
maxi score, test score, baseline:  0.07010000000000001 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0721 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.   ]
 [0.261]
 [0.094]
 [0.074]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.062]
 [0.   ]
 [0.261]
 [0.094]
 [0.074]]
printing an ep nov before normalisation:  36.26333669426165
maxi score, test score, baseline:  0.0721 0.35 0.35
probs:  [0.05251624512529311, 0.09742788638747289, 0.3668977339605515, 0.2883023617517368, 0.05251624512529311, 0.14233952764965266]
actor:  0 policy actor:  0  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.90991193
siam score:  -0.9119345
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  77.07924564205194
maxi score, test score, baseline:  0.0741 0.8 0.8
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.053200834522894466, 0.09712309212693533, 0.3716372021521906, 0.2837926869441089, 0.053200834522894466, 0.14104534973097616]
printing an ep nov before normalisation:  43.62445241899799
printing an ep nov before normalisation:  27.60609770047202
Printing some Q and Qe and total Qs values:  [[1.23 ]
 [1.394]
 [1.23 ]
 [1.201]
 [1.23 ]] [[29.563]
 [37.037]
 [29.563]
 [32.543]
 [29.563]] [[2.112]
 [2.499]
 [2.112]
 [2.172]
 [2.112]]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[45.405]
 [45.405]
 [45.405]
 [45.405]
 [45.405]] [[2.833]
 [2.833]
 [2.833]
 [2.833]
 [2.833]]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0741 0.8 0.8
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.   ]
 [0.86 ]
 [0.86 ]
 [0.854]
 [0.86 ]] [[45.783]
 [28.974]
 [28.974]
 [52.167]
 [28.974]] [[1.45 ]
 [1.083]
 [1.083]
 [1.39 ]
 [1.083]]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.052623995157349925, 0.0960688223989944, 0.36759899265927237, 0.2915705449863946, 0.052623995157349925, 0.1395136496406389]
using explorer policy with actor:  1
printing an ep nov before normalisation:  23.54663372039795
actions average: 
K:  3  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0000],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0007,     0.9391,     0.0018,     0.0004,     0.0580],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0002,     0.0137,     0.9678,     0.0007,     0.0176],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0019,     0.0006,     0.0001,     0.8422,     0.1553],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0036, 0.1991, 0.0448, 0.0322, 0.7203], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.052623995157349925, 0.0960688223989944, 0.36759899265927237, 0.2915705449863946, 0.052623995157349925, 0.1395136496406389]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.053277857858623996, 0.09579866116164, 0.37218388263124397, 0.28714227602521203, 0.053277857858623996, 0.13831946446465598]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.053277857858623996, 0.09579866116164, 0.37218388263124397, 0.28714227602521203, 0.053277857858623996, 0.13831946446465598]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.053277857858623996, 0.09579866116164, 0.37218388263124397, 0.28714227602521203, 0.053277857858623996, 0.13831946446465598]
printing an ep nov before normalisation:  30.72886099225576
maxi score, test score, baseline:  0.0741 0.8 0.8
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  38.58625839987299
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970444, 0.05271841950625698, 0.13686481740932874]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970444, 0.05271841950625698, 0.13686481740932874]
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970444, 0.05271841950625698, 0.13686481740932874]
maxi score, test score, baseline:  0.0741 0.8 0.8
maxi score, test score, baseline:  0.0741 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
printing an ep nov before normalisation:  32.143333366771024
Printing some Q and Qe and total Qs values:  [[0.976]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]] [[42.033]
 [39.733]
 [39.733]
 [39.733]
 [39.733]] [[0.976]
 [0.97 ]
 [0.97 ]
 [0.97 ]
 [0.97 ]]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0761 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
maxi score, test score, baseline:  0.0761 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
maxi score, test score, baseline:  0.0761 0.8 0.8
printing an ep nov before normalisation:  44.36606944255044
maxi score, test score, baseline:  0.0761 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
siam score:  -0.9074118
maxi score, test score, baseline:  0.0761 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
printing an ep nov before normalisation:  42.832821982367264
actor:  0 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
printing an ep nov before normalisation:  58.01494239239183
printing an ep nov before normalisation:  53.80955734986001
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05271841950625698, 0.10530991819567684, 0.3682674116427761, 0.28412101373970433, 0.05271841950625698, 0.13686481740932874]
printing an ep nov before normalisation:  41.82159940878853
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.18563815101121
printing an ep nov before normalisation:  43.90948109992262
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.052170636556292484, 0.10421428660646258, 0.3644325368573129, 0.29157142678707487, 0.052170636556292484, 0.1354404766365646]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
siam score:  -0.9084724
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
Printing some Q and Qe and total Qs values:  [[0.913]
 [0.928]
 [0.868]
 [0.868]
 [0.913]] [[25.198]
 [33.911]
 [28.695]
 [28.695]
 [28.847]] [[0.913]
 [0.928]
 [0.868]
 [0.868]
 [0.913]]
maxi score, test score, baseline:  0.0781 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
actor:  0 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
printing an ep nov before normalisation:  22.617146968841553
printing an ep nov before normalisation:  18.978031873703003
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05280706405367376, 0.10378897567143176, 0.36889491608377345, 0.28732385749536066, 0.05280706405367376, 0.13437812264208657]
printing an ep nov before normalisation:  30.632987022399902
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0801 0.8 0.8
actions average: 
K:  1  action  0 :  tensor([    0.9965,     0.0003,     0.0000,     0.0016,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9400,     0.0073,     0.0012,     0.0514],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0003,     0.9690,     0.0000,     0.0306],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0002,     0.0013,     0.9747,     0.0237],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0017, 0.0855, 0.0493, 0.1665, 0.6970], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0801 0.8 0.8
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.059]
 [1.059]
 [1.059]] [[25.238]
 [25.238]
 [25.238]
 [25.238]
 [25.238]] [[1.415]
 [1.415]
 [1.415]
 [1.415]
 [1.415]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05289044372067377, 0.1023583667406707, 0.36948515104865404, 0.2804428896126596, 0.06278402832467316, 0.13203912055266884]
maxi score, test score, baseline:  0.0801 0.8 0.8
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05289044372067377, 0.1023583667406707, 0.36948515104865404, 0.2804428896126596, 0.06278402832467316, 0.13203912055266884]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05289044372067377, 0.1023583667406707, 0.36948515104865404, 0.2804428896126596, 0.06278402832467316, 0.13203912055266884]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05289044372067377, 0.1023583667406707, 0.36948515104865404, 0.2804428896126596, 0.06278402832467316, 0.13203912055266884]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05348201269479078, 0.10198972153988047, 0.37363289107238257, 0.2766174733822032, 0.06318355446380872, 0.13109434684693422]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05348201269479078, 0.10198972153988047, 0.37363289107238257, 0.2766174733822032, 0.06318355446380872, 0.13109434684693422]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05348201269479078, 0.10198972153988047, 0.37363289107238257, 0.2766174733822032, 0.06318355446380872, 0.13109434684693422]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05348201269479078, 0.10198972153988047, 0.37363289107238257, 0.2766174733822032, 0.06318355446380872, 0.13109434684693422]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05348201269479078, 0.10198972153988047, 0.37363289107238257, 0.2766174733822032, 0.06318355446380872, 0.13109434684693422]
printing an ep nov before normalisation:  48.79130204630797
printing an ep nov before normalisation:  0.005657211056728784
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0801 0.8 0.8
printing an ep nov before normalisation:  22.13439431202544
maxi score, test score, baseline:  0.0801 0.8 0.8
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05405105325735109, 0.10163511526128724, 0.37762267488411694, 0.2729377384754574, 0.06356786565813832, 0.1301855524636489]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05405105325735109, 0.10163511526128724, 0.37762267488411694, 0.2729377384754574, 0.06356786565813832, 0.1301855524636489]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05405105325735109, 0.10163511526128724, 0.37762267488411694, 0.2729377384754574, 0.06356786565813832, 0.1301855524636489]
Printing some Q and Qe and total Qs values:  [[0.265]
 [0.462]
 [0.265]
 [0.265]
 [0.265]] [[36.399]
 [51.004]
 [36.399]
 [36.399]
 [36.399]] [[0.598]
 [1.411]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05405105325735109, 0.10163511526128724, 0.37762267488411694, 0.2729377384754574, 0.06356786565813832, 0.1301855524636489]
siam score:  -0.9139116
printing an ep nov before normalisation:  35.86324063649799
printing an ep nov before normalisation:  63.43110385408308
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05405105325735109, 0.10163511526128724, 0.37762267488411694, 0.2729377384754574, 0.06356786565813832, 0.1301855524636489]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.9165091
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05354236190978965, 0.11010451428822816, 0.37406122538760794, 0.2703639460271372, 0.06296938730619607, 0.12895856508104098]
printing an ep nov before normalisation:  50.72843563804863
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.91641325
maxi score, test score, baseline:  0.0801 0.8 0.8
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
siam score:  -0.9192628
printing an ep nov before normalisation:  48.06069252279882
printing an ep nov before normalisation:  27.000366497487597
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
maxi score, test score, baseline:  0.0801 0.8 0.8
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
maxi score, test score, baseline:  0.0801 0.8 0.8
printing an ep nov before normalisation:  59.333699025626174
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
maxi score, test score, baseline:  0.0801 0.8 0.8
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
siam score:  -0.9219565
maxi score, test score, baseline:  0.0801 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
actor:  0 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
printing an ep nov before normalisation:  22.65690950549245
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05409448168065029, 0.10960953181074057, 0.3779322741061769, 0.2669021738459963, 0.06334699003566534, 0.12811454852077064]
actor:  1 policy actor:  1  step number:  86 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
printing an ep nov before normalisation:  39.69924834554557
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
printing an ep nov before normalisation:  36.717059041620445
Printing some Q and Qe and total Qs values:  [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.294]
 [0.294]
 [0.294]
 [0.294]
 [0.294]]
siam score:  -0.9255911
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
printing an ep nov before normalisation:  44.78184044227306
printing an ep nov before normalisation:  41.052713165815724
siam score:  -0.92736834
using explorer policy with actor:  1
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.1507],
        [0.0000],
        [0.1860],
        [0.4837],
        [0.1266],
        [0.2657],
        [0.5304],
        [0.7520],
        [0.2956],
        [0.4771]], dtype=torch.float64)
0.0 0.15074988325321106
0.0 0.0
0.0 0.18602319703937373
0.0 0.48371989822321126
0.0 0.12660489509203648
0.0 0.2656645079898432
0.0 0.5303956763591221
0.0 0.7519684908521608
0.0 0.29557084254198074
0.0 0.4770901250399006
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05462653086190962, 0.10913254287503465, 0.3816626029406599, 0.26356624357888897, 0.06371086619743047, 0.12730121354607632]
printing an ep nov before normalisation:  28.672095405909296
printing an ep nov before normalisation:  21.45377391537209
actor:  1 policy actor:  1  step number:  101 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05668802137733484, 0.10382172650133421, 0.3960506982701302, 0.264076323922932, 0.05668802137733484, 0.12267520855093393]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05668802137733484, 0.10382172650133421, 0.3960506982701302, 0.264076323922932, 0.05668802137733484, 0.12267520855093393]
Printing some Q and Qe and total Qs values:  [[0.737]
 [0.722]
 [0.745]
 [0.92 ]
 [0.737]] [[29.549]
 [16.392]
 [25.504]
 [33.29 ]
 [29.549]] [[1.344]
 [0.961]
 [1.238]
 [1.63 ]
 [1.344]]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05668802137733484, 0.10382172650133421, 0.3960506982701302, 0.264076323922932, 0.05668802137733484, 0.12267520855093393]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05668802137733484, 0.10382172650133421, 0.3960506982701302, 0.264076323922932, 0.05668802137733484, 0.12267520855093393]
maxi score, test score, baseline:  0.0821 0.8 0.8
probs:  [0.05668802137733484, 0.10382172650133421, 0.3960506982701302, 0.264076323922932, 0.05668802137733484, 0.12267520855093393]
printing an ep nov before normalisation:  23.862428665161133
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.249]
 [0.27 ]
 [0.249]
 [0.249]
 [0.249]] [[ 0.   ]
 [42.754]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.249]
 [0.27 ]
 [0.249]
 [0.249]
 [0.249]]
maxi score, test score, baseline:  0.0821 0.8 0.8
printing an ep nov before normalisation:  23.00824043567733
actor:  0 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057181892760434984, 0.10344306483349062, 0.39951456610104674, 0.26073104988187984, 0.057181892760434984, 0.1219475336627129]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057181892760434984, 0.10344306483349062, 0.39951456610104674, 0.26073104988187984, 0.057181892760434984, 0.1219475336627129]
using explorer policy with actor:  1
siam score:  -0.93234724
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057181892760434984, 0.10344306483349062, 0.39951456610104674, 0.26073104988187984, 0.057181892760434984, 0.1219475336627129]
Printing some Q and Qe and total Qs values:  [[0.97 ]
 [1.21 ]
 [1.013]
 [1.044]
 [1.088]] [[44.106]
 [30.956]
 [44.156]
 [41.963]
 [33.413]] [[2.32 ]
 [1.927]
 [2.366]
 [2.291]
 [1.924]]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
printing an ep nov before normalisation:  19.418634125218908
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057181892760434984, 0.10344306483349062, 0.39951456610104674, 0.26073104988187984, 0.057181892760434984, 0.1219475336627129]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.05665848939215593, 0.10249522992320208, 0.39585036932189743, 0.2675074958349682, 0.05665848939215593, 0.12082992613562052]
actions average: 
K:  2  action  0 :  tensor([0.9797, 0.0083, 0.0034, 0.0029, 0.0058], grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0004,     0.9819,     0.0004,     0.0002,     0.0172],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0000,     0.9592,     0.0227,     0.0181],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0003,     0.0038,     0.9777,     0.0182],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0010, 0.0763, 0.1179, 0.0943, 0.7106], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.412]
 [1.353]
 [1.353]
 [1.353]] [[29.115]
 [26.67 ]
 [29.115]
 [29.115]
 [29.115]] [[2.527]
 [2.412]
 [2.527]
 [2.527]
 [2.527]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.93678063
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057139547752023394, 0.10215069251146583, 0.39922424792378586, 0.2641908136454586, 0.057139547752023394, 0.1201551504152428]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057139547752023394, 0.10215069251146583, 0.39922424792378586, 0.2641908136454586, 0.057139547752023394, 0.1201551504152428]
printing an ep nov before normalisation:  33.09618949890137
printing an ep nov before normalisation:  36.73247795918312
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.057139547752023394, 0.10215069251146583, 0.39922424792378586, 0.2641908136454586, 0.057139547752023394, 0.1201551504152428]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.93892896
printing an ep nov before normalisation:  37.511166245566464
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.05613053992329087, 0.10034499062064117, 0.39216036522315323, 0.27720279341004245, 0.05613053992329087, 0.1180307708995813]
UNIT TEST: sample policy line 217 mcts : [0.231 0.026 0.41  0.231 0.103]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.05613053992329087, 0.10034499062064117, 0.39216036522315323, 0.27720279341004245, 0.05613053992329087, 0.1180307708995813]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.05613053992329087, 0.10034499062064117, 0.39216036522315323, 0.27720279341004245, 0.05613053992329087, 0.1180307708995813]
maxi score, test score, baseline:  0.08410000000000001 0.8 0.8
probs:  [0.05613053992329087, 0.10034499062064117, 0.39216036522315323, 0.27720279341004245, 0.05613053992329087, 0.1180307708995813]
actor:  0 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.137]
 [0.151]
 [0.192]
 [0.167]
 [0.164]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.137]
 [0.151]
 [0.192]
 [0.167]
 [0.164]]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05613053992329087, 0.10034499062064117, 0.39216036522315323, 0.27720279341004245, 0.05613053992329087, 0.1180307708995813]
Printing some Q and Qe and total Qs values:  [[1.061]
 [1.069]
 [1.069]
 [1.069]
 [1.069]] [[64.677]
 [40.72 ]
 [40.72 ]
 [40.72 ]
 [40.72 ]] [[1.945]
 [1.515]
 [1.515]
 [1.515]
 [1.515]]
maxi score, test score, baseline:  0.0861 0.8 0.8
printing an ep nov before normalisation:  31.2445874196195
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055639313032858254, 0.0994658999935721, 0.3887213739342836, 0.28353756522857027, 0.055639313032858254, 0.11699653477785762]
printing an ep nov before normalisation:  64.73300185211453
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055639313032858254, 0.0994658999935721, 0.3887213739342836, 0.28353756522857027, 0.055639313032858254, 0.11699653477785762]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055639313032858254, 0.0994658999935721, 0.3887213739342836, 0.28353756522857027, 0.055639313032858254, 0.11699653477785762]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.098]
 [1.098]
 [1.098]
 [1.098]] [[39.852]
 [32.204]
 [32.204]
 [32.204]
 [32.204]] [[2.142]
 [1.808]
 [1.808]
 [1.808]
 [1.808]]
actions average: 
K:  1  action  0 :  tensor([    0.9995,     0.0001,     0.0000,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9712,     0.0003,     0.0005,     0.0280],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9827,     0.0002,     0.0170],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0011,     0.0001,     0.0155,     0.7830,     0.2003],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0015, 0.0593, 0.1242, 0.1509, 0.6641], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  47.370290756225586
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055156629604785255, 0.09860209858993386, 0.3853421938919146, 0.28107306832755796, 0.06384572340181498, 0.1159802861839933]
printing an ep nov before normalisation:  36.98292255401611
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05468226867756188, 0.09775319098106372, 0.38202127818417586, 0.27865106465577144, 0.06329645313826227, 0.12359574436316484]
printing an ep nov before normalisation:  26.662267132148795
actions average: 
K:  3  action  0 :  tensor([    0.9999,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0003,     0.9856,     0.0001,     0.0002,     0.0138],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0138,     0.9074,     0.0222,     0.0565],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0002,     0.0184,     0.9568,     0.0243],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0048, 0.0275, 0.0205, 0.0906, 0.8566], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05468226867756188, 0.09775319098106372, 0.38202127818417586, 0.27865106465577144, 0.06329645313826227, 0.12359574436316484]
siam score:  -0.9391769
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054216016844253284, 0.10545935094054293, 0.37875713278742096, 0.27627046459484167, 0.06275657252696822, 0.12254046230597279]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054216016844253284, 0.10545935094054293, 0.37875713278742096, 0.27627046459484167, 0.06275657252696822, 0.12254046230597279]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054216016844253284, 0.10545935094054293, 0.37875713278742096, 0.27627046459484167, 0.06275657252696822, 0.12254046230597279]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054216016844253284, 0.10545935094054293, 0.37875713278742096, 0.27627046459484167, 0.06275657252696822, 0.12254046230597279]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  63.99800871013244
printing an ep nov before normalisation:  70.01589003092941
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055179343188262175, 0.10472926473421973, 0.38551215349464585, 0.26989566988741154, 0.06343766344592176, 0.12124590524953889]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055179343188262175, 0.10472926473421973, 0.38551215349464585, 0.26989566988741154, 0.06343766344592176, 0.12124590524953889]
using explorer policy with actor:  1
printing an ep nov before normalisation:  32.7737357005977
printing an ep nov before normalisation:  38.65193031261917
printing an ep nov before normalisation:  37.53079255822589
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055179343188262175, 0.10472926473421973, 0.38551215349464585, 0.26989566988741154, 0.06343766344592176, 0.12124590524953889]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055637518536451686, 0.10438202259361924, 0.3887249629270967, 0.2668637027841777, 0.06376160254597961, 0.12063019061267508]
printing an ep nov before normalisation:  44.98349278013894
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
actions average: 
K:  0  action  0 :  tensor([    0.9933,     0.0009,     0.0000,     0.0011,     0.0047],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9544,     0.0001,     0.0011,     0.0443],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0246,     0.9581,     0.0004,     0.0168],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0009,     0.0423,     0.8413,     0.1151],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0031, 0.0267, 0.0564, 0.2129, 0.7009], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055637518536451686, 0.10438202259361924, 0.3887249629270967, 0.2668637027841777, 0.06376160254597961, 0.12063019061267508]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05518987586589262, 0.10354125500839702, 0.38559096667300613, 0.27277108200716244, 0.06324843905631002, 0.11965838138923184]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773744, 0.10322112990156425, 0.38872604534452515, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773744, 0.10322112990156425, 0.38872604534452515, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
line 256 mcts: sample exp_bonus 29.003439882540825
printing an ep nov before normalisation:  55.80553081029044
siam score:  -0.9355832
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
printing an ep nov before normalisation:  53.94307821096669
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
printing an ep nov before normalisation:  28.90347035581435
Printing some Q and Qe and total Qs values:  [[0.895]
 [1.098]
 [0.895]
 [0.895]
 [0.926]] [[39.143]
 [42.12 ]
 [39.143]
 [39.143]
 [39.175]] [[1.733]
 [2.044]
 [1.733]
 [1.733]
 [1.766]]
Printing some Q and Qe and total Qs values:  [[0.317]
 [1.01 ]
 [0.871]
 [0.877]
 [0.89 ]] [[31.521]
 [35.545]
 [29.025]
 [34.044]
 [35.258]] [[0.916]
 [1.754]
 [1.379]
 [1.567]
 [1.625]]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05563697732773743, 0.10322112990156425, 0.3887260453445251, 0.26976566390995804, 0.06356766942337523, 0.11908251409283985]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055199911036415264, 0.10240936047981586, 0.3856660571402194, 0.275510675105618, 0.06306815261031536, 0.11814584362761604]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055199911036415264, 0.10240936047981586, 0.3856660571402194, 0.275510675105618, 0.06306815261031536, 0.11814584362761604]
Printing some Q and Qe and total Qs values:  [[0.978]
 [0.882]
 [0.882]
 [0.882]
 [0.882]] [[36.885]
 [32.136]
 [32.136]
 [32.136]
 [32.136]] [[2.079]
 [1.746]
 [1.746]
 [1.746]
 [1.746]]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055199911036415264, 0.10240936047981586, 0.3856660571402194, 0.275510675105618, 0.06306815261031536, 0.11814584362761604]
printing an ep nov before normalisation:  56.49531237961436
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054769674353729814, 0.10161027578705223, 0.3826538843869866, 0.2811659146147881, 0.06257644125928354, 0.11722380959815969]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054769674353729814, 0.10161027578705223, 0.3826538843869866, 0.2811659146147881, 0.06257644125928354, 0.11722380959815969]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054769674353729814, 0.10161027578705223, 0.3826538843869866, 0.2811659146147881, 0.06257644125928354, 0.11722380959815969]
maxi score, test score, baseline:  0.0861 0.8 0.8
siam score:  -0.9319449
printing an ep nov before normalisation:  41.417636093067856
Printing some Q and Qe and total Qs values:  [[0.463]
 [0.458]
 [0.442]
 [0.515]
 [0.458]] [[54.115]
 [50.652]
 [49.418]
 [49.86 ]
 [50.652]] [[1.196]
 [1.107]
 [1.061]
 [1.145]
 [1.107]]
printing an ep nov before normalisation:  47.92625276801269
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.054769674353729814, 0.10161027578705223, 0.3826538843869866, 0.2811659146147881, 0.06257644125928354, 0.11722380959815969]
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05520948313588757, 0.10132969701069271, 0.3857376825719911, 0.2781238501974457, 0.0628961854483551, 0.11670310163562776]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  40.2928933198307
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05520948313588757, 0.10132969701069271, 0.3857376825719911, 0.2781238501974457, 0.0628961854483551, 0.11670310163562776]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05520948313588757, 0.10132969701069271, 0.3857376825719911, 0.2781238501974457, 0.0628961854483551, 0.11670310163562776]
Printing some Q and Qe and total Qs values:  [[1.393]
 [1.281]
 [1.24 ]
 [1.103]
 [1.248]] [[33.451]
 [35.01 ]
 [37.364]
 [27.464]
 [36.286]] [[2.182]
 [2.149]
 [2.226]
 [1.591]
 [2.18 ]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
printing an ep nov before normalisation:  1.410965553643564
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.572]
 [0.626]
 [0.635]
 [0.665]] [[28.061]
 [26.089]
 [34.549]
 [35.314]
 [29.253]] [[1.349]
 [1.144]
 [1.551]
 [1.592]
 [1.369]]
printing an ep nov before normalisation:  35.778082424505115
printing an ep nov before normalisation:  42.100677166291234
Printing some Q and Qe and total Qs values:  [[0.017]
 [0.017]
 [0.012]
 [0.017]
 [0.017]] [[36.464]
 [36.464]
 [ 3.776]
 [36.464]
 [36.464]] [[0.971]
 [0.971]
 [0.012]
 [0.971]
 [0.971]]
printing an ep nov before normalisation:  72.8908225308789
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055635968697418124, 0.10105761786665617, 0.3887280626051637, 0.2751739396820686, 0.0632062435589578, 0.11619816758973552]
maxi score, test score, baseline:  0.0861 0.8 0.8
Printing some Q and Qe and total Qs values:  [[1.337]
 [1.377]
 [1.337]
 [1.337]
 [1.337]] [[35.247]
 [40.115]
 [35.247]
 [35.247]
 [35.247]] [[2.075]
 [2.298]
 [2.075]
 [2.075]
 [2.075]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055218623493948744, 0.10029873084471107, 0.385806077399539, 0.2806191602477603, 0.0627319747190758, 0.11532543329496514]
printing an ep nov before normalisation:  44.74026892982696
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055218623493948744, 0.10029873084471107, 0.385806077399539, 0.2806191602477603, 0.0627319747190758, 0.11532543329496514]
Printing some Q and Qe and total Qs values:  [[0.818]
 [0.623]
 [0.623]
 [0.623]
 [0.623]] [[52.735]
 [42.923]
 [42.923]
 [42.923]
 [42.923]] [[1.72 ]
 [1.272]
 [1.272]
 [1.272]
 [1.272]]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0026,     0.9117,     0.0283,     0.0003,     0.0572],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0262,     0.9269,     0.0114,     0.0355],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0049,     0.0003,     0.0119,     0.8456,     0.1373],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0112, 0.0900, 0.0262, 0.1670, 0.7056], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.055218623493948744, 0.10029873084471107, 0.385806077399539, 0.2806191602477603, 0.0627319747190758, 0.11532543329496514]
Printing some Q and Qe and total Qs values:  [[1.501]
 [1.5  ]
 [1.5  ]
 [1.502]
 [1.5  ]] [[ 9.807]
 [15.004]
 [ 8.051]
 [31.511]
 [10.276]] [[1.604]
 [1.658]
 [1.585]
 [1.835]
 [1.608]]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05651475143958062, 0.09585472116353992, 0.39483849106563057, 0.2846865758385445, 0.05651475143958062, 0.11159070905312363]
printing an ep nov before normalisation:  33.34552500187714
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05651475143958062, 0.09585472116353992, 0.39483849106563057, 0.2846865758385445, 0.05651475143958062, 0.11159070905312363]
printing an ep nov before normalisation:  17.571899083620917
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.20096778869629
printing an ep nov before normalisation:  62.072754518209365
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05693091036783424, 0.09566117729683392, 0.39775725934303136, 0.2815664585560323, 0.05693091036783424, 0.11115328406843379]
siam score:  -0.9257521
printing an ep nov before normalisation:  36.05663983228897
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05693091036783424, 0.09566117729683392, 0.39775725934303136, 0.2815664585560323, 0.05693091036783424, 0.11115328406843379]
printing an ep nov before normalisation:  0.43992038466484473
printing an ep nov before normalisation:  0.16608833177770066
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.095]
 [0.564]
 [0.621]
 [0.772]] [[31.831]
 [28.777]
 [21.225]
 [35.45 ]
 [28.46 ]] [[0.789]
 [0.095]
 [0.564]
 [0.621]
 [0.772]]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05733436665069906, 0.09547354107487381, 0.40058693646827176, 0.27854157831091253, 0.05733436665069906, 0.11072921084454371]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05733436665069906, 0.09547354107487381, 0.40058693646827176, 0.27854157831091253, 0.05733436665069906, 0.11072921084454371]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05733436665069906, 0.09547354107487381, 0.40058693646827176, 0.27854157831091253, 0.05733436665069906, 0.11072921084454371]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.05733436665069906, 0.09547354107487381, 0.40058693646827176, 0.27854157831091253, 0.05733436665069906, 0.11072921084454371]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
using explorer policy with actor:  1
UNIT TEST: sample policy line 217 mcts : [0.051 0.026 0.128 0.436 0.359]
maxi score, test score, baseline:  0.0861 0.8 0.8
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
printing an ep nov before normalisation:  40.37768978269596
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
printing an ep nov before normalisation:  45.63835501123988
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
printing an ep nov before normalisation:  52.99768803328242
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
maxi score, test score, baseline:  0.0861 0.8 0.8
probs:  [0.057725693140074544, 0.09529154608027873, 0.40333154018995304, 0.27560764019325873, 0.057725693140074544, 0.11031788725636038]
printing an ep nov before normalisation:  31.400516835909546
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  48.22477765418929
using another actor
from probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
printing an ep nov before normalisation:  36.16400948286174
actor:  0 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.157]
 [0.245]
 [0.157]
 [0.157]
 [0.157]] [[37.168]
 [67.969]
 [37.168]
 [37.168]
 [37.168]] [[0.157]
 [0.245]
 [0.157]
 [0.157]
 [0.157]]
printing an ep nov before normalisation:  57.08960398182565
maxi score, test score, baseline:  0.0881 0.8 0.8
maxi score, test score, baseline:  0.0881 0.8 0.8
printing an ep nov before normalisation:  32.68968454769865
rdn beta is 0 so we're just using the maxi policy
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.0881 0.8 0.8
maxi score, test score, baseline:  0.0881 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
printing an ep nov before normalisation:  30.859697102578217
printing an ep nov before normalisation:  36.99703740244267
actor:  0 policy actor:  0  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0901 0.8 0.8
from probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
from probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
printing an ep nov before normalisation:  30.737038841145832
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
printing an ep nov before normalisation:  50.46172855327375
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05810542875273994, 0.09511494167794222, 0.40599485024964155, 0.27276060371891325, 0.05810542875273994, 0.10991874684802314]
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
Printing some Q and Qe and total Qs values:  [[0.097]
 [0.082]
 [0.119]
 [0.112]
 [0.112]] [[27.86 ]
 [32.826]
 [31.811]
 [28.607]
 [28.607]] [[0.218]
 [0.246]
 [0.275]
 [0.239]
 [0.239]]
siam score:  -0.9083781
printing an ep nov before normalisation:  45.053848137199296
printing an ep nov before normalisation:  22.688454151543993
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0901 0.8 0.8
maxi score, test score, baseline:  0.0901 0.8 0.8
from probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
line 256 mcts: sample exp_bonus 0.0
maxi score, test score, baseline:  0.0901 0.8 0.8
probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
maxi score, test score, baseline:  0.0901 0.8 0.8
actor:  0 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0921 0.8 0.8
probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
from probs:  [0.05767913875851205, 0.09441650771631696, 0.40301040696187834, 0.2707558787137806, 0.06502661255007304, 0.10911145529943894]
maxi score, test score, baseline:  0.0921 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
maxi score, test score, baseline:  0.0921 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
actor:  0 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
printing an ep nov before normalisation:  54.495359645676444
printing an ep nov before normalisation:  74.17944885362735
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.057259072320851145, 0.093728270436123, 0.4000695346044065, 0.27607426101248217, 0.06455291194390551, 0.10831594968223171]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05763459350446815, 0.09357923300848965, 0.40270313274307445, 0.27330243052859704, 0.06482352140527245, 0.10795708881009823]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05763459350446815, 0.09357923300848965, 0.40270313274307445, 0.27330243052859704, 0.06482352140527245, 0.10795708881009823]
printing an ep nov before normalisation:  37.313103228672816
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05763459350446815, 0.09357923300848965, 0.40270313274307445, 0.27330243052859704, 0.06482352140527245, 0.10795708881009823]
printing an ep nov before normalisation:  23.231598610524987
printing an ep nov before normalisation:  33.50562320028965
maxi score, test score, baseline:  0.0941 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.734]
 [0.631]
 [0.689]
 [0.686]
 [0.62 ]] [[30.218]
 [32.198]
 [24.039]
 [42.539]
 [27.757]] [[1.112]
 [1.057]
 [0.916]
 [1.365]
 [0.937]]
printing an ep nov before normalisation:  38.30612956895977
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05763459350446815, 0.09357923300848965, 0.40270313274307445, 0.27330243052859704, 0.06482352140527245, 0.10795708881009823]
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.078927484982096
printing an ep nov before normalisation:  36.05898380279541
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05799946516838219, 0.09343442217869234, 0.4052620438694217, 0.2706092072302431, 0.06508645657044422, 0.10760840498281642]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05799946516838219, 0.09343442217869234, 0.4052620438694217, 0.2706092072302431, 0.06508645657044422, 0.10760840498281642]
printing an ep nov before normalisation:  29.60433389930291
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05799946516838219, 0.09343442217869234, 0.4052620438694217, 0.2706092072302431, 0.06508645657044422, 0.10760840498281642]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05799946516838219, 0.09343442217869234, 0.4052620438694217, 0.2706092072302431, 0.06508645657044422, 0.10760840498281642]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05799946516838219, 0.09343442217869234, 0.4052620438694217, 0.2706092072302431, 0.06508645657044422, 0.10760840498281642]
actor:  1 policy actor:  1  step number:  109 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
printing an ep nov before normalisation:  36.21952333655317
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
printing an ep nov before normalisation:  43.935809020796725
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.386]
 [0.475]
 [0.386]
 [0.386]
 [0.386]] [[50.776]
 [47.356]
 [50.776]
 [50.776]
 [50.776]] [[1.401]
 [1.36 ]
 [1.401]
 [1.401]
 [1.401]]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05759193006456286, 0.0927773289684673, 0.4024088393228265, 0.2757414032687704, 0.06462900984534375, 0.10685148853002908]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 33.0990875871286
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
actions average: 
K:  0  action  0 :  tensor([    0.9981,     0.0010,     0.0000,     0.0001,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0005,     0.8993,     0.0165,     0.0018,     0.0819],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0131,     0.9003,     0.0146,     0.0715],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0015,     0.0004,     0.0003,     0.9711,     0.0267],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0023, 0.0366, 0.0243, 0.0815, 0.8553], grad_fn=<DivBackward0>)
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0000,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0007,     0.9137,     0.0104,     0.0013,     0.0740],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0079,     0.9603,     0.0021,     0.0296],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0004,     0.0003,     0.0007,     0.9716,     0.0271],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0192, 0.0748, 0.0395, 0.1158, 0.7506], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
printing an ep nov before normalisation:  42.226403868229134
printing an ep nov before normalisation:  53.16943270442586
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
Printing some Q and Qe and total Qs values:  [[0.442]
 [0.442]
 [0.442]
 [0.442]
 [0.442]] [[46.293]
 [46.293]
 [46.293]
 [46.293]
 [46.293]] [[1.028]
 [1.028]
 [1.028]
 [1.028]
 [1.028]]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
actions average: 
K:  1  action  0 :  tensor([    0.9830,     0.0118,     0.0000,     0.0001,     0.0052],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9328,     0.0126,     0.0003,     0.0541],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0140,     0.9565,     0.0001,     0.0292],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0007,     0.0003,     0.9136,     0.0852],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0044, 0.1310, 0.0501, 0.0387, 0.7756], grad_fn=<DivBackward0>)
siam score:  -0.9181406
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
printing an ep nov before normalisation:  27.612524032592773
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.10610515900190909]
siam score:  -0.91763717
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.734]
 [0.754]
 [0.754]
 [0.754]] [[38.756]
 [48.396]
 [38.756]
 [38.756]
 [38.756]] [[0.987]
 [1.067]
 [0.987]
 [0.987]
 [0.987]]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.1061051590019091]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.1061051590019091]
maxi score, test score, baseline:  0.0941 0.8 0.8
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05719009511883566, 0.09212942646388811, 0.39959554230034977, 0.28080181572717133, 0.06417796138784615, 0.1061051590019091]
line 256 mcts: sample exp_bonus 34.99616513681176
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05679384157032555, 0.0984298595015706, 0.3968213213421602, 0.2788526038702991, 0.06373317789219972, 0.10536919582344478]
printing an ep nov before normalisation:  47.54659700766028
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05679384157032555, 0.0984298595015706, 0.3968213213421602, 0.2788526038702991, 0.06373317789219972, 0.10536919582344478]
maxi score, test score, baseline:  0.0941 0.8 0.8
printing an ep nov before normalisation:  57.96979766693288
maxi score, test score, baseline:  0.0941 0.8 0.8
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05679384157032555, 0.0984298595015706, 0.3968213213421602, 0.2788526038702992, 0.06373317789219972, 0.1053691958234448]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05679384157032555, 0.0984298595015706, 0.3968213213421602, 0.2788526038702992, 0.06373317789219972, 0.1053691958234448]
printing an ep nov before normalisation:  43.89323590942476
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05679384157032555, 0.0984298595015706, 0.3968213213421602, 0.2788526038702992, 0.06373317789219972, 0.1053691958234448]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0941 0.8 0.8
line 256 mcts: sample exp_bonus 70.73261525677698
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
actions average: 
K:  0  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9642,     0.0002,     0.0003,     0.0351],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0139,     0.8915,     0.0155,     0.0788],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0004,     0.0164,     0.8659,     0.1168],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0011, 0.0624, 0.0249, 0.1923, 0.7192], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
printing an ep nov before normalisation:  60.749692292566216
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
printing an ep nov before normalisation:  45.45123024511654
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
maxi score, test score, baseline:  0.0941 0.8 0.8
probs:  [0.05715773210646824, 0.09822358256654264, 0.3993731526070884, 0.276175601226865, 0.06400204051648065, 0.10506789097655504]
printing an ep nov before normalisation:  40.283743550945886
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.0941 0.8 0.8
Printing some Q and Qe and total Qs values:  [[0.094]
 [0.209]
 [0.149]
 [0.135]
 [0.167]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.094]
 [0.209]
 [0.149]
 [0.135]
 [0.167]]
using another actor
printing an ep nov before normalisation:  45.550282055152415
actor:  0 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.246684001364194
maxi score, test score, baseline:  0.0961 0.8 0.8
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
actions average: 
K:  0  action  0 :  tensor([    0.9981,     0.0000,     0.0001,     0.0010,     0.0007],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9510,     0.0097,     0.0004,     0.0387],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0003,     0.0000,     0.9820,     0.0006,     0.0171],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0002,     0.0446,     0.8415,     0.1137],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0571,     0.0019,     0.2630,     0.6774],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  64.15252989833785
maxi score, test score, baseline:  0.0961 0.8 0.8
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
printing an ep nov before normalisation:  38.434064153135935
using another actor
maxi score, test score, baseline:  0.0961 0.8 0.8
printing an ep nov before normalisation:  32.70697928978849
printing an ep nov before normalisation:  34.81715120504578
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
maxi score, test score, baseline:  0.0961 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
printing an ep nov before normalisation:  44.51623959029367
printing an ep nov before normalisation:  42.621968141369784
actor:  0 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  54.44600189638045
printing an ep nov before normalisation:  37.35774082571761
printing an ep nov before normalisation:  29.781752655108257
printing an ep nov before normalisation:  34.30014301559752
Printing some Q and Qe and total Qs values:  [[1.281]
 [1.275]
 [1.281]
 [1.281]
 [1.281]] [[37.527]
 [39.265]
 [37.527]
 [37.527]
 [37.527]] [[1.87 ]
 [1.915]
 [1.87 ]
 [1.87 ]
 [1.87 ]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.058003161635546384, 0.10052366360424564, 0.4052539277132569, 0.2776924218071591, 0.058003161635546384, 0.10052366360424564]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.00424848132809
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.27573771906862227, 0.05759561426471098, 0.09981666680740348]
maxi score, test score, baseline:  0.0981 0.8 0.8
printing an ep nov before normalisation:  57.05911893761915
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
printing an ep nov before normalisation:  29.873160483166977
actions average: 
K:  0  action  0 :  tensor([    0.9095,     0.0325,     0.0000,     0.0015,     0.0565],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0023,     0.9451,     0.0101,     0.0004,     0.0421],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([0.0030, 0.0307, 0.9155, 0.0193, 0.0315], grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0005,     0.0002,     0.0138,     0.9331,     0.0525],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0049, 0.1633, 0.0191, 0.2295, 0.5833], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0981 0.8 0.8
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
printing an ep nov before normalisation:  35.948143481550005
printing an ep nov before normalisation:  40.17721128183966
Printing some Q and Qe and total Qs values:  [[1.012]
 [0.859]
 [0.885]
 [0.924]
 [0.933]] [[35.911]
 [23.759]
 [24.574]
 [24.666]
 [36.595]] [[1.588]
 [1.152]
 [1.197]
 [1.238]
 [1.525]]
printing an ep nov before normalisation:  27.347109922913038
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
printing an ep nov before normalisation:  38.21923494338989
Printing some Q and Qe and total Qs values:  [[0.049]
 [0.393]
 [0.505]
 [0.264]
 [0.348]] [[37.53 ]
 [45.261]
 [38.45 ]
 [29.907]
 [37.429]] [[0.775]
 [1.377]
 [1.261]
 [0.736]
 [1.07 ]]
siam score:  -0.91871184
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
printing an ep nov before normalisation:  47.139351380382536
printing an ep nov before normalisation:  33.310962554489215
using explorer policy with actor:  1
printing an ep nov before normalisation:  22.25358866535715
maxi score, test score, baseline:  0.0981 0.8 0.8
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
Printing some Q and Qe and total Qs values:  [[0.319]
 [0.319]
 [0.319]
 [0.319]
 [0.319]] [[56.618]
 [56.618]
 [56.618]
 [56.618]
 [56.618]] [[2.089]
 [2.089]
 [2.089]
 [2.089]
 [2.089]]
maxi score, test score, baseline:  0.0981 0.8 0.8
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
printing an ep nov before normalisation:  41.25667651853667
printing an ep nov before normalisation:  26.44909143447876
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0981 0.8 0.8
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05759561426471098, 0.10685350889785225, 0.4024008766966999, 0.2757377190686223, 0.05759561426471098, 0.09981666680740348]
maxi score, test score, baseline:  0.0981 0.8 0.8
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  40.015885548254744
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05795341521522911, 0.10652742118289271, 0.40491060069854046, 0.27306687021488213, 0.05795341521522911, 0.09958827747322647]
printing an ep nov before normalisation:  30.271210397602065
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
main train batch thing paused
add a thread
from probs:  [0.05795341521522911, 0.10652742118289271, 0.40491060069854046, 0.27306687021488213, 0.05795341521522911, 0.09958827747322647]
Adding thread: now have 2 threads
UNIT TEST: sample policy line 217 mcts : [0.282 0.308 0.026 0.026 0.359]
printing an ep nov before normalisation:  37.625552656186045
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05790569880281365, 0.10548862224324937, 0.4045812838688452, 0.27542763453051966, 0.05790569880281365, 0.09869106175175853]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.9979,     0.0002,     0.0000,     0.0002,     0.0018],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.8969,     0.0078,     0.0001,     0.0950],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0006,     0.0063,     0.9690,     0.0029,     0.0212],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0003,     0.0002,     0.0015,     0.9478,     0.0501],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0031, 0.1254, 0.0843, 0.1343, 0.6528], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05824733556699509, 0.10519178820809001, 0.40697755518655726, 0.27285054764057176, 0.05824733556699509, 0.09848543783079072]
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05824733556699509, 0.10519178820809001, 0.40697755518655726, 0.27285054764057176, 0.05824733556699509, 0.09848543783079072]
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05824733556699509, 0.10519178820809001, 0.40697755518655726, 0.27285054764057176, 0.05824733556699509, 0.09848543783079072]
printing an ep nov before normalisation:  43.304616515167695
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.128]
 [1.128]
 [1.128]
 [1.128]
 [1.128]] [[31.194]
 [31.194]
 [31.194]
 [31.194]
 [31.194]] [[1.624]
 [1.624]
 [1.624]
 [1.624]
 [1.624]]
printing an ep nov before normalisation:  0.03126420544532493
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05857992551279437, 0.10490281457873965, 0.4093103712978085, 0.2703417040999727, 0.05857992551279437, 0.09828525899789028]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.0589038232950612, 0.10462139321028778, 0.4115822197839518, 0.26789842862181124, 0.0589038232950612, 0.09809031179382681]
printing an ep nov before normalisation:  45.39144668993832
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[1.096]
 [0.821]
 [0.962]
 [0.981]
 [0.943]] [[32.839]
 [25.311]
 [21.74 ]
 [24.031]
 [22.258]] [[1.688]
 [1.194]
 [1.232]
 [1.317]
 [1.228]]
printing an ep nov before normalisation:  45.79171180725098
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.399]
 [1.394]
 [1.316]
 [1.254]
 [1.362]] [[19.753]
 [18.997]
 [19.232]
 [19.006]
 [19.213]] [[2.146]
 [2.112]
 [2.043]
 [1.973]
 [2.089]]
actions average: 
K:  2  action  0 :  tensor([    0.9675,     0.0141,     0.0000,     0.0000,     0.0183],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9852,     0.0003,     0.0002,     0.0143],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0008,     0.0181,     0.9319,     0.0153,     0.0339],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0012,     0.0002,     0.0004,     0.9162,     0.0820],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0080, 0.0542, 0.0826, 0.1000, 0.7552], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05921936527010962, 0.10434723185666359, 0.41379545987874783, 0.26551818395149906, 0.05921936527010962, 0.09790039377287015]
Printing some Q and Qe and total Qs values:  [[1.502]
 [1.502]
 [1.5  ]
 [1.501]
 [1.502]] [[4.907]
 [7.244]
 [5.316]
 [6.466]
 [4.458]] [[1.705]
 [1.803]
 [1.72 ]
 [1.769]
 [1.686]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05884058565148731, 0.10367915399443318, 0.4111436226317763, 0.27022240783966067, 0.05884058565148731, 0.09727364423115518]
printing an ep nov before normalisation:  52.51497444593532
printing an ep nov before normalisation:  49.90660944221361
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05884058565148731, 0.10367915399443318, 0.4111436226317763, 0.27022240783966067, 0.05884058565148731, 0.09727364423115518]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  39.721967319915
siam score:  -0.91602445
maxi score, test score, baseline:  0.0981 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
Printing some Q and Qe and total Qs values:  [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]] [[43.203]
 [43.203]
 [43.203]
 [43.203]
 [43.203]] [[0.924]
 [0.924]
 [0.924]
 [0.924]
 [0.924]]
printing an ep nov before normalisation:  24.65765361241424
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
printing an ep nov before normalisation:  61.91558979541135
printing an ep nov before normalisation:  47.97724539528228
printing an ep nov before normalisation:  44.97297381500028
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.064]
 [1.064]
 [1.062]
 [1.064]] [[40.118]
 [31.635]
 [31.635]
 [43.128]
 [31.635]] [[1.674]
 [1.41 ]
 [1.41 ]
 [1.648]
 [1.41 ]]
printing an ep nov before normalisation:  37.02920260276321
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05846663153347879, 0.10938429512556722, 0.40852556872908663, 0.2685019938508434, 0.05846663153347879, 0.0966548792275451]
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
printing an ep nov before normalisation:  37.52909226826341
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.058779733848173976, 0.10905713943349096, 0.4107215729453929, 0.2661740318876065, 0.058779733848173976, 0.0964877880371617]
Printing some Q and Qe and total Qs values:  [[0.2  ]
 [0.231]
 [0.268]
 [0.224]
 [0.316]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.2  ]
 [0.231]
 [0.268]
 [0.224]
 [0.316]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.059085059811235664, 0.10873810912912692, 0.4128630362012108, 0.263903888247537, 0.059085059811235664, 0.09632484679965408]
printing an ep nov before normalisation:  38.672542553038724
line 256 mcts: sample exp_bonus 0.0
printing an ep nov before normalisation:  49.57623774345215
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.059085059811235664, 0.10873810912912692, 0.4128630362012108, 0.263903888247537, 0.059085059811235664, 0.09632484679965408]
printing an ep nov before normalisation:  82.23165460071682
printing an ep nov before normalisation:  34.10631456546055
line 256 mcts: sample exp_bonus 42.93438345017764
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using another actor
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.05872113536492992, 0.10806766396000957, 0.41031515160487253, 0.2684438818940184, 0.05872113536492992, 0.09573103181123965]
Printing some Q and Qe and total Qs values:  [[0.123]
 [0.123]
 [0.146]
 [0.123]
 [0.123]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.123]
 [0.123]
 [0.146]
 [0.123]
 [0.123]]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
siam score:  -0.91519713
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
printing an ep nov before normalisation:  52.27683788851184
maxi score, test score, baseline:  0.10010000000000001 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
Printing some Q and Qe and total Qs values:  [[0.87 ]
 [0.842]
 [0.87 ]
 [0.87 ]
 [0.87 ]] [[34.955]
 [37.814]
 [34.955]
 [34.955]
 [34.955]] [[0.87 ]
 [0.842]
 [0.87 ]
 [0.87 ]
 [0.87 ]]
actor:  0 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  52.328069651780076
printing an ep nov before normalisation:  60.63824885210581
siam score:  -0.91697776
maxi score, test score, baseline:  0.1021 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
actor:  0 policy actor:  0  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05836167654489917, 0.10740544565664294, 0.40779853146607364, 0.2667976952698102, 0.06449214768386716, 0.095144503378707]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05836167654489916, 0.10740544565664294, 0.4077985314660736, 0.2667976952698102, 0.06449214768386714, 0.095144503378707]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.547]
 [0.727]
 [0.561]
 [0.672]
 [0.636]] [[49.318]
 [36.523]
 [60.64 ]
 [50.764]
 [50.649]] [[1.266]
 [1.128]
 [1.561]
 [1.427]
 [1.388]]
printing an ep nov before normalisation:  45.124419447551986
siam score:  -0.91365373
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05866466731750488, 0.10711416235264287, 0.4099235063222554, 0.26457502121684134, 0.06472085419689713, 0.09500178859385837]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05866466731750488, 0.10711416235264287, 0.4099235063222554, 0.26457502121684134, 0.06472085419689713, 0.09500178859385837]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.05866466731750488, 0.10711416235264287, 0.4099235063222554, 0.26457502121684134, 0.06472085419689713, 0.09500178859385837]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05896040318842453, 0.10682985362319883, 0.41199760014488485, 0.26240556753621525, 0.06494408449277132, 0.09486249101450525]
Printing some Q and Qe and total Qs values:  [[0.593]
 [0.593]
 [0.593]
 [0.661]
 [0.579]] [[30.406]
 [30.406]
 [30.406]
 [36.161]
 [29.598]] [[0.593]
 [0.593]
 [0.593]
 [0.661]
 [0.579]]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05896040318842453, 0.10682985362319883, 0.41199760014488485, 0.26240556753621525, 0.06494408449277132, 0.09486249101450525]
printing an ep nov before normalisation:  65.87417198591973
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05896040318842453, 0.10682985362319883, 0.41199760014488485, 0.26240556753621525, 0.06494408449277132, 0.09486249101450525]
maxi score, test score, baseline:  0.1041 0.8 0.8
probs:  [0.05896040318842453, 0.10682985362319883, 0.41199760014488485, 0.26240556753621525, 0.06494408449277132, 0.09486249101450525]
printing an ep nov before normalisation:  0.05572309504259465
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  0
using explorer policy with actor:  1
printing an ep nov before normalisation:  34.982692281358204
maxi score, test score, baseline:  0.1061 0.8 0.8
printing an ep nov before normalisation:  26.117992401123047
maxi score, test score, baseline:  0.1061 0.8 0.8
actor:  0 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.186]
 [0.09 ]
 [0.146]
 [0.186]
 [0.186]] [[ 0.   ]
 [26.622]
 [27.445]
 [ 0.   ]
 [ 0.   ]] [[0.186]
 [0.09 ]
 [0.146]
 [0.186]
 [0.186]]
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05896040318842453, 0.1068298536231988, 0.41199760014488485, 0.2624055675362152, 0.06494408449277131, 0.09486249101450522]
using another actor
Printing some Q and Qe and total Qs values:  [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]] [[76.179]
 [76.179]
 [76.179]
 [76.179]
 [76.179]] [[0.191]
 [0.191]
 [0.191]
 [0.191]
 [0.191]]
printing an ep nov before normalisation:  52.36586392760796
printing an ep nov before normalisation:  37.931430663563276
maxi score, test score, baseline:  0.1081 0.8 0.8
printing an ep nov before normalisation:  37.438340152882574
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05896040318842453, 0.1068298536231988, 0.41199760014488485, 0.2624055675362152, 0.06494408449277131, 0.09486249101450522]
actor:  1 policy actor:  1  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  0
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05890137398788796, 0.1059262289749914, 0.4115877863911637, 0.2587570076830776, 0.07065758773466382, 0.09417001522821554]
printing an ep nov before normalisation:  55.20857785712041
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05890137398788796, 0.1059262289749914, 0.4115877863911637, 0.2587570076830776, 0.07065758773466382, 0.09417001522821554]
using explorer policy with actor:  0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  49.414184247924524
printing an ep nov before normalisation:  51.91881170684428
actions average: 
K:  0  action  0 :  tensor([    0.9646,     0.0152,     0.0000,     0.0011,     0.0191],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9503,     0.0091,     0.0012,     0.0392],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0000,     0.9813,     0.0096,     0.0088],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0027,     0.0004,     0.0128,     0.8402,     0.1439],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0260, 0.1482, 0.0017, 0.0767, 0.7474], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05890137398788796, 0.1059262289749914, 0.4115877863911637, 0.2587570076830776, 0.07065758773466382, 0.09417001522821554]
printing an ep nov before normalisation:  0.7878347969062816
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  80.99758109313962
maxi score, test score, baseline:  0.1081 0.8 0.8
from probs:  [0.05855767410222786, 0.1053075087246879, 0.40918143377067806, 0.25724447124768296, 0.07024513275784287, 0.09946377939688042]
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.05855767410222786, 0.1053075087246879, 0.40918143377067806, 0.25724447124768296, 0.07024513275784287, 0.09946377939688042]
from probs:  [0.05855767410222786, 0.1053075087246879, 0.40918143377067806, 0.25724447124768296, 0.07024513275784287, 0.09946377939688042]
maxi score, test score, baseline:  0.1081 0.8 0.8
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  39.78691164788314
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.058844391662873634, 0.10505393809307066, 0.4111921831931259, 0.25523496399121093, 0.0703967782704229, 0.09927774478929605]
Printing some Q and Qe and total Qs values:  [[0.891]
 [0.838]
 [0.831]
 [0.831]
 [0.77 ]] [[32.524]
 [36.952]
 [41.939]
 [41.939]
 [41.552]] [[0.891]
 [0.838]
 [0.831]
 [0.831]
 [0.77 ]]
maxi score, test score, baseline:  0.1081 0.8 0.8
maxi score, test score, baseline:  0.1081 0.8 0.8
maxi score, test score, baseline:  0.1081 0.8 0.8
probs:  [0.058844391662873634, 0.10505393809307066, 0.4111921831931259, 0.25523496399121093, 0.0703967782704229, 0.09927774478929605]
actor:  0 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.601399119719495
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.058844391662873634, 0.10505393809307066, 0.4111921831931259, 0.25523496399121093, 0.0703967782704229, 0.09927774478929605]
line 256 mcts: sample exp_bonus 25.043957199054447
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.059124557744973306, 0.10480616153471915, 0.4131569871155037, 0.25327137385139314, 0.07054495869240977, 0.09909596106100095]
printing an ep nov before normalisation:  28.54339361190796
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.059124557744973306, 0.10480616153471915, 0.4131569871155037, 0.25327137385139314, 0.07054495869240977, 0.09909596106100095]
printing an ep nov before normalisation:  34.80403661727905
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.059124557744973306, 0.10480616153471915, 0.4131569871155037, 0.25327137385139314, 0.07054495869240977, 0.09909596106100095]
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
Starting evaluation
printing an ep nov before normalisation:  42.83802017971458
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
printing an ep nov before normalisation:  28.523963689804077
printing an ep nov before normalisation:  59.43820956977945
printing an ep nov before normalisation:  38.97233593245144
printing an ep nov before normalisation:  44.62314570706229
Printing some Q and Qe and total Qs values:  [[0.533]
 [0.533]
 [0.002]
 [0.627]
 [0.553]] [[31.12 ]
 [31.12 ]
 [29.142]
 [34.419]
 [28.446]] [[0.533]
 [0.533]
 [0.002]
 [0.627]
 [0.553]]
printing an ep nov before normalisation:  39.87119566465547
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
printing an ep nov before normalisation:  40.63725948067017
printing an ep nov before normalisation:  42.367506065647056
printing an ep nov before normalisation:  51.05027955427306
Printing some Q and Qe and total Qs values:  [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]] [[29.537]
 [29.537]
 [29.537]
 [29.537]
 [29.537]] [[0.653]
 [0.653]
 [0.653]
 [0.653]
 [0.653]]
maxi score, test score, baseline:  0.1101 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.57876462479424
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.36794227889964
maxi score, test score, baseline:  0.1261 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1321 0.8 0.8
probs:  [0.05939839436333458, 0.10456398270157968, 0.4150774025270147, 0.2513521448008762, 0.07068979144789586, 0.09891828415929906]
actor:  0 policy actor:  0  step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  21.020691394805908
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.9201426
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
printing an ep nov before normalisation:  32.776031494140625
printing an ep nov before normalisation:  27.692815995981825
line 256 mcts: sample exp_bonus 52.929413756811535
siam score:  -0.9183824
printing an ep nov before normalisation:  48.34526962295228
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.256078954206366
Printing some Q and Qe and total Qs values:  [[0.537]
 [0.537]
 [0.537]
 [0.537]
 [0.537]] [[60.695]
 [60.695]
 [60.695]
 [60.695]
 [60.695]] [[1.825]
 [1.825]
 [1.825]
 [1.825]
 [1.825]]
printing an ep nov before normalisation:  58.01430047889226
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.81889943425339
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.349]
 [0.349]
 [0.349]
 [0.349]
 [0.349]] [[51.089]
 [51.089]
 [51.089]
 [51.089]
 [51.089]] [[0.726]
 [0.726]
 [0.726]
 [0.726]
 [0.726]]
printing an ep nov before normalisation:  61.23198022966842
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.353]
 [1.353]
 [1.353]
 [1.353]
 [1.353]] [[43.724]
 [43.724]
 [43.724]
 [43.724]
 [43.724]] [[1.739]
 [1.739]
 [1.739]
 [1.739]
 [1.739]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  22.42655971075814
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
printing an ep nov before normalisation:  24.399538151050404
using explorer policy with actor:  1
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  73.86957479608752
Printing some Q and Qe and total Qs values:  [[1.091]
 [1.176]
 [1.091]
 [1.091]
 [1.105]] [[32.795]
 [34.355]
 [32.795]
 [32.795]
 [40.684]] [[1.592]
 [1.741]
 [1.592]
 [1.592]
 [1.934]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
printing an ep nov before normalisation:  36.587485671043396
printing an ep nov before normalisation:  59.78206492169764
printing an ep nov before normalisation:  44.86981178110434
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.542]
 [0.542]
 [0.551]
 [0.542]
 [0.542]] [[37.389]
 [37.389]
 [44.414]
 [37.389]
 [37.389]] [[1.009]
 [1.009]
 [1.182]
 [1.009]
 [1.009]]
printing an ep nov before normalisation:  38.20116768485002
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.09791898727417
printing an ep nov before normalisation:  30.89934310082524
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.357]
 [0.357]
 [0.357]
 [0.357]
 [0.357]] [[47.693]
 [47.693]
 [47.693]
 [47.693]
 [47.693]] [[1.803]
 [1.803]
 [1.803]
 [1.803]
 [1.803]]
Printing some Q and Qe and total Qs values:  [[1.325]
 [1.258]
 [1.356]
 [1.285]
 [1.27 ]] [[29.425]
 [30.057]
 [33.051]
 [28.767]
 [31.384]] [[1.782]
 [1.736]
 [1.934]
 [1.72 ]
 [1.792]]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.618]
 [0.618]
 [0.618]
 [0.618]] [[42.995]
 [42.995]
 [42.995]
 [42.995]
 [42.995]] [[1.944]
 [1.944]
 [1.944]
 [1.944]
 [1.944]]
printing an ep nov before normalisation:  54.27616061190129
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.897]
 [0.875]
 [0.897]] [[29.326]
 [29.326]
 [29.326]
 [34.041]
 [29.326]] [[1.558]
 [1.558]
 [1.558]
 [1.709]
 [1.558]]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.41711632379492
printing an ep nov before normalisation:  57.95409512212467
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.91618365
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  23.97350311279297
printing an ep nov before normalisation:  60.646850930578374
printing an ep nov before normalisation:  37.16385364532471
Printing some Q and Qe and total Qs values:  [[1.337]
 [1.292]
 [1.337]
 [1.337]
 [1.33 ]] [[31.117]
 [35.218]
 [31.117]
 [31.117]
 [30.395]] [[2.373]
 [2.551]
 [2.373]
 [2.373]
 [2.327]]
printing an ep nov before normalisation:  27.825141085772792
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.143]
 [0.139]
 [0.143]
 [0.143]
 [0.143]] [[43.132]
 [56.816]
 [43.132]
 [43.132]
 [43.132]] [[1.036]
 [1.472]
 [1.036]
 [1.036]
 [1.036]]
siam score:  -0.9131517
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.774982210597166
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.82427327499427
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.556]
 [0.612]
 [0.556]
 [0.55 ]
 [0.556]] [[29.711]
 [36.403]
 [29.711]
 [30.817]
 [29.711]] [[1.602]
 [1.894]
 [1.602]
 [1.635]
 [1.602]]
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  30.396758290000214
printing an ep nov before normalisation:  43.30882391215706
printing an ep nov before normalisation:  21.370913198905647
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  24.98905176612021
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.15209999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.663129552150814
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  44.0804766334898
printing an ep nov before normalisation:  29.966630935668945
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 50.66223868213261
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 38.63777145261347
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.52688485289906
printing an ep nov before normalisation:  28.57362352747259
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.229]
 [1.496]
 [1.496]
 [1.497]
 [1.497]] [[24.978]
 [16.918]
 [10.734]
 [15.888]
 [17.499]] [[1.944]
 [1.979]
 [1.803]
 [1.95 ]
 [1.997]]
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.051 0.077 0.692 0.103 0.077]
printing an ep nov before normalisation:  36.05602264404297
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.325]
 [0.008]
 [0.383]
 [0.383]
 [0.383]] [[24.723]
 [26.894]
 [33.484]
 [33.484]
 [33.484]] [[2.038]
 [2.008]
 [3.253]
 [3.253]
 [3.253]]
printing an ep nov before normalisation:  58.10460041942291
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.56423604663615
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.996297888677574
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.70389505288478
siam score:  -0.9249533
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  56.74132610873455
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  63.717963464948305
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  36.27470444991032
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.02594406582353
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.601]
 [0.677]
 [0.601]
 [0.601]
 [0.601]] [[ 0.   ]
 [50.613]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.003]
 [ 1.21 ]
 [-0.003]
 [-0.003]
 [-0.003]]
printing an ep nov before normalisation:  26.683120727539062
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.26956094132363
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.64710141259515
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.735]
 [0.744]
 [0.689]
 [0.725]
 [0.744]] [[34.853]
 [34.551]
 [39.674]
 [46.151]
 [34.551]] [[0.735]
 [0.744]
 [0.689]
 [0.725]
 [0.744]]
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.672]
 [0.726]
 [0.726]
 [0.726]] [[33.398]
 [24.675]
 [26.581]
 [26.581]
 [26.581]] [[0.738]
 [0.672]
 [0.726]
 [0.726]
 [0.726]]
printing an ep nov before normalisation:  23.33518780538897
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.829]
 [0.829]] [[45.757]
 [45.757]
 [45.757]
 [45.757]
 [45.757]] [[1.808]
 [1.808]
 [1.808]
 [1.808]
 [1.808]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.826]
 [0.845]
 [0.891]
 [0.846]] [[34.346]
 [32.081]
 [31.203]
 [29.766]
 [29.735]] [[1.456]
 [1.373]
 [1.365]
 [1.368]
 [1.321]]
printing an ep nov before normalisation:  46.50613325007404
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.414652451296035
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.475]
 [1.428]
 [1.395]
 [1.417]
 [1.399]] [[30.239]
 [29.039]
 [29.155]
 [27.971]
 [28.709]] [[2.019]
 [1.939]
 [1.91 ]
 [1.898]
 [1.901]]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  47.28627500615104
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9259018
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.53073646208144
printing an ep nov before normalisation:  51.550072874368944
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.8106],
        [0.2393],
        [0.0000],
        [0.0000],
        [0.8812],
        [0.2255],
        [0.2635],
        [0.8506],
        [0.6529],
        [0.5377]], dtype=torch.float64)
0.0 0.8105702511389649
0.0 0.23927014868585508
0.0 0.0
0.0 0.0
0.0 0.8812266350271615
0.0 0.22546329561451353
0.0 0.26347094545218874
0.0 0.8506000373186875
0.0 0.6529329990879728
0.0 0.5376573551503125
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.194]
 [1.129]
 [1.129]
 [1.128]
 [1.129]] [[35.774]
 [27.957]
 [27.957]
 [37.499]
 [27.957]] [[1.35 ]
 [1.231]
 [1.231]
 [1.296]
 [1.231]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.29574242682107
printing an ep nov before normalisation:  51.800688781478584
printing an ep nov before normalisation:  54.98950371819901
Printing some Q and Qe and total Qs values:  [[1.372]
 [1.252]
 [1.242]
 [1.209]
 [1.234]] [[31.352]
 [29.585]
 [31.286]
 [23.675]
 [29.761]] [[2.512]
 [2.269]
 [2.377]
 [1.815]
 [2.263]]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.299]
 [0.408]
 [0.549]
 [0.452]
 [0.385]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.299]
 [0.408]
 [0.549]
 [0.452]
 [0.385]]
printing an ep nov before normalisation:  38.73875255130195
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.775860274713104
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.841765557206536
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.131]
 [1.165]
 [1.165]
 [1.165]
 [1.165]] [[53.76 ]
 [39.479]
 [39.479]
 [39.479]
 [39.479]] [[1.984]
 [1.7  ]
 [1.7  ]
 [1.7  ]
 [1.7  ]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1541 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.930407918444054
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.69283273072461
Printing some Q and Qe and total Qs values:  [[0.604]
 [0.604]
 [0.604]
 [0.604]
 [0.604]] [[42.16]
 [42.16]
 [42.16]
 [42.16]
 [42.16]] [[1.297]
 [1.297]
 [1.297]
 [1.297]
 [1.297]]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9286409
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 28.61572425207318
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.685586157776854
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  43.30855599541536
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]] [[37.074]
 [37.074]
 [37.074]
 [37.074]
 [37.074]] [[0.431]
 [0.431]
 [0.431]
 [0.431]
 [0.431]]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1541 1.0 1.0
maxi score, test score, baseline:  0.1541 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.449766497973826
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
printing an ep nov before normalisation:  26.199906056033
printing an ep nov before normalisation:  2.092602069842542
printing an ep nov before normalisation:  22.644962256888128
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9670,     0.0145,     0.0000,     0.0001,     0.0184],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9821,     0.0000,     0.0001,     0.0177],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9981,     0.0006,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0021,     0.0145,     0.8402,     0.1426],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0025, 0.0646, 0.0512, 0.1630, 0.7186], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1561 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.745]
 [0.849]
 [0.804]
 [0.94 ]
 [0.914]] [[35.907]
 [27.873]
 [29.285]
 [34.45 ]
 [30.098]] [[0.982]
 [0.98 ]
 [0.954]
 [1.159]
 [1.075]]
maxi score, test score, baseline:  0.1561 1.0 1.0
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.12095814887411
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
printing an ep nov before normalisation:  39.483043308788595
printing an ep nov before normalisation:  33.25360668787333
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.30681371440482
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 29.047214279204198
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.884697682639732
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.107942327625544
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  45.2885816618226
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.859050160526394
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.18790189000012
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.7884826013212
maxi score, test score, baseline:  0.1561 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.2891535802106
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.546560164409904
printing an ep nov before normalisation:  33.20687887863101
maxi score, test score, baseline:  0.1581 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.02 ]
 [1.317]
 [1.233]
 [1.162]
 [1.252]] [[55.781]
 [31.809]
 [41.666]
 [49.727]
 [36.438]] [[1.353]
 [1.463]
 [1.457]
 [1.448]
 [1.435]]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.52921475786851
printing an ep nov before normalisation:  38.19225247745856
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.149]
 [1.118]
 [1.118]
 [1.118]
 [1.118]] [[46.286]
 [37.49 ]
 [37.49 ]
 [37.49 ]
 [37.49 ]] [[1.844]
 [1.568]
 [1.568]
 [1.568]
 [1.568]]
printing an ep nov before normalisation:  30.04204599153862
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  31.44440879596005
Printing some Q and Qe and total Qs values:  [[1.124]
 [1.124]
 [1.283]
 [1.164]
 [1.124]] [[42.118]
 [42.118]
 [33.408]
 [46.724]
 [42.118]] [[1.804]
 [1.804]
 [1.713]
 [1.976]
 [1.804]]
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9171939
printing an ep nov before normalisation:  56.35051955100643
printing an ep nov before normalisation:  47.87948835418902
printing an ep nov before normalisation:  56.75573425942746
siam score:  -0.9180292
printing an ep nov before normalisation:  21.09377452396672
Printing some Q and Qe and total Qs values:  [[1.344]
 [1.363]
 [1.355]
 [1.344]
 [1.235]] [[22.587]
 [20.092]
 [19.541]
 [22.587]
 [20.544]] [[2.295]
 [2.131]
 [2.083]
 [2.295]
 [2.036]]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.1581 1.0 1.0
maxi score, test score, baseline:  0.1581 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
printing an ep nov before normalisation:  28.603636898289192
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.68675719483184
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  1  action  0 :  tensor([    0.9212,     0.0162,     0.0000,     0.0138,     0.0488],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0002,     0.9188,     0.0100,     0.0006,     0.0703],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0016,     0.0009,     0.9415,     0.0118,     0.0442],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0013,     0.0008,     0.0167,     0.9061,     0.0751],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0178, 0.0798, 0.0236, 0.0394, 0.8393], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1601 1.0 1.0
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.80493825906749
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1601 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.113]
 [1.089]
 [1.113]
 [1.058]
 [1.055]] [[47.017]
 [48.845]
 [47.017]
 [48.641]
 [49.491]] [[2.664]
 [2.726]
 [2.664]
 [2.685]
 [2.722]]
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.478]
 [0.699]
 [0.478]
 [0.478]
 [0.478]] [[45.563]
 [40.586]
 [45.563]
 [45.563]
 [45.563]] [[0.691]
 [0.87 ]
 [0.691]
 [0.691]
 [0.691]]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1621 1.0 1.0
printing an ep nov before normalisation:  73.45297009895269
Printing some Q and Qe and total Qs values:  [[1.132]
 [1.132]
 [1.132]
 [1.132]
 [1.132]] [[63.94]
 [63.94]
 [63.94]
 [63.94]
 [63.94]] [[2.753]
 [2.753]
 [2.753]
 [2.753]
 [2.753]]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1621 1.0 1.0
printing an ep nov before normalisation:  30.100027538074574
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1621 1.0 1.0
printing an ep nov before normalisation:  22.852292235229125
printing an ep nov before normalisation:  27.807378218992092
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.229]
 [1.295]
 [1.201]
 [1.182]
 [1.253]] [[20.238]
 [33.193]
 [24.641]
 [19.768]
 [23.253]] [[1.329]
 [1.508]
 [1.339]
 [1.278]
 [1.38 ]]
Printing some Q and Qe and total Qs values:  [[1.391]
 [1.391]
 [1.391]
 [1.391]
 [1.391]] [[29.43]
 [29.43]
 [29.43]
 [29.43]
 [29.43]] [[1.724]
 [1.724]
 [1.724]
 [1.724]
 [1.724]]
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  66.1244833721966
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  50.048779988471125
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.56641555675183
printing an ep nov before normalisation:  37.33479339791944
Printing some Q and Qe and total Qs values:  [[1.085]
 [1.085]
 [1.085]
 [1.085]
 [1.085]] [[45.793]
 [45.793]
 [45.793]
 [45.793]
 [45.793]] [[2.099]
 [2.099]
 [2.099]
 [2.099]
 [2.099]]
printing an ep nov before normalisation:  28.26254607793423
printing an ep nov before normalisation:  50.85904187797344
actor:  1 policy actor:  1  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  45.37131331284481
actions average: 
K:  4  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0002,     0.9515,     0.0004,     0.0011,     0.0467],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0008,     0.0321,     0.8884,     0.0013,     0.0774],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0010,     0.9952,     0.0037],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0053, 0.0667, 0.0250, 0.1107, 0.7923], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.087]
 [1.082]
 [1.082]
 [1.11 ]
 [1.083]] [[51.759]
 [42.756]
 [42.756]
 [49.149]
 [43.574]] [[1.477]
 [1.36 ]
 [1.36 ]
 [1.468]
 [1.371]]
Printing some Q and Qe and total Qs values:  [[0.33 ]
 [0.561]
 [0.397]
 [0.423]
 [0.483]] [[43.896]
 [35.495]
 [36.972]
 [41.694]
 [40.343]] [[0.33 ]
 [0.561]
 [0.397]
 [0.423]
 [0.483]]
printing an ep nov before normalisation:  38.53074312210083
maxi score, test score, baseline:  0.1621 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  35.93935969489667
actor:  0 policy actor:  0  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
UNIT TEST: sample policy line 217 mcts : [0.077 0.256 0.385 0.103 0.179]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.154]
 [1.125]
 [1.001]
 [1.045]
 [1.068]] [[36.007]
 [39.027]
 [43.985]
 [39.394]
 [39.121]] [[1.832]
 [1.913]
 [1.971]
 [1.847]
 [1.86 ]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.449]
 [1.356]
 [1.328]
 [1.328]
 [1.328]] [[24.672]
 [26.498]
 [25.4  ]
 [25.4  ]
 [25.4  ]] [[2.637]
 [2.632]
 [2.552]
 [2.552]
 [2.552]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.883440266820045
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.16291267061956
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
printing an ep nov before normalisation:  26.27066764575823
printing an ep nov before normalisation:  2.754796652174747
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  101 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  52.89657744390167
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.93328515042659
line 256 mcts: sample exp_bonus 39.95442102376413
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.29866313934326
maxi score, test score, baseline:  0.1641 1.0 1.0
printing an ep nov before normalisation:  53.323467169528364
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.269]
 [1.337]
 [1.269]
 [1.269]
 [1.32 ]] [[37.56 ]
 [35.725]
 [37.56 ]
 [37.56 ]
 [37.224]] [[2.144]
 [2.15 ]
 [2.144]
 [2.144]
 [2.184]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  71.59522028702872
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.022310061639516
printing an ep nov before normalisation:  42.5791767541687
printing an ep nov before normalisation:  18.571872669939342
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9989,     0.0001,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9712,     0.0088,     0.0010,     0.0188],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0158,     0.9572,     0.0002,     0.0267],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0002,     0.0204,     0.8529,     0.1264],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0034, 0.0576, 0.0405, 0.1736, 0.7250], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.779603804780848
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.34654258006327
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.95003485424767
printing an ep nov before normalisation:  0.6644335431587933
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.65420902600892
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.64417292008969
Printing some Q and Qe and total Qs values:  [[0.643]
 [0.643]
 [0.643]
 [0.643]
 [0.643]] [[59.011]
 [59.011]
 [59.011]
 [59.011]
 [59.011]] [[1.68]
 [1.68]
 [1.68]
 [1.68]
 [1.68]]
printing an ep nov before normalisation:  31.99122905731201
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.64850667007994
siam score:  -0.91268677
line 256 mcts: sample exp_bonus 49.73718959436812
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 57.75529938892912
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.421]
 [0.593]
 [0.356]
 [0.393]
 [0.401]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.421]
 [0.593]
 [0.356]
 [0.393]
 [0.401]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.1641 1.0 1.0
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  46.267429747091896
printing an ep nov before normalisation:  30.380261011485388
printing an ep nov before normalisation:  29.29041819250223
siam score:  -0.9159758
maxi score, test score, baseline:  0.1641 1.0 1.0
printing an ep nov before normalisation:  35.181756019592285
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4060],
        [0.5790],
        [0.2334],
        [0.0594],
        [0.2671],
        [0.6419],
        [0.3742],
        [0.1187],
        [0.0000],
        [0.4824]], dtype=torch.float64)
0.0 0.40604743263279686
0.0 0.578991758812899
0.0 0.23336397271358822
0.0 0.05942035812393835
0.0 0.2670732279409879
0.0 0.6418960399909246
0.0 0.3741565195086074
0.0 0.11869585871359929
0.0 0.0
0.0 0.4823663870791858
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actor:  1 policy actor:  1  step number:  104 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.59984229349875
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1641 1.0 1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.665]
 [0.527]
 [0.527]
 [0.527]] [[51.102]
 [58.949]
 [51.102]
 [51.102]
 [51.102]] [[1.79 ]
 [2.332]
 [1.79 ]
 [1.79 ]
 [1.79 ]]
maxi score, test score, baseline:  0.1641 1.0 1.0
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.99725036835292
printing an ep nov before normalisation:  43.89081937865809
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.9160908
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.986]
 [0.971]
 [0.971]
 [0.971]
 [0.971]] [[37.448]
 [36.525]
 [36.525]
 [36.525]
 [36.525]] [[0.986]
 [0.971]
 [0.971]
 [0.971]
 [0.971]]
maxi score, test score, baseline:  0.1641 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1661 1.0 1.0
maxi score, test score, baseline:  0.1661 1.0 1.0
actor:  1 policy actor:  1  step number:  106 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  4  action  0 :  tensor([    0.9997,     0.0001,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0001,     0.9871,     0.0000,     0.0000,     0.0128],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0241,     0.9396,     0.0015,     0.0346],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0029,     0.0007,     0.0003,     0.8963,     0.0998],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0291, 0.0398, 0.0689, 0.1189, 0.7433], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.631]
 [0.711]
 [0.631]
 [0.631]
 [0.631]] [[19.804]
 [28.171]
 [19.804]
 [19.804]
 [19.804]] [[0.767]
 [0.957]
 [0.767]
 [0.767]
 [0.767]]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  53.64845305771723
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[50.075]
 [50.075]
 [50.075]
 [50.075]
 [50.075]] [[1.087]
 [1.087]
 [1.087]
 [1.087]
 [1.087]]
printing an ep nov before normalisation:  38.96480377947809
printing an ep nov before normalisation:  29.94729815588157
printing an ep nov before normalisation:  32.51885847745387
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.91407305
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  61.12876574936458
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  118 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.672]
 [0.672]
 [0.904]
 [0.672]
 [0.722]] [[38.074]
 [38.074]
 [38.257]
 [38.074]
 [41.181]] [[0.896]
 [0.896]
 [1.129]
 [0.896]
 [0.98 ]]
siam score:  -0.91001076
Printing some Q and Qe and total Qs values:  [[0.381]
 [0.342]
 [0.489]
 [0.381]
 [0.381]] [[29.963]
 [35.391]
 [50.946]
 [29.963]
 [29.963]] [[0.61]
 [0.67]
 [1.1 ]
 [0.61]
 [0.61]]
maxi score, test score, baseline:  0.1661 1.0 1.0
printing an ep nov before normalisation:  58.86099460920752
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.70509897770263
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1661 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.759390061683845
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
actions average: 
K:  1  action  0 :  tensor([    0.9933,     0.0043,     0.0000,     0.0005,     0.0019],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9906,     0.0013,     0.0001,     0.0080],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0270,     0.9578,     0.0000,     0.0148],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([0.0010, 0.0009, 0.0023, 0.8342, 0.1616], grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0184, 0.0052, 0.0044, 0.1330, 0.8390], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.1681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.115]
 [1.084]
 [1.084]
 [1.084]
 [1.084]] [[31.105]
 [23.228]
 [23.228]
 [23.228]
 [23.228]] [[1.674]
 [1.385]
 [1.385]
 [1.385]
 [1.385]]
printing an ep nov before normalisation:  27.58691060335131
siam score:  -0.9117377
printing an ep nov before normalisation:  61.86470374193072
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.272]
 [1.311]
 [1.272]
 [1.272]
 [1.287]] [[34.417]
 [43.858]
 [34.417]
 [34.417]
 [36.914]] [[1.54 ]
 [1.715]
 [1.54 ]
 [1.54 ]
 [1.591]]
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  73.0993851366141
maxi score, test score, baseline:  0.1681 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.088]
 [1.088]
 [1.088]
 [1.088]
 [1.088]] [[39.604]
 [39.604]
 [39.604]
 [39.604]
 [39.604]] [[1.552]
 [1.552]
 [1.552]
 [1.552]
 [1.552]]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.75202546473398
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.544]
 [0.545]
 [0.544]
 [0.544]
 [0.544]] [[ 0.   ]
 [30.996]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[0.302]
 [0.886]
 [0.302]
 [0.302]
 [0.302]]
siam score:  -0.91551995
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
printing an ep nov before normalisation:  44.69518184661865
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  74.25403055606907
siam score:  -0.91342604
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.55646322412063
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.593]
 [1.404]
 [1.371]
 [0.145]
 [1.168]] [[22.759]
 [22.207]
 [24.538]
 [22.259]
 [20.314]] [[0.812]
 [1.613]
 [1.621]
 [0.356]
 [1.344]]
printing an ep nov before normalisation:  26.82084737711192
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.268]
 [0.268]
 [0.268]
 [0.268]
 [0.268]]
Printing some Q and Qe and total Qs values:  [[0.543]
 [0.536]
 [0.535]
 [0.538]
 [0.535]] [[26.169]
 [23.298]
 [23.43 ]
 [22.319]
 [22.854]] [[0.543]
 [0.536]
 [0.535]
 [0.538]
 [0.535]]
Printing some Q and Qe and total Qs values:  [[0.217]
 [0.211]
 [0.217]
 [0.217]
 [0.217]] [[24.93 ]
 [43.585]
 [24.93 ]
 [24.93 ]
 [24.93 ]] [[0.651]
 [1.189]
 [0.651]
 [0.651]
 [0.651]]
Printing some Q and Qe and total Qs values:  [[0.679]
 [0.711]
 [0.679]
 [0.679]
 [0.679]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.679]
 [0.711]
 [0.679]
 [0.679]
 [0.679]]
maxi score, test score, baseline:  0.1681 1.0 1.0
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1681 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.560454080721044
printing an ep nov before normalisation:  38.78567035874422
actor:  0 policy actor:  0  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1701 1.0 1.0
printing an ep nov before normalisation:  65.66762630877588
printing an ep nov before normalisation:  40.51974241767848
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.95685384761541
actions average: 
K:  0  action  0 :  tensor([    0.9980,     0.0001,     0.0000,     0.0003,     0.0016],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9363,     0.0072,     0.0003,     0.0562],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0002,     0.0189,     0.9681,     0.0005,     0.0124],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0008,     0.0002,     0.0245,     0.8798,     0.0946],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0026, 0.0608, 0.0211, 0.1437, 0.7717], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  19.077705604243146
printing an ep nov before normalisation:  40.676492637891144
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  35.48424455585017
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.598]
 [0.598]
 [0.598]
 [0.598]
 [0.598]]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.91506976
printing an ep nov before normalisation:  47.83838971517716
actor:  1 policy actor:  1  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.83361382218358
maxi score, test score, baseline:  0.1701 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.91298723
actions average: 
K:  0  action  0 :  tensor([    0.9971,     0.0009,     0.0000,     0.0009,     0.0011],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9864,     0.0001,     0.0002,     0.0131],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0003,     0.9481,     0.0272,     0.0244],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0004,     0.0002,     0.0316,     0.8569,     0.1110],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0227, 0.0378, 0.0053, 0.1107, 0.8234], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.14452086285769
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
maxi score, test score, baseline:  0.17209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  0  action  0 :  tensor([    0.9908,     0.0000,     0.0002,     0.0054,     0.0035],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0014,     0.8927,     0.0156,     0.0005,     0.0898],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0005,     0.0425,     0.8893,     0.0002,     0.0676],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0003,     0.0001,     0.1460,     0.8004,     0.0533],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0211, 0.0905, 0.2048, 0.1081, 0.5755], grad_fn=<DivBackward0>)
Starting evaluation
Printing some Q and Qe and total Qs values:  [[1.357]
 [1.403]
 [1.413]
 [1.413]
 [1.403]] [[34.604]
 [40.416]
 [37.954]
 [37.954]
 [39.803]] [[2.157]
 [2.484]
 [2.375]
 [2.375]
 [2.454]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  43.432969200428126
printing an ep nov before normalisation:  38.51848840713501
printing an ep nov before normalisation:  35.32413154487919
printing an ep nov before normalisation:  38.52390128031793
printing an ep nov before normalisation:  42.83265722301153
rdn beta is 0 so we're just using the maxi policy
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  39.298323492363636
printing an ep nov before normalisation:  34.64548225490388
printing an ep nov before normalisation:  37.1342658996582
printing an ep nov before normalisation:  34.20493273969426
line 256 mcts: sample exp_bonus 33.45177013616843
actions average: 
K:  3  action  0 :  tensor([    0.9131,     0.0039,     0.0000,     0.0334,     0.0496],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0001,     0.9746,     0.0008,     0.0004,     0.0242],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0001,     0.9788,     0.0123,     0.0089],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0005,     0.0005,     0.0326,     0.9132,     0.0533],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0012, 0.0785, 0.1183, 0.0942, 0.7078], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  53.434237701111414
printing an ep nov before normalisation:  34.472128594767376
printing an ep nov before normalisation:  49.02548191907115
printing an ep nov before normalisation:  40.94173519983314
Printing some Q and Qe and total Qs values:  [[0.966]
 [0.966]
 [0.966]
 [0.992]
 [0.966]] [[42.209]
 [42.209]
 [42.209]
 [35.668]
 [42.209]] [[0.966]
 [0.966]
 [0.966]
 [0.992]
 [0.966]]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.1981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  60.66108499239524
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  37.875816314199554
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  34.41075038162114
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.672819116608494
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9990,     0.0000,     0.0005,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0001,     0.9491,     0.0089,     0.0002,     0.0417],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0167,     0.8807,     0.0024,     0.1002],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0008,     0.9019,     0.0969],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0018, 0.1106, 0.0379, 0.1365, 0.7132], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  29.73766922950745
printing an ep nov before normalisation:  28.39185558404081
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.859]
 [0.859]
 [0.859]
 [0.859]
 [0.859]] [[22.431]
 [22.431]
 [22.431]
 [22.431]
 [22.431]] [[1.137]
 [1.137]
 [1.137]
 [1.137]
 [1.137]]
Printing some Q and Qe and total Qs values:  [[1.178]
 [1.165]
 [1.188]
 [1.182]
 [1.186]] [[38.835]
 [33.088]
 [29.153]
 [30.638]
 [30.734]] [[1.706]
 [1.587]
 [1.538]
 [1.559]
 [1.565]]
Printing some Q and Qe and total Qs values:  [[0.902]
 [0.897]
 [0.837]
 [1.06 ]
 [0.965]] [[21.839]
 [24.854]
 [22.873]
 [26.704]
 [20.316]] [[1.139]
 [1.203]
 [1.098]
 [1.408]
 [1.167]]
Printing some Q and Qe and total Qs values:  [[0.497]
 [0.53 ]
 [0.497]
 [0.497]
 [0.497]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.497]
 [0.53 ]
 [0.497]
 [0.497]
 [0.497]]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]] [[36.086]
 [36.086]
 [36.086]
 [36.086]
 [36.086]] [[0.548]
 [0.548]
 [0.548]
 [0.548]
 [0.548]]
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.724]
 [0.749]
 [0.749]
 [0.749]
 [0.749]] [[31.158]
 [30.62 ]
 [30.62 ]
 [30.62 ]
 [30.62 ]] [[0.724]
 [0.749]
 [0.749]
 [0.749]
 [0.749]]
printing an ep nov before normalisation:  37.5426995340199
maxi score, test score, baseline:  0.21209999999999998 1.0 1.0
actor:  0 policy actor:  0  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.84136152267456
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2161 1.0 1.0
siam score:  -0.9129002
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.788]
 [0.799]
 [0.757]
 [0.85 ]
 [0.826]] [[32.447]
 [33.354]
 [34.967]
 [29.22 ]
 [30.507]] [[1.511]
 [1.56 ]
 [1.584]
 [1.441]
 [1.47 ]]
printing an ep nov before normalisation:  30.01084433276863
Printing some Q and Qe and total Qs values:  [[0.878]
 [0.891]
 [0.934]
 [0.12 ]
 [0.861]] [[29.471]
 [22.898]
 [23.76 ]
 [23.984]
 [23.584]] [[1.673]
 [1.399]
 [1.48 ]
 [0.675]
 [1.399]]
printing an ep nov before normalisation:  33.96808329103177
printing an ep nov before normalisation:  27.162074040774336
maxi score, test score, baseline:  0.2161 1.0 1.0
printing an ep nov before normalisation:  41.4094987828351
printing an ep nov before normalisation:  1.6851705570175568
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.61764663162195
printing an ep nov before normalisation:  45.90192794799805
Printing some Q and Qe and total Qs values:  [[1.483]
 [1.401]
 [1.334]
 [1.343]
 [1.392]] [[20.623]
 [25.665]
 [26.276]
 [26.385]
 [25.8  ]] [[2.364]
 [2.498]
 [2.457]
 [2.471]
 [2.495]]
siam score:  -0.9080025
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2161 1.0 1.0
printing an ep nov before normalisation:  48.541635633882734
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.50900850976427
printing an ep nov before normalisation:  25.23815155029297
printing an ep nov before normalisation:  24.861035332883752
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.506214190715298
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2161 1.0 1.0
printing an ep nov before normalisation:  35.99902209733301
printing an ep nov before normalisation:  29.38092895526694
printing an ep nov before normalisation:  29.569497457026348
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.52135944366455
maxi score, test score, baseline:  0.2181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  72 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  38.7915883745466
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  23.11468142631109
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.53 ]
 [0.53 ]
 [0.665]
 [0.53 ]
 [0.53 ]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.53 ]
 [0.53 ]
 [0.665]
 [0.53 ]
 [0.53 ]]
Printing some Q and Qe and total Qs values:  [[1.151]
 [1.322]
 [1.234]
 [1.234]
 [1.173]] [[34.127]
 [29.008]
 [28.14 ]
 [28.14 ]
 [28.549]] [[1.427]
 [1.53 ]
 [1.431]
 [1.431]
 [1.375]]
Printing some Q and Qe and total Qs values:  [[1.334]
 [1.354]
 [1.334]
 [1.334]
 [1.334]] [[25.86 ]
 [34.369]
 [25.86 ]
 [25.86 ]
 [25.86 ]] [[1.555]
 [1.687]
 [1.555]
 [1.555]
 [1.555]]
Printing some Q and Qe and total Qs values:  [[1.408]
 [1.371]
 [1.374]
 [1.374]
 [1.361]] [[36.967]
 [41.097]
 [41.867]
 [41.867]
 [40.355]] [[2.114]
 [2.22 ]
 [2.25 ]
 [2.25 ]
 [2.184]]
maxi score, test score, baseline:  0.2201 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
line 256 mcts: sample exp_bonus 26.248115268036422
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.9142767
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.671419398313528
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
printing an ep nov before normalisation:  62.45140468180845
siam score:  -0.9099631
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  78 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  37.1037483215332
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 57.11327955844702
printing an ep nov before normalisation:  35.63322256878274
maxi score, test score, baseline:  0.2201 1.0 1.0
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.657]
 [0.761]
 [0.657]
 [0.657]
 [0.639]] [[39.972]
 [48.118]
 [39.972]
 [39.972]
 [40.695]] [[0.779]
 [0.931]
 [0.779]
 [0.779]
 [0.766]]
printing an ep nov before normalisation:  85.00187993049622
Printing some Q and Qe and total Qs values:  [[1.15 ]
 [1.067]
 [1.067]
 [1.065]
 [1.067]] [[40.44 ]
 [25.863]
 [25.863]
 [38.296]
 [25.863]] [[1.334]
 [1.159]
 [1.159]
 [1.235]
 [1.159]]
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.47063995925087
printing an ep nov before normalisation:  35.625199601824484
printing an ep nov before normalisation:  36.59828300873593
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.524716600117575
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  32.094461097740066
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  21.40462530860112
maxi score, test score, baseline:  0.2201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  31.314650755555
Printing some Q and Qe and total Qs values:  [[0.441]
 [0.441]
 [0.663]
 [0.441]
 [0.441]] [[24.414]
 [24.414]
 [30.463]
 [24.414]
 [24.414]] [[0.704]
 [0.704]
 [1.066]
 [0.704]
 [0.704]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  58.48305613766641
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.6585839230283
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.932899475097656
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2221 1.0 1.0
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.558]
 [0.558]
 [0.559]
 [0.573]
 [0.558]] [[33.318]
 [33.318]
 [35.198]
 [31.889]
 [33.318]] [[0.558]
 [0.558]
 [0.559]
 [0.573]
 [0.558]]
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  38.33061191982246
printing an ep nov before normalisation:  28.70011926411613
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  78.89159576553128
maxi score, test score, baseline:  0.2221 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2221 1.0 1.0
actor:  0 policy actor:  0  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.664]
 [0.706]
 [0.805]
 [0.805]
 [0.857]] [[59.597]
 [70.507]
 [55.371]
 [55.371]
 [55.513]] [[1.343]
 [1.602]
 [1.401]
 [1.401]
 [1.456]]
Printing some Q and Qe and total Qs values:  [[0.595]
 [0.725]
 [0.595]
 [0.595]
 [0.595]] [[40.863]
 [39.542]
 [40.863]
 [40.863]
 [40.863]] [[1.862]
 [1.951]
 [1.862]
 [1.862]
 [1.862]]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.202046394348145
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  103 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  33.65109988621303
printing an ep nov before normalisation:  33.68557182047356
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  30.39331004314116
printing an ep nov before normalisation:  42.552418501675426
printing an ep nov before normalisation:  57.85002238905067
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.66263341900592
siam score:  -0.91097534
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9845,     0.0002,     0.0078,     0.0032,     0.0043],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9966,     0.0002,     0.0019,     0.0012],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9947,     0.0028,     0.0025],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0003,     0.0003,     0.0181,     0.9108,     0.0706],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0055, 0.1222, 0.0252, 0.1473, 0.6997], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.849]
 [0.849]
 [0.849]
 [0.849]
 [0.849]] [[34.27]
 [34.27]
 [34.27]
 [34.27]
 [34.27]] [[1.061]
 [1.061]
 [1.061]
 [1.061]
 [1.061]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  49.742326831102815
printing an ep nov before normalisation:  51.84629798247783
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.768510521215994
maxi score, test score, baseline:  0.2281 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  36.92226442320269
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  20.410339508542584
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.534]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  113 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[0.678]
 [0.779]
 [0.678]
 [0.678]
 [0.678]] [[40.4  ]
 [56.848]
 [40.4  ]
 [40.4  ]
 [40.4  ]] [[0.967]
 [1.356]
 [0.967]
 [0.967]
 [0.967]]
printing an ep nov before normalisation:  34.24038887023926
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.682]
 [0.702]
 [0.716]
 [0.718]
 [0.696]] [[42.8  ]
 [40.487]
 [43.023]
 [37.14 ]
 [40.694]] [[1.333]
 [1.285]
 [1.373]
 [1.205]
 [1.286]]
Printing some Q and Qe and total Qs values:  [[0.779]
 [0.828]
 [0.779]
 [0.779]
 [0.779]] [[43.581]
 [46.033]
 [43.581]
 [43.581]
 [43.581]] [[1.562]
 [1.705]
 [1.562]
 [1.562]
 [1.562]]
Printing some Q and Qe and total Qs values:  [[1.114]
 [1.052]
 [1.087]
 [1.087]
 [1.083]] [[38.686]
 [26.366]
 [31.199]
 [31.199]
 [31.635]] [[1.716]
 [1.344]
 [1.5  ]
 [1.5  ]
 [1.507]]
Printing some Q and Qe and total Qs values:  [[1.042]
 [1.042]
 [1.042]
 [1.13 ]
 [1.042]] [[33.904]
 [33.904]
 [33.904]
 [36.448]
 [33.904]] [[1.743]
 [1.743]
 [1.743]
 [1.931]
 [1.743]]
printing an ep nov before normalisation:  35.2709916116302
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.874]
 [0.958]
 [1.032]
 [0.958]] [[30.261]
 [25.776]
 [28.127]
 [31.132]
 [28.127]] [[1.182]
 [1.165]
 [1.305]
 [1.451]
 [1.305]]
printing an ep nov before normalisation:  37.78419735299616
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.30440378189087
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2281 1.0 1.0
printing an ep nov before normalisation:  43.50758181781688
printing an ep nov before normalisation:  57.981252670288086
printing an ep nov before normalisation:  33.238372718123635
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.486]
 [0.489]
 [0.621]
 [0.48 ]
 [0.484]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.486]
 [0.489]
 [0.621]
 [0.48 ]
 [0.484]]
printing an ep nov before normalisation:  92.6063579536556
maxi score, test score, baseline:  0.2281 1.0 1.0
printing an ep nov before normalisation:  28.31852849042348
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  62.99810972242018
printing an ep nov before normalisation:  60.74305040396817
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.693070747990216
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9983,     0.0000,     0.0000,     0.0006,     0.0010],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0006,     0.9442,     0.0170,     0.0011,     0.0371],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9662,     0.0152,     0.0184],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([0.0151, 0.0011, 0.0215, 0.7457, 0.2166], grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0155, 0.0249, 0.0311, 0.0902, 0.8383], grad_fn=<DivBackward0>)
siam score:  -0.90718305
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.6044097253517
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  74.8178603463111
printing an ep nov before normalisation:  14.025097401267498
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  39.886978687877075
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  48.62396795723615
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.751388450537696
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.45281629240354
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.99132305652723
printing an ep nov before normalisation:  30.501744142614147
printing an ep nov before normalisation:  40.833101324051604
maxi score, test score, baseline:  0.2281 1.0 1.0
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.53445827737668
Printing some Q and Qe and total Qs values:  [[1.322]
 [1.297]
 [0.346]
 [1.222]
 [1.267]] [[37.248]
 [39.376]
 [33.659]
 [28.412]
 [37.043]] [[2.169]
 [2.236]
 [1.037]
 [1.685]
 [2.104]]
maxi score, test score, baseline:  0.2281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  85 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.662]
 [0.624]
 [0.782]
 [0.478]
 [0.588]] [[54.145]
 [53.51 ]
 [41.288]
 [51.68 ]
 [51.954]] [[2.383]
 [2.308]
 [1.752]
 [2.055]
 [2.181]]
printing an ep nov before normalisation:  37.50196497754045
maxi score, test score, baseline:  0.2281 1.0 1.0
printing an ep nov before normalisation:  36.518379844937634
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  35.98250180224114
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  60.60252662027518
printing an ep nov before normalisation:  36.52089836567572
printing an ep nov before normalisation:  35.042859684059756
printing an ep nov before normalisation:  37.8163379858284
actor:  1 policy actor:  1  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  41.71295195396228
Printing some Q and Qe and total Qs values:  [[0.07 ]
 [1.143]
 [1.083]
 [1.111]
 [1.119]] [[34.802]
 [33.216]
 [33.978]
 [38.072]
 [39.082]] [[0.904]
 [1.885]
 [1.869]
 [2.137]
 [2.204]]
printing an ep nov before normalisation:  22.84364639114144
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.28158333338369
maxi score, test score, baseline:  0.2301 1.0 1.0
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  87 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
printing an ep nov before normalisation:  47.817667911128524
Printing some Q and Qe and total Qs values:  [[1.38 ]
 [1.333]
 [1.333]
 [1.333]
 [1.333]] [[27.661]
 [22.677]
 [22.677]
 [22.677]
 [22.677]] [[2.347]
 [1.977]
 [1.977]
 [1.977]
 [1.977]]
printing an ep nov before normalisation:  33.17358621344039
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.218]
 [1.29 ]
 [1.191]
 [0.849]
 [1.243]] [[21.296]
 [20.934]
 [22.289]
 [29.897]
 [24.025]] [[1.721]
 [1.774]
 [1.746]
 [1.803]
 [1.889]]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.97373795803211
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.475]
 [0.761]
 [0.632]
 [0.511]
 [0.632]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.475]
 [0.761]
 [0.632]
 [0.511]
 [0.632]]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.118915381938514
printing an ep nov before normalisation:  21.756589652231813
printing an ep nov before normalisation:  22.318506027867762
Printing some Q and Qe and total Qs values:  [[0.024]
 [0.028]
 [0.031]
 [0.024]
 [0.024]] [[1.808]
 [1.846]
 [1.861]
 [1.779]
 [1.847]] [[0.04 ]
 [0.045]
 [0.047]
 [0.04 ]
 [0.041]]
maxi score, test score, baseline:  0.2301 1.0 1.0
printing an ep nov before normalisation:  49.74878393323423
using explorer policy with actor:  1
printing an ep nov before normalisation:  45.8998712942022
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.525]
 [0.525]
 [0.525]
 [0.525]
 [0.525]]
Printing some Q and Qe and total Qs values:  [[1.459]
 [1.39 ]
 [1.374]
 [1.374]
 [1.351]] [[27.391]
 [27.233]
 [27.141]
 [27.141]
 [26.831]] [[2.723]
 [2.641]
 [2.618]
 [2.618]
 [2.57 ]]
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.52874820886141
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2321 1.0 1.0
actions average: 
K:  1  action  0 :  tensor([    0.9947,     0.0011,     0.0002,     0.0001,     0.0038],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9817,     0.0000,     0.0004,     0.0177],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0004,     0.0002,     0.9817,     0.0002,     0.0175],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0004,     0.0003,     0.0008,     0.7803,     0.2181],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.1282, 0.0235, 0.0668, 0.7798], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  33.583670929124835
printing an ep nov before normalisation:  26.77164316177368
maxi score, test score, baseline:  0.2321 1.0 1.0
printing an ep nov before normalisation:  28.120660180877184
Printing some Q and Qe and total Qs values:  [[1.074]
 [1.074]
 [1.074]
 [1.065]
 [1.076]] [[50.805]
 [50.805]
 [50.805]
 [50.25 ]
 [50.887]] [[2.57 ]
 [2.57 ]
 [2.57 ]
 [2.537]
 [2.575]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.106]
 [0.132]
 [0.184]
 [0.135]
 [0.136]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.106]
 [0.132]
 [0.184]
 [0.135]
 [0.136]]
maxi score, test score, baseline:  0.2321 1.0 1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  18.721545645976146
printing an ep nov before normalisation:  52.53056195764945
printing an ep nov before normalisation:  49.56610219265496
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.941]
 [0.941]
 [0.941]
 [1.071]
 [0.946]] [[29.9  ]
 [29.9  ]
 [29.9  ]
 [32.014]
 [30.903]] [[1.214]
 [1.214]
 [1.214]
 [1.377]
 [1.235]]
Printing some Q and Qe and total Qs values:  [[0.391]
 [0.563]
 [0.539]
 [0.507]
 [0.547]] [[35.713]
 [37.666]
 [29.398]
 [28.208]
 [32.15 ]] [[0.391]
 [0.563]
 [0.539]
 [0.507]
 [0.547]]
Printing some Q and Qe and total Qs values:  [[0.55 ]
 [0.57 ]
 [0.57 ]
 [0.619]
 [0.573]] [[26.195]
 [24.954]
 [23.477]
 [20.766]
 [22.433]] [[0.55 ]
 [0.57 ]
 [0.57 ]
 [0.619]
 [0.573]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.146031801664297
Printing some Q and Qe and total Qs values:  [[0.023]
 [0.032]
 [0.039]
 [0.028]
 [0.033]] [[19.013]
 [20.069]
 [21.238]
 [20.761]
 [19.545]] [[0.501]
 [0.588]
 [0.682]
 [0.635]
 [0.55 ]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.128692024847258
printing an ep nov before normalisation:  28.460335731506348
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  24.455310325358155
maxi score, test score, baseline:  0.2321 1.0 1.0
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.09647939411479456
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2321 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.799]
 [0.821]
 [0.799]
 [0.799]
 [0.799]] [[61.473]
 [47.494]
 [61.473]
 [61.473]
 [61.473]] [[1.452]
 [1.267]
 [1.452]
 [1.452]
 [1.452]]
maxi score, test score, baseline:  0.2321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.883979083538158
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.102]
 [1.102]
 [1.086]
 [1.085]] [[51.56 ]
 [37.814]
 [37.814]
 [39.063]
 [39.437]] [[1.737]
 [1.498]
 [1.498]
 [1.507]
 [1.513]]
printing an ep nov before normalisation:  28.23185240408662
siam score:  -0.9061464
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  54.71118688583375
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.964]
 [0.964]
 [0.964]
 [0.964]
 [0.964]] [[44.669]
 [44.669]
 [44.669]
 [44.669]
 [44.669]] [[1.301]
 [1.301]
 [1.301]
 [1.301]
 [1.301]]
printing an ep nov before normalisation:  33.79329844538794
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.79 ]
 [0.844]
 [0.84 ]
 [0.989]
 [0.934]] [[25.391]
 [24.154]
 [27.274]
 [29.861]
 [28.997]] [[1.069]
 [1.094]
 [1.164]
 [1.374]
 [1.299]]
printing an ep nov before normalisation:  28.252220557501886
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.095]
 [1.148]
 [1.148]
 [1.148]
 [1.148]] [[54.026]
 [38.267]
 [38.267]
 [38.267]
 [38.267]] [[1.762]
 [1.551]
 [1.551]
 [1.551]
 [1.551]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.19416581209164
printing an ep nov before normalisation:  0.09011821941953713
actor:  1 policy actor:  1  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.8528356552124
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.554]
 [0.554]
 [0.638]
 [0.554]] [[40.871]
 [40.871]
 [40.871]
 [34.983]
 [40.871]] [[0.554]
 [0.554]
 [0.554]
 [0.638]
 [0.554]]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.06468105316162
printing an ep nov before normalisation:  36.494937691832696
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.27299404144287
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
using explorer policy with actor:  1
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9989,     0.0001,     0.0003,     0.0002,     0.0006],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0002,     0.9592,     0.0029,     0.0008,     0.0369],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0001,     0.9637,     0.0216,     0.0146],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0043,     0.0006,     0.0249,     0.8930,     0.0772],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0039, 0.0775, 0.0096, 0.1220, 0.7870], grad_fn=<DivBackward0>)
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.743]
 [0.799]
 [0.784]
 [0.952]
 [0.784]] [[36.096]
 [26.799]
 [32.831]
 [40.411]
 [32.831]] [[1.522]
 [1.225]
 [1.439]
 [1.895]
 [1.439]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  38.028574663720235
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.321]
 [1.321]
 [1.321]
 [1.321]
 [1.321]] [[33.423]
 [33.423]
 [33.423]
 [33.423]
 [33.423]] [[2.89]
 [2.89]
 [2.89]
 [2.89]
 [2.89]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  71 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89932173
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.773]
 [0.84 ]
 [0.731]
 [0.731]] [[30.678]
 [31.969]
 [30.77 ]
 [30.678]
 [30.678]] [[2.582]
 [2.773]
 [2.702]
 [2.582]
 [2.582]]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
Printing some Q and Qe and total Qs values:  [[1.108]
 [1.108]
 [1.108]
 [1.108]
 [1.108]] [[31.65]
 [31.65]
 [31.65]
 [31.65]
 [31.65]] [[2.991]
 [2.991]
 [2.991]
 [2.991]
 [2.991]]
Printing some Q and Qe and total Qs values:  [[1.105]
 [1.105]
 [1.105]
 [1.105]
 [1.105]] [[30.644]
 [30.644]
 [30.644]
 [30.644]
 [30.644]] [[3.105]
 [3.105]
 [3.105]
 [3.105]
 [3.105]]
printing an ep nov before normalisation:  0.01101034278235602
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  2  action  0 :  tensor([    0.9501,     0.0434,     0.0000,     0.0030,     0.0035],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0010,     0.9777,     0.0000,     0.0002,     0.0211],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0004,     0.0002,     0.9792,     0.0009,     0.0193],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0002,     0.0011,     0.0007,     0.9423,     0.0558],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0016, 0.0627, 0.0415, 0.0077, 0.8864], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  33.915429877115535
printing an ep nov before normalisation:  34.30661663768707
printing an ep nov before normalisation:  44.82094693462966
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.38279827719194
printing an ep nov before normalisation:  27.59669318631038
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.44483329988071
line 256 mcts: sample exp_bonus 2.1163748700042317
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8997579
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.98006746982382
printing an ep nov before normalisation:  33.020687098240586
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.497]
 [0.499]
 [0.608]
 [0.528]] [[49.072]
 [48.509]
 [40.471]
 [32.358]
 [39.938]] [[0.514]
 [0.497]
 [0.499]
 [0.608]
 [0.528]]
printing an ep nov before normalisation:  35.02519101810298
printing an ep nov before normalisation:  32.82157920912737
printing an ep nov before normalisation:  40.434367192938005
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.026]
 [0.831]
 [0.717]
 [0.669]
 [0.77 ]] [[45.384]
 [42.74 ]
 [42.705]
 [35.657]
 [42.639]] [[0.451]
 [1.212]
 [1.097]
 [0.932]
 [1.149]]
printing an ep nov before normalisation:  20.836495662779626
Printing some Q and Qe and total Qs values:  [[0.929]
 [0.929]
 [0.929]
 [1.006]
 [0.929]] [[27.991]
 [27.991]
 [27.991]
 [34.692]
 [27.991]] [[1.176]
 [1.176]
 [1.176]
 [1.346]
 [1.176]]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.0031623771997
printing an ep nov before normalisation:  29.924741836062466
Printing some Q and Qe and total Qs values:  [[0.666]
 [0.791]
 [0.666]
 [0.666]
 [0.666]] [[22.19 ]
 [26.337]
 [22.19 ]
 [22.19 ]
 [22.19 ]] [[1.064]
 [1.376]
 [1.064]
 [1.064]
 [1.064]]
printing an ep nov before normalisation:  24.130516980018925
printing an ep nov before normalisation:  26.886398395150795
printing an ep nov before normalisation:  19.05362039441899
actor:  1 policy actor:  1  step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23609999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.707]
 [0.811]
 [0.707]
 [0.68 ]
 [0.707]] [[40.088]
 [37.249]
 [40.088]
 [28.745]
 [40.088]] [[1.35 ]
 [1.374]
 [1.35 ]
 [1.008]
 [1.35 ]]
Printing some Q and Qe and total Qs values:  [[0.731]
 [0.81 ]
 [0.751]
 [0.7  ]
 [0.778]] [[44.655]
 [40.892]
 [24.156]
 [20.262]
 [21.653]] [[1.092]
 [1.124]
 [0.858]
 [0.759]
 [0.854]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.256]
 [1.38 ]
 [0.108]
 [1.337]
 [1.38 ]] [[33.054]
 [39.082]
 [29.325]
 [33.121]
 [32.521]] [[1.529]
 [1.762]
 [0.312]
 [1.611]
 [1.643]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
siam score:  -0.896349
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Printing some Q and Qe and total Qs values:  [[1.106]
 [0.996]
 [0.996]
 [0.958]
 [0.996]] [[51.277]
 [42.432]
 [42.432]
 [55.099]
 [42.432]] [[2.073]
 [1.709]
 [1.709]
 [2.035]
 [1.709]]
printing an ep nov before normalisation:  25.90639474899877
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9975,     0.0008,     0.0000,     0.0003,     0.0014],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0005,     0.9724,     0.0002,     0.0003,     0.0267],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0006,     0.9958,     0.0011,     0.0025],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0013,     0.0002,     0.0004,     0.9686,     0.0295],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0026, 0.0019, 0.0269, 0.0748, 0.8938], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.05232837793137435
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  0.029222471339380718
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
siam score:  -0.90336764
printing an ep nov before normalisation:  36.23170767337488
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  15.120402315227892
printing an ep nov before normalisation:  34.62751178552693
printing an ep nov before normalisation:  30.716722011566162
actor:  1 policy actor:  1  step number:  110 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.55074812140595
Printing some Q and Qe and total Qs values:  [[0.85 ]
 [0.85 ]
 [0.85 ]
 [0.948]
 [0.85 ]] [[33.398]
 [33.398]
 [33.398]
 [37.111]
 [33.398]] [[1.285]
 [1.285]
 [1.285]
 [1.456]
 [1.285]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.693604309627847
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.256]
 [1.298]
 [1.256]
 [1.256]
 [1.256]] [[31.994]
 [41.238]
 [31.994]
 [31.994]
 [31.994]] [[1.698]
 [2.055]
 [1.698]
 [1.698]
 [1.698]]
Printing some Q and Qe and total Qs values:  [[1.247]
 [1.264]
 [1.284]
 [1.208]
 [1.28 ]] [[35.576]
 [36.086]
 [37.029]
 [34.69 ]
 [36.52 ]] [[1.945]
 [1.981]
 [2.038]
 [1.871]
 [2.014]]
printing an ep nov before normalisation:  30.111387439887174
siam score:  -0.90179807
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.39798298795757
printing an ep nov before normalisation:  49.48176443510533
printing an ep nov before normalisation:  44.031404115579946
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
line 256 mcts: sample exp_bonus 41.42310967280847
Printing some Q and Qe and total Qs values:  [[0.588]
 [0.644]
 [0.533]
 [0.588]
 [0.588]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.588]
 [0.644]
 [0.533]
 [0.588]
 [0.588]]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
using explorer policy with actor:  1
printing an ep nov before normalisation:  70.07898607880726
Printing some Q and Qe and total Qs values:  [[0.972]
 [0.972]
 [0.972]
 [1.077]
 [1.14 ]] [[73.933]
 [73.933]
 [73.933]
 [75.703]
 [74.423]] [[2.699]
 [2.699]
 [2.699]
 [2.864]
 [2.883]]
Printing some Q and Qe and total Qs values:  [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]] [[36.746]
 [36.746]
 [36.746]
 [36.746]
 [36.746]] [[3.067]
 [3.067]
 [3.067]
 [3.067]
 [3.067]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.23809999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.612420082092285
UNIT TEST: sample policy line 217 mcts : [0.026 0.897 0.026 0.026 0.026]
printing an ep nov before normalisation:  31.29541046948342
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.89174056
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.62435258442497
Printing some Q and Qe and total Qs values:  [[0.59 ]
 [0.575]
 [0.586]
 [0.631]
 [0.602]] [[53.404]
 [40.976]
 [40.024]
 [40.511]
 [43.51 ]] [[0.59 ]
 [0.575]
 [0.586]
 [0.631]
 [0.602]]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.086174246494664
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]] [[33.103]
 [33.103]
 [33.103]
 [33.103]
 [33.103]] [[0.715]
 [0.715]
 [0.715]
 [0.715]
 [0.715]]
Printing some Q and Qe and total Qs values:  [[0.398]
 [0.507]
 [0.398]
 [0.398]
 [0.398]] [[ 0.   ]
 [30.473]
 [ 0.   ]
 [ 0.   ]
 [ 0.   ]] [[-0.254]
 [ 1.573]
 [-0.254]
 [-0.254]
 [-0.254]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.346389293670654
printing an ep nov before normalisation:  40.9817094581095
Printing some Q and Qe and total Qs values:  [[0.685]
 [0.82 ]
 [0.82 ]
 [0.724]
 [0.823]] [[50.227]
 [45.138]
 [40.178]
 [39.274]
 [39.187]] [[1.258]
 [1.314]
 [1.236]
 [1.125]
 [1.223]]
printing an ep nov before normalisation:  43.752113832932025
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  31.99245682401508
printing an ep nov before normalisation:  39.23395984915928
line 256 mcts: sample exp_bonus 40.87027538488286
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.891225
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
printing an ep nov before normalisation:  32.199842288007886
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8895732
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
maxi score, test score, baseline:  0.24009999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.88775235
printing an ep nov before normalisation:  0.03761617472108014
actor:  0 policy actor:  0  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 43.89838104472458
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.711603164672855
printing an ep nov before normalisation:  33.765633425992156
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0031,     0.9402,     0.0173,     0.0003,     0.0392],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0236,     0.8853,     0.0009,     0.0900],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([0.0015, 0.0043, 0.0011, 0.8996, 0.0935], grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0019, 0.1474, 0.0884, 0.0704, 0.6918], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24209999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.24409999999999998 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.88795
using explorer policy with actor:  1
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
maxi score, test score, baseline:  0.24609999999999999 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.22818223677407
Printing some Q and Qe and total Qs values:  [[0.88 ]
 [0.88 ]
 [0.88 ]
 [0.953]
 [0.88 ]] [[30.589]
 [30.589]
 [30.589]
 [34.586]
 [30.589]] [[1.946]
 [1.946]
 [1.946]
 [2.291]
 [1.946]]
printing an ep nov before normalisation:  48.109531401866406
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Starting evaluation
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.267188477750782
printing an ep nov before normalisation:  60.37594868792282
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.829]
 [0.829]
 [0.829]
 [0.925]
 [0.829]] [[28.144]
 [28.144]
 [28.144]
 [34.019]
 [28.144]] [[1.003]
 [1.003]
 [1.003]
 [1.173]
 [1.003]]
maxi score, test score, baseline:  0.2481 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.733369827270508
Printing some Q and Qe and total Qs values:  [[0.738]
 [0.853]
 [0.751]
 [0.688]
 [0.77 ]] [[71.389]
 [48.737]
 [46.998]
 [71.654]
 [56.538]] [[0.738]
 [0.853]
 [0.751]
 [0.688]
 [0.77 ]]
actions average: 
K:  1  action  0 :  tensor([    0.9976,     0.0001,     0.0013,     0.0000,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0008,     0.9189,     0.0275,     0.0009,     0.0519],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9998,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0001,     0.0001,     0.9486,     0.0511],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0044, 0.0896, 0.0472, 0.1720, 0.6867], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]] [[42.167]
 [42.167]
 [42.167]
 [42.167]
 [42.167]] [[0.884]
 [0.884]
 [0.884]
 [0.884]
 [0.884]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  42.52084441047763
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  0.002437673256281414
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.28409999999999996 1.0 1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actions average: 
K:  2  action  0 :  tensor([    0.9982,     0.0001,     0.0000,     0.0009,     0.0008],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0015,     0.9726,     0.0000,     0.0003,     0.0256],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0019,     0.9616,     0.0224,     0.0140],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0004,     0.0011,     0.0372,     0.9119,     0.0494],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0116, 0.0741, 0.0211, 0.0381, 0.8551], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.822187639391146
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  42.54149715230547
printing an ep nov before normalisation:  38.2038303663681
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.28809999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.2950768138803
printing an ep nov before normalisation:  33.61304607505946
UNIT TEST: sample policy line 217 mcts : [0.282 0.59  0.026 0.051 0.051]
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.24944795441059
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  25.880869759748375
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.29209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.7526803202845
maxi score, test score, baseline:  0.2941 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.274]
 [1.34 ]
 [1.274]
 [1.274]
 [1.274]] [[38.034]
 [40.251]
 [38.034]
 [38.034]
 [38.034]] [[2.831]
 [3.05 ]
 [2.831]
 [2.831]
 [2.831]]
printing an ep nov before normalisation:  31.4971892617113
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.383]
 [1.446]
 [1.387]
 [1.343]
 [1.34 ]] [[28.87 ]
 [24.346]
 [27.93 ]
 [29.537]
 [26.279]] [[2.594]
 [2.297]
 [2.523]
 [2.607]
 [2.345]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  30.302266246127385
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.883907392023552
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  51.84251805321169
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.910638685836567
printing an ep nov before normalisation:  24.533720016479492
Printing some Q and Qe and total Qs values:  [[0.292]
 [0.398]
 [0.561]
 [0.398]
 [0.398]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.292]
 [0.398]
 [0.561]
 [0.398]
 [0.398]]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([    0.9996,     0.0000,     0.0000,     0.0001,     0.0002],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0003,     0.9731,     0.0001,     0.0001,     0.0264],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0000,     0.0001,     0.9998,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0001,     0.0178,     0.8846,     0.0973],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0300, 0.0019, 0.0704, 0.1567, 0.7410], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.957995414733887
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.79712247848511
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.053883946477185
printing an ep nov before normalisation:  23.292743350380405
printing an ep nov before normalisation:  48.05265712223004
printing an ep nov before normalisation:  35.91107130050659
actor:  1 policy actor:  1  step number:  81 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.2941 1.0 1.0
printing an ep nov before normalisation:  43.820669380597124
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.2941 1.0 1.0
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  42.217431922025945
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.133]
 [1.197]
 [1.133]
 [1.133]
 [1.061]] [[46.52 ]
 [41.122]
 [46.52 ]
 [46.52 ]
 [45.046]] [[2.965]
 [2.606]
 [2.965]
 [2.965]
 [2.778]]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8876796
actor:  1 policy actor:  1  step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  42.987152598026924
printing an ep nov before normalisation:  91.79419159462951
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.69676208496094
actor:  0 policy actor:  0  step number:  100 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.39957162973309
Printing some Q and Qe and total Qs values:  [[1.057]
 [1.057]
 [1.057]
 [1.057]
 [1.057]] [[47.962]
 [47.962]
 [47.962]
 [47.962]
 [47.962]] [[2.168]
 [2.168]
 [2.168]
 [2.168]
 [2.168]]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.002367780563844
printing an ep nov before normalisation:  44.518961906433105
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.7522053416264
using explorer policy with actor:  1
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.2961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  0 policy actor:  0  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.2981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  53.07550098993837
line 256 mcts: sample exp_bonus 34.27000849769651
printing an ep nov before normalisation:  32.73903407889147
siam score:  -0.88988554
printing an ep nov before normalisation:  28.428305404105096
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  72.12170568428823
printing an ep nov before normalisation:  42.03537464141846
printing an ep nov before normalisation:  31.33083816507642
maxi score, test score, baseline:  0.3001 1.0 1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.036811469332065
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  44.21054207922429
actor:  1 policy actor:  1  step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  6.217076392465515
maxi score, test score, baseline:  0.3001 1.0 1.0
printing an ep nov before normalisation:  51.189025369704424
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.63312513945388
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.493]
 [1.493]
 [1.464]
 [1.464]
 [1.493]] [[28.912]
 [29.915]
 [31.429]
 [31.429]
 [30.876]] [[2.916]
 [3.006]
 [3.113]
 [3.113]
 [3.092]]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  49.82891528261121
printing an ep nov before normalisation:  24.357067636263828
printing an ep nov before normalisation:  0.07942527091699958
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.483]
 [1.405]
 [1.284]
 [1.418]
 [1.379]] [[37.931]
 [36.55 ]
 [31.906]
 [35.341]
 [36.712]] [[2.381]
 [2.254]
 [1.965]
 [2.223]
 [2.234]]
Printing some Q and Qe and total Qs values:  [[1.503]
 [1.503]
 [1.503]
 [1.503]
 [1.503]] [[38.421]
 [38.421]
 [38.421]
 [38.421]
 [38.421]] [[2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3001 1.0 1.0
printing an ep nov before normalisation:  23.861135755266464
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.92227752012938
printing an ep nov before normalisation:  39.809035628097575
actions average: 
K:  1  action  0 :  tensor([    0.9771,     0.0002,     0.0002,     0.0002,     0.0222],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9698,     0.0183,     0.0003,     0.0116],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0000,     0.9743,     0.0118,     0.0139],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0010,     0.0003,     0.0002,     0.8207,     0.1778],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0013, 0.0788, 0.0358, 0.1109, 0.7732], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.854]
 [0.842]
 [0.854]
 [0.906]
 [0.854]] [[45.037]
 [50.849]
 [45.037]
 [39.033]
 [45.037]] [[1.948]
 [2.175]
 [1.948]
 [1.753]
 [1.948]]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[0.567]
 [0.003]
 [0.689]
 [0.508]
 [0.612]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.567]
 [0.003]
 [0.689]
 [0.508]
 [0.612]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  42.34996136640552
printing an ep nov before normalisation:  42.82105073933455
printing an ep nov before normalisation:  25.718891620635986
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.393371247833908
printing an ep nov before normalisation:  47.982844055706764
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.598512982739692
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.65582385898867
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.031713937207474
printing an ep nov before normalisation:  49.76910685687571
printing an ep nov before normalisation:  44.046308146491036
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  50.98321935036376
maxi score, test score, baseline:  0.3021 1.0 1.0
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3021 1.0 1.0
printing an ep nov before normalisation:  37.75694268520437
maxi score, test score, baseline:  0.3021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  36.110991603440525
maxi score, test score, baseline:  0.3041 1.0 1.0
printing an ep nov before normalisation:  35.485280475008274
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.204289693223316
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.405]
 [1.405]
 [1.405]
 [1.405]
 [1.405]] [[37.735]
 [37.735]
 [37.735]
 [37.735]
 [37.735]] [[2.345]
 [2.345]
 [2.345]
 [2.345]
 [2.345]]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8851283
Printing some Q and Qe and total Qs values:  [[0.938]
 [0.938]
 [0.938]
 [1.077]
 [0.962]] [[31.571]
 [31.571]
 [31.571]
 [32.915]
 [32.633]] [[1.123]
 [1.123]
 [1.123]
 [1.275]
 [1.158]]
printing an ep nov before normalisation:  45.65668895247309
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.142583071513847
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.674]
 [0.674]
 [0.674]
 [0.674]
 [0.674]]
printing an ep nov before normalisation:  46.0924282184765
printing an ep nov before normalisation:  41.33229222532684
actor:  0 policy actor:  0  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  33.76552215309321
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.618]
 [0.726]
 [0.611]
 [0.626]
 [0.619]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.618]
 [0.726]
 [0.611]
 [0.626]
 [0.619]]
printing an ep nov before normalisation:  51.02841380592803
Printing some Q and Qe and total Qs values:  [[0.784]
 [0.784]
 [0.784]
 [0.784]
 [1.015]] [[36.166]
 [36.166]
 [36.166]
 [36.166]
 [40.484]] [[1.274]
 [1.274]
 [1.274]
 [1.274]
 [1.64 ]]
printing an ep nov before normalisation:  42.58758054997069
using explorer policy with actor:  1
printing an ep nov before normalisation:  37.62834074693449
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 47.65287699276941
Printing some Q and Qe and total Qs values:  [[0.035]
 [0.765]
 [0.993]
 [0.868]
 [0.894]] [[26.082]
 [29.545]
 [28.445]
 [35.305]
 [24.885]] [[0.19 ]
 [0.959]
 [1.174]
 [1.126]
 [1.036]]
printing an ep nov before normalisation:  56.81861644157272
printing an ep nov before normalisation:  45.69019943801437
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
printing an ep nov before normalisation:  30.692858695983887
printing an ep nov before normalisation:  42.830013407078155
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.710698608697854
printing an ep nov before normalisation:  26.04992979380121
printing an ep nov before normalisation:  37.442907007788776
siam score:  -0.8865764
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8868111
Printing some Q and Qe and total Qs values:  [[0.683]
 [0.79 ]
 [0.683]
 [0.683]
 [0.669]] [[39.419]
 [54.466]
 [39.419]
 [39.419]
 [45.717]] [[0.811]
 [1.036]
 [0.811]
 [0.811]
 [0.846]]
printing an ep nov before normalisation:  42.484339511932816
Printing some Q and Qe and total Qs values:  [[0.834]
 [0.862]
 [0.864]
 [0.952]
 [0.947]] [[50.843]
 [50.975]
 [45.975]
 [36.727]
 [44.027]] [[1.113]
 [1.142]
 [1.097]
 [1.096]
 [1.161]]
printing an ep nov before normalisation:  49.461272092895385
printing an ep nov before normalisation:  63.074140548706055
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.309]
 [0.309]
 [0.362]
 [0.304]
 [0.357]] [[29.624]
 [30.371]
 [31.213]
 [31.29 ]
 [31.389]] [[1.817]
 [1.899]
 [2.046]
 [1.996]
 [2.06 ]]
printing an ep nov before normalisation:  40.4601294581582
Printing some Q and Qe and total Qs values:  [[0.775]
 [0.799]
 [0.808]
 [0.808]
 [0.808]] [[67.255]
 [61.696]
 [59.134]
 [59.134]
 [59.134]] [[2.512]
 [2.33 ]
 [2.244]
 [2.244]
 [2.244]]
maxi score, test score, baseline:  0.3081 1.0 1.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.99069358742767
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.845462377731415
printing an ep nov before normalisation:  46.57720433477398
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
printing an ep nov before normalisation:  44.87909175462565
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  43.168431253898895
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3081 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.469]
 [0.672]
 [0.683]
 [0.535]
 [0.523]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.469]
 [0.672]
 [0.683]
 [0.535]
 [0.523]]
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 0.0
Printing some Q and Qe and total Qs values:  [[0.527]
 [0.54 ]
 [0.538]
 [0.667]
 [0.563]] [[47.683]
 [49.789]
 [40.328]
 [33.474]
 [42.332]] [[0.527]
 [0.54 ]
 [0.538]
 [0.667]
 [0.563]]
line 256 mcts: sample exp_bonus 34.204529317343315
printing an ep nov before normalisation:  35.931350128690156
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3101 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.2663654589148
actor:  0 policy actor:  0  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  39.08860298194019
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  53.56364595461855
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.716]
 [0.741]
 [0.811]
 [0.749]] [[38.494]
 [34.022]
 [36.865]
 [32.935]
 [35.836]] [[1.787]
 [1.548]
 [1.716]
 [1.588]
 [1.673]]
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  32.268647538770985
actor:  1 policy actor:  1  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
siam score:  -0.88310426
maxi score, test score, baseline:  0.3121 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.205 0.051 0.436 0.128 0.179]
actor:  0 policy actor:  0  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.772]
 [0.647]
 [0.689]
 [0.691]
 [0.703]] [[33.968]
 [29.091]
 [34.287]
 [38.198]
 [34.568]] [[0.772]
 [0.647]
 [0.689]
 [0.691]
 [0.703]]
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3141 1.0 1.0
maxi score, test score, baseline:  0.3141 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8789555
printing an ep nov before normalisation:  33.89084741177476
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.67445571945018
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3161 1.0 1.0
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  35.793137550354004
maxi score, test score, baseline:  0.3161 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.906679964645214
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.772]
 [0.772]
 [0.842]
 [0.772]] [[30.996]
 [28.166]
 [28.166]
 [29.193]
 [28.166]] [[1.101]
 [1.069]
 [1.069]
 [1.16 ]
 [1.069]]
Printing some Q and Qe and total Qs values:  [[1.04]
 [1.04]
 [1.04]
 [1.04]
 [1.04]] [[56.599]
 [56.599]
 [56.599]
 [56.599]
 [56.599]] [[1.938]
 [1.938]
 [1.938]
 [1.938]
 [1.938]]
printing an ep nov before normalisation:  35.686645615421014
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3161 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.553]
 [0.558]
 [0.569]
 [0.637]
 [0.548]] [[33.781]
 [42.209]
 [33.601]
 [26.612]
 [33.456]] [[0.553]
 [0.558]
 [0.569]
 [0.637]
 [0.548]]
printing an ep nov before normalisation:  33.624524535432734
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3161 1.0 1.0
actor:  0 policy actor:  0  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3181 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3201 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.028]
 [0.026]
 [0.012]
 [0.01 ]
 [0.028]] [[1.904]
 [1.943]
 [2.305]
 [2.382]
 [1.925]] [[0.028]
 [0.026]
 [0.012]
 [0.01 ]
 [0.028]]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.081]
 [0.721]
 [0.841]
 [0.479]
 [0.724]] [[34.968]
 [37.186]
 [28.748]
 [29.5  ]
 [31.971]] [[1.343]
 [2.118]
 [1.727]
 [1.41 ]
 [1.805]]
Printing some Q and Qe and total Qs values:  [[0.107]
 [0.776]
 [0.79 ]
 [0.728]
 [0.795]] [[38.52 ]
 [39.133]
 [40.974]
 [35.927]
 [33.861]] [[1.247]
 [1.948]
 [2.055]
 [1.738]
 [1.699]]
Printing some Q and Qe and total Qs values:  [[1.455]
 [1.455]
 [1.455]
 [1.455]
 [1.455]] [[34.312]
 [34.312]
 [34.312]
 [34.312]
 [34.312]] [[3.167]
 [3.167]
 [3.167]
 [3.167]
 [3.167]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[1.013]
 [1.013]
 [1.013]
 [1.049]
 [0.993]] [[53.276]
 [53.276]
 [53.276]
 [51.142]
 [53.625]] [[2.358]
 [2.358]
 [2.358]
 [2.317]
 [2.351]]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.76113450308555
printing an ep nov before normalisation:  36.072313276250526
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.2944375549934
siam score:  -0.8752446
printing an ep nov before normalisation:  36.15511890110007
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  29.37594175338745
printing an ep nov before normalisation:  30.831215381622314
printing an ep nov before normalisation:  67.19362441413556
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  43.73082940353093
Printing some Q and Qe and total Qs values:  [[1.454]
 [1.396]
 [1.353]
 [1.223]
 [1.25 ]] [[31.421]
 [31.204]
 [29.406]
 [25.234]
 [30.666]] [[3.005]
 [2.928]
 [2.725]
 [2.222]
 [2.734]]
printing an ep nov before normalisation:  42.887007604548245
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  28.663139186118638
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 31.997977850001014
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.783]
 [0.783]
 [0.783]
 [0.783]] [[48.478]
 [45.106]
 [45.106]
 [45.106]
 [45.106]] [[0.798]
 [0.783]
 [0.783]
 [0.783]
 [0.783]]
maxi score, test score, baseline:  0.3201 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.675807652209016
actor:  0 policy actor:  0  step number:  74 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  55.88689794996977
printing an ep nov before normalisation:  35.74129122481759
Printing some Q and Qe and total Qs values:  [[0.514]
 [0.523]
 [0.512]
 [0.571]
 [0.52 ]] [[30.442]
 [30.241]
 [30.229]
 [29.41 ]
 [30.055]] [[0.514]
 [0.523]
 [0.512]
 [0.571]
 [0.52 ]]
printing an ep nov before normalisation:  37.75609774243302
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.557]
 [0.719]
 [0.726]
 [0.67 ]
 [0.683]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.557]
 [0.719]
 [0.726]
 [0.67 ]
 [0.683]]
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
rdn beta is 0 so we're just using the maxi policy
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3261 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.746]
 [0.735]
 [0.746]
 [0.746]
 [0.746]] [[33.634]
 [39.578]
 [33.634]
 [33.634]
 [33.634]] [[2.236]
 [2.703]
 [2.236]
 [2.236]
 [2.236]]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.30177489920119
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9922,     0.0002,     0.0049,     0.0012,     0.0015],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9411,     0.0203,     0.0012,     0.0373],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0000,     0.0211,     0.9381,     0.0003,     0.0405],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0003,     0.0001,     0.9989,     0.0007],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([    0.0005,     0.0024,     0.0052,     0.2937,     0.6982],
       grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  55.01899913969677
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.165627031745004
actor:  1 policy actor:  1  step number:  94 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.771679932493264
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.11568318267585
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3241 1.0 1.0
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.586]
 [0.458]
 [0.458]
 [0.461]] [[34.509]
 [46.164]
 [34.509]
 [34.509]
 [35.349]] [[0.458]
 [0.586]
 [0.458]
 [0.458]
 [0.461]]
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.813052819965144
maxi score, test score, baseline:  0.3241 1.0 1.0
maxi score, test score, baseline:  0.3241 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actions average: 
K:  4  action  0 :  tensor([    0.9552,     0.0073,     0.0000,     0.0010,     0.0365],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0000,     0.9369,     0.0164,     0.0001,     0.0466],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0191,     0.9651,     0.0005,     0.0153],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0008,     0.0187,     0.0078,     0.9117,     0.0611],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0011, 0.0760, 0.0288, 0.0981, 0.7959], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.70424907904042
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  32.42436910103919
maxi score, test score, baseline:  0.3261 1.0 1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3261 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.179]
 [1.079]
 [1.1  ]
 [1.028]
 [1.119]] [[42.902]
 [43.807]
 [38.22 ]
 [49.981]
 [43.65 ]] [[1.347]
 [1.255]
 [1.228]
 [1.256]
 [1.292]]
Printing some Q and Qe and total Qs values:  [[1.328]
 [1.386]
 [1.328]
 [1.328]
 [1.328]] [[34.129]
 [42.72 ]
 [34.129]
 [34.129]
 [34.129]] [[1.559]
 [1.719]
 [1.559]
 [1.559]
 [1.559]]
Printing some Q and Qe and total Qs values:  [[1.127]
 [1.04 ]
 [1.   ]
 [1.096]
 [1.035]] [[48.791]
 [26.877]
 [37.405]
 [47.093]
 [33.529]] [[1.486]
 [1.155]
 [1.233]
 [1.437]
 [1.225]]
printing an ep nov before normalisation:  47.99339760875048
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.007]
 [1.119]
 [1.139]
 [1.109]
 [1.102]] [[25.221]
 [32.381]
 [26.699]
 [27.118]
 [24.85 ]] [[0.254]
 [1.5  ]
 [1.414]
 [1.392]
 [1.342]]
printing an ep nov before normalisation:  50.71135324683837
printing an ep nov before normalisation:  50.80705635076693
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  42.07163198253378
Printing some Q and Qe and total Qs values:  [[1.411]
 [1.372]
 [1.394]
 [1.324]
 [1.337]] [[42.048]
 [44.497]
 [47.466]
 [42.022]
 [44.13 ]] [[1.673]
 [1.662]
 [1.719]
 [1.586]
 [1.623]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  23.42791794821299
using explorer policy with actor:  1
printing an ep nov before normalisation:  60.356622114773636
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  2  action  0 :  tensor([    0.9755,     0.0009,     0.0000,     0.0003,     0.0232],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9778,     0.0001,     0.0002,     0.0219],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0003,     0.0202,     0.9449,     0.0002,     0.0345],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0003,     0.0003,     0.0283,     0.9281,     0.0431],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0009, 0.0624, 0.0589, 0.1090, 0.7688], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3261 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
actions average: 
K:  0  action  0 :  tensor([    0.9992,     0.0000,     0.0000,     0.0004,     0.0004],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0009,     0.9794,     0.0001,     0.0003,     0.0193],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0185,     0.9494,     0.0003,     0.0317],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0002,     0.0002,     0.0006,     0.9629,     0.0361],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0081, 0.1385, 0.0258, 0.1201, 0.7075], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.059]
 [1.059]
 [1.059]
 [1.059]] [[56.662]
 [36.246]
 [36.246]
 [36.246]
 [36.246]] [[2.143]
 [1.578]
 [1.578]
 [1.578]
 [1.578]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.78909610973929
printing an ep nov before normalisation:  48.62047953024682
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  38.5860192326996
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.87630624
printing an ep nov before normalisation:  33.91794395068995
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.64480569051745
printing an ep nov before normalisation:  33.5604542581544
maxi score, test score, baseline:  0.3281 1.0 1.0
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.8  ]
 [0.823]
 [0.852]
 [0.852]
 [0.852]] [[53.486]
 [58.007]
 [50.72 ]
 [50.72 ]
 [50.72 ]] [[1.795]
 [1.966]
 [1.756]
 [1.756]
 [1.756]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.65392355251334
actor:  1 policy actor:  1  step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  40.2546842513541
Printing some Q and Qe and total Qs values:  [[0.592]
 [0.592]
 [0.592]
 [0.592]
 [0.592]] [[39.79]
 [39.79]
 [39.79]
 [39.79]
 [39.79]] [[0.763]
 [0.763]
 [0.763]
 [0.763]
 [0.763]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8759372
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.144525810855164
printing an ep nov before normalisation:  69.89907575105262
Printing some Q and Qe and total Qs values:  [[1.346]
 [1.291]
 [1.292]
 [1.292]
 [1.292]] [[31.873]
 [34.666]
 [32.053]
 [32.053]
 [32.053]] [[1.79 ]
 [1.803]
 [1.74 ]
 [1.74 ]
 [1.74 ]]
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
siam score:  -0.8710855
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  22.45830442407098
printing an ep nov before normalisation:  38.04944959957453
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  40.728108708585104
maxi score, test score, baseline:  0.3281 1.0 1.0
maxi score, test score, baseline:  0.3281 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
rdn beta is 0 so we're just using the maxi policy
actor:  0 policy actor:  0  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.27585216342951
Printing some Q and Qe and total Qs values:  [[0.994]
 [0.994]
 [0.994]
 [1.005]
 [1.022]] [[64.442]
 [64.442]
 [64.442]
 [64.928]
 [65.154]] [[2.201]
 [2.201]
 [2.201]
 [2.224]
 [2.247]]
printing an ep nov before normalisation:  46.32921826264535
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  30.246127763138492
printing an ep nov before normalisation:  39.66793061295542
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  8.394936974164011
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.96864091725492
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  11.430882215499878
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  19.781754311828905
maxi score, test score, baseline:  0.3301 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  70.20130912068913
actor:  0 policy actor:  0  step number:  103 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3321 1.0 1.0
printing an ep nov before normalisation:  39.227151628570695
printing an ep nov before normalisation:  39.93016401426997
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.754]
 [0.754]
 [0.754]
 [0.754]
 [0.754]] [[53.971]
 [53.971]
 [53.971]
 [53.971]
 [53.971]] [[2.045]
 [2.045]
 [2.045]
 [2.045]
 [2.045]]
printing an ep nov before normalisation:  56.982584780777245
maxi score, test score, baseline:  0.3321 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.983685417740364
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  69 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  66.33860721319857
printing an ep nov before normalisation:  45.22786312960586
printing an ep nov before normalisation:  47.46892349291853
line 256 mcts: sample exp_bonus 53.2106988600207
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.312]
 [1.417]
 [1.168]
 [0.713]
 [1.138]] [[24.643]
 [20.897]
 [29.941]
 [28.121]
 [26.963]] [[1.947]
 [1.843]
 [2.098]
 [1.542]
 [1.902]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
Printing some Q and Qe and total Qs values:  [[0.652]
 [0.748]
 [0.809]
 [0.809]
 [0.813]] [[51.636]
 [49.723]
 [47.868]
 [47.868]
 [49.777]] [[1.905]
 [1.918]
 [1.899]
 [1.899]
 [1.985]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  92 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3341 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.789]
 [0.806]
 [0.789]
 [0.789]
 [0.789]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.789]
 [0.806]
 [0.789]
 [0.789]
 [0.789]]
siam score:  -0.8585785
printing an ep nov before normalisation:  37.71077884621433
Printing some Q and Qe and total Qs values:  [[1.1  ]
 [1.05 ]
 [1.05 ]
 [1.042]
 [1.017]] [[49.016]
 [47.604]
 [47.604]
 [53.407]
 [47.561]] [[1.541]
 [1.472]
 [1.472]
 [1.541]
 [1.438]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  1 policy actor:  1  step number:  82 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  51.154041289576654
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  25.70593357008531
maxi score, test score, baseline:  0.3341 1.0 1.0
printing an ep nov before normalisation:  67.45748321518253
printing an ep nov before normalisation:  26.003257469730954
printing an ep nov before normalisation:  67.39008475948656
maxi score, test score, baseline:  0.3341 1.0 1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.554]
 [0.568]
 [0.716]
 [0.568]
 [0.568]] [[34.681]
 [34.389]
 [39.251]
 [34.389]
 [34.389]] [[1.221]
 [1.223]
 [1.562]
 [1.223]
 [1.223]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.965]
 [0.908]
 [0.012]
 [1.002]
 [0.963]] [[42.705]
 [32.086]
 [35.001]
 [49.934]
 [34.973]] [[1.487]
 [1.19 ]
 [0.359]
 [1.687]
 [1.309]]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  55.74707690812687
printing an ep nov before normalisation:  25.755402449998194
maxi score, test score, baseline:  0.3341 1.0 1.0
printing an ep nov before normalisation:  37.33945574334061
Printing some Q and Qe and total Qs values:  [[0.062]
 [0.523]
 [0.562]
 [0.577]
 [0.534]] [[25.039]
 [19.187]
 [24.254]
 [30.001]
 [21.067]] [[0.062]
 [0.523]
 [0.562]
 [0.577]
 [0.534]]
printing an ep nov before normalisation:  27.961967120123155
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3341 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  80 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actor:  1 policy actor:  1  step number:  105 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.191050146846536
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.458]
 [0.523]
 [0.458]
 [0.458]
 [0.458]] [[46.669]
 [48.174]
 [46.669]
 [46.669]
 [48.496]] [[0.458]
 [0.523]
 [0.458]
 [0.458]
 [0.458]]
printing an ep nov before normalisation:  52.555888384534555
actor:  0 policy actor:  0  step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.860621
printing an ep nov before normalisation:  24.534374359075013
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85927826
maxi score, test score, baseline:  0.3381 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  90 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  83 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  36.41679502426762
Printing some Q and Qe and total Qs values:  [[1.046]
 [0.958]
 [0.958]
 [0.958]
 [0.958]] [[32.148]
 [29.436]
 [29.436]
 [29.436]
 [29.436]] [[1.385]
 [1.247]
 [1.247]
 [1.247]
 [1.247]]
line 256 mcts: sample exp_bonus 39.062133452917536
maxi score, test score, baseline:  0.3401 1.0 1.0
printing an ep nov before normalisation:  32.492756843566895
actions average: 
K:  1  action  0 :  tensor([    0.9585,     0.0056,     0.0002,     0.0002,     0.0356],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0000,     0.9520,     0.0003,     0.0006,     0.0470],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([0.0020, 0.0015, 0.9653, 0.0177, 0.0135], grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0002,     0.0038,     0.9090,     0.0865],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([    0.0005,     0.1782,     0.0259,     0.0731,     0.7223],
       grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[0.806]
 [0.782]
 [0.782]
 [0.782]
 [0.782]] [[53.986]
 [52.83 ]
 [52.83 ]
 [52.83 ]
 [52.83 ]] [[2.041]
 [1.967]
 [1.967]
 [1.967]
 [1.967]]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
siam score:  -0.85243154
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.57825677072011
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.762]
 [0.77 ]
 [0.757]
 [0.771]] [[33.65 ]
 [31.898]
 [28.429]
 [29.692]
 [27.96 ]] [[1.916]
 [1.781]
 [1.593]
 [1.651]
 [1.567]]
printing an ep nov before normalisation:  49.12964070147132
siam score:  -0.85427785
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
Starting evaluation
maxi score, test score, baseline:  0.3401 1.0 1.0
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.528]
 [0.528]
 [0.528]
 [0.528]
 [0.528]]
printing an ep nov before normalisation:  43.07546705116838
printing an ep nov before normalisation:  36.67596971348178
printing an ep nov before normalisation:  36.893136501312256
printing an ep nov before normalisation:  47.216427190614716
printing an ep nov before normalisation:  55.48517386218868
printing an ep nov before normalisation:  35.3465466570205
printing an ep nov before normalisation:  51.38451963789457
printing an ep nov before normalisation:  38.23269166801074
Printing some Q and Qe and total Qs values:  [[0.797]
 [0.913]
 [0.797]
 [0.812]
 [0.845]] [[32.587]
 [40.546]
 [32.587]
 [26.942]
 [29.792]] [[0.797]
 [0.913]
 [0.797]
 [0.812]
 [0.845]]
Printing some Q and Qe and total Qs values:  [[0.758]
 [0.904]
 [0.893]
 [0.893]
 [0.838]] [[30.643]
 [41.646]
 [29.444]
 [29.444]
 [30.476]] [[0.758]
 [0.904]
 [0.893]
 [0.893]
 [0.838]]
Printing some Q and Qe and total Qs values:  [[0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]] [[43.308]
 [43.308]
 [43.308]
 [43.308]
 [43.308]] [[0.952]
 [0.952]
 [0.952]
 [0.952]
 [0.952]]
maxi score, test score, baseline:  0.3401 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[1.045]
 [0.968]
 [0.968]
 [0.996]
 [0.968]] [[48.675]
 [39.028]
 [39.028]
 [51.357]
 [39.028]] [[1.977]
 [1.625]
 [1.625]
 [2.004]
 [1.625]]
printing an ep nov before normalisation:  57.044901034415126
Printing some Q and Qe and total Qs values:  [[0.98 ]
 [1.114]
 [0.98 ]
 [0.984]
 [1.052]] [[57.078]
 [50.992]
 [57.078]
 [58.917]
 [57.938]] [[2.521]
 [2.418]
 [2.521]
 [2.596]
 [2.626]]
printing an ep nov before normalisation:  57.33016641504424
Printing some Q and Qe and total Qs values:  [[0.606]
 [0.688]
 [0.628]
 [0.628]
 [0.628]] [[47.066]
 [47.524]
 [45.93 ]
 [45.93 ]
 [45.93 ]] [[1.526]
 [1.625]
 [1.506]
 [1.506]
 [1.506]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.50779914855957
printing an ep nov before normalisation:  55.12225409584081
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  56.521160878106905
actor:  1 policy actor:  1  step number:  73 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  56.19253017727318
printing an ep nov before normalisation:  38.21709845979341
printing an ep nov before normalisation:  33.59742098048206
printing an ep nov before normalisation:  34.404405553207745
siam score:  -0.86374253
maxi score, test score, baseline:  0.3801 1.0 1.0
maxi score, test score, baseline:  0.3801 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  70.95308096856647
UNIT TEST: sample policy line 217 mcts : [0.564 0.179 0.077 0.051 0.128]
actor:  0 policy actor:  0  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.904]
 [0.904]
 [0.904]
 [1.041]
 [0.904]] [[49.53]
 [49.53]
 [49.53]
 [52.17]
 [49.53]] [[1.84 ]
 [1.84 ]
 [1.84 ]
 [2.066]
 [1.84 ]]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.264723777770996
UNIT TEST: sample policy line 217 mcts : [0.051 0.872 0.    0.026 0.051]
line 256 mcts: sample exp_bonus 44.47745504121508
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  0  action  0 :  tensor([    0.9699,     0.0001,     0.0000,     0.0001,     0.0299],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9887,     0.0001,     0.0002,     0.0108],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0003,     0.9754,     0.0112,     0.0130],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0009,     0.0003,     0.0000,     0.8902,     0.1086],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0271, 0.0462, 0.0221, 0.1659, 0.7387], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.649]
 [0.649]
 [0.649]
 [0.649]
 [0.649]]
printing an ep nov before normalisation:  59.91659868474176
Printing some Q and Qe and total Qs values:  [[0.535]
 [0.535]
 [0.535]
 [0.548]
 [0.535]] [[37.403]
 [37.403]
 [37.403]
 [38.605]
 [37.403]] [[0.535]
 [0.535]
 [0.535]
 [0.548]
 [0.535]]
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  17.277600331709436
maxi score, test score, baseline:  0.3821 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.72 ]
 [0.848]
 [0.72 ]
 [0.72 ]
 [0.719]] [[34.304]
 [31.971]
 [34.304]
 [34.304]
 [34.322]] [[0.72 ]
 [0.848]
 [0.72 ]
 [0.72 ]
 [0.719]]
actor:  0 policy actor:  0  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  28.917613810936018
actor:  0 policy actor:  0  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9673,     0.0001,     0.0002,     0.0002,     0.0323],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0004,     0.9209,     0.0083,     0.0003,     0.0701],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0001,     0.0002,     0.9995,     0.0001,     0.0001],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0000,     0.0005,     0.0000,     0.9993,     0.0003],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([    0.0006,     0.0022,     0.0110,     0.1729,     0.8133],
       grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.043]
 [1.262]
 [1.137]
 [1.161]
 [1.187]] [[33.555]
 [39.107]
 [30.144]
 [34.52 ]
 [32.101]] [[0.197]
 [1.464]
 [1.26 ]
 [1.323]
 [1.328]]
actions average: 
K:  2  action  0 :  tensor([    0.9866,     0.0001,     0.0000,     0.0066,     0.0067],
       grad_fn=<DivBackward0>)
K:  2  action  1 :  tensor([    0.0001,     0.9646,     0.0001,     0.0002,     0.0351],
       grad_fn=<DivBackward0>)
K:  2  action  2 :  tensor([    0.0001,     0.0000,     0.9667,     0.0002,     0.0330],
       grad_fn=<DivBackward0>)
K:  2  action  3 :  tensor([    0.0000,     0.0002,     0.0010,     0.9378,     0.0610],
       grad_fn=<DivBackward0>)
K:  2  action  4 :  tensor([0.0248, 0.0384, 0.0023, 0.1003, 0.8342], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  63.80797031741618
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.019]
 [1.037]
 [0.944]
 [0.961]
 [0.968]] [[40.963]
 [31.389]
 [36.781]
 [37.636]
 [36.994]] [[2.425]
 [1.809]
 [2.073]
 [2.147]
 [2.112]]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  26.851492570530038
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.196]
 [0.146]
 [1.171]
 [0.317]
 [1.171]] [[41.196]
 [42.095]
 [42.851]
 [41.889]
 [42.851]] [[0.516]
 [0.479]
 [1.516]
 [0.647]
 [1.516]]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.79889467030241
actor:  1 policy actor:  1  step number:  93 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3861 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
Printing some Q and Qe and total Qs values:  [[1.286]
 [1.277]
 [1.179]
 [1.277]
 [1.179]] [[24.919]
 [28.551]
 [28.559]
 [20.235]
 [28.559]] [[2.398]
 [2.558]
 [2.46 ]
 [2.17 ]
 [2.46 ]]
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3881 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.369]
 [1.418]
 [1.308]
 [1.308]
 [1.387]] [[34.779]
 [40.558]
 [31.301]
 [31.301]
 [38.162]] [[1.579]
 [1.687]
 [1.484]
 [1.484]
 [1.632]]
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3881 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  46.998904892228
printing an ep nov before normalisation:  52.87709704417731
maxi score, test score, baseline:  0.3901 1.0 1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
printing an ep nov before normalisation:  47.70415916248177
Printing some Q and Qe and total Qs values:  [[0.688]
 [0.688]
 [0.688]
 [0.688]
 [0.688]] [[49.46]
 [49.46]
 [49.46]
 [49.46]
 [49.46]] [[2.355]
 [2.355]
 [2.355]
 [2.355]
 [2.355]]
printing an ep nov before normalisation:  36.78832354612403
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.43354701123555
printing an ep nov before normalisation:  48.413180161497515
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.8511023
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3901 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
printing an ep nov before normalisation:  31.853494710223682
printing an ep nov before normalisation:  29.973913139504464
siam score:  -0.850532
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actions average: 
K:  3  action  0 :  tensor([    0.9987,     0.0008,     0.0000,     0.0000,     0.0006],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9533,     0.0003,     0.0001,     0.0462],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0001,     0.0000,     0.9276,     0.0299,     0.0423],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0002,     0.0012,     0.0005,     0.9709,     0.0272],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0012, 0.1293, 0.0285, 0.0023, 0.8387], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  30.968470573425293
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.3921 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.93513992231114
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.048]
 [1.048]
 [1.048]
 [1.072]
 [1.048]] [[30.083]
 [30.083]
 [30.083]
 [47.914]
 [30.083]] [[1.368]
 [1.368]
 [1.368]
 [1.755]
 [1.368]]
actor:  1 policy actor:  1  step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.026]
 [1.026]
 [1.026]
 [1.07 ]
 [1.018]] [[30.946]
 [30.946]
 [30.946]
 [44.399]
 [30.677]] [[1.248]
 [1.248]
 [1.248]
 [1.475]
 [1.236]]
maxi score, test score, baseline:  0.3941 1.0 1.0
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.585]
 [0.585]
 [0.585]
 [0.585]
 [0.585]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.896]
 [0.896]
 [0.896]
 [0.896]
 [0.868]] [[47.094]
 [47.094]
 [47.094]
 [47.094]
 [61.034]] [[1.56 ]
 [1.56 ]
 [1.56 ]
 [1.56 ]
 [1.857]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.821]
 [0.786]
 [0.786]
 [0.694]
 [0.727]] [[51.825]
 [46.017]
 [46.017]
 [57.941]
 [48.687]] [[0.821]
 [0.786]
 [0.786]
 [0.694]
 [0.727]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.898]
 [0.834]
 [0.898]
 [0.898]
 [0.898]] [[29.141]
 [54.437]
 [29.141]
 [29.141]
 [29.141]] [[1.208]
 [1.656]
 [1.208]
 [1.208]
 [1.208]]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3941 1.0 1.0
printing an ep nov before normalisation:  70.11519949812345
printing an ep nov before normalisation:  68.53945624233076
printing an ep nov before normalisation:  59.3622245851794
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
printing an ep nov before normalisation:  64.64997763470494
printing an ep nov before normalisation:  47.27087262108747
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
line 256 mcts: sample exp_bonus 48.896847791267874
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3941 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  26.355331443904248
line 256 mcts: sample exp_bonus 48.907423748038816
printing an ep nov before normalisation:  38.70434490669661
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  29.787462811606446
printing an ep nov before normalisation:  24.336538314819336
actor:  1 policy actor:  1  step number:  67 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.8617951
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  37.11699399387141
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  25.25905132293701
maxi score, test score, baseline:  0.3961 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.08877896069875
siam score:  -0.85884947
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.585]
 [0.58 ]
 [0.58 ]
 [0.649]
 [0.608]] [[36.615]
 [43.783]
 [33.375]
 [32.105]
 [31.775]] [[0.585]
 [0.58 ]
 [0.58 ]
 [0.649]
 [0.608]]
printing an ep nov before normalisation:  40.98846841006227
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.675]
 [0.638]
 [0.673]
 [0.68 ]
 [0.653]] [[48.816]
 [43.118]
 [47.218]
 [50.536]
 [49.053]] [[0.675]
 [0.638]
 [0.673]
 [0.68 ]
 [0.653]]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  120.066424240651
printing an ep nov before normalisation:  45.07056908884299
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.3981 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  63 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  73.92349999478986
Printing some Q and Qe and total Qs values:  [[1.243]
 [1.359]
 [1.243]
 [1.271]
 [1.36 ]] [[34.897]
 [40.817]
 [34.897]
 [30.467]
 [35.831]] [[2.094]
 [2.546]
 [2.094]
 [1.871]
 [2.264]]
actor:  1 policy actor:  1  step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  24.746992588043213
maxi score, test score, baseline:  0.3981 1.0 1.0
printing an ep nov before normalisation:  0.04234553505284566
printing an ep nov before normalisation:  30.139991256513248
printing an ep nov before normalisation:  45.69432723075756
printing an ep nov before normalisation:  40.481651805799096
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
siam score:  -0.84691024
printing an ep nov before normalisation:  0.011018529028632429
actor:  0 policy actor:  0  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
Printing some Q and Qe and total Qs values:  [[0.736]
 [0.736]
 [0.736]
 [0.736]
 [0.736]] [[44.597]
 [44.597]
 [44.597]
 [44.597]
 [44.597]] [[0.926]
 [0.926]
 [0.926]
 [0.926]
 [0.926]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  58.88554303375959
Printing some Q and Qe and total Qs values:  [[1.133]
 [1.071]
 [1.071]
 [1.071]
 [1.071]] [[52.569]
 [50.03 ]
 [50.03 ]
 [50.03 ]
 [50.03 ]] [[1.367]
 [1.291]
 [1.291]
 [1.291]
 [1.291]]
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  38.434132565508676
maxi score, test score, baseline:  0.4001 1.0 1.0
actor:  1 policy actor:  1  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.83701229095459
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  1  action  0 :  tensor([    0.9998,     0.0000,     0.0000,     0.0000,     0.0001],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0006,     0.9593,     0.0004,     0.0002,     0.0394],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0005,     0.0000,     0.9641,     0.0182,     0.0171],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0006,     0.0003,     0.0023,     0.9111,     0.0857],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0020, 0.0836, 0.0018, 0.0845, 0.8281], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  52.803961361924074
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  46.90373953779444
printing an ep nov before normalisation:  33.8902949435515
printing an ep nov before normalisation:  39.62676455413347
maxi score, test score, baseline:  0.4001 1.0 1.0
maxi score, test score, baseline:  0.4001 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.36197566986084
siam score:  -0.853815
Printing some Q and Qe and total Qs values:  [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.61 ]] [[38.326]
 [38.326]
 [38.326]
 [38.326]
 [38.688]] [[0.534]
 [0.534]
 [0.534]
 [0.534]
 [0.61 ]]
line 256 mcts: sample exp_bonus 60.1184102043608
printing an ep nov before normalisation:  35.25178909301758
line 256 mcts: sample exp_bonus 31.926172349165682
printing an ep nov before normalisation:  37.341622628464364
actor:  1 policy actor:  1  step number:  84 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  45.64987876023717
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  57.77960163286844
printing an ep nov before normalisation:  43.85291083605955
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.016807980302928627
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
using explorer policy with actor:  1
printing an ep nov before normalisation:  39.15103070491609
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  0.05932295746191585
actor:  1 policy actor:  1  step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.845]
 [0.845]
 [0.944]
 [0.845]] [[42.516]
 [42.516]
 [42.516]
 [39.719]
 [42.516]] [[1.176]
 [1.176]
 [1.176]
 [1.239]
 [1.176]]
printing an ep nov before normalisation:  41.29425797981652
printing an ep nov before normalisation:  39.60001766681671
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.03 ]
 [1.261]
 [1.185]
 [1.185]
 [1.118]] [[53.536]
 [40.111]
 [42.407]
 [42.407]
 [43.698]] [[1.913]
 [1.818]
 [1.798]
 [1.798]
 [1.762]]
printing an ep nov before normalisation:  40.713593375594414
printing an ep nov before normalisation:  32.717756400140544
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  72.19915946427099
printing an ep nov before normalisation:  46.31897892398045
printing an ep nov before normalisation:  30.385220274914182
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.08 ]
 [1.052]
 [1.034]
 [1.052]
 [1.057]] [[49.99 ]
 [35.779]
 [36.219]
 [40.944]
 [49.616]] [[2.406]
 [1.702]
 [1.705]
 [1.948]
 [2.366]]
Printing some Q and Qe and total Qs values:  [[1.24]
 [1.31]
 [1.24]
 [1.24]
 [1.24]] [[48.444]
 [50.429]
 [48.444]
 [48.444]
 [48.444]] [[2.995]
 [3.187]
 [2.995]
 [2.995]
 [2.995]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
UNIT TEST: sample policy line 217 mcts : [0.205 0.385 0.026 0.    0.385]
printing an ep nov before normalisation:  57.34892190323916
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  50.24496385698896
maxi score, test score, baseline:  0.4021 1.0 1.0
printing an ep nov before normalisation:  30.939680982811964
printing an ep nov before normalisation:  31.722434105873223
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.85328984
printing an ep nov before normalisation:  28.375833702736625
printing an ep nov before normalisation:  35.89928616013698
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4021 1.0 1.0
printing an ep nov before normalisation:  33.53642463684082
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.09 ]
 [1.074]
 [1.074]
 [1.074]
 [1.074]] [[47.713]
 [33.517]
 [33.517]
 [33.517]
 [33.517]] [[2.088]
 [1.58 ]
 [1.58 ]
 [1.58 ]
 [1.58 ]]
printing an ep nov before normalisation:  62.38034040329312
printing an ep nov before normalisation:  41.654556320298425
actor:  1 policy actor:  1  step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.897]
 [0.897]
 [0.897]
 [0.897]
 [0.897]]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  32.87704629837828
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
line 256 mcts: sample exp_bonus 33.83669108582856
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8587736
actor:  1 policy actor:  1  step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  64.78452623397108
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.765]
 [0.038]
 [0.657]
 [0.676]
 [0.689]] [[32.233]
 [27.309]
 [24.445]
 [24.797]
 [24.72 ]] [[1.398]
 [0.506]
 [1.028]
 [1.059]
 [1.069]]
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.62270898813695
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
actions average: 
K:  3  action  0 :  tensor([    0.9992,     0.0002,     0.0000,     0.0000,     0.0005],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9677,     0.0006,     0.0008,     0.0308],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0000,     0.0006,     0.9748,     0.0143,     0.0103],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0004,     0.0002,     0.0149,     0.9324,     0.0521],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0860, 0.0820, 0.1149, 0.0420, 0.6751], grad_fn=<DivBackward0>)
printing an ep nov before normalisation:  42.307217081971636
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  69.5286646065924
printing an ep nov before normalisation:  37.27139704535471
printing an ep nov before normalisation:  63.839162404735006
printing an ep nov before normalisation:  53.22750502683243
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  27.89794429889771
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  41.810879707336426
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  49.16193708402485
actor:  1 policy actor:  1  step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  29.64420588642281
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.189]
 [0.011]
 [1.078]
 [0.97 ]
 [1.149]] [[30.905]
 [27.035]
 [29.317]
 [26.049]
 [30.271]] [[1.615]
 [0.351]
 [1.468]
 [1.288]
 [1.56 ]]
printing an ep nov before normalisation:  35.757314695315195
printing an ep nov before normalisation:  0.018886254563881266
actor:  1 policy actor:  1  step number:  51 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
siam score:  -0.8530417
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.4943],
        [0.9228],
        [0.4344],
        [0.5219],
        [0.5652],
        [0.8410],
        [0.3864],
        [0.0000],
        [0.6818],
        [0.0000]], dtype=torch.float64)
0.0 0.4943286317492936
0.0 0.9227804679051237
0.0 0.4343699497597074
0.0 0.5219143290062095
0.0 0.5652302643475527
0.0 0.8410274888282361
0.0 0.38637296419641437
0.0 0.0
0.0 0.6818082349885174
0.0 0.0
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4021 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.694]
 [0.694]
 [0.684]
 [0.694]
 [0.694]] [[34.054]
 [34.054]
 [41.292]
 [34.054]
 [34.054]] [[1.36 ]
 [1.36 ]
 [1.684]
 [1.36 ]
 [1.36 ]]
maxi score, test score, baseline:  0.4041 1.0 1.0
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.6268545481049
using explorer policy with actor:  1
using explorer policy with actor:  1
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  62 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.739]
 [0.739]
 [0.739]
 [0.739]
 [0.739]] [[50.461]
 [50.461]
 [50.461]
 [50.461]
 [50.461]] [[2.709]
 [2.709]
 [2.709]
 [2.709]
 [2.709]]
printing an ep nov before normalisation:  40.829690347233246
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.36037007888333
Printing some Q and Qe and total Qs values:  [[0.845]
 [0.796]
 [0.828]
 [0.747]
 [0.79 ]] [[34.503]
 [43.502]
 [31.818]
 [38.358]
 [38.388]] [[1.189]
 [1.34 ]
 [1.113]
 [1.176]
 [1.22 ]]
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  54.840772380967785
maxi score, test score, baseline:  0.4041 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  28.168875023053687
maxi score, test score, baseline:  0.4041 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
printing an ep nov before normalisation:  58.732545892534375
Printing some Q and Qe and total Qs values:  [[1.066]
 [1.03 ]
 [1.03 ]
 [1.038]
 [1.03 ]] [[43.905]
 [27.631]
 [27.631]
 [36.617]
 [27.631]] [[1.862]
 [1.432]
 [1.432]
 [1.657]
 [1.432]]
Printing some Q and Qe and total Qs values:  [[1.085]
 [1.203]
 [1.203]
 [1.056]
 [1.116]] [[48.912]
 [35.555]
 [35.555]
 [49.824]
 [37.034]] [[1.916]
 [1.687]
 [1.687]
 [1.911]
 [1.639]]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.37625195074096
Printing some Q and Qe and total Qs values:  [[1.186]
 [1.268]
 [1.194]
 [1.194]
 [1.332]] [[29.737]
 [40.607]
 [32.494]
 [32.494]
 [41.725]] [[1.717]
 [2.347]
 [1.865]
 [1.865]
 [2.467]]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Training Flag: True
Self play flag: True
add more workers flag:  False
expV_train_flag:  True
expV_train_start_flag:  20051
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  44.44836039900564
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  37.84326615366359
printing an ep nov before normalisation:  45.21126921773016
printing an ep nov before normalisation:  62.148784527961986
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  0  action  0 :  tensor([    0.9552,     0.0000,     0.0006,     0.0093,     0.0349],
       grad_fn=<DivBackward0>)
K:  0  action  1 :  tensor([    0.0002,     0.9432,     0.0004,     0.0004,     0.0557],
       grad_fn=<DivBackward0>)
K:  0  action  2 :  tensor([    0.0010,     0.0000,     0.9975,     0.0002,     0.0013],
       grad_fn=<DivBackward0>)
K:  0  action  3 :  tensor([    0.0027,     0.0002,     0.0145,     0.9198,     0.0627],
       grad_fn=<DivBackward0>)
K:  0  action  4 :  tensor([0.0074, 0.0336, 0.0029, 0.1099, 0.8462], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  49.89319316647646
printing an ep nov before normalisation:  22.39123897589761
printing an ep nov before normalisation:  52.21936131697649
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.10642580018184
printing an ep nov before normalisation:  36.88658381119017
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[0.677]
 [0.677]
 [0.717]
 [0.677]
 [0.677]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.677]
 [0.677]
 [0.717]
 [0.677]
 [0.677]]
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  28.111505982110188
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  49.663890078363366
maxi score, test score, baseline:  0.4061 1.0 1.0
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.69922193003013
Printing some Q and Qe and total Qs values:  [[0.502]
 [0.566]
 [0.502]
 [0.502]
 [0.527]] [[43.67 ]
 [43.949]
 [43.67 ]
 [43.67 ]
 [47.561]] [[0.502]
 [0.566]
 [0.502]
 [0.502]
 [0.527]]
rdn beta is 0 so we're just using the maxi policy
printing an ep nov before normalisation:  36.007245084829194
printing an ep nov before normalisation:  65.1784405688439
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  75.4745488580639
printing an ep nov before normalisation:  52.59066382932392
using explorer policy with actor:  1
line 256 mcts: sample exp_bonus 38.548079645679636
actions average: 
K:  3  action  0 :  tensor([    0.9320,     0.0241,     0.0002,     0.0092,     0.0345],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0000,     0.9931,     0.0021,     0.0000,     0.0047],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0242,     0.0003,     0.9319,     0.0012,     0.0424],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0000,     0.0011,     0.0177,     0.9489,     0.0323],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0019, 0.1201, 0.0275, 0.1045, 0.7461], grad_fn=<DivBackward0>)
actions average: 
K:  1  action  0 :  tensor([    0.9977,     0.0001,     0.0000,     0.0012,     0.0010],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0001,     0.9802,     0.0003,     0.0012,     0.0182],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0000,     0.0001,     0.9719,     0.0002,     0.0278],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0000,     0.0000,     0.0158,     0.9395,     0.0446],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0009, 0.1427, 0.0062, 0.0575, 0.7926], grad_fn=<DivBackward0>)
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  37.374497894598676
printing an ep nov before normalisation:  47.103981025671516
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.041]
 [1.168]
 [1.289]
 [1.041]
 [1.047]] [[25.063]
 [36.538]
 [38.271]
 [30.452]
 [26.014]] [[0.283]
 [1.624]
 [1.777]
 [1.384]
 [1.308]]
printing an ep nov before normalisation:  51.03071815499774
maxi score, test score, baseline:  0.4061 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  89 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  30.671988727133115
maxi score, test score, baseline:  0.4061 1.0 1.0
maxi score, test score, baseline:  0.4061 1.0 1.0
actor:  1 policy actor:  1  step number:  79 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  0 policy actor:  0  step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
printing an ep nov before normalisation:  46.15782369447499
Printing some Q and Qe and total Qs values:  [[0.895]
 [0.868]
 [0.868]
 [0.888]
 [0.868]] [[40.807]
 [36.895]
 [36.895]
 [39.843]
 [36.895]] [[1.508]
 [1.365]
 [1.365]
 [1.472]
 [1.365]]
printing an ep nov before normalisation:  49.85564688424608
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  61.056330175834944
actor:  0 policy actor:  0  step number:  52 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  44.67487087044782
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.614]
 [0.88 ]
 [0.799]
 [0.799]
 [0.762]] [[41.565]
 [41.161]
 [35.545]
 [35.545]
 [37.664]] [[0.843]
 [1.105]
 [0.972]
 [0.972]
 [0.955]]
from probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  54.68806421266401
actor:  1 policy actor:  1  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  34.75813342859898
actions average: 
K:  1  action  0 :  tensor([    0.9951,     0.0012,     0.0004,     0.0005,     0.0028],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0003,     0.9620,     0.0087,     0.0009,     0.0281],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0011,     0.0001,     0.9966,     0.0007,     0.0015],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0001,     0.0024,     0.0076,     0.9581,     0.0319],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0016, 0.0344, 0.0670, 0.1343, 0.7627], grad_fn=<DivBackward0>)
siam score:  -0.8421261
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  42.71270534723597
printing an ep nov before normalisation:  25.251553841250367
using explorer policy with actor:  1
printing an ep nov before normalisation:  29.247103310737806
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
printing an ep nov before normalisation:  31.859706534825772
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  88 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.345]
 [1.361]
 [1.345]
 [1.345]
 [1.345]] [[38.781]
 [46.242]
 [38.781]
 [38.781]
 [38.781]] [[2.152]
 [2.464]
 [2.152]
 [2.152]
 [2.152]]
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.73 ]
 [0.815]
 [0.73 ]
 [0.73 ]
 [0.79 ]] [[42.888]
 [42.582]
 [42.888]
 [42.888]
 [41.484]] [[1.486]
 [1.562]
 [1.486]
 [1.486]
 [1.505]]
printing an ep nov before normalisation:  28.566377657062347
printing an ep nov before normalisation:  45.9319204253086
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.279]
 [1.279]
 [1.279]
 [1.279]
 [1.279]] [[45.918]
 [45.918]
 [45.918]
 [45.918]
 [45.918]] [[2.688]
 [2.688]
 [2.688]
 [2.688]
 [2.688]]
actor:  1 policy actor:  1  step number:  75 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Printing some Q and Qe and total Qs values:  [[1.13]
 [1.13]
 [1.13]
 [1.13]
 [1.13]] [[31.024]
 [31.024]
 [31.024]
 [31.024]
 [31.024]] [[52.847]
 [52.847]
 [52.847]
 [52.847]
 [52.847]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  44.62832712870729
printing an ep nov before normalisation:  48.54440654243895
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.003]
 [0.802]
 [0.01 ]
 [0.738]
 [0.766]] [[47.7  ]
 [47.437]
 [38.77 ]
 [45.118]
 [51.186]] [[0.187]
 [0.983]
 [0.105]
 [0.896]
 [0.985]]
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.875]
 [0.875]
 [0.875]
 [0.951]
 [0.875]] [[22.548]
 [22.548]
 [22.548]
 [26.695]
 [22.548]] [[0.936]
 [0.936]
 [0.936]
 [1.045]
 [0.936]]
printing an ep nov before normalisation:  42.20084203979039
siam score:  -0.8382933
maxi score, test score, baseline:  0.40809999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.77 ]
 [0.814]
 [0.77 ]
 [0.77 ]
 [0.77 ]] [[51.876]
 [47.05 ]
 [51.876]
 [51.876]
 [51.876]] [[2.043]
 [1.896]
 [2.043]
 [2.043]
 [2.043]]
line 256 mcts: sample exp_bonus 41.99750015066886
actor:  0 policy actor:  0  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]] [[0.]
 [0.]
 [0.]
 [0.]
 [0.]] [[0.611]
 [0.611]
 [0.611]
 [0.611]
 [0.611]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  47.02414517090823
using explorer policy with actor:  1
printing an ep nov before normalisation:  44.96133232352131
printing an ep nov before normalisation:  26.699839327099987
printing an ep nov before normalisation:  50.4774367742146
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.560853381782564
printing an ep nov before normalisation:  37.48767202214183
Printing some Q and Qe and total Qs values:  [[1.315]
 [1.375]
 [1.315]
 [1.315]
 [1.246]] [[31.365]
 [40.496]
 [31.365]
 [31.365]
 [31.293]] [[1.609]
 [1.832]
 [1.609]
 [1.609]
 [1.539]]
Printing some Q and Qe and total Qs values:  [[1.045]
 [1.253]
 [1.143]
 [1.118]
 [1.08 ]] [[56.597]
 [39.535]
 [45.709]
 [42.605]
 [49.621]] [[1.647]
 [1.622]
 [1.597]
 [1.529]
 [1.587]]
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[0.808]
 [0.808]
 [0.808]
 [0.808]
 [0.808]] [[62.059]
 [62.059]
 [62.059]
 [62.059]
 [62.059]] [[1.702]
 [1.702]
 [1.702]
 [1.702]
 [1.702]]
printing an ep nov before normalisation:  47.42535817392359
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.5]
 [1.5]
 [1.5]
 [1.5]
 [1.5]] [[40.056]
 [40.056]
 [40.056]
 [40.056]
 [40.056]] [[2.167]
 [2.167]
 [2.167]
 [2.167]
 [2.167]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  60.289968437350616
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.53836357744407
printing an ep nov before normalisation:  33.15421956755009
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.501]
 [1.501]
 [0.001]
 [1.501]
 [1.501]] [[32.499]
 [32.499]
 [ 0.015]
 [32.499]
 [32.499]] [[2.695]
 [2.695]
 [0.001]
 [2.695]
 [2.695]]
actor:  1 policy actor:  1  step number:  55 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  45.55103543600193
Printing some Q and Qe and total Qs values:  [[0.7  ]
 [0.807]
 [0.7  ]
 [0.687]
 [0.751]] [[54.225]
 [45.508]
 [54.225]
 [41.456]
 [47.692]] [[0.981]
 [1.015]
 [0.981]
 [0.861]
 [0.977]]
printing an ep nov before normalisation:  27.42009401321411
printing an ep nov before normalisation:  34.847000622693734
actor:  1 policy actor:  1  step number:  96 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
Printing some Q and Qe and total Qs values:  [[0.868]
 [0.868]
 [0.868]
 [0.868]
 [0.868]] [[28.634]
 [28.634]
 [28.634]
 [28.634]
 [28.634]] [[1.067]
 [1.067]
 [1.067]
 [1.067]
 [1.067]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  41.356912259365096
Printing some Q and Qe and total Qs values:  [[1.059]
 [1.059]
 [1.059]
 [1.093]
 [1.059]] [[34.437]
 [34.437]
 [34.437]
 [46.707]
 [34.437]] [[1.542]
 [1.542]
 [1.542]
 [1.905]
 [1.542]]
printing an ep nov before normalisation:  47.3916898121087
siam score:  -0.8356099
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.825]
 [0.81 ]
 [0.81 ]
 [0.81 ]
 [0.81 ]] [[45.687]
 [36.613]
 [36.613]
 [36.613]
 [36.613]] [[0.967]
 [0.896]
 [0.896]
 [0.896]
 [0.896]]
actions average: 
K:  4  action  0 :  tensor([    0.9989,     0.0001,     0.0000,     0.0002,     0.0008],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0003,     0.9371,     0.0129,     0.0005,     0.0493],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0001,     0.0136,     0.9156,     0.0008,     0.0700],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0000,     0.0001,     0.0002,     0.9960,     0.0037],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0023, 0.1137, 0.0024, 0.1415, 0.7402], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  34.4658471426192
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  74.02620732454515
printing an ep nov before normalisation:  37.207255363464355
Printing some Q and Qe and total Qs values:  [[0.729]
 [0.82 ]
 [0.729]
 [0.729]
 [0.729]] [[39.464]
 [50.516]
 [39.464]
 [39.464]
 [39.464]] [[0.981]
 [1.265]
 [0.981]
 [0.981]
 [0.981]]
Printing some Q and Qe and total Qs values:  [[0.069]
 [0.849]
 [0.02 ]
 [0.732]
 [0.83 ]] [[51.409]
 [47.895]
 [42.671]
 [47.621]
 [50.914]] [[0.556]
 [1.264]
 [0.328]
 [1.141]
 [1.307]]
line 256 mcts: sample exp_bonus 44.47782039642334
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
actor:  1 policy actor:  1  step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  41.623270229412164
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.081]
 [1.054]
 [1.038]
 [1.048]
 [1.05 ]] [[38.702]
 [32.046]
 [26.885]
 [30.472]
 [31.329]] [[2.181]
 [1.803]
 [1.516]
 [1.714]
 [1.762]]
actor:  1 policy actor:  1  step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  41.34720593834673
printing an ep nov before normalisation:  25.957622793505102
printing an ep nov before normalisation:  36.587112066427714
printing an ep nov before normalisation:  62.0927753792352
actor:  1 policy actor:  1  step number:  77 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
printing an ep nov before normalisation:  34.84294414520264
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.798]
 [0.799]
 [0.818]
 [0.796]
 [0.798]] [[43.035]
 [42.892]
 [37.206]
 [40.418]
 [43.024]] [[2.798]
 [2.789]
 [2.377]
 [2.598]
 [2.797]]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 54.503561478158225
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
printing an ep nov before normalisation:  34.34434571067675
using explorer policy with actor:  1
Printing some Q and Qe and total Qs values:  [[1.36]
 [1.36]
 [1.36]
 [1.36]
 [1.36]] [[40.036]
 [40.036]
 [40.036]
 [40.036]
 [40.036]] [[2.36]
 [2.36]
 [2.36]
 [2.36]
 [2.36]]
actor:  1 policy actor:  1  step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
actor:  1 policy actor:  1  step number:  60 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
printing an ep nov before normalisation:  29.802111784201134
printing an ep nov before normalisation:  39.415177703000914
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
printing an ep nov before normalisation:  46.180715560913086
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actions average: 
K:  3  action  0 :  tensor([    0.9974,     0.0002,     0.0006,     0.0000,     0.0017],
       grad_fn=<DivBackward0>)
K:  3  action  1 :  tensor([    0.0002,     0.9716,     0.0000,     0.0001,     0.0280],
       grad_fn=<DivBackward0>)
K:  3  action  2 :  tensor([    0.0016,     0.0163,     0.9677,     0.0002,     0.0141],
       grad_fn=<DivBackward0>)
K:  3  action  3 :  tensor([    0.0003,     0.0003,     0.0202,     0.8898,     0.0895],
       grad_fn=<DivBackward0>)
K:  3  action  4 :  tensor([0.0011, 0.0437, 0.0009, 0.0387, 0.9155], grad_fn=<DivBackward0>)
Printing some Q and Qe and total Qs values:  [[1.116]
 [1.167]
 [1.167]
 [1.051]
 [1.167]] [[54.624]
 [40.28 ]
 [40.28 ]
 [40.98 ]
 [40.28 ]] [[1.816]
 [1.616]
 [1.616]
 [1.512]
 [1.616]]
printing an ep nov before normalisation:  25.014970302581787
printing an ep nov before normalisation:  28.305806587093283
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[1.119]
 [1.095]
 [1.095]
 [1.095]
 [1.095]] [[51.143]
 [46.524]
 [46.524]
 [46.524]
 [46.524]] [[1.938]
 [1.793]
 [1.793]
 [1.793]
 [1.793]]
printing an ep nov before normalisation:  41.430107957403216
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 33.6374718163076
actor:  1 policy actor:  1  step number:  70 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
printing an ep nov before normalisation:  47.87173051259157
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  31.172068119049072
actor:  1 policy actor:  1  step number:  46 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
printing an ep nov before normalisation:  0.004060436774864229
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[0.866]
 [0.787]
 [0.866]
 [0.866]
 [0.929]] [[55.441]
 [75.266]
 [55.441]
 [55.441]
 [61.826]] [[1.537]
 [1.787]
 [1.537]
 [1.537]
 [1.706]]
actor:  1 policy actor:  1  step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  48.20791764768984
Printing some Q and Qe and total Qs values:  [[1.366]
 [1.444]
 [1.278]
 [1.348]
 [1.289]] [[28.594]
 [25.972]
 [18.623]
 [22.859]
 [23.442]] [[2.087]
 [2.072]
 [1.643]
 [1.864]
 [1.826]]
actor:  1 policy actor:  1  step number:  59 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
maxi score, test score, baseline:  0.41009999999999996 1.0 1.0
Printing some Q and Qe and total Qs values:  [[0.839]
 [0.895]
 [0.839]
 [0.839]
 [0.839]] [[32.525]
 [35.252]
 [32.525]
 [32.525]
 [32.525]] [[0.839]
 [0.895]
 [0.839]
 [0.839]
 [0.839]]
actor:  0 policy actor:  0  step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.333
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.15 ]
 [1.182]
 [1.182]
 [1.059]
 [1.11 ]] [[43.759]
 [37.524]
 [37.524]
 [41.03 ]
 [39.725]] [[1.58 ]
 [1.519]
 [1.519]
 [1.448]
 [1.481]]
Printing some Q and Qe and total Qs values:  [[1.08 ]
 [1.023]
 [1.011]
 [1.054]
 [1.045]] [[42.017]
 [37.507]
 [33.86 ]
 [38.771]
 [41.473]] [[1.447]
 [1.319]
 [1.25 ]
 [1.369]
 [1.403]]
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
line 256 mcts: sample exp_bonus 0.0
actions average: 
K:  1  action  0 :  tensor([    0.9987,     0.0000,     0.0009,     0.0001,     0.0003],
       grad_fn=<DivBackward0>)
K:  1  action  1 :  tensor([    0.0004,     0.9442,     0.0003,     0.0004,     0.0547],
       grad_fn=<DivBackward0>)
K:  1  action  2 :  tensor([    0.0001,     0.0156,     0.9026,     0.0390,     0.0427],
       grad_fn=<DivBackward0>)
K:  1  action  3 :  tensor([    0.0002,     0.0005,     0.0020,     0.9403,     0.0569],
       grad_fn=<DivBackward0>)
K:  1  action  4 :  tensor([0.0474, 0.1735, 0.0605, 0.0447, 0.6739], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41209999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  0 policy actor:  0  step number:  44 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
siam score:  -0.8321582
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
actor:  1 policy actor:  1  step number:  68 total reward:  1.0  reward:  1.0 rdn_beta:  1.0
printing an ep nov before normalisation:  36.7022750577868
printing an ep nov before normalisation:  43.06452412037301
actions average: 
K:  4  action  0 :  tensor([    0.9984,     0.0003,     0.0000,     0.0002,     0.0011],
       grad_fn=<DivBackward0>)
K:  4  action  1 :  tensor([    0.0014,     0.9653,     0.0011,     0.0003,     0.0319],
       grad_fn=<DivBackward0>)
K:  4  action  2 :  tensor([    0.0000,     0.0244,     0.9227,     0.0007,     0.0522],
       grad_fn=<DivBackward0>)
K:  4  action  3 :  tensor([    0.0016,     0.0014,     0.0002,     0.9570,     0.0398],
       grad_fn=<DivBackward0>)
K:  4  action  4 :  tensor([0.0170, 0.0630, 0.0038, 0.0491, 0.8670], grad_fn=<DivBackward0>)
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  36.767597199787524
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
actor:  1 policy actor:  1  step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0.667
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
siam score:  -0.83949065
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
Printing some Q and Qe and total Qs values:  [[1.158]
 [1.371]
 [1.158]
 [1.158]
 [1.158]] [[31.745]
 [42.527]
 [31.745]
 [31.745]
 [31.745]] [[1.855]
 [2.597]
 [1.855]
 [1.855]
 [1.855]]
actor:  1 policy actor:  1  step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  1.667
line 256 mcts: sample exp_bonus 23.37451218560613
Printing some Q and Qe and total Qs values:  [[0.67 ]
 [0.63 ]
 [0.63 ]
 [0.652]
 [0.63 ]] [[31.408]
 [31.676]
 [31.676]
 [38.424]
 [31.676]] [[0.67 ]
 [0.63 ]
 [0.63 ]
 [0.652]
 [0.63 ]]
Printing some Q and Qe and total Qs values:  [[0.168]
 [0.168]
 [0.606]
 [0.537]
 [0.589]] [[27.952]
 [29.189]
 [25.708]
 [31.676]
 [29.751]] [[1.069]
 [1.18 ]
 [1.308]
 [1.771]
 [1.651]]
Printing some Q and Qe and total Qs values:  [[0.285]
 [0.554]
 [0.57 ]
 [0.58 ]
 [0.556]] [[36.753]
 [38.301]
 [34.355]
 [37.295]
 [35.134]] [[0.285]
 [0.554]
 [0.57 ]
 [0.58 ]
 [0.556]]
using explorer policy with actor:  1
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
printing an ep nov before normalisation:  51.34506135336744
maxi score, test score, baseline:  0.41409999999999997 1.0 1.0
printing an ep nov before normalisation:  37.53147461778513
using explorer policy with actor:  1
actor:  1 policy actor:  1  step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  2.0
actor:  0 policy actor:  0  step number:  54 total reward:  1.0  reward:  1.0 rdn_beta:  1.333
Starting evaluation
rdn beta is 0 so we're just using the maxi policy
Printing some Q and Qe and total Qs values:  [[0.58 ]
 [0.58 ]
 [0.58 ]
 [0.6  ]
 [0.561]] [[40.753]
 [40.753]
 [40.753]
 [41.826]
 [45.933]] [[0.58 ]
 [0.58 ]
 [0.58 ]
 [0.6  ]
 [0.561]]
maxi score, test score, baseline:  0.41609999999999997 1.0 1.0
probs:  [0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669, 0.16666666666666669]
printing an ep nov before normalisation:  45.88313192348649
